{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax\n",
    "from dataclasses import dataclass\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "#with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "import gpjax as gpx\n",
    "from gpjax.distributions import GaussianDistribution\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "# plt.style.use(\n",
    "#     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    "# )\n",
    "# colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from gpjax.kernels.base import AdditiveKernel\n",
    "\n",
    "\n",
    "import optax as ox\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "# custom bits\n",
    "from gpjax.dataset import VerticalDataset\n",
    "from gpjax.kernels.stationary.rbf import OrthogonalRBF, OrthogonalRBFUnif\n",
    "from gpjax.gps import CustomAdditiveConjugatePosterior, VerticalSmoother\n",
    "from gpjax.objectives import CustomConjugateMLL, CustomELBO, custom_variational_expectation\n",
    "from gpjax.optim_utils import optim_builder, zero_grads\n",
    "from gpjax.variational_families import CustomVariationalGaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100_000 entries sampled across time/lat/lon over first day of data\n",
    "\n",
    "## X2D has:\n",
    "\n",
    "0\"Sea surface temperature (K)\"\n",
    "\n",
    "1\"Sensible heat flux (W/m^2)\"\n",
    "\n",
    "2\"Latent heat flux (W/m^2)\"\n",
    "\n",
    "3\"Vertically-integrated moisture convergence (kg/m^2)\"\n",
    "\n",
    "4\"Column relative humidity (%)\"\n",
    "\n",
    "\n",
    "## X3D has:\n",
    "\n",
    "0\"Absolute temperature (K)\"\n",
    "\n",
    "1\"Relative humidity (%)\"\n",
    "\n",
    "2\"Specific humidity (kg/kg)\"\n",
    "\n",
    "3\"Geopotential height (m^2 s^-2)\"\n",
    "\n",
    "4\"Zonal wind (m/s)\"\n",
    "\n",
    "5\"Meridional wind (m/s)\"\n",
    "\n",
    "6\"Potential temperature (K)\"\n",
    "\n",
    "7\"Equivalent potential temperature (K)\"\n",
    "\n",
    "8\"Equivalent potential temperature saturation deficit (K)\"\n",
    "\n",
    "9\"Saturated equivalent potential temperature (K)\"\n",
    "\n",
    "10\"MSE-conserving plume buoyancy (m/s^2)\"\n",
    "\n",
    "\n",
    "## static has:\n",
    "\n",
    "0\"Land-sea mask\"\n",
    "\n",
    "1\"Angle of sub-gridscale orography (rad)Anisotropy of sub-gridscale orography\"\n",
    "\n",
    "2\"Standard deviation of sub-gridscale orography\"\n",
    "\n",
    "3\"Slope of sub-gridscale orography\"\n",
    "\n",
    "## Y has:\n",
    "\n",
    "0\"ERA5 Precipitation (mm/hr)\"\n",
    "\n",
    "1\"TRMM Precipitation (mm/hr)\"\n",
    "\n",
    "2\"TRMM Relative Error (%)\"\n",
    "\n",
    "# plev are\n",
    "1000.,   2000.,   3000.,   5000.,   7000.,  10000., 15000.,\n",
    "20000.,  25000.,  30000.,  40000.,  50000.,  60000.,  70000.,80000.,  85000.,  90000.,  92500.,  95000.,  97500., 100000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X2d_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/X2d_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "X3d_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/X3d_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Xstatic_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/XStatic_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Y_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "pressure = jnp.array([[1000,2000,3000,5000,7000,10000,15000,20000,25000,30000,40000,50000,60000,70000,80000, 85000,90000,92500,95000,97500,100000]], dtype=jnp.float64)\n",
    "pressure_mean = jnp.mean(pressure)\n",
    "pressure_std = jnp.std(pressure)\n",
    "pressure = (pressure - pressure_mean) / pressure_std\n",
    "\n",
    "\n",
    "\n",
    "# random shuffle\n",
    "X2d = jr.permutation(key, X2d_raw)\n",
    "X3d = jr.permutation(key, X3d_raw)\n",
    "Xstatic = jr.permutation(key, Xstatic_raw)\n",
    "Y = jr.permutation(key, Y_raw)\n",
    "\n",
    "# look at ERA5 rain\n",
    "Y = Y[:,0:1]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# just keep the \"prognostic\" 3d inputs that joe considered (for now)\n",
    "# RH, tehta_e^+, theta_e, theta_e^*\n",
    "names_3d =  [\"K,\",\"RH\", \"q\", \"gh\", \"wind_z\",\"wind_m\",\"theta\",\"tehta_e\", \"theta_e+\", \"theta_e*\", \"plume\"]\n",
    "idx_3d = [i for i in range(len(names_3d))]\n",
    "# idx_3d = [1, 7, 8, 9]\n",
    "names_3d = [names_3d[i] for i in idx_3d]\n",
    "X3d = X3d[:,idx_3d,:]\n",
    "\n",
    "\n",
    "# # also use his \"normalisatopm\" for sigma_o\n",
    "# sigma_o = jnp.where(Xstatic[:,0]<0.5, 0.0, 1.0+jnp.log(1+Xstatic[:,2])) # optimize threshold?\n",
    "# Xstatic = Xstatic.at[:,2].set(sigma_o)\n",
    "names_static = [\"LSM\",\"O_angle\",\"O_sd\",\"O_slope\"]\n",
    "idx_static = [i for i in range(len(names_static))]\n",
    "# idx_static = [0]\n",
    "#idx_static = []\n",
    "names_static = [names_static[i] for i in idx_static]\n",
    "Xstatic = Xstatic[:,idx_static]\n",
    "\n",
    "\n",
    "names_2d = [\"K_surface\",\"flux_s\",\"flux_l\",\"moisture\",\"CRH\"]\n",
    "idx_2d =[i for i in range(len(names_2d))]\n",
    "idx_2d = [1,2,3,4]\n",
    "names_2d = [names_2d[i] for i in idx_2d]\n",
    "X2d = X2d[:,idx_2d]\n",
    "\n",
    "\n",
    "#remove all pressure levels above 500 hPA\n",
    "lowest_idx =  11 #7\n",
    "print(f\"Removed all pressure levels below {pressure[:,lowest_idx]} hPa\")\n",
    "X3d = X3d[:, :, lowest_idx:]\n",
    "pressure_levels = pressure[:,lowest_idx:]\n",
    "\n",
    "\n",
    "\n",
    "# remove any entries with nan\n",
    "X3d_nan_idx = jnp.isnan(X3d).any(axis=1).any(axis=1)\n",
    "X2d_nan_idx = jnp.isnan(X2d).any(axis=1)\n",
    "Xstatic_nan_idx = jnp.isnan(Xstatic).any(axis=1)\n",
    "Y_nan_idx = jnp.isnan(Y).any(axis=1)\n",
    "any_nan = X3d_nan_idx | X2d_nan_idx | Y_nan_idx | Xstatic_nan_idx\n",
    "no_nan = ~ any_nan\n",
    "print(f\"Removed {any_nan.sum()} entries with nan\")\n",
    "X2d = X2d[no_nan,:]\n",
    "X3d = X3d[no_nan,:,:]\n",
    "Xstatic = Xstatic[no_nan,:]\n",
    "Y = Y[no_nan,:]\n",
    "\n",
    "\n",
    "# # remove no rain days\n",
    "# print(f\"Removed {(Y[:,0]==0).sum()} entries with zero rain\")\n",
    "# X3d = X3d[Y[:,0]>0,:]\n",
    "# X2d = X2d[Y[:,0]>0,:]\n",
    "# Xstatic = Xstatic[Y[:,0]>0,:]\n",
    "# Y = Y[Y[:,0]>0,:]\n",
    "\n",
    "\n",
    "# also log Y\n",
    "# print(f\"Applied log transform to Y\")\n",
    "#Y = jnp.log(Y-jnp.min(Y)+1e-1)\n",
    "print(f\"then standardized Y as Gaussian\")\n",
    "Y_mean = jnp.mean(Y)\n",
    "Y_std = jnp.std(Y)\n",
    "Y = (Y - Y_mean) / Y_std\n",
    "plt.hist(Y.T)\n",
    "\n",
    "\n",
    "#standardize inputs \n",
    "print(\"standardized inputs to be Gaussian\")\n",
    "X3d_before_standardization = X3d\n",
    "X3d_mean = jnp.mean(X3d,axis=(0))\n",
    "X3d_std = jnp.std(X3d, axis=(0,2))\n",
    "X3d = (X3d - X3d_mean[None,:,:]) / X3d_std[None,:,None]\n",
    "X3d_std = jnp.std(X3d, axis=(0))\n",
    "# X3d = (X3d - X3d_mean[None,:,:]) / X3d_std[None,:,:]\n",
    "# X2d_std = jnp.std(X2d, axis=0)\n",
    "X2d_mean = jnp.mean(X2d,axis=0)\n",
    "X2d = (X2d - X2d_mean) / X2d_std\n",
    "Xstatic_std = jnp.std(Xstatic, axis=0)\n",
    "Xstatic_mean = jnp.mean(Xstatic,axis=0)\n",
    "Xstatic = (Xstatic - Xstatic_mean) / Xstatic_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"standardized inputs with max and min\")\n",
    "# X3d_before_standardization = X3d\n",
    "# X3d_min = jnp.min(X3d, axis=(0))\n",
    "# X3d_max = jnp.max(X3d,axis=(0))\n",
    "# X3d = (X3d_max - X3d) / (X3d_max - X3d_min)\n",
    "# X2d_min = jnp.min(X2d, axis=0)\n",
    "# X2d_max = jnp.max(X2d,axis=0)\n",
    "# X2d = (X2d_max - X2d) / (X2d_max - X2d_min)\n",
    "# Xstatic_min = jnp.min(Xstatic, axis=0)\n",
    "# Xstatic_max = jnp.max(Xstatic,axis=0)\n",
    "# Xstatic = (Xstatic_max - Xstatic) / (Xstatic_max - Xstatic_min)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# look at all data but 1_000 test\n",
    "\n",
    "N_test = 100\n",
    "N_train = len(X2d) - N_test \n",
    "print(f\"Using {N_train} training and {N_test} testing points!\")\n",
    "num_2d_variables= X2d.shape[1]\n",
    "num_3d_variables= X3d.shape[1]\n",
    "num_static_variables= Xstatic.shape[1]\n",
    "num_not_3d_variables = num_2d_variables + num_static_variables\n",
    "num_variables = num_2d_variables + num_3d_variables + num_static_variables\n",
    "print(f\"using {num_static_variables} static variables\")\n",
    "print(f\"using {num_2d_variables} 2d variables\")\n",
    "print(f\"using {num_3d_variables} 3d variables\")\n",
    "names = names_3d + names_2d + names_static\n",
    "print(f\"using variables with names {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [X3d_before_standardization, X3d]:\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=4)\n",
    "    i,j=0,0\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            col.boxplot(data[:,i,:].T, showfliers=False);\n",
    "            col.set_title(names_3d[i])\n",
    "            i+=1\n",
    "            if i==data.shape[1]:\n",
    "                break\n",
    "        if i==data.shape[1]:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=num_2d_variables)\n",
    "i=0\n",
    "for col in ax:\n",
    "    col.hist(X2d[:1000,i].T);\n",
    "    col.set_title(names_2d[i])\n",
    "    i+=1\n",
    "fig, ax = plt.subplots(nrows=1, ncols=num_static_variables)\n",
    "i=0\n",
    "for col in ax:\n",
    "    col.hist(Xstatic[:1000,i].T);\n",
    "    col.set_title(names_static[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_params(model, title=\"\"):\n",
    "    if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "        model = model.posterior\n",
    "    plt.figure()\n",
    "    lengthscales = jnp.array([model.base_kernels[i].lengthscale[0] for i in range(len(model.base_kernels))])\n",
    "    z_to_plot = jnp.linspace(jnp.min(model.smoother.Z_levels),jnp.max(model.smoother.Z_levels),100)\n",
    "    smoothing_weights = model.smoother.smooth_fn(z_to_plot) \n",
    "    z_unscaled = z_to_plot * pressure_std+ pressure_mean\n",
    "    for i in range(num_3d_variables):\n",
    "        plt.plot(smoothing_weights[i,:].T,z_unscaled, label=f\"{names_3d[i]} with lengthscales_ {lengthscales[i]:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.title(title+f\" other lengthscales are {lengthscales[num_3d_variables:]}\")\n",
    "    \n",
    "def plot_interactions(model, data, k=10):\n",
    "    plt.figure()\n",
    "    idx_2 = []\n",
    "    for i in range(num_variables):\n",
    "        for j in range(i+1,num_variables):\n",
    "            idx_2.append([i,j])\n",
    "    idxs = [[]] + [[i] for i in range(num_variables)] + idx_2\n",
    "    if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "        sobols = model.get_sobol_indicies(idxs)\n",
    "        z = model.inducing_inputs\n",
    "    else:\n",
    "        sobols = model.get_sobol_indicies(data, idxs)\n",
    "        z = model.smoother.smooth_data(data)[0]\n",
    "    sobols = sobols / jnp.sum(sobols)\n",
    "\n",
    "    plt.plot(sobols)\n",
    "    plt.title(\"sobol indicies (red lines between orders)\")\n",
    "    plt.axvline(x=1, color=\"red\")\n",
    "    plt.axvline(x=num_variables+1, color=\"red\")\n",
    "    for idx in jax.lax.top_k(sobols, k)[1]:\n",
    "        chosen_idx = idxs[idx]\n",
    "        plt.figure()\n",
    "        num_plot = 1_000 if len(chosen_idx)==1 else 10_000\n",
    "        x_plot = jr.uniform(key, (num_plot, num_variables), minval=jnp.min(z, axis=0), maxval=jnp.max(z, axis=0))\n",
    "        if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "            posterior_dist = model.predict(x_plot,chosen_idx)\n",
    "        else:\n",
    "            posterior_dist = model.predict(x_plot, data, chosen_idx)\n",
    "        mean = posterior_dist.mean() * Y_std + Y_mean\n",
    "        std = jnp.sqrt(posterior_dist.variance())* Y_std\n",
    "        if len(chosen_idx)==1:\n",
    "            plt.scatter(x_plot[:,chosen_idx[0]],mean, color=\"blue\") \n",
    "            plt.scatter(x_plot[:,chosen_idx[0]],mean+ 1.96*std, color=\"red\") \n",
    "            plt.scatter(x_plot[:,chosen_idx[0]],mean- 1.96*std, color=\"red\") \n",
    "            plt.xlim([jnp.min(z[:,chosen_idx[0]]),jnp.max(z[:,chosen_idx[0]])])\n",
    "            plt.scatter(z[:,chosen_idx[0]],jnp.zeros_like(z[:,chosen_idx[0]]), color=\"black\")\n",
    "        elif len(chosen_idx)==2:\n",
    "            col = plt.scatter(x_plot[:,chosen_idx[0]],x_plot[:,chosen_idx[1]],c=mean)\n",
    "            plt.ylim([jnp.min(z[:,chosen_idx[1]]),jnp.max(z[:,chosen_idx[1]])])\n",
    "            plt.colorbar(col)\n",
    "            plt.scatter(z[:,chosen_idx[0]],z[:,chosen_idx[1]], color=\"black\")\n",
    "        plt.xlim([jnp.min(z[:,chosen_idx[0]]),jnp.max(z[:,chosen_idx[0]])])   \n",
    "        plt.title(f\"variable {[names[i] for i in chosen_idx]} with sobol index {sobols[idx]}\")\n",
    "        \n",
    "        \n",
    "    #     top_idx = jax.lax.top_k(sobols, k)[1]\n",
    "    # zeroth = self.predict_additive_component(x, train_data, []).mean()[0]\n",
    "    # plt.figure()\n",
    "    # plt.hist(train_data.y.T, label=\"data\")\n",
    "    # plt.hist(jnp.sum(mean_components[:,:,0],0).T+ zeroth, label=\"all components\")\n",
    "    # plt.hist(jnp.sum(mean_components[:,:,0][top_idx],0).T + zeroth, label=f\"top {k} components\")\n",
    "    # plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from jaxtyping import Num, Float\n",
    "from gpjax.typing import Array, ScalarFloat\n",
    "from beartype.typing import Optional\n",
    "from gpjax.base import Module, param_field,static_field\n",
    "import cola\n",
    "from jax import vmap\n",
    "\n",
    "@dataclass\n",
    "class ConjugatePrecipGP(Module):\n",
    "    base_kernels:List[gpx.kernels.AbstractKernel]\n",
    "    likelihood: gpx.likelihoods.AbstractLikelihood\n",
    "    smoother: VerticalSmoother\n",
    "    max_interaction_depth: bool = static_field(2)\n",
    "    interaction_variances: Float[Array, \" D\"] = param_field(jnp.array([1.0,1.0,1.0]), bijector=tfb.Softplus())\n",
    "    jitter: float = static_field(1e-6)\n",
    "    zeroth_order: bool = static_field(True)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.mean_function = gpx.mean_functions.Zero()\n",
    "        if self.zeroth_order:\n",
    "            if not self.max_interaction_depth == len(self.interaction_variances) - 1:\n",
    "                raise ValueError(\"Number of interaction variances must be equal to max_interaction_depth + 1\")\n",
    "        else:\n",
    "            if not self.max_interaction_depth == len(self.interaction_variances):\n",
    "                raise ValueError(\"Number of interaction variances must be equal to max_interaction_depth\")\n",
    "        \n",
    "        \n",
    "    def predict(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: VerticalDataset,\n",
    "        component_list: Optional[List[List[int]]]=None,\n",
    "    ) -> Union[GaussianDistribution,  Num[Array, \"N 1\"]]:\n",
    "        r\"\"\"Get the posterior predictive distribution (for a specific additive component if componen specified).\"\"\"\n",
    "        #smooth data to get in form for preds\n",
    "        x, y = self.smoother.smooth_data(train_data)\n",
    "        t = test_inputs\n",
    "        \n",
    "        obs_noise = self.likelihood.obs_stddev**2\n",
    "        mx = self.mean_function(x)\n",
    "        mt = self.mean_function(t)\n",
    "        \n",
    "        \n",
    "        Kxx = self.eval_K_xt(x, x) # [N, N]\n",
    "        Kxx += cola.ops.I_like(Kxx) * self.jitter\n",
    "        Sigma = Kxx + cola.ops.I_like(Kxx) * obs_noise\n",
    "        Sigma = cola.PSD(Sigma)\n",
    "        \n",
    "        if component_list is None:\n",
    "            Ktt = self.eval_K_xt(t,t)\n",
    "            Kxt = self.eval_K_xt(x,t)\n",
    "        else:\n",
    "            Ktt = self.eval_specific_K_xt(t, t, component_list)\n",
    "            Kxt = self.eval_specific_K_xt(x, t, component_list)\n",
    "        Sigma_inv_Kxt = cola.solve(Sigma, Kxt)\n",
    "\n",
    "        mean = mt + jnp.matmul(Sigma_inv_Kxt.T, y - mx)\n",
    "        covariance = Ktt - jnp.matmul(Kxt.T, Sigma_inv_Kxt)\n",
    "        covariance += cola.ops.I_like(covariance) * self.jitter\n",
    "        covariance = cola.PSD(covariance)\n",
    "        return GaussianDistribution(jnp.atleast_1d(mean.squeeze()), covariance)\n",
    "    \n",
    "\n",
    "    def eval_K_xt(self, x: Num[Array, \"N d\"], t: Num[Array, \"M d\"]) -> Num[Array, \"N M\"]:\n",
    "        ks = jnp.stack([k.cross_covariance(x,t) for k in self.base_kernels]) # [d, N, M]\n",
    "        ks_orthogonal = self._orthogonalise(ks)\n",
    "        if self.zeroth_order:\n",
    "            return jnp.sum(self._compute_additive_terms_girad_newton(ks_orthogonal) * self.interaction_variances[:, None, None], 0)\n",
    "        else:\n",
    "            return jnp.sum(self._compute_additive_terms_girad_newton(ks_orthogonal)[1:,:,:] * self.interaction_variances[:, None, None], 0)\n",
    "        \n",
    "    def eval_specific_K_xt(self, x: Num[Array, \"N d\"], t: Num[Array, \"M d\"], component_list: List[int])-> Num[Array, \"N M\"]:\n",
    "        ks = jnp.stack([self.base_kernels[i].cross_covariance(x,t) for i in component_list]) # [p, N, M]\n",
    "        var = self.interaction_variances[len(component_list)]\n",
    "        return var * jnp.prod(self._orthogonalise(ks),0) # [p, N, M]\n",
    "        \n",
    "        \n",
    "    def _orthogonalise(self, ks: Num[Array, \"d N M\"])->Num[Array, \"d N M\"]:\n",
    "        denom = jnp.sum(ks, (1,2))[:, None, None] # [d]\n",
    "        Kx =  jnp.sum(ks, -1) # [d, N]\n",
    "        Ky = jnp.sum(ks, 1) # [d, M]\n",
    "        numerator = jnp.matmul(Kx[:,:,None], Ky[:, None, :])# [d, N, M]\n",
    "        return ks -  numerator / denom \n",
    "\n",
    "\n",
    "    @jax.jit   \n",
    "    def _compute_additive_terms_girad_newton(self, ks: Num[Array, \"D N N\"]) -> Num[Array, \"p N N\"]:\n",
    "        N = jnp.shape(ks)[-1]\n",
    "        powers = jnp.arange(self.max_interaction_depth + 1)[:, None] # [p + 1, 1]\n",
    "        s = jnp.power(ks[None, :,:,:],powers[:,:,None,None]) # [p + 1, d, N, N]\n",
    "        e = jnp.ones(shape=(self.max_interaction_depth+1, N, N), dtype=jnp.float64) # [p+1, N, N]lazy init then populate\n",
    "        for n in range(1, self.max_interaction_depth + 1): # has to be for loop because iterative\n",
    "            thing = jax.vmap(lambda k: ((-1.0)**(k-1))*e[n-k]*jnp.sum(s[k], 0))(jnp.arange(1, n+1))\n",
    "            e = e.at[n].set((1.0/n) *jnp.sum(thing,0))\n",
    "        return e\n",
    "\n",
    "\n",
    "    \n",
    "    def loss_fn(self, negative=False)->gpx.objectives.AbstractObjective:\n",
    "        class Loss(gpx.objectives.AbstractObjective):\n",
    "            def step(\n",
    "                self,\n",
    "                posterior: ConjugatePrecipGP,\n",
    "                train_data: gpx.Dataset,\n",
    "            ) -> ScalarFloat:\n",
    "                #smooth data to get in form for preds\n",
    "                x, y = posterior.smoother.smooth_data(train_data)\n",
    "                \n",
    "                obs_noise = posterior.likelihood.obs_stddev**2\n",
    "                mx = posterior.mean_function(x)\n",
    "                Kxx = posterior.eval_K_xt(x,x) # [N, N]\n",
    "                # Σ = (Kxx + Io²) = LLᵀ\n",
    "                Kxx += cola.ops.I_like(Kxx) * posterior.jitter\n",
    "                Sigma = Kxx + cola.ops.I_like(Kxx) * obs_noise\n",
    "                Sigma = cola.PSD(Sigma)\n",
    "\n",
    "                # p(y | x, θ), where θ are the model hyperparameters:\n",
    "                mll = GaussianDistribution(jnp.atleast_1d(mx.squeeze()), Sigma)\n",
    "\n",
    "                return self.constant * (mll.log_prob(jnp.atleast_1d(y.squeeze())).squeeze())\n",
    "            \n",
    "        return Loss(negative=negative)\n",
    "\n",
    "\n",
    "    def get_sobol_indicies(self, train_data: VerticalDataset, component_list: List[List[int]]) -> Num[Array, \"c\"]:\n",
    "        if not isinstance(component_list, List):\n",
    "            raise ValueError(\"Use get_sobol_index if you want to calc for single components (TODO)\")\n",
    "        x,y = self.smoother.smooth_data(train_data)\n",
    "        m_x = self.mean_function(x)\n",
    "\n",
    "\n",
    "        Kxx_indiv = jnp.stack([k.gram(x).to_dense() for k in self.base_kernels], axis=0) # [d, N, N]\n",
    "        Kxx_indiv =  self._orthogonalise(Kxx_indiv)\n",
    "        Kxx_components = [self.interaction_variances[len(c)]*jnp.prod(Kxx_indiv[c, :, :], axis=0) for c in component_list] \n",
    "        Kxx_components = jnp.stack(Kxx_components, axis=0) # [c, N, N]\n",
    "        assert Kxx_components.shape[0] == len(component_list)\n",
    "\n",
    "        Kxx = self.eval_K_xt(x,x)\n",
    "        Sigma = cola.PSD(Kxx + cola.ops.I_like(Kxx) * self.likelihood.obs_stddev**2)\n",
    "\n",
    "        def get_mean_from_covar(K): # [N,N] -> [N, 1]\n",
    "            Sigma_inv_Kxx = cola.solve(Sigma, K)\n",
    "            return m_x + jnp.matmul(Sigma_inv_Kxx.T, y - m_x) # [N, 1] \n",
    "\n",
    "        mean_overall =  get_mean_from_covar(Kxx) # [N, 1]\n",
    "        mean_components = vmap(get_mean_from_covar)(Kxx_components) # [c, N, 1]\n",
    "\n",
    "        sobols = jnp.var(mean_components[:,:,0], axis=-1) / jnp.var(mean_overall) # [c]\n",
    "        return sobols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_log_prior(tau=None) -> callable:\n",
    "\n",
    "    def log_prior(model):\n",
    "        log_prob = 0.0\n",
    "\n",
    "        # if hasattr(model, \"smoother\"):\n",
    "        #     smoother_input_scale_prior = tfd.LogNormal(0.0,1.0)\n",
    "        #     smoother_mean_prior = tfd.Uniform(jnp.min(model.smoother.Z_levels),jnp.max(model.smoother.Z_levels))\n",
    "        #     log_prob += jnp.sum(smoother_mean_prior.log_prob(model.smoother.smoother_mean))\n",
    "        #     log_prob += jnp.sum(smoother_input_scale_prior.log_prob(model.smoother.smoother_input_scale))\n",
    "\n",
    "\n",
    "        if isinstance(model.prior.kernel, AdditiveKernel):\n",
    "            lengthscales = jnp.vstack([k.lengthscale for k in model.prior.kernel.kernels])\n",
    "            variances = model.prior.kernel.interaction_variances\n",
    "        else:\n",
    "            lengthscales = model.prior.kernel.lengthscale\n",
    "            variances = model.prior.kernel.variance\n",
    "            \n",
    "        variance_prior = tfd.Gamma(1.0,0.2)\n",
    "        log_prob += jnp.sum(variance_prior.log_prob(variances))\n",
    "\n",
    "        d = lengthscales.size\n",
    "        #l_prior = tfd.LogNormal(jnp.sqrt(2.0) + jnp.log(d)/2.0,jnp.sqrt(1.0))\n",
    "        #l_prior = tfd.Gamma(3.0*d,6.0)\n",
    "        #l_prior = tfd.Gamma(3.0,6.0/d)\n",
    "        #log_prob += jnp.sum(l_prior.log_prob(lengthscales))\n",
    "        # #l_prior = tfd.HalfCauchy(0.0,tau)\n",
    "        # #log_prob += jnp.sum(l_prior.log_prob((1.0 / lengthscales**2)))\n",
    "\n",
    "            \n",
    "        noise_prior = tfd.LogNormal(0.0,10)\n",
    "        log_prob += noise_prior.log_prob(model.likelihood.obs_stddev)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return log_prob\n",
    "    \n",
    "    return log_prior\n",
    "\n",
    "\n",
    "\n",
    "def init_smoother(data: VerticalDataset, fit=True, tau=0.1, verbose=False) -> VerticalSmoother:\n",
    "    smoother_input_scale_bijector = tfb.Softplus(low=jnp.array(1e-1, dtype=jnp.float64))\n",
    "    smoother_mean_bijector =  tfb.SoftClip(low=jnp.min(pressure_levels), high=jnp.max(pressure_levels))\n",
    "\n",
    "    if fit:\n",
    "        kernels = []\n",
    "        smoothers = []\n",
    "        likelihoods = []\n",
    "        scores = []\n",
    "        for i in range(num_3d_variables):\n",
    "            print(f\"doing the {i} input\")\n",
    "            scores_i = []\n",
    "            kernels_i = []\n",
    "            likelihoods_i = []\n",
    "            smoothers_i = []\n",
    "            for j in range(1,len(pressure_levels[0])):\n",
    "                smoother_input_scale = 2.0\n",
    "                smoother_mean = pressure_levels[0][j]\n",
    "                smoother_i = VerticalSmoother(\n",
    "                    jnp.array([[smoother_mean]]), jnp.array([[smoother_input_scale]]), Z_levels=pressure_levels\n",
    "                    ).replace_bijector(smoother_input_scale=smoother_input_scale_bijector, smoother_mean=smoother_mean_bijector).replace_trainable(smoother_mean=False)\n",
    "                data_i = VerticalDataset(X3d=data.X3d[:,i:i+1,:]  ,X2d = jnp.array([[]]*len(data.X3d)),Xstatic = jnp.array([[]]*len(data.X3d)), y = data.y)\n",
    "                posterior_i = CustomAdditiveConjugatePosterior(\n",
    "                    prior= init_prior(data_i), \n",
    "                    likelihood=init_likelihood(data_i),\n",
    "                    smoother=smoother_i,\n",
    "                    )\n",
    "                obj_i = CustomConjugateMLL(negative=True, log_prior=build_log_prior(tau))\n",
    "                opt_posterior_i, history_i = gpx.fit_scipy(model=posterior_i,objective=obj_i,train_data=data_i,safe=False, verbose=False)\n",
    "                if verbose:\n",
    "                    print(\"##################################\")\n",
    "                    print(f\"found loss is {history_i[-1]}\")\n",
    "                    print(f\"lengthscale is {opt_posterior_i.prior.kernel.kernels[0].lengthscale} and variance is {opt_posterior_i.prior.kernel.interaction_variances[1]} and noise is {opt_posterior_i.likelihood.obs_stddev}\")\n",
    "                    print(f\"smoother mean is {opt_posterior_i.smoother.smoother_mean} and smoother input scale is {opt_posterior_i.smoother.smoother_input_scale}\")\n",
    "                scores_i.append(history_i[-1])\n",
    "                smoothers_i.append(opt_posterior_i.smoother)\n",
    "                kernels_i.append(opt_posterior_i.prior.kernel)\n",
    "                likelihoods_i.append(opt_posterior_i.likelihood)\n",
    "            best_idx = jnp.nanargmin(jnp.array(scores_i))\n",
    "            smoothers.append(smoothers_i[best_idx])\n",
    "            kernels.append(kernels_i[best_idx])\n",
    "            likelihoods.append(likelihoods_i[best_idx])\n",
    "            scores.append(scores_i[best_idx])\n",
    "\n",
    "\n",
    "        smoother = VerticalSmoother(\n",
    "            jnp.array([[s.smoother_mean[0,0] for s in smoothers]]), \n",
    "            jnp.array([[s.smoother_input_scale[0][0] for s in smoothers]]), \n",
    "            Z_levels=pressure_levels\n",
    "        ).replace_bijector(smoother_input_scale=smoother_input_scale_bijector,smoother_mean=smoother_mean_bijector).replace_trainable(smoother_mean=False)\n",
    "\n",
    "        z_to_plot = jnp.linspace(jnp.min(smoother.Z_levels),jnp.max(smoother.Z_levels),100)\n",
    "        smoothing_weights = smoother.smooth_fn(z_to_plot) \n",
    "        z_unscaled = z_to_plot * pressure_std+ pressure_mean\n",
    "        for i in range(num_3d_variables):\n",
    "            plt.plot(smoothing_weights[i,:].T,z_unscaled, label=f\"{names_3d[i]} with l {kernels[i].kernels[0].lengthscale}, v {kernels[i].interaction_variances[1]}, n {likelihoods[i].obs_stddev}, s {scores[i]}\")\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(1.0, 1.05))\n",
    "    else:\n",
    "        smoother = VerticalSmoother(\n",
    "            jnp.array([[1.0]*num_3d_variables]), \n",
    "            jnp.array([[1.0]*num_3d_variables]), \n",
    "            Z_levels=pressure_levels\n",
    "            ).replace_bijector(smoother_input_scale=smoother_input_scale_bijector,smoother_mean=smoother_mean_bijector)\n",
    "    return smoother\n",
    "\n",
    "\n",
    "def init_prior(data):\n",
    "    lengthscale_bij = tfb.Softplus(low=jnp.array(1e-3, dtype=jnp.float64))\n",
    "    variance_bij =  tfb.Softplus(low=jnp.array(1e-3, dtype=jnp.float64))\n",
    "    \n",
    "    #base_kernels = [gpx.kernels.RBF(lengthscale=jnp.array([1.0]), active_dims=[i]).replace_trainable(variance=False).replace_bijector(lengthscale = lengthscale_bij) for i in range(num_variables)]\n",
    "    #base_kernels= [OrthogonalRBF(lengthscale=jnp.array([1.]), active_dims=[i]).replace_bijector(lengthscale = lengthscale_bij) for i in range(data.dim)]\n",
    "    base_kernels= [OrthogonalRBFUnif(lengthscale=jnp.array([1.]), active_dims=[i]).replace_bijector(lengthscale = lengthscale_bij) for i in range(data.dim)]\n",
    "    max_interaction = 2\n",
    "    max_interaction = min(max_interaction, data.dim)\n",
    "    zeroth_order = False\n",
    "    kernel = gpx.kernels.AdditiveKernel(kernels=base_kernels,interaction_variances=jnp.array([1.0]*(max_interaction+1*zeroth_order)), max_interaction_depth=max_interaction, zeroth_order=zeroth_order).replace_bijector(\n",
    "        interaction_variances=variance_bij,\n",
    "        )\n",
    "    return gpx.gps.Prior(mean_function= gpx.mean_functions.Zero(), kernel = kernel)\n",
    "\n",
    "\n",
    "def init_likelihood(data):\n",
    "    obs_bij=tfb.Softplus(low=jnp.array(1e-2, dtype=jnp.float64))\n",
    "    return gpx.likelihoods.Gaussian(num_datapoints=data.n, obs_stddev=jnp.array(0.1, dtype=jnp.float64)).replace_bijector(obs_stddev=obs_bij)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep fancy kernel for model\n",
    "# first fit with small data to get init for SVGP\n",
    "num_data_for_init=100\n",
    "D_small = VerticalDataset(\n",
    "    X2d = X2d[:num_data_for_init,:],\n",
    "    X3d = X3d[:num_data_for_init,:,:],\n",
    "    Xstatic = Xstatic[:num_data_for_init,:],\n",
    "    y=Y[:num_data_for_init,:],\n",
    ")\n",
    "\n",
    "\n",
    "smoother = init_smoother(D_small, fit=False, verbose=False)\n",
    "base_kernels = [gpx.kernels.RBF(lengthscale=jnp.array([1.0]), active_dims=[i]).replace_trainable(variance=False) for i in range(num_variables)]\n",
    "\n",
    "# fit with small data\n",
    "posterior = ConjugatePrecipGP(\n",
    "    base_kernels=base_kernels, \n",
    "    likelihood=init_likelihood(D_small), \n",
    "    smoother=smoother,\n",
    "    )\n",
    "plt.figure()\n",
    "opt_posterior, history = gpx.fit_scipy(\n",
    "        model=posterior,\n",
    "        objective=jax.jit(posterior.loss_fn(negative=True)),\n",
    "        train_data=D_small,\n",
    "        safe=False,\n",
    "    )\n",
    "plt.plot(history)\n",
    "plot_params(opt_posterior, title=\"initial fit with small data\")\n",
    "print(f\"noise is {opt_posterior.likelihood.obs_stddev}\")\n",
    "print(f\"interaction vars {opt_posterior.interaction_variances}\")\n",
    "plot_interactions(opt_posterior, D_small, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
