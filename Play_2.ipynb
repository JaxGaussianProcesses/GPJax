{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax\n",
    "from dataclasses import dataclass\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "#with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "import gpjax as gpx\n",
    "from gpjax.distributions import GaussianDistribution\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "# plt.style.use(\n",
    "#     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    "# )\n",
    "# colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from gpjax.kernels.base import AdditiveKernel\n",
    "\n",
    "\n",
    "import optax as ox\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "\n",
    "\n",
    "from typing import List, Union, Callable\n",
    "from jaxtyping import Num, Float\n",
    "from gpjax.typing import Array, ScalarFloat\n",
    "from beartype.typing import Optional\n",
    "from gpjax.base import Module, param_field,static_field\n",
    "import cola\n",
    "from cola.linalg.decompositions.decompositions import Cholesky\n",
    "from jax import vmap\n",
    "from scipy.stats import qmc\n",
    "\n",
    "# custom bits\n",
    "from gpjax.precip_gp import VerticalDataset, VerticalSmoother, ConjugatePrecipGP, ProblemInfo, VariationalPrecipGP, SwitchKernelPositive, SwitchKernelNegative\n",
    "from gpjax.normalizer import Normalizer\n",
    "from gpjax.plotting import plot_data, plot_marginals, plot_interactions, plot_params, plot_component\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100_000 entries sampled across time/lat/lon over first day of data\n",
    "\n",
    "## X2D has:\n",
    "\n",
    "0\"Sea surface temperature (K)\"\n",
    "\n",
    "1\"Sensible heat flux (W/m^2)\"\n",
    "\n",
    "2\"Latent heat flux (W/m^2)\"\n",
    "\n",
    "3\"Vertically-integrated moisture convergence (kg/m^2)\"\n",
    "\n",
    "4\"Column relative humidity (%)\"\n",
    "\n",
    "\n",
    "## X3D has:\n",
    "\n",
    "0\"Absolute temperature (K)\"\n",
    "\n",
    "1\"Relative humidity (%)\"\n",
    "\n",
    "2\"Specific humidity (kg/kg)\"\n",
    "\n",
    "3\"Geopotential height (m^2 s^-2)\"\n",
    "\n",
    "4\"Zonal wind (m/s)\"\n",
    "\n",
    "5\"Meridional wind (m/s)\"\n",
    "\n",
    "6\"Potential temperature (K)\"\n",
    "\n",
    "7\"Equivalent potential temperature (K)\"\n",
    "\n",
    "8\"Equivalent potential temperature saturation deficit (K)\"\n",
    "\n",
    "9\"Saturated equivalent potential temperature (K)\"\n",
    "\n",
    "10\"MSE-conserving plume buoyancy (m/s^2)\"\n",
    "\n",
    "\n",
    "## static has:\n",
    "\n",
    "0\"Land-sea mask\"\n",
    "\n",
    "1\"Angle of sub-gridscale orography (rad)\n",
    "\n",
    "2\"Anisotropy of sub-gridscale orography\"\n",
    "\n",
    "3\"Standard deviation of sub-gridscale orography\"\n",
    "\n",
    "4\"Slope of sub-gridscale orography\"\n",
    "\n",
    "## Y has:\n",
    "\n",
    "0\"ERA5 Precipitation (mm/hr)\"\n",
    "\n",
    "1\"TRMM Precipitation (mm/hr)\"\n",
    "\n",
    "2\"TRMM Relative Error (%)\"\n",
    "\n",
    "# plev are\n",
    "1000.,   2000.,   3000.,   5000.,   7000.,  10000., 15000.,\n",
    "20000.,  25000.,  30000.,  40000.,  50000.,  60000.,  70000.,80000.,  85000.,  90000.,  92500.,  95000.,  97500., 100000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "# X2d_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/X2d_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "# X3d_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/X3d_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "# Xstatic_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/XStatic_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "# Y_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "X2d_raw = jnp.array(jnp.load(\"../data/100_000_one_day/X2D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "X3d_raw = jnp.array(jnp.load(\"../data/100_000_one_day/X3D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Xstatic_raw = jnp.array(jnp.load(\"../data/100_000_one_day/XSTATIC_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Y_raw = jnp.array(jnp.load(\"../data/100_000_one_day/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "\n",
    "\n",
    "\n",
    "# X2d_raw = jnp.hstack([X2d_raw[:,:2],X2d_raw[:,1:2],X2d_raw[:,2:3],X2d_raw[:,2:]]) # repeat latent fluxes so they can be split into land and sea\n",
    "\n",
    "\n",
    "pressure = jnp.array([[1000,2000,3000,5000,7000,10000,15000,20000,25000,30000,40000,50000,60000,70000,80000, 85000,90000,92500,95000,97500,100000]], dtype=jnp.float64)\n",
    "# random shuffle\n",
    "X2d = jr.permutation(key, X2d_raw)\n",
    "X3d = jr.permutation(key, X3d_raw)\n",
    "Xstatic = jr.permutation(key, Xstatic_raw)\n",
    "Y = jr.permutation(key, Y_raw)\n",
    "\n",
    "# look at ERA5 rain\n",
    "Y = Y[:,0:1]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# just keep the \"prognostic\" 3d inputs that joe considered (for now)\n",
    "# RH, tehta_e^+, theta_e, theta_e^*\n",
    "names_3d =  [\"Absolute temperature,\",\"Relative Humidity\", \"Specific Humidity\", \"Geopotential Height\", \"Zonal Wind\",\"Meridional Wind\",\"Potential Temperature (theta)\",\"Equivalent Potential Temperature (tehta_e)\", \"Equivalent Potential Temperature Saturation Deficit (theta_e+)\", \"Saturated Equivalent Potential Temperature (theta_e*)\", \"MSE-conserving Plume Buoyancy\"]\n",
    "names_3d_short =  [\"K,\",\"RH\", \"q\", \"gh\", \"wind_z\",\"wind_m\",\"theta\",\"tehta_e\", \"theta_e+\", \"theta_e*\", \"plume\"]\n",
    "idx_3d = [1,4,5,6,7,8,9]\n",
    "# idx_3d = [1, 7, 8, 9]\n",
    "names_3d = [names_3d[i] for i in idx_3d]\n",
    "names_3d_short = [names_3d_short[i] for i in idx_3d]\n",
    "X3d = X3d[:,idx_3d,:]\n",
    "\n",
    "\n",
    "\n",
    "names_static = [\"Land-sea Mask\",\"Angle of sub-gridscale orography\",\"Anisotropy of sub-gridscale orography\",\"Stdev of sub-gridscale orography\",\"Slope of sub-gridscale orography\"]\n",
    "names_static_short = [\"LSM\",\"O_angle\",\"O_anisotrophy\",\"O_sd\",\"O_slope\"]\n",
    "idx_static = [0, 3]\n",
    "names_static = [names_static[i] for i in idx_static]\n",
    "names_static_short = [names_static_short[i] for i in idx_static]\n",
    "Xstatic = Xstatic[:,idx_static]\n",
    "lsm_threshold = 0.5\n",
    "# # also use his \"normalisatopm\" for sigma_o\n",
    "o_sd_idx = names_static.index(\"Stdev of sub-gridscale orography\")\n",
    "lsm_idx = names_static.index(\"Land-sea Mask\")\n",
    "# Xstatic = Xstatic.at[:, o_sd_idx].set(jnp.where(Xstatic[:,lsm_idx]<lsm_threshold, jnp.nanmean(1.0+jnp.log(1+Xstatic[:,o_sd_idx][Xstatic[:,lsm_idx]>lsm_threshold])), 1.0+jnp.log(1+Xstatic[:,o_sd_idx]))) # optimize lsm_threshold?\n",
    "Xstatic = Xstatic.at[:, o_sd_idx].set(jnp.log(Xstatic[:,o_sd_idx]+1.0))\n",
    "# map = lambda x: jnp.log((x+1e-5)/(1-x+1e-5))\n",
    "# Xstatic = Xstatic.at[:,lsm_idx].set(map(Xstatic[:,lsm_idx]))\n",
    "# lsm_threshold = map(lsm_threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# names_2d = [\"Sea Surface temperature\", \"Sensible heat flux land\",\"Sensible heat flux sea\", \"Latent heat flux land\",\"Latent heat flux sea\", \"Vertically-integrated moisture convergence\", \"Column relative humidity\"]\n",
    "# names_2d_short = [\"T_surface\",\"flux_s_land\",\"flux_s_sea\",\"flux_l_land\",\"flux_l_sea\",\"moisture\",\"CRH\"]\n",
    "names_2d = [\"Sea Surface temperature\", \"Sensible heat flux\", \"Latent heat flux\", \"Vertically-integrated moisture convergence\", \"Column relative humidity\"]\n",
    "names_2d_short = [\"T_surface\",\"flux_s\",\"flux_l\",\"moisture\",\"CRH\"]\n",
    "idx_2d = [1,2]\n",
    "names_2d = [names_2d[i] for i in idx_2d]\n",
    "names_2d_short = [names_2d_short[i] for i in idx_2d]\n",
    "X2d = X2d[:,idx_2d]\n",
    "#sea_surface_idx = names_2d.index(\"Sea Surface temperature\")\n",
    "# sea_surface = jnp.where(Xstatic[:,lsm_idx]>lsm_threshold, jnp.nanmean(X2d[:,sea_surface_idx]), X2d[:, sea_surface_idx])\n",
    "# X2d = X2d.at[:,sea_surface_idx].set(sea_surface) # turn nans corresonping to land for sea surface to fixed value, they will be ignored by switch kernels anyway \n",
    "flux_s_idx = names_2d.index(\"Sensible heat flux\")\n",
    "flux_l_idx = names_2d.index(\"Latent heat flux\")\n",
    "X2d = X2d.at[:,flux_s_idx].set(jnp.log(X2d[:,flux_s_idx]+0.01 - jnp.min(X2d[:,flux_s_idx])) )\n",
    "# X2d = X2d.at[:,flux_l_idx].set(jnp.log(X2d[:,flux_l_idx]+1.0 - jnp.min(X2d[:,flux_l_idx])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove all pressure levels above 500 hPA\n",
    "lowest_idx =  11 #7\n",
    "print(f\"Removed all pressure levels below {pressure[:,lowest_idx]} hPa\")\n",
    "X3d = X3d[:, :, lowest_idx:]\n",
    "pressure_levels = pressure[:,lowest_idx:]\n",
    "pressure_mean = jnp.mean(pressure_levels)\n",
    "pressure_std = jnp.std(pressure_levels)\n",
    "pressure_levels = (pressure_levels - pressure_mean) / pressure_std\n",
    "\n",
    "\n",
    "\n",
    "# remove any entries with nan\n",
    "X3d_nan_idx = jnp.isnan(X3d).any(axis=1).any(axis=1)\n",
    "X2d_nan_idx = jnp.isnan(X2d).any(axis=1) # ignore sea surface\n",
    "Xstatic_nan_idx = jnp.isnan(Xstatic).any(axis=1)\n",
    "Y_nan_idx = jnp.isnan(Y).any(axis=1)\n",
    "any_nan = X3d_nan_idx | X2d_nan_idx | Y_nan_idx | Xstatic_nan_idx\n",
    "no_nan = ~ any_nan\n",
    "print(f\"Removed {any_nan.sum()} entries with nan\")\n",
    "X2d = X2d[no_nan,:]\n",
    "X3d = X3d[no_nan,:,:]\n",
    "Xstatic = Xstatic[no_nan,:]\n",
    "Y = Y[no_nan,:]\n",
    "\n",
    "\n",
    "# remove no rain days\n",
    "print(f\"Removed {(Y[:,0]<0.01).sum()} entries with zero rain\")\n",
    "X3d = X3d[Y[:,0]>0,:]\n",
    "X2d = X2d[Y[:,0]>0,:]\n",
    "Xstatic = Xstatic[Y[:,0]>0,:]\n",
    "Y = Y[Y[:,0]>0,:]\n",
    "\n",
    "\n",
    "\n",
    "# # also log Y\n",
    "# print(f\"Applied log transform to Y\")\n",
    "# Y = jnp.log(Y-jnp.min(Y)+1e-3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_2d_variables= X2d.shape[1]\n",
    "num_3d_variables= X3d.shape[1]\n",
    "num_static_variables= Xstatic.shape[1]\n",
    "num_not_3d_variables = num_2d_variables + num_static_variables\n",
    "num_variables = num_2d_variables + num_3d_variables + num_static_variables\n",
    "print(f\"using {num_static_variables} static variables\")\n",
    "print(f\"using {num_2d_variables} 2d variables\")\n",
    "print(f\"using {num_3d_variables} 3d variables\")\n",
    "names = names_3d + names_2d + names_static\n",
    "names_short = names_3d_short + names_2d_short + names_static_short\n",
    "print(f\"using variables with names {names_short}\")\n",
    "\n",
    "\n",
    "\n",
    "problem_info = ProblemInfo(\n",
    "    num_2d_variables = num_2d_variables,\n",
    "    num_3d_variables = num_3d_variables,\n",
    "    num_static_variables = num_static_variables,\n",
    "    names_2d_short = names_2d_short,\n",
    "    names_3d_short = names_3d_short,\n",
    "    names_static_short = names_static_short,\n",
    "    names_2d = names_2d,\n",
    "    names_3d = names_3d,\n",
    "    names =names,\n",
    "    names_short = names_short,\n",
    "    names_static = names_static,\n",
    "    num_variables = num_variables,\n",
    "    pressure_levels = pressure_levels,\n",
    "    pressure_mean = pressure_mean,\n",
    "    pressure_std = pressure_std,\n",
    "    lsm_threshold = lsm_threshold,\n",
    ")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = VerticalDataset(\n",
    "    X2d = X2d,\n",
    "    X3d = X3d,\n",
    "    Xstatic = Xstatic,\n",
    "    y=Y,\n",
    "    standardize=True,\n",
    "    standardize_with_NF=False,\n",
    ")\n",
    "\n",
    "plot_data(problem_info,D)\n",
    "\n",
    "# plot_marginals(problem_info, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_log_prior(tau=None) -> callable:\n",
    "\n",
    "    def log_prior(model):\n",
    "        log_prob = 0.0\n",
    "\n",
    "        # if hasattr(model, \"smoother\"):\n",
    "        #     smoother_input_scale_prior = tfd.LogNormal(0.0,1.0)\n",
    "        #     smoother_mean_prior = tfd.Uniform(jnp.min(model.smoother.Z_levels),jnp.max(model.smoother.Z_levels))\n",
    "        #     log_prob += jnp.sum(smoother_mean_prior.log_prob(model.smoother.smoother_mean))\n",
    "        #     log_prob += jnp.sum(smoother_input_scale_prior.log_prob(model.smoother.smoother_input_scale))\n",
    "\n",
    "        lengthscales = jnp.vstack([k.lengthscale for k in model.base_kernels])\n",
    "        variances = model.interaction_variances\n",
    "       \n",
    "        #variance_prior = tfd.Gamma(1.0,0.2)\n",
    "        #log_prob += jnp.sum(variance_prior.log_prob(variances))\n",
    "\n",
    "        d = lengthscales.size\n",
    "        # #l_prior = tfd.LogNormal(jnp.sqrt(2.0) + jnp.log(d)/2.0,jnp.sqrt(1.0))\n",
    "        # #l_prior = tfd.Gamma(3.0*d,6.0)\n",
    "        #l_prior = tfd.Gamma(3.0,6.0)\n",
    "        #log_prob += jnp.sum(l_prior.log_prob(lengthscales))\n",
    "        l_prior = tfd.HalfCauchy(0.0,tau)\n",
    "        log_prob += jnp.sum(l_prior.log_prob((1.0 / lengthscales**2)))\n",
    "            \n",
    "        #noise_prior = tfd.LogNormal(0.0,10)\n",
    "        #log_prob += noise_prior.log_prob(model.likelihood.obs_stddev)\n",
    "\n",
    "        return log_prob\n",
    "    \n",
    "    return log_prior\n",
    "\n",
    "\n",
    "\n",
    "def init_smoother():\n",
    "    smoother_input_scale_bijector = tfb.Softplus(low=jnp.array(0.1, dtype=jnp.float64))\n",
    "    smoother_mean_bijector =  tfb.SoftClip(low=jnp.min(pressure_levels+1e-3), high=jnp.max(pressure_levels-1e-3))\n",
    "    smoother = VerticalSmoother(\n",
    "        jnp.array([[0.0]*num_3d_variables]), \n",
    "        jnp.array([[1.0]*num_3d_variables]), \n",
    "        Z_levels=pressure_levels\n",
    "        ).replace_bijector(smoother_input_scale=smoother_input_scale_bijector,smoother_mean=smoother_mean_bijector)\n",
    "    return smoother\n",
    "\n",
    "\n",
    "def init_kernels(data, linear=False):\n",
    "    lengthscale_bij = tfb.SoftClip(low=jnp.array(1e-3, dtype=jnp.float64), high=jnp.array(1e2, dtype=jnp.float64))\n",
    "    kernels = []\n",
    "    if linear:\n",
    "        kernel = gpx.kernels.Linear(active_dims=[i for i in range(len(names_short))])\n",
    "        kernels.append(kernel)\n",
    "    else:\n",
    "        lsm_idx = names_short.index(\"LSM\")\n",
    "        for i, name in enumerate(names_short):\n",
    "            kernel = gpx.kernels.RBF(lengthscale=jnp.array([1.1]), active_dims=[i]).replace_trainable(variance=False).replace_bijector(lengthscale = lengthscale_bij)\n",
    "            # if name in [\"O_sd\"]:\n",
    "            #     kernel *= SwitchKernelPositive(threshold = jnp.array([problem_info.lsm_threshold]), active_dims=[lsm_idx])\n",
    "            # elif name in [\"T_surface\"]:\n",
    "            #     kernel *= SwitchKernelNegative(threshold = jnp.array([problem_info.lsm_threshold]), active_dims=[lsm_idx])\n",
    "            kernels.append(kernel)\n",
    "    return kernels\n",
    "    \n",
    "def init_likelihood(data, obs_stddev = jnp.array(1.0, dtype=jnp.float64) ):\n",
    "    obs_bij=tfb.Softplus(low=jnp.array(1e-3, dtype=jnp.float64))\n",
    "    return gpx.likelihoods.Gaussian(num_datapoints=data.n, obs_stddev=obs_stddev).replace_bijector(obs_stddev=obs_bij)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit with small data\n",
    "# # prep fancy kernel for model\n",
    "# # first fit with small data to get init for SVGP\n",
    "# num_data_for_init=200\n",
    "# D_small = D.get_subset(num_data_for_init, space_filling=False,use_output=True)\n",
    "\n",
    "# plot_data(problem_info, D_small)\n",
    "# starting_var = jnp.var(D_small.y)\n",
    "# posterior = ConjugatePrecipGP(\n",
    "#     base_kernels=init_kernels(D_small), \n",
    "#     likelihood=init_likelihood(D_small, obs_stddev = jnp.array(1.0, dtype=jnp.float64) ),\n",
    "#     smoother=init_smoother(),\n",
    "#     max_interaction_depth=2,\n",
    "#     interaction_variances=jnp.array([starting_var/3.0]*3, dtype=jnp.float64),\n",
    "#     jitter=jnp.array(1e-5, dtype=jnp.float64),\n",
    "#     measure=None,\n",
    "#     second_order_empirical=False,\n",
    "#     )\n",
    "# plt.figure()\n",
    "# # opt_posterior, history = gpx.fit_scipy(\n",
    "# #         model=posterior,\n",
    "# #         objective=jax.jit(posterior.loss_fn(negative=True, log_prior=None, use_loocv=True)),#build_log_prior(tau=1.0))),\n",
    "# #         train_data=D_small,\n",
    "# #         safe=False,\n",
    "# #     )\n",
    "\n",
    "# opt_posterior, history = gpx.fit(\n",
    "#         model=posterior,\n",
    "#         objective=jax.jit(posterior.loss_fn(negative=True, log_prior=None)),\n",
    "#         train_data=D_small,\n",
    "#         optim=ox.adam(1e-1),\n",
    "#         num_iters=200,\n",
    "#         safe=False,\n",
    "#         key=key,\n",
    "#     )\n",
    "# plt.plot(history)\n",
    "# plot_params(problem_info, opt_posterior,D_small, title=\"initial fit with small data\", print_corr=True)\n",
    "# print(f\"noise is {opt_posterior.likelihood.obs_stddev}\")\n",
    "# print(f\"interaction vars {opt_posterior.interaction_variances}\")\n",
    "# plot_interactions(problem_info, opt_posterior, D_small, k=5, use_range=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#likelihood = init_likelihood(D, obs_stddev = jnp.array(1.0, dtype=jnp.float64) )\n",
    "#likelihood = gpx.likelihoods.Exponential(num_datapoints=D.n)\n",
    "likelihood = gpx.likelihoods.Gamma(num_datapoints=D.n).replace_bijector(\n",
    "    scale1=tfb.SoftClip(low=jnp.array(1e-3, dtype=jnp.float64),high=jnp.array(1e3, dtype=jnp.float64)),\n",
    "    scale2=tfb.SoftClip(low=jnp.array(1e-3, dtype=jnp.float64),high=jnp.array(1e3, dtype=jnp.float64)),\n",
    "    )\n",
    "assert jnp.min(D.y) >= 0\n",
    "\n",
    "num_inducing=25\n",
    "\n",
    "\n",
    "# smoothed = opt_posterior.smoother.smooth_data(D_small)[0]\n",
    "#\n",
    "# sampler = qmc.Halton(d=problem_info.num_variables)\n",
    "#inducing_inputs  = sampler.random(n=num_inducing) * (jnp.max(smoothed, axis=0) - jnp.min(smoothed, axis=0)) + jnp.min(smoothed, axis=0)\n",
    "#inducing_inputs = jr.uniform(key, (num_inducing, D_small.dim), minval=jnp.array(1.0, dtype=jnp.float64), maxval=jnp.array(1.0, dtype=jnp.float64))\n",
    "# variational_mean = opt_posterior.predict(inducing_inputs, D_small).mean()[:,None]\n",
    "# if isinstance(likelihood, (gpx.likelihoods.Exponential, gpx.likelihoods.Gamma)):\n",
    "#     variational_mean = jnp.log(jnp.maximum(variational_mean,1e-5))\n",
    "   \n",
    "variational_root_covariance = jnp.eye(num_inducing)\n",
    "smoother = init_smoother()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#init_data = D.get_subset(num_inducing, space_filling=True, use_output=False)\n",
    "# inducing_inputs_3d = smoother.smooth_data(init_data)[0]\n",
    "# inducing_inputs = smoother.smooth_data(init_data)[0]\n",
    "#inducing_inputs  = qmc.Halton(d=problem_info.num_variables).random(n=num_inducing)\n",
    "\n",
    "# inducing_inputs_3d = init_data.X3d[:num_inducing,:,:]\n",
    "# inducing_inputs_2d = init_data.X2d[:num_inducing,:]\n",
    "# inducing_inputs_static = init_data.Xstatic[:num_inducing,:]\n",
    "# #variational_mean = opt_posterior.predict(opt_posterior.smoother.smooth_data(init_data)[0], D_small).mean()[:,None]\n",
    "# variational_mean = jnp.log(init_data.y[:num_inducing,:])\n",
    "\n",
    "\n",
    "\n",
    "inducing_inputs = qmc.Halton(d=problem_info.num_3d_variables*jnp.shape(problem_info.pressure_levels)[1]+ problem_info.num_2d_variables+problem_info.num_static_variables, seed=1234).random(n=num_inducing)\n",
    "inducing_inputs_3d = inducing_inputs[:,:num_3d_variables*jnp.shape(problem_info.pressure_levels)[1]].reshape(num_inducing,problem_info.num_3d_variables, jnp.shape(problem_info.pressure_levels)[1])\n",
    "inducing_inputs_2d =inducing_inputs[:,num_3d_variables*jnp.shape(problem_info.pressure_levels)[1]:num_3d_variables*jnp.shape(problem_info.pressure_levels)[1]+num_2d_variables]\n",
    "inducing_inputs_static = inducing_inputs[:,num_3d_variables*jnp.shape(problem_info.pressure_levels)[1]+num_2d_variables:]\n",
    "\n",
    "inducing_inputs = qmc.Halton(d=problem_info.num_3d_variables+ problem_info.num_2d_variables+problem_info.num_static_variables, seed=1234).random(n=num_inducing)\n",
    "inducing_inputs_3d = inducing_inputs[:,:num_3d_variables]\n",
    "inducing_inputs_2d =inducing_inputs[:,num_3d_variables:num_3d_variables+num_2d_variables]\n",
    "inducing_inputs_static = inducing_inputs[:,num_3d_variables+num_2d_variables:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "variational_mean  = jnp.zeros((num_inducing,1), dtype=jnp.float64)\n",
    "max_depth = 2\n",
    "variational_posterior = VariationalPrecipGP(\n",
    "    base_kernels=init_kernels(D), \n",
    "    likelihood=likelihood,\n",
    "    smoother=init_smoother(),\n",
    "    max_interaction_depth=max_depth,\n",
    "    interaction_variances=jnp.array([1.0/(max_depth+1)]*(max_depth+1), dtype=jnp.float64),\n",
    "    jitter=jnp.array(1e-6, dtype=jnp.float64),\n",
    "    measure = \"empirical\",\n",
    "    second_order_empirical=False,\n",
    "    inducing_inputs_3d=inducing_inputs_3d,\n",
    "    inducing_inputs_2d = inducing_inputs_2d,\n",
    "    inducing_inputs_static = inducing_inputs_static,\n",
    "    variational_mean=variational_mean,\n",
    "    variational_root_covariance=variational_root_covariance,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#variational_posterior = variational_posterior.replace_trainable(inducing_inputs_3d=False, inducing_inputs_2d=False, inducing_inputs_static=False)\n",
    "variational_posterior = variational_posterior.replace_bijector(\n",
    "    inducing_inputs_3d = tfb.SoftClip(low=jnp.array(-1e-3, dtype=jnp.float64),high=jnp.array(1.0+1e-3, dtype=jnp.float64)),\n",
    "    inducing_inputs_static = tfb.SoftClip(low=jnp.array(-1e-3, dtype=jnp.float64),high=jnp.array(1.0+1e-3, dtype=jnp.float64)),\n",
    "    inducing_inputs_2d = tfb.SoftClip(low=jnp.array(-1e-3, dtype=jnp.float64),high=jnp.array(1.0+1e-3, dtype=jnp.float64)),\n",
    "    variational_mean = tfb.SoftClip(low=jnp.array(-1e5, dtype=jnp.float64),high=jnp.array(1e3, dtype=jnp.float64)),\n",
    "    variational_root_covariance = tfb.Chain([tfb.FillTriangular(), tfb.SoftClip(low=jnp.array(-1e3, dtype=jnp.float64),high=jnp.array(1e3, dtype=jnp.float64))]),\n",
    "    )\n",
    "opt_variational_posterior, history = gpx.fit(\n",
    "        model=variational_posterior,\n",
    "        objective=jax.jit(variational_posterior.loss_fn(negative=True, log_prior=None)), #todo rejit\n",
    "        train_data=D,\n",
    "        optim=ox.adamw(1e-1),\n",
    "        num_iters=500,\n",
    "        batch_size=512,\n",
    "        safe=False,\n",
    "        key=key,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# variational_posterior = variational_posterior.replace_trainable(inducing_inputs_3d=True, inducing_inputs_2d=True, inducing_inputs_static=True)\n",
    "# opt_variational_posterior, history = gpx.fit(\n",
    "#         model=opt_variational_posterior,\n",
    "#         objective=opt_variational_posterior.loss_fn(negative=True, log_prior=None), #todo rejit\n",
    "#         train_data=D,\n",
    "#         optim=ox.adamw(1e-1),\n",
    "#         num_iters=1000,\n",
    "#         batch_size=1024,\n",
    "#         safe=False,\n",
    "#         key=key,\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prep for infernece \n",
    "\n",
    "plt.plot(history[5:])\n",
    "plot_params(problem_info, opt_variational_posterior,D, title=\"initial fit with small data\", print_corr=True)\n",
    "try:\n",
    "    print(f\"noise is {opt_variational_posterior.likelihood.obs_stddev}\")\n",
    "except:\n",
    "    try:\n",
    "        print(f\"scalee is {opt_variational_posterior.likelihood.scale1}\")\n",
    "        print(f\"scalee is {opt_variational_posterior.likelihood.scale2}\")\n",
    "    except:\n",
    "        pass\n",
    "    pass\n",
    "print(f\"interaction vars {opt_variational_posterior.interaction_variances}\")\n",
    "plot_interactions(problem_info, opt_variational_posterior, D, k=10, use_range=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smol = D.get_subset(1_000)\n",
    "smol_smoothed = opt_variational_posterior.smoother.smooth_data(smol)[0]\n",
    "pred_f_mean = opt_variational_posterior.predict_indiv_mean(smol_smoothed)\n",
    "pred_f_var = opt_variational_posterior.predict_indiv_var(smol_smoothed)\n",
    "pred_y = opt_variational_posterior.likelihood.link_function(pred_f_mean).mean()\n",
    "plt.hist(smol.y.T, label=\"truth\", alpha=0.5, bins=100, density=True)\n",
    "plt.hist(pred_y, label=\"pred\", alpha=0.5, bins=100, density=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
