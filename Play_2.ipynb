{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax\n",
    "from dataclasses import dataclass\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "#with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "import gpjax as gpx\n",
    "from gpjax.distributions import GaussianDistribution\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "# plt.style.use(\n",
    "#     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    "# )\n",
    "# colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from gpjax.kernels.base import AdditiveKernel\n",
    "\n",
    "\n",
    "import optax as ox\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "# custom bits\n",
    "from gpjax.dataset import VerticalDataset\n",
    "from gpjax.kernels.stationary.rbf import OrthogonalRBF, OrthogonalRBFUnif\n",
    "from gpjax.gps import CustomAdditiveConjugatePosterior, VerticalSmoother\n",
    "from gpjax.objectives import CustomConjugateMLL, CustomELBO, custom_variational_expectation\n",
    "from gpjax.optim_utils import optim_builder, zero_grads\n",
    "from gpjax.variational_families import CustomVariationalGaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100_000 entries sampled across time/lat/lon over first day of data\n",
    "\n",
    "## X2D has:\n",
    "\n",
    "0\"Sea surface temperature (K)\"\n",
    "\n",
    "1\"Sensible heat flux (W/m^2)\"\n",
    "\n",
    "2\"Latent heat flux (W/m^2)\"\n",
    "\n",
    "3\"Vertically-integrated moisture convergence (kg/m^2)\"\n",
    "\n",
    "4\"Column relative humidity (%)\"\n",
    "\n",
    "\n",
    "## X3D has:\n",
    "\n",
    "0\"Absolute temperature (K)\"\n",
    "\n",
    "1\"Relative humidity (%)\"\n",
    "\n",
    "2\"Specific humidity (kg/kg)\"\n",
    "\n",
    "3\"Geopotential height (m^2 s^-2)\"\n",
    "\n",
    "4\"Zonal wind (m/s)\"\n",
    "\n",
    "5\"Meridional wind (m/s)\"\n",
    "\n",
    "6\"Potential temperature (K)\"\n",
    "\n",
    "7\"Equivalent potential temperature (K)\"\n",
    "\n",
    "8\"Equivalent potential temperature saturation deficit (K)\"\n",
    "\n",
    "9\"Saturated equivalent potential temperature (K)\"\n",
    "\n",
    "10\"MSE-conserving plume buoyancy (m/s^2)\"\n",
    "\n",
    "\n",
    "## static has:\n",
    "\n",
    "0\"Land-sea mask\"\n",
    "\n",
    "1\"Angle of sub-gridscale orography (rad)\n",
    "\n",
    "2\"Anisotropy of sub-gridscale orography\"\n",
    "\n",
    "3\"Standard deviation of sub-gridscale orography\"\n",
    "\n",
    "4\"Slope of sub-gridscale orography\"\n",
    "\n",
    "## Y has:\n",
    "\n",
    "0\"ERA5 Precipitation (mm/hr)\"\n",
    "\n",
    "1\"TRMM Precipitation (mm/hr)\"\n",
    "\n",
    "2\"TRMM Relative Error (%)\"\n",
    "\n",
    "# plev are\n",
    "1000.,   2000.,   3000.,   5000.,   7000.,  10000., 15000.,\n",
    "20000.,  25000.,  30000.,  40000.,  50000.,  60000.,  70000.,80000.,  85000.,  90000.,  92500.,  95000.,  97500., 100000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X2d_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/X2d_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "X3d_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/X3d_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Xstatic_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/XStatic_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Y_raw = jnp.array(jnp.load(\"../data/ERA/NPY_DATA/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "# X2d_raw = jnp.array(jnp.load(\"../data/100_000_one_day/X2D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "# X3d_raw = jnp.array(jnp.load(\"../data/100_000_one_day/X3D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "# Xstatic_raw = jnp.array(jnp.load(\"../data/100_000_one_day/XSTATIC_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "# Y_raw = jnp.array(jnp.load(\"../data/100_000_one_day/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pressure = jnp.array([[1000,2000,3000,5000,7000,10000,15000,20000,25000,30000,40000,50000,60000,70000,80000, 85000,90000,92500,95000,97500,100000]], dtype=jnp.float64)\n",
    "\n",
    "# random shuffle\n",
    "X2d = jr.permutation(key, X2d_raw)\n",
    "X3d = jr.permutation(key, X3d_raw)\n",
    "Xstatic = jr.permutation(key, Xstatic_raw)\n",
    "Y = jr.permutation(key, Y_raw)\n",
    "\n",
    "# look at ERA5 rain\n",
    "Y = Y[:,0:1]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# just keep the \"prognostic\" 3d inputs that joe considered (for now)\n",
    "# RH, tehta_e^+, theta_e, theta_e^*\n",
    "names_3d =  [\"Absolute temperature,\",\"Relative Humidity\", \"Specific Humidity\", \"Geopotential Height\", \"Zonal Wind\",\"Meridional Wind\",\"Potential Temperature (theta)\",\"Equivalent Potential Temperature (tehta_e)\", \"Equivalent Potential Temperature Saturation Deficit (theta_e+)\", \"Saturated Equivalent Potential Temperature (theta_e*)\", \"MSE-conserving Plume Buoyancy\"]\n",
    "names_3d_short =  [\"K,\",\"RH\", \"q\", \"gh\", \"wind_z\",\"wind_m\",\"theta\",\"tehta_e\", \"theta_e+\", \"theta_e*\", \"plume\"]\n",
    "idx_3d = [i for i in range(len(names_3d))]\n",
    "# idx_3d = [1, 7, 8, 9]\n",
    "names_3d = [names_3d[i] for i in idx_3d]\n",
    "names_3d_short = [names_3d_short[i] for i in idx_3d]\n",
    "X3d = X3d[:,idx_3d,:]\n",
    "\n",
    "\n",
    "# # also use his \"normalisatopm\" for sigma_o\n",
    "#sigma_o = jnp.where(Xstatic[:,0]<0.5, 0.0, 1.0+jnp.log(1+Xstatic[:,3])) # optimize threshold?\n",
    "sigma_o = jnp.log(Xstatic[:,3]+1.0)\n",
    "Xstatic = Xstatic.at[:,3].set(sigma_o)\n",
    "names_static = [\"Land-sea Mask\",\"Angle of sub-gridscale orography\",\"Anisotropy of sub-gridscale orography\",\"Stdev of sub-gridscale orography\",\"Slope of sub-gridscale orography\"]\n",
    "names_static_short = [\"LSM\",\"O_angle\",\"O_anisotrophy\",\"O_sd\",\"O_slope\"]\n",
    "idx_static = [0, 1, 2, 3, 4]\n",
    "names_static = [names_static[i] for i in idx_static]\n",
    "names_static_short = [names_static_short[i] for i in idx_static]\n",
    "Xstatic = Xstatic[:,idx_static]\n",
    "\n",
    "names_2d = [\"Sea Surface temperature\", \"Sensible heat flux\", \"Latent heat flux\", \"Vertically-integrated moisture convergence\", \"Column relative humidity\"]\n",
    "names_2d_short = [\"T_surface\",\"flux_s\",\"flux_l\",\"moisture\",\"CRH\"]\n",
    "idx_2d = [1,2,4]\n",
    "names_2d = [names_2d[i] for i in idx_2d]\n",
    "names_2d_short = [names_2d_short[i] for i in idx_2d]\n",
    "X2d = X2d[:,idx_2d]\n",
    "\n",
    "\n",
    "\n",
    "#remove all pressure levels above 500 hPA\n",
    "lowest_idx =  11 #7\n",
    "print(f\"Removed all pressure levels below {pressure[:,lowest_idx]} hPa\")\n",
    "X3d = X3d[:, :, lowest_idx:]\n",
    "pressure_levels = pressure[:,lowest_idx:]\n",
    "pressure_mean = jnp.mean(pressure_levels)\n",
    "pressure_std = jnp.std(pressure_levels)\n",
    "pressure_levels = (pressure_levels - pressure_mean) / pressure_std\n",
    "\n",
    "\n",
    "\n",
    "# remove any entries with nan\n",
    "X3d_nan_idx = jnp.isnan(X3d).any(axis=1).any(axis=1)\n",
    "X2d_nan_idx = jnp.isnan(X2d).any(axis=1)\n",
    "Xstatic_nan_idx = jnp.isnan(Xstatic).any(axis=1)\n",
    "Y_nan_idx = jnp.isnan(Y).any(axis=1)\n",
    "any_nan = X3d_nan_idx | X2d_nan_idx | Y_nan_idx | Xstatic_nan_idx\n",
    "no_nan = ~ any_nan\n",
    "print(f\"Removed {any_nan.sum()} entries with nan\")\n",
    "X2d = X2d[no_nan,:]\n",
    "X3d = X3d[no_nan,:,:]\n",
    "Xstatic = Xstatic[no_nan,:]\n",
    "Y = Y[no_nan,:]\n",
    "\n",
    "\n",
    "# # remove no rain days\n",
    "# print(f\"Removed {(Y[:,0]==0).sum()} entries with zero rain\")\n",
    "# X3d = X3d[Y[:,0]>0,:]\n",
    "# X2d = X2d[Y[:,0]>0,:]\n",
    "# Xstatic = Xstatic[Y[:,0]>0,:]\n",
    "# Y = Y[Y[:,0]>0,:]\n",
    "\n",
    "\n",
    "# also log Y\n",
    "# print(f\"Applied log transform to Y\")\n",
    "#Y = jnp.log(Y-jnp.min(Y)+1e-1)\n",
    "\n",
    "\n",
    "num_2d_variables= X2d.shape[1]\n",
    "num_3d_variables= X3d.shape[1]\n",
    "num_static_variables= Xstatic.shape[1]\n",
    "num_not_3d_variables = num_2d_variables + num_static_variables\n",
    "num_variables = num_2d_variables + num_3d_variables + num_static_variables\n",
    "print(f\"using {num_static_variables} static variables\")\n",
    "print(f\"using {num_2d_variables} 2d variables\")\n",
    "print(f\"using {num_3d_variables} 3d variables\")\n",
    "names = names_3d + names_2d + names_static\n",
    "names_short = names_3d_short + names_2d_short + names_static_short\n",
    "print(f\"using variables with names {names_short}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_params(model,data,title=\"\", print_corr=False):\n",
    "    if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "        model = model.posterior\n",
    "    plt.figure()\n",
    "    lengthscales = jnp.array([model.base_kernels[i].lengthscale[0] for i in range(len(model.base_kernels))])\n",
    "    z_to_plot = jnp.linspace(jnp.min(model.smoother.Z_levels),jnp.max(model.smoother.Z_levels),100)\n",
    "    smoothing_weights = model.smoother.smooth_fn(z_to_plot) \n",
    "    z_unscaled = z_to_plot * pressure_std+ pressure_mean\n",
    "    for i in range(num_3d_variables):\n",
    "        plt.plot(smoothing_weights[i,:].T,z_unscaled, label=f\"{names_3d_short[i]} with lengthscales_ {lengthscales[i]:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.title(title+f\" other lengthscales are {lengthscales[num_3d_variables:]}\")\n",
    "    smoothed = model.smoother.smooth_data(data)[0]\n",
    "    plt.figure()\n",
    "    for i in range(num_3d_variables):\n",
    "        plt.hist(smoothed[i,:], label=names_3d_short[i], alpha=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    corr = jnp.corrcoef(smoothed.T)\n",
    "    plt.figure()\n",
    "    plt.imshow(corr)\n",
    "    plt.colorbar()\n",
    "    plt.figure()\n",
    "    plt.hist(corr.flatten(), bins=10);\n",
    "    pairs = []\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i):\n",
    "            if jnp.absolute(corr[i][j])>0.75:\n",
    "                pairs.append([names[i], names[j]])\n",
    "                if print_corr:\n",
    "                    print(f\"{names[i]} and {names[j]} have correlation {corr[i][j]}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_interactions(model, data, k=10):\n",
    "        \n",
    "    plt.figure()\n",
    "    idx_2 = []\n",
    "    for i in range(num_variables):\n",
    "        for j in range(i+1,num_variables):\n",
    "            idx_2.append([i,j])\n",
    "    idxs = [[]] + [[i] for i in range(num_variables)] + idx_2\n",
    "    if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "        sobols = model.get_sobol_indicies(idxs)\n",
    "        z = model.inducing_inputs\n",
    "    else:\n",
    "        sobols = model.get_sobol_indicies(data, idxs)\n",
    "        z = model.smoother.smooth_data(data)[0]\n",
    "    sobols = sobols / jnp.sum(sobols)\n",
    "\n",
    "    plt.plot(sobols)\n",
    "    plt.title(\"sobol indicies (red lines between orders)\")\n",
    "    plt.axvline(x=1, color=\"red\")\n",
    "    plt.axvline(x=num_variables+1, color=\"red\")\n",
    "    for idx in jax.lax.top_k(sobols, k)[1]:\n",
    "        chosen_idx = idxs[idx]\n",
    "        plt.figure()\n",
    "        num_plot = 1_000 if len(chosen_idx)==1 else 10_000\n",
    "        from scipy.stats import qmc\n",
    "        sampler = qmc.Halton(d=num_variables)\n",
    "        x_plot = sampler.random(n=num_plot) * (jnp.max(z, axis=0) - jnp.min(z, axis=0)) + jnp.min(z, axis=0)\n",
    "        if len(chosen_idx)==1:\n",
    "            if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "                mean = model.predict_indiv_mean(x_plot,chosen_idx)\n",
    "                std = jnp.sqrt(model.predict_indiv_var(x_plot,chosen_idx))\n",
    "            else:\n",
    "                mean = model.predict_indiv_mean(x_plot, data, chosen_idx)\n",
    "                std = jnp.sqrt(model.predict_indiv_var(x_plot, data, chosen_idx))\n",
    "            mean = mean * data.Y_std + data.Y_mean\n",
    "            std = std* data.Y_std\n",
    "            plt.scatter(x_plot[:,chosen_idx[0]],mean, color=\"blue\") \n",
    "            plt.scatter(x_plot[:,chosen_idx[0]],mean+ 1.96*std, color=\"red\") \n",
    "            plt.scatter(x_plot[:,chosen_idx[0]],mean- 1.96*std, color=\"red\") \n",
    "            plt.xlim([jnp.min(x_plot[:,chosen_idx[0]]),jnp.max(x_plot[:,chosen_idx[0]])])\n",
    "            plt.scatter(z[:,chosen_idx[0]],jnp.zeros_like(z[:,chosen_idx[0]]), color=\"black\")\n",
    "            plt.title(f\"Best guess (and uncertainty) at additive contributions from {[names[i] for i in chosen_idx]}with sobol index {sobols[idx]}\")\n",
    "        elif len(chosen_idx)==2:\n",
    "            if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "                mean = model.predict_indiv_mean(x_plot,chosen_idx)\n",
    "            else:\n",
    "                mean = model.predict_indiv_mean(x_plot, data, chosen_idx)\n",
    "            mean = mean * data.Y_std + data.Y_mean\n",
    "            col = plt.scatter(x_plot[:,chosen_idx[0]],x_plot[:,chosen_idx[1]],c=mean)\n",
    "            plt.ylim([jnp.min(z[:,chosen_idx[1]]),jnp.max(z[:,chosen_idx[1]])])\n",
    "            plt.colorbar(col)\n",
    "            plt.scatter(z[:,chosen_idx[0]],z[:,chosen_idx[1]], color=\"black\")\n",
    "            plt.title(f\"Best guess at additive contribution from {[names[i] for i in chosen_idx]} with sobol index {sobols[idx]}\")\n",
    "        plt.xlim([jnp.min(x_plot[:,chosen_idx[0]]),jnp.max(x_plot[:,chosen_idx[0]])])   \n",
    "       \n",
    "        \n",
    "        \n",
    "    # top_idx = jax.lax.top_k(sobols, k)[1]\n",
    "    # zeroth = self.predict_additive_component(x, train_data, []).mean()[0]\n",
    "    # plt.figure()\n",
    "    # plt.hist(train_data.y.T, label=\"data\")\n",
    "    # plt.hist(jnp.sum(mean_components[:,:,0],0).T+ zeroth, label=\"all components\")\n",
    "    # plt.hist(jnp.sum(mean_components[:,:,0][top_idx],0).T + zeroth, label=f\"top {k} components\")\n",
    "    # plt.legend()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def plot_data(data:VerticalDataset):\n",
    "    plt.hist(data.y.T)\n",
    "    plt.title(\"Y\")\n",
    "\n",
    "    for X in [data.X3d_raw, data.X3d]:\n",
    "        fig, ax = plt.subplots(nrows=3, ncols=4)\n",
    "        i,j=0,0\n",
    "        for row in ax:\n",
    "            for col in row:\n",
    "                col.boxplot(X[:,i,:].T, showfliers=False);\n",
    "                col.set_title(names_3d_short[i])\n",
    "                i+=1\n",
    "                if i==X.shape[1]:\n",
    "                    break\n",
    "            if i==X.shape[1]:\n",
    "                break\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=data.X2d.shape[1])\n",
    "    i=0\n",
    "    for col in ax:\n",
    "        col.hist(data.X2d[:100000,i].T);\n",
    "        col.set_title(names_2d_short[i])\n",
    "        i+=1\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=data.Xstatic.shape[1])\n",
    "    i=0\n",
    "    for col in ax:\n",
    "        col.hist(data.Xstatic[:100000,i].T);\n",
    "        col.set_title(names_static_short[i])\n",
    "        i+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Callable\n",
    "from jaxtyping import Num, Float\n",
    "from gpjax.typing import Array, ScalarFloat\n",
    "from beartype.typing import Optional\n",
    "from gpjax.base import Module, param_field,static_field\n",
    "import cola\n",
    "from cola.linalg.decompositions.decompositions import Cholesky\n",
    "from jax import vmap\n",
    "\n",
    "@dataclass\n",
    "class ConjugatePrecipGP(Module):\n",
    "    base_kernels:List[gpx.kernels.AbstractKernel]\n",
    "    likelihood: gpx.likelihoods.AbstractLikelihood\n",
    "    smoother: VerticalSmoother\n",
    "    max_interaction_depth: bool = static_field(2)\n",
    "    interaction_variances: Float[Array, \" D\"] = param_field(jnp.array([1.0,1.0,1.0]), bijector=tfb.Softplus(low=jnp.array(1e-5, dtype=jnp.float64)))\n",
    "    jitter: float = static_field(1e-6)\n",
    "    zeroth_order: bool = static_field(True)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.mean_function = gpx.mean_functions.Zero()\n",
    "        if self.zeroth_order:\n",
    "            if not self.max_interaction_depth == len(self.interaction_variances) - 1:\n",
    "                raise ValueError(\"Number of interaction variances must be equal to max_interaction_depth + 1\")\n",
    "        else:\n",
    "            if not self.max_interaction_depth == len(self.interaction_variances):\n",
    "                raise ValueError(\"Number of interaction variances must be equal to max_interaction_depth\")\n",
    "        \n",
    "        \n",
    "    def predict(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: VerticalDataset,\n",
    "        component_list: Optional[List[List[int]]]=None,\n",
    "    ) -> Union[GaussianDistribution,  Num[Array, \"N 1\"]]:\n",
    "        r\"\"\"Get the posterior predictive distribution (for a specific additive component if componen specified).\"\"\"\n",
    "        #smooth data to get in form for preds\n",
    "        x, y = self.smoother.smooth_data(train_data)\n",
    "        t = test_inputs\n",
    "        \n",
    "        obs_noise = self.likelihood.obs_stddev**2\n",
    "        mx = self.mean_function(x)\n",
    "        mt = self.mean_function(t)\n",
    "        \n",
    "        \n",
    "        Kxx = self.eval_K_xt(x, x, ref = x) # [N, N]\n",
    "        Kxx += cola.ops.I_like(Kxx) * self.jitter\n",
    "        Sigma = Kxx + cola.ops.I_like(Kxx) * obs_noise\n",
    "        Sigma = cola.PSD(Sigma)\n",
    "        \n",
    "        if component_list is None:\n",
    "            Ktt = self.eval_K_xt(t,t,ref=x)\n",
    "            Kxt = self.eval_K_xt(x,t, ref=x)\n",
    "        else:\n",
    "            Ktt = self.eval_specific_K_xt(t, t, component_list, ref = x)\n",
    "            Kxt = self.eval_specific_K_xt(x, t, component_list, ref = x)\n",
    "        Sigma_inv_Kxt = cola.solve(Sigma, Kxt, Cholesky())\n",
    "        \n",
    "        mean = mt + jnp.matmul(Sigma_inv_Kxt.T, y - mx)\n",
    "        covariance = Ktt - jnp.matmul(Kxt.T, Sigma_inv_Kxt)\n",
    "        covariance += cola.ops.I_like(covariance) * self.jitter\n",
    "        covariance = cola.PSD(covariance)\n",
    "        return GaussianDistribution(jnp.atleast_1d(mean.squeeze()), covariance)\n",
    "    \n",
    "    def predict_indiv_mean(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: VerticalDataset,\n",
    "        component_list: Optional[List[List[int]]]=None,\n",
    "    ):\n",
    "        predictor = lambda x: self.predict(x, train_data, component_list).mean()\n",
    "        return jax.vmap(predictor,1)(test_inputs[:,None,:])\n",
    "    \n",
    "    def predict_indiv_var(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: VerticalDataset,\n",
    "        component_list: Optional[List[List[int]]]=None,\n",
    "    ):\n",
    "        predictor = lambda x: self.predict(x, train_data, component_list).variance()\n",
    "        return jax.vmap(predictor,1)(test_inputs[:,None,:])\n",
    "\n",
    "    def eval_K_xt(self, x: Num[Array, \"N d\"], t: Num[Array, \"M d\"], ref:  Num[Array, \"n d\"]) -> Num[Array, \"N M\"]:\n",
    "        x_all, t_all = jnp.vstack([x, ref]), jnp.vstack([t, ref])  # [N+n, d] [M+n, d]\n",
    "        ks_all = jnp.stack([k.cross_covariance(x_all,t_all) for k in self.base_kernels]) # [d, N+n, M+n]\n",
    "        ks_orthogonal = self._orthogonalise(ks_all, num_ref = jnp.shape(ref)[0])\n",
    "        if self.zeroth_order:\n",
    "            return jnp.sum(self._compute_additive_terms_girad_newton(ks_orthogonal) * self.interaction_variances[:, None, None], 0)\n",
    "        else:\n",
    "            return jnp.sum(self._compute_additive_terms_girad_newton(ks_orthogonal)[1:,:,:] * self.interaction_variances[:, None, None], 0)\n",
    "        \n",
    "    def eval_specific_K_xt(self, x: Num[Array, \"N d\"], t: Num[Array, \"M d\"], component_list: List[int], ref =  Num[Array, \"n d\"])-> Num[Array, \"N M\"]:\n",
    "        x_all, t_all = jnp.vstack([x, ref]), jnp.vstack([t, ref])  # [N+n, d] [M+n, d]\n",
    "        ks_all = jnp.stack([self.base_kernels[i].cross_covariance(x_all,t_all) for i in component_list]) # [p, N+n, M+n]\n",
    "        var = self.interaction_variances[len(component_list)]\n",
    "        return var * jnp.prod(self._orthogonalise(ks_all, num_ref = jnp.shape(ref)[0]),0) # [N, M]\n",
    "        \n",
    "    def _orthogonalise(self, ks: Num[Array, \"d N+n M+n\"], num_ref: int)->Num[Array, \"d N M\"]:\n",
    "        ks_xt, ks_xX, ks_Xt, ks_XX = ks[:,:-num_ref,:-num_ref], ks[:,:-num_ref,-num_ref:], ks[:,-num_ref:,:-num_ref], ks[:,-num_ref:,-num_ref:] # [d, N, M], [d, N, n], [d, n, M], [d, n, n]\n",
    "        denom = jnp.mean(ks_XX, (1,2))[:, None, None] # [d, 1, 1]\n",
    "        Kx =  jnp.mean(ks_xX, 2) # [d, N]\n",
    "        Kt = jnp.mean(ks_Xt, 1) # [d, M]\n",
    "        numerator = jnp.matmul(Kx[:,:,None], Kt[:, None, :])# [d, N, M]\n",
    "        return ks_xt -  numerator / denom \n",
    "\n",
    "    @jax.jit   \n",
    "    def _compute_additive_terms_girad_newton(self, ks: Num[Array, \"D N N\"]) -> Num[Array, \"p N N\"]:\n",
    "        N = jnp.shape(ks)[-1]\n",
    "        powers = jnp.arange(self.max_interaction_depth + 1)[:, None] # [p + 1, 1]\n",
    "        s = jnp.power(ks[None, :,:,:],powers[:,:,None,None]) # [p + 1, d, N, N]\n",
    "        e = jnp.ones(shape=(self.max_interaction_depth+1, N, N), dtype=jnp.float64) # [p+1, N, N]lazy init then populate\n",
    "        for n in range(1, self.max_interaction_depth + 1): # has to be for loop because iterative\n",
    "            thing = jax.vmap(lambda k: ((-1.0)**(k-1))*e[n-k]*jnp.sum(s[k], 0))(jnp.arange(1, n+1))\n",
    "            e = e.at[n].set((1.0/n) *jnp.sum(thing,0))\n",
    "        return e\n",
    "    \n",
    "    def loss_fn(self, negative=False, log_prior: Optional[Callable] = None)->gpx.objectives.AbstractObjective:\n",
    "        class Loss(gpx.objectives.AbstractObjective):\n",
    "            def step(\n",
    "                self,\n",
    "                posterior: ConjugatePrecipGP,\n",
    "                train_data: gpx.Dataset,\n",
    "            ) -> ScalarFloat:\n",
    "                #smooth data to get in form for preds\n",
    "                x, y = posterior.smoother.smooth_data(train_data)\n",
    "                \n",
    "                obs_noise = posterior.likelihood.obs_stddev**2\n",
    "                mx = posterior.mean_function(x)\n",
    "                Kxx = posterior.eval_K_xt(x,x, ref=x) # [N, N]\n",
    "                \n",
    "                \n",
    "                # Σ = (Kxx + Io²) = LLᵀ\n",
    "                Kxx += cola.ops.I_like(Kxx) * posterior.jitter\n",
    "                Sigma = Kxx + cola.ops.I_like(Kxx) * obs_noise\n",
    "                Sigma = cola.PSD(Sigma)\n",
    "\n",
    "                # p(y | x, θ), where θ are the model hyperparameters:\n",
    "                mll = GaussianDistribution(jnp.atleast_1d(mx.squeeze()), Sigma)\n",
    "                log_prob =jnp.array(0.0, dtype=jnp.float64)\n",
    "                if log_prior is not None:\n",
    "                    log_prob += log_prior(posterior)\n",
    "                return self.constant * (mll.log_prob(jnp.atleast_1d(y.squeeze())).squeeze() + log_prob.squeeze())\n",
    "            \n",
    "        return Loss(negative=negative)\n",
    "\n",
    "\n",
    "    def get_sobol_indicies(self, train_data: VerticalDataset, component_list: List[List[int]]) -> Num[Array, \"c\"]:\n",
    "        if not isinstance(component_list, List):\n",
    "            raise ValueError(\"Use get_sobol_index if you want to calc for single components (TODO)\")\n",
    "        x,y = self.smoother.smooth_data(train_data)\n",
    "        m_x = self.mean_function(x)\n",
    "        x_all = jnp.vstack([x,x]) # waste of memory here\n",
    "        Kxx_indiv = jnp.stack([k.cross_covariance(x_all,x_all) for k in self.base_kernels], axis=0) # [d, 2N, 2N]\n",
    "        Kxx_indiv =  self._orthogonalise(Kxx_indiv, num_ref = jnp.shape(x)[0]) # [d, N, N]\n",
    "        Kxx_components = [self.interaction_variances[len(c)]*jnp.prod(Kxx_indiv[c, :, :], axis=0) for c in component_list] \n",
    "        Kxx_components = jnp.stack(Kxx_components, axis=0) # [c, N, N]\n",
    "        \n",
    "        assert Kxx_components.shape[0] == len(component_list)\n",
    "\n",
    "        Kxx = self.eval_K_xt(x,x, ref=x)\n",
    "        Sigma = cola.PSD(Kxx + cola.ops.I_like(Kxx) * (self.likelihood.obs_stddev**2+self.jitter))\n",
    "\n",
    "        def get_mean_from_covar(K): # [N,N] -> [N, 1]\n",
    "            Sigma_inv_Kxx = cola.solve(Sigma, K)\n",
    "            return m_x + jnp.matmul(Sigma_inv_Kxx.T, y - m_x) # [N, 1] \n",
    "\n",
    "        mean_overall =  get_mean_from_covar(Kxx) # [N, 1]\n",
    "        mean_components = vmap(get_mean_from_covar)(Kxx_components) # [c, N, 1]\n",
    "\n",
    "        sobols = jnp.var(mean_components[:,:,0], axis=-1) / jnp.var(mean_overall) # [c]\n",
    "        return sobols\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# @dataclass\n",
    "# class NonConjugatePrecipGP(ConjugatePrecipGP):\n",
    "#     inducing_inputs: Float[Array, \"N D\"]\n",
    "#     variational_mean: Union[Float[Array, \"N 1\"], None] = param_field(None)\n",
    "#     variational_root_covariance: Float[Array, \"N N\"] = param_field(\n",
    "#         None, bijector=tfb.FillTriangular()\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_log_prior(tau=None) -> callable:\n",
    "\n",
    "    def log_prior(model):\n",
    "        log_prob = 0.0\n",
    "\n",
    "        # if hasattr(model, \"smoother\"):\n",
    "        #     smoother_input_scale_prior = tfd.LogNormal(0.0,1.0)\n",
    "        #     smoother_mean_prior = tfd.Uniform(jnp.min(model.smoother.Z_levels),jnp.max(model.smoother.Z_levels))\n",
    "        #     log_prob += jnp.sum(smoother_mean_prior.log_prob(model.smoother.smoother_mean))\n",
    "        #     log_prob += jnp.sum(smoother_input_scale_prior.log_prob(model.smoother.smoother_input_scale))\n",
    "\n",
    "        lengthscales = jnp.vstack([k.lengthscale for k in model.base_kernels])\n",
    "        variances = model.interaction_variances\n",
    "       \n",
    "        # variance_prior = tfd.Gamma(1.0,0.2)\n",
    "        # log_prob += jnp.sum(variance_prior.log_prob(variances))\n",
    "\n",
    "        # d = lengthscales.size\n",
    "        # #l_prior = tfd.LogNormal(jnp.sqrt(2.0) + jnp.log(d)/2.0,jnp.sqrt(1.0))\n",
    "        # #l_prior = tfd.Gamma(3.0*d,6.0)\n",
    "        #l_prior = tfd.Gamma(3.0,6.0)\n",
    "        #log_prob += jnp.sum(l_prior.log_prob(lengthscales))\n",
    "        # l_prior = tfd.HalfCauchy(0.0,tau)\n",
    "        # log_prob += jnp.sum(l_prior.log_prob((1.0 / lengthscales**2)))\n",
    "            \n",
    "        # noise_prior = tfd.LogNormal(0.0,10)\n",
    "        # log_prob += noise_prior.log_prob(model.likelihood.obs_stddev)\n",
    "\n",
    "        return log_prob\n",
    "    \n",
    "    return log_prior\n",
    "\n",
    "\n",
    "\n",
    "def init_smoother():\n",
    "    smoother_input_scale_bijector = tfb.Softplus(low=jnp.array(1e-1, dtype=jnp.float64))\n",
    "    smoother_mean_bijector =  tfb.SoftClip(low=jnp.min(pressure_levels+1e-3), high=jnp.max(pressure_levels-1e-3))\n",
    "    smoother = VerticalSmoother(\n",
    "        jnp.array([[0.0]*num_3d_variables]), \n",
    "        jnp.array([[1.0]*num_3d_variables]), \n",
    "        Z_levels=pressure_levels\n",
    "        ).replace_bijector(smoother_input_scale=smoother_input_scale_bijector,smoother_mean=smoother_mean_bijector)\n",
    "    return smoother\n",
    "\n",
    "\n",
    "def init_kernels(data):\n",
    "    lengthscale_bij = tfb.SoftClip(low=jnp.array(1e-3, dtype=jnp.float64), high=jnp.array(1e3, dtype=jnp.float64))\n",
    "    return  [gpx.kernels.RBF(lengthscale=jnp.array([1.0]), active_dims=[i]).replace_trainable(variance=False).replace_bijector(lengthscale = lengthscale_bij) for i in range(data.dim)]\n",
    "\n",
    "def init_likelihood(data, obs_stddev = jnp.array(1.0, dtype=jnp.float64) ):\n",
    "    obs_bij=tfb.Softplus(low=jnp.array(1e-3, dtype=jnp.float64))\n",
    "    return gpx.likelihoods.Gaussian(num_datapoints=data.n, obs_stddev=obs_stddev).replace_bijector(obs_stddev=obs_bij)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep fancy kernel for model\n",
    "# first fit with small data to get init for SVGP\n",
    "num_data_for_init=100\n",
    "D_small = VerticalDataset(\n",
    "    X2d_raw = X2d[:num_data_for_init,:],\n",
    "    X3d_raw = X3d[:num_data_for_init,:,:],\n",
    "    Xstatic_raw = Xstatic[:num_data_for_init,:],\n",
    "    y_raw=Y[:num_data_for_init,:],\n",
    ")\n",
    "\n",
    "\n",
    "plot_data(D_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with small data\n",
    "posterior = ConjugatePrecipGP(\n",
    "    base_kernels=init_kernels(D_small), \n",
    "    likelihood=init_likelihood(D_small, obs_stddev = jnp.array(0.1, dtype=jnp.float64) ),\n",
    "    smoother=init_smoother(),\n",
    "    zeroth_order=True,\n",
    "    max_interaction_depth=2,\n",
    "    interaction_variances=jnp.array([1.0, 1.0,1.0], dtype=jnp.float64),\n",
    "    jitter=jnp.array(1e-5, dtype=jnp.float64)\n",
    "    )\n",
    "plt.figure()\n",
    "# opt_posterior, history = gpx.fit_scipy(\n",
    "#         model=posterior,\n",
    "#         objective=jax.jit(posterior.loss_fn(negative=True, log_prior=None)),\n",
    "#         train_data=D_small,\n",
    "#         safe=False,\n",
    "#     )\n",
    "opt_posterior, history = gpx.fit(\n",
    "        model=posterior,\n",
    "        objective=jax.jit(posterior.loss_fn(negative=True, log_prior=None)),\n",
    "        train_data=D_small,\n",
    "        optim=ox.adam(1e-1),\n",
    "        num_iters=500,\n",
    "        safe=False,\n",
    "        key=key,\n",
    "    )\n",
    "\n",
    "\n",
    "plt.plot(history)\n",
    "plot_params(opt_posterior,D_small, title=\"initial fit with small data\")\n",
    "print(f\"noise is {opt_posterior.likelihood.obs_stddev}\")\n",
    "print(f\"interaction vars {opt_posterior.interaction_variances}\")\n",
    "plot_interactions(opt_posterior, D_small, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
