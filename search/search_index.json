{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GPJax!","text":"<p>GPJax is a didactic Gaussian process library that supports GPU acceleration and just-in-time compilation. We seek to provide a flexible API as close as possible to how the underlying mathematics is written on paper to enable researchers to rapidly prototype and develop new ideas.</p> <p></p> <p>You can view the source code for GPJax here on Github.</p>"},{"location":"#hello-world-example","title":"<code>Hello World</code> example","text":"<p>Defining a Gaussian process posterior is as simple as typing the maths we would write on paper. To see this, consider the following example.</p> PythonMath <pre><code>import gpjax as gpx\nmeanf = gpx.Zero()\nkernel = gpx.kernels.RBF()\nprior = gpx.gps.Prior(mean_function = meanf, kernel = kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints = 123)\nposterior = prior * likelihood\n</code></pre> \\[ \\begin{align} k(\\cdot, \\cdot') &amp; = \\sigma^2\\exp\\left(-\\frac{\\lVert \\cdot- \\cdot'\\rVert_2^2}{2\\ell^2}\\right)\\\\ p(f(\\cdot)) &amp; = \\mathcal{GP}(\\mathbf{0}, k(\\cdot, \\cdot')) \\\\ p(y\\,|\\, f(\\cdot)) &amp; = \\mathcal{N}(y\\,|\\, f(\\cdot), \\sigma_n^2) \\\\ p(f(\\cdot) \\,|\\, y) &amp; \\propto p(f(\\cdot))p(y\\,|\\, f(\\cdot))\\,. \\end{align} \\] <p>Note</p> <p>If you're new to Gaussian processes and want a gentle introduction, we have put together an introduction to GPs notebook that starts from Bayes' theorem and univariate Gaussian random variables. The notebook is linked here.</p> <p>Seealso</p> <p>To learn more, checkout the regression notebook.</p>"},{"location":"#citing-gpjax","title":"Citing GPJax","text":"<p>If you use GPJax in your research, please cite our JOSS paper.</p> <pre><code>@article{Pinder2022,\n  doi = {10.21105/joss.04455},\n  url = {https://doi.org/10.21105/joss.04455},\n  year = {2022},\n  publisher = {The Open Journal},\n  volume = {7},\n  number = {75},\n  pages = {4455},\n  author = {Thomas Pinder and Daniel Dodd},\n  title = {GPJax: A Gaussian Process Framework in JAX},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"CODE OF CONDUCT","text":"<p>Like the technical community as a whole, the GPJax team and community is made up of a mixture of professionals and volunteers from all over the world, working on every aspect of the mission - including mentorship, teaching and connecting people.</p> <p>Diversity is one of our huge strengths, but it can also lead to communication issues and unhappiness. To that end, we have a few ground rules that we ask people to adhere to when they're participating within this community and project. These rules apply equally to founders, mentors and those seeking help and guidance.</p> <p>This isn't an exhaustive list of things that you can't do. Rather, take it in the spirit in which it's intended - a guide to make it easier to enrich all of us, the technical community and the conferences and usergroups we hope to guide new speakers to.</p> <p>This code of conduct applies to all communication: this includes IRC, the mailing list, and other forums such as Skype, Google+ Hangouts, etc.</p> <ul> <li>Be welcoming, friendly, and patient.</li> <li>Be considerate. Your work will be used by other people, and you in turn will depend on the work of others. Any decision you make will affect users and colleagues, and you should take those consequences into account when making decisions.</li> <li>Be respectful. Not all of us will agree all the time, but disagreement is no excuse for poor behaviour and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It's important to remember that a community where people feel uncomfortable or threatened is not a productive one. Members of the GPJax community should be respectful when dealing with other members as well as with people outside the GPJax community and with user groups/conferences, usergroup/conference organizers.</li> <li>Be careful in the words that you choose. Remember that sexist, racist, and other exclusionary jokes can be offensive to those around you. Be kind and welcoming to others. Do not insult or put down other participants. Behave professionally. The harassment and exclusionary behaviour towards others is unacceptable. This includes, but is not limited to:</li> <li>Violent threats or language directed against another person.</li> <li>Discriminatory jokes and language.</li> <li>Posting sexually explicit or violent material.</li> <li>Posting (or threatening to post) other people's personally identifying information (\"doxing\").</li> <li>Personal insults, especially those using racist or sexist terms.</li> <li>Unwelcome sexual attention.</li> <li>Advocating for, or encouraging, any of the above behaviour.</li> <li>Repeated harassment of others. In general, if someone asks you to stop, then stop.</li> <li>When we disagree, we try to understand why. Disagreements, both social and technical, happen all the time and GPJax is no exception. It is important that we resolve disagreements and differing views constructively. Remember that we're different. The strength of GPJax comes from its varied community, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn't mean that they're wrong. Don't forget that it is human to err and blaming each other doesn't get us anywhere, rather offer to help resolving issues and to help learn from mistakes.</li> </ul> <p>Text adapted from the Speak Up! project and Django</p>"},{"location":"contributing/","title":"How can I contribute?","text":"<p>GPJax welcomes contributions from interested individuals or groups. There are many ways to contribute, including:</p> <ul> <li>Answering questions on our discussions   page.</li> <li>Raising issues related to bugs   or desired enhancements.</li> <li>Contributing or improving the   docs or   examples.</li> <li>Fixing outstanding issues   (bugs).</li> <li>Extending or improving our codebase.</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of conduct","text":"<p>As a contributor to GPJax, you can help us keep the community open and inclusive. Please read and follow our Code of Conduct.</p>"},{"location":"contributing/#opening-issues-and-getting-support","title":"Opening issues and getting support","text":"<p>Please open issues on Github Issue Tracker. Here you can mention</p> <p>You can ask a question or start a discussion in the Discussion section on Github.</p>"},{"location":"contributing/#contributing-to-the-source-code","title":"Contributing to the source code","text":"<p>Submitting code contributions to GPJax is done via a GitHub pull request. Our preferred workflow is to first fork the GitHub repository, clone it to your local machine, and develop on a feature branch. Once you're happy with your changes, install our <code>pre-commit hooks</code>, <code>commit</code> and <code>push</code> your code.</p> <p>New to this? Don't panic, our guide below will walk you through every detail!</p> <p>Note</p> <p>Before opening a pull request we recommend you check our pull request checklist.</p>"},{"location":"contributing/#step-by-step-guide","title":"Step-by-step guide:","text":"<ol> <li> <p>Click here to Fork GPJax's   codebase (alternatively, click the 'Fork' button towards the top right of   the main repository page). This   adds a copy of the codebase to your GitHub user account.</p> </li> <li> <p>Clone your GPJax fork from your GitHub account to your local disk, and add   the base repository as a remote:   <pre><code>$ git clone git@github.com:&lt;your GitHub handle&gt;/GPJax.git\n$ cd GPJax\n$ git remote add upstream git@github.com:GPJax.git\n</code></pre></p> </li> <li> <p>Create a <code>feature</code> branch to hold your development changes:</p> </li> </ol> <p><pre><code>$ git checkout -b my-feature\n</code></pre>   Always use a <code>feature</code> branch. It's good practice to avoid   work on the <code>main</code> branch of any repository.</p> <ol> <li>Project requirements are in <code>requirements.txt</code>. We suggest using a   virtual environment for   development. Once the virtual environment is activated, run:</li> </ol> <pre><code>$ pip install -e .\n$ pip install -r requirements-dev.txt\n</code></pre> <ol> <li>Install the pre-commit hooks.</li> </ol> <pre><code>$ pre-commit install\n</code></pre> <p>Please ensure you have done this before committing any files. If   successful, this will print the following output <code>pre-commit installed at   .git/hooks/pre-commit</code>.</p> <ol> <li>Add changed files using <code>git add</code> and then <code>git commit</code> files to record your   changes locally:</li> </ol> <p><pre><code>$ git add modified_files\n$ git commit\n</code></pre>   After committing, it is a good idea to sync with the base repository in case   there have been any changes:</p> <pre><code>$ git fetch upstream\n$ git rebase upstream/main\n</code></pre> <p>Then push the changes to your GitHub account with:</p> <pre><code>$ git push -u origin my-feature\n</code></pre> <ol> <li>Go to the GitHub web page of your fork of the GPJax repo. Click the 'Pull   request' button to send your changes to the project's maintainers for   review.</li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<p>We welcome both complete or \"work in progress\" pull requests. Before opening one, we recommended you check the following guidelines to ensure a smooth review process.</p> <p>My contribution is a \"work in progress\":</p> <p>Please prefix the title of incomplete contributions with <code>[WIP]</code> (to indicate a work in progress). WIPs are useful to:</p> <ol> <li>Indicate you are working on something to avoid duplicated work.</li> <li>Request broad review of functionality or API.</li> <li>Seek collaborators.</li> </ol> <p>In the description of the pull request, we recommend you outline where work needs doing. For example, do some tests need writing?</p> <p>My contribution is complete:</p> <p>If addressing an issue, please use the pull request title to describe the issue and mention the issue number in the pull request description. This will make sure a link back to the original issue is created. Then before making your pull request, we recommend you check the following:</p> <ul> <li>Do all public methods have informative docstrings that describe their   function, input(s) and output(s)?</li> <li>Do the tests pass when everything is rebuilt from scratch?</li> <li> <p>Documentation and high-coverage tests are necessary for enhancements to be   accepted. Test coverage can be checked with:</p> <pre><code>$ pip install -r requirements-dev.txt\n$ pytest tests --cov=./ --cov-report=html\n</code></pre> </li> </ul> <p>Navigate to the newly created folder <code>htmlcov</code> and open <code>index.html</code> to view   the coverage report.</p> <p>This guide was derived from PyMC's guide to contributing.</p>"},{"location":"design/","title":"Design Principles","text":"<p>\\(\\require{bm}\\)</p> <p><code>GPJax</code> is designed to be a Gaussian process package that provides an accurate representation of the underlying maths. Variable names are chosen to closely match the notation in (Rasmussen and Williams, 2006)1. We here list the notation used in <code>GPJax</code> with its corresponding mathematical quantity.</p>"},{"location":"design/#gaussian-process-notation","title":"Gaussian process notation","text":"On paper GPJax code Description \\(n\\) n Number of train inputs \\(\\boldsymbol{x} = (x_1,\\dotsc,x_{n})\\) x Train inputs \\(\\boldsymbol{y} = (y_1,\\dotsc,y_{n})\\) y Train labels \\(\\boldsymbol{t}\\) t Test inputs \\(f(\\cdot)\\) f Latent function modelled as a GP \\(f({\\boldsymbol{x}})\\) fx Latent function at inputs \\(\\boldsymbol{x}\\) \\(\\boldsymbol{\\mu}_{\\boldsymbol{x}}\\) \u03bcx Prior mean at inputs \\(\\boldsymbol{x}\\) \\(\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) Kxx Kernel Gram matrix at inputs \\(\\boldsymbol{x}\\) \\(\\mathbf{L}_{\\boldsymbol{x}}\\) Lx Lower Cholesky decomposition of \\(\\boldsymbol{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) \\(\\mathbf{K}_{\\boldsymbol{t}\\boldsymbol{x}}\\) Ktx Cross-covariance between inputs \\(\\boldsymbol{t}\\) and \\(\\boldsymbol{x}\\)"},{"location":"design/#sparse-gaussian-process-notation","title":"Sparse Gaussian process notation","text":"On paper GPJax code Description \\(m\\) m Number of inducing inputs \\(\\boldsymbol{z} = (z_1,\\dotsc,z_{m})\\) z Inducing inputs \\(\\boldsymbol{u} = (u_1,\\dotsc,u_{m})\\) u Inducing outputs"},{"location":"design/#package-style","title":"Package style","text":"<p>Prior to building GPJax, the developers of GPJax have benefited greatly from the GPFlow and GPyTorch packages. As such, many of the design principles in GPJax are inspired by the excellent precursory packages. Documentation designs have been greatly inspired by the exceptional Flax docs.</p> <ol> <li> <p>Rasmussen, C. E. and Williams, C. K. (2006) Gaussian Processes for Machine Learning. 3. MIT press Cambridge, MA.\u00a0\u21a9</p> </li> </ol>"},{"location":"index%20copy/","title":"Welcome to GPJax!","text":"<p>GPJax is a didactic Gaussian process library that supports GPU acceleration and just-in-time compilation. We seek to provide a flexible API as close as possible to how the underlying mathematics is written on paper to enable researchers to rapidly prototype and develop new ideas.</p> <p></p> <p>You can view the source code for GPJax here on Github.</p>"},{"location":"index%20copy/#hello-world-example","title":"<code>Hello World</code> example","text":"<p>Defining a Gaussian process posterior is as simple as typing the maths we would write on paper. To see this, consider the following example.</p> PythonMaths <pre><code>import gpjax as gpx\nmeanf = gpx.Zero()\nkernel = gpx.kernels.RBF()\nprior = gpx.gps.Prior(mean_function = meanf, kernel = kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints = 123)\nposterior = prior * likelihood\n</code></pre> \\[ \\begin{align} k(\\cdot, \\cdot') &amp; = \\sigma^2\\exp\\left(-\\frac{\\lVert \\cdot- \\cdot'\\rVert_2^2}{2\\ell^2}\\right)\\\\ p(f(\\cdot)) &amp; = \\mathcal{GP}(\\mathbf{0}, k(\\cdot, \\cdot')) \\\\ p(y\\,|\\, f(\\cdot)) &amp; = \\mathcal{N}(y\\,|\\, f(\\cdot), \\sigma_n^2) \\\\ p(f(\\cdot) \\,|\\, y) &amp; \\propto p(f(\\cdot))p(y\\,|\\, f(\\cdot))\\,. \\end{align} \\] <p>Note</p> <p>If you're new to Gaussian processes and want a gentle introduction, we have put together an introduction to GPs notebook that starts from Bayes' theorem and univariate Gaussian random variables. The notebook is linked here.</p> <p>Seealso</p> <p>To learn more, checkout the regression notebook.</p>"},{"location":"index%20copy/#citing-gpjax","title":"Citing GPJax","text":"<p>If you use GPJax in your research, please cite our JOSS paper.</p> <pre><code>@article{Pinder2022,\n  doi = {10.21105/joss.04455},\n  url = {https://doi.org/10.21105/joss.04455},\n  year = {2022},\n  publisher = {The Open Journal},\n  volume = {7},\n  number = {75},\n  pages = {4455},\n  author = {Thomas Pinder and Daniel Dodd},\n  title = {GPJax: A Gaussian Process Framework in JAX},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-version","title":"Stable version","text":"<p>The latest stable release of <code>GPJax</code> can be installed via <code>pip</code>:</p> <pre><code>pip install gpjax\n</code></pre> <p>Check your installation</p> <p>We recommend you check your installation version: <pre><code>python -c 'import gpjax; print(gpjax.__version__)'\n</code></pre></p>"},{"location":"installation/#gpu-support","title":"GPU support","text":"<p>GPU support is enabled through proper configuration of the underlying Jax installation. CPU enabled forms of both packages are installed as part of the <code>GPJax</code> installation. For GPU Jax support, the following commands should be run:</p> <pre><code># Specify your installed CUDA version.\nCUDA_VERSION=11.0\npip install jaxlib\n</code></pre> <p>Then, within a Python shell run</p> <pre><code>import jaxlib\nprint(jaxlib.__version__)\n</code></pre>"},{"location":"installation/#development-version","title":"Development version","text":"<p>Warning</p> <p>This version is possibly unstable and may contain bugs.</p> <p>The latest development version of <code>GPJax</code> can be installed via running following:</p> <pre><code>git clone https://github.com/thomaspinder/GPJax.git\ncd GPJax\npoetry install\n</code></pre> <p>Tip</p> <p>We advise you create virtual environment before installing:</p> <pre><code>conda create -n gpjax_experimental python=3.10.0\nconda activate gpjax_experimental\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>poetry run pytest tests/\n</code></pre>"},{"location":"sharp_bits/","title":"\ud83d\udd2a The sharp bits","text":""},{"location":"sharp_bits/#pseudo-randomness","title":"Pseudo-randomness","text":"<p>Can briefly acknowledge and then point to the Jax docs for more information.</p>"},{"location":"sharp_bits/#float64","title":"Float64","text":"<p>The need for Float64 when inverting the Gram matrix</p>"},{"location":"sharp_bits/#positive-definiteness","title":"Positive-definiteness","text":"<p>The need for jitter in the kernel Gram matrix</p>"},{"location":"sharp_bits/#slow-to-evaluate","title":"Slow-to-evaluate","text":"<p>More than several thousand data points will require the use of inducing points - don't try and use the ConjugateMLL objective on a million data points.</p>"},{"location":"_static/jaxkern/main/","title":"Main","text":"In\u00a0[\u00a0]: Copied! <pre>import jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.patheffects as path_effects\nimport matplotlib.pyplot as plt\n</pre> import jax.numpy as jnp import jax.random as jr import matplotlib as mpl import matplotlib.patheffects as path_effects import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>import gpjax.kernels as jk\n</pre> import gpjax.kernels as jk In\u00a0[\u00a0]: Copied! <pre>key = jr.PRNGKey(123)\n</pre> key = jr.PRNGKey(123) In\u00a0[\u00a0]: Copied! <pre>def set_font(font_path):\n    font = mpl.font_manager.FontEntry(fname=font_path, name=\"my_font\")\n    mpl.font_manager.fontManager.ttflist.append(font)\n\n    mpl.rcParams.update(\n        {\n            \"font.family\": font.name,\n        }\n    )\n</pre> def set_font(font_path):     font = mpl.font_manager.FontEntry(fname=font_path, name=\"my_font\")     mpl.font_manager.fontManager.ttflist.append(font)      mpl.rcParams.update(         {             \"font.family\": font.name,         }     ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    set_font(\"lato.ttf\")\n    x1 = jnp.linspace(-3.0, 3.0, 500).reshape(-1, 1)\n\n    kern = jk.Matern52()\n    focal_points = [-2.5, 0.0, 2.5]\n    cols = [\"#5E97F6\", \"#30A89C\", \"#9C26B0\"]\n\n    fig, ax = plt.subplots(figsize=(6, 2.5), tight_layout=True)\n    for c, f in zip(cols, focal_points):\n        x2 = jnp.array([[0]])\n        params = kern.init_params(key)\n        Kxx = kern.cross_covariance(params, x1, x2)\n        ax.plot(x1 + f, Kxx, color=c)\n        ax.fill_between((x1 + f).squeeze(), Kxx.squeeze(), color=c, alpha=0.2)\n    ax.axis(\"off\")\n    text = ax.text(\n        x=0.0,\n        y=0.25,\n        s=\"JaxKern\",\n        fontsize=42,\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n    )\n    text.set_path_effects(\n        [\n            path_effects.Stroke(linewidth=3, foreground=\"white\"),\n            path_effects.Normal(),\n        ]\n    )\n    plt.savefig(\n        \"logo.png\",\n        dpi=450,\n        transparent=True,\n        bbox_inches=\"tight\",\n        pad_inches=0.0,\n    )\n</pre> if __name__ == \"__main__\":     set_font(\"lato.ttf\")     x1 = jnp.linspace(-3.0, 3.0, 500).reshape(-1, 1)      kern = jk.Matern52()     focal_points = [-2.5, 0.0, 2.5]     cols = [\"#5E97F6\", \"#30A89C\", \"#9C26B0\"]      fig, ax = plt.subplots(figsize=(6, 2.5), tight_layout=True)     for c, f in zip(cols, focal_points):         x2 = jnp.array([[0]])         params = kern.init_params(key)         Kxx = kern.cross_covariance(params, x1, x2)         ax.plot(x1 + f, Kxx, color=c)         ax.fill_between((x1 + f).squeeze(), Kxx.squeeze(), color=c, alpha=0.2)     ax.axis(\"off\")     text = ax.text(         x=0.0,         y=0.25,         s=\"JaxKern\",         fontsize=42,         horizontalalignment=\"center\",         verticalalignment=\"center\",     )     text.set_path_effects(         [             path_effects.Stroke(linewidth=3, foreground=\"white\"),             path_effects.Normal(),         ]     )     plt.savefig(         \"logo.png\",         dpi=450,         transparent=True,         bbox_inches=\"tight\",         pad_inches=0.0,     )"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gpjax<ul> <li>base<ul> <li>module</li> <li>param</li> </ul> </li> <li>dataset</li> <li>fit</li> <li>gaussian_distribution</li> <li>gps</li> <li>kernels<ul> <li>approximations<ul> <li>rff</li> </ul> </li> <li>base</li> <li>computations<ul> <li>base</li> <li>basis_functions</li> <li>constant_diagonal</li> <li>dense</li> <li>diagonal</li> <li>eigen</li> </ul> </li> <li>non_euclidean<ul> <li>graph</li> <li>utils</li> </ul> </li> <li>nonstationary<ul> <li>arccosine</li> <li>linear</li> <li>polynomial</li> </ul> </li> <li>stationary<ul> <li>matern12</li> <li>matern32</li> <li>matern52</li> <li>periodic</li> <li>powered_exponential</li> <li>rational_quadratic</li> <li>rbf</li> <li>utils</li> <li>white</li> </ul> </li> </ul> </li> <li>likelihoods</li> <li>linops<ul> <li>constant_diagonal_linear_operator</li> <li>dense_linear_operator</li> <li>diagonal_linear_operator</li> <li>identity_linear_operator</li> <li>linear_operator</li> <li>triangular_linear_operator</li> <li>utils</li> <li>zero_linear_operator</li> </ul> </li> <li>mean_functions</li> <li>objectives</li> <li>progress_bar</li> <li>quadrature</li> <li>scan</li> <li>typing</li> <li>variational_families</li> </ul> </li> </ul>"},{"location":"api/dataset/","title":"dataset","text":""},{"location":"api/dataset/#gpjax.dataset","title":"<code>gpjax.dataset</code>","text":""},{"location":"api/dataset/#gpjax.dataset.__all__","title":"<code>__all__ = ['Dataset']</code>  <code>module-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset","title":"<code>Dataset</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Base class for datasets.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset--attributes","title":"Attributes","text":"<pre><code>X (Optional[Num[Array, \"N D\"]]): Input data.\ny (Optional[Num[Array, \"N Q\"]]): Output data.\n</code></pre>"},{"location":"api/dataset/#gpjax.dataset.Dataset.X","title":"<code>X: Optional[Num[Array, N D]] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.y","title":"<code>y: Optional[Num[Array, N Q]] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>Number of observations.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.in_dim","title":"<code>in_dim: int</code>  <code>property</code>","text":"<p>Dimension of the inputs, X.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Dimension of the outputs, y.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Checks that the shapes of X and y are compatible.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Returns a string representation of the dataset.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_supervised","title":"<code>is_supervised() -&gt; bool</code>","text":"<p>Returns <code>True</code> if the dataset is supervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_unsupervised","title":"<code>is_unsupervised() -&gt; bool</code>","text":"<p>Returns <code>True</code> if the dataset is unsupervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__add__","title":"<code>__add__(other: Dataset) -&gt; Dataset</code>","text":"<p>Combine two datasets. Right hand dataset is stacked beneath the left.</p>"},{"location":"api/fit/","title":"fit","text":""},{"location":"api/fit/#gpjax.fit","title":"<code>gpjax.fit</code>","text":""},{"location":"api/fit/#gpjax.fit.__all__","title":"<code>__all__ = ['fit', 'get_batch']</code>  <code>module-attribute</code>","text":""},{"location":"api/fit/#gpjax.fit.fit","title":"<code>fit(*, model: Module, objective: Union[AbstractObjective, Callable[[Module, Dataset], ScalarFloat]], train_data: Dataset, optim: ox.GradientTransformation, key: KeyArray, num_iters: Optional[int] = 100, batch_size: Optional[int] = -1, log_rate: Optional[int] = 10, verbose: Optional[bool] = True, unroll: Optional[int] = 1, safe: Optional[bool] = True) -&gt; Tuple[Module, Array]</code>","text":"<p>Train a Module model with respect to a supplied Objective function. Optimisers used here should originate from Optax.</p> Example <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; import optax as ox\n&gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (1) Create a dataset:\n&gt;&gt;&gt; X = jnp.linspace(0.0, 10.0, 100)[:, None]\n&gt;&gt;&gt; y = 2.0 * X + 1.0 + 10 * jr.normal(jr.PRNGKey(0), X.shape)\n&gt;&gt;&gt; D = gpx.Dataset(X, y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (2) Define your model:\n&gt;&gt;&gt; class LinearModel(gpx.Module):\n...     weight: float = gpx.param_field()\n...     bias: float = gpx.param_field()\n...\n...     def __call__(self, x):\n...         return self.weight * x + self.bias\n...\n&gt;&gt;&gt; model = LinearModel(weight=1.0, bias=1.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (3) Define your loss function:\n&gt;&gt;&gt; class MeanSquareError(gpx.AbstractObjective):\n...     def evaluate(self, model: LinearModel, train_data: gpx.Dataset) -&gt; float:\n...         return jnp.mean((train_data.y - model(train_data.X)) ** 2)\n...\n&gt;&gt;&gt; loss = MeanSqaureError()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (4) Train!\n&gt;&gt;&gt; trained_model, history = gpx.fit(\n...     model=model, objective=loss, train_data=D, optim=ox.sgd(0.001), num_iters=1000\n... )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model Module to be optimised.</p> required <code>objective</code> <code>Objective</code> <p>The objective function that we are optimising with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data to be used for the optimisation.</p> required <code>optim</code> <code>GradientTransformation</code> <p>The Optax optimiser that is to be used for learning a parameter set.</p> required <code>num_iters</code> <code>Optional[int]</code> <p>The number of optimisation steps to run. Defaults to 100.</p> <code>100</code> <code>batch_size</code> <code>Optional[int]</code> <p>The size of the mini-batch to use. Defaults to -1 (i.e. full batch).</p> <code>-1</code> <code>key</code> <code>Optional[KeyArray]</code> <p>The random key to use for the optimisation batch selection. Defaults to jr.PRNGKey(42).</p> required <code>log_rate</code> <code>Optional[int]</code> <p>How frequently the objective function's value should be printed. Defaults to 10.</p> <code>10</code> <code>verbose</code> <code>Optional[bool]</code> <p>Whether to print the training loading bar. Defaults to True.</p> <code>True</code> <code>unroll</code> <code>int</code> <p>The number of unrolled steps to use for the optimisation. Defaults to 1.</p> <code>1</code>"},{"location":"api/fit/#gpjax.fit.fit--returns","title":"Returns","text":"<pre><code>Tuple[Module, Array]: A Tuple comprising the optimised model and training\n    history respectively.\n</code></pre>"},{"location":"api/fit/#gpjax.fit.get_batch","title":"<code>get_batch(train_data: Dataset, batch_size: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Batch the data into mini-batches. Sampling is done with replacement.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>The training dataset.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>key</code> <code>KeyArray</code> <p>The random key to use for the batch selection.</p> required"},{"location":"api/fit/#gpjax.fit.get_batch--returns","title":"Returns","text":"<pre><code>Dataset: The batched dataset.\n</code></pre>"},{"location":"api/gaussian_distribution/","title":"gaussian_distribution","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution","title":"<code>gpjax.gaussian_distribution</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.__all__","title":"<code>__all__ = ['GaussianDistribution']</code>  <code>module-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>         Bases: <code>tfd.Distribution</code></p> <p>Multivariate Gaussian distribution with a linear operator scale matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>Optional[Float[Array,  N]]</code> <p>The mean of the distribution. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Optional[LinearOperator]</code> <p>The scale matrix of the distribution. Defaults to None.</p> <code>None</code>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate Gaussian distribution with a linear operator scale matrix.\n</code></pre>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.loc","title":"<code>loc = loc</code>  <code>instance-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.scale","title":"<code>scale = scale</code>  <code>instance-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.event_shape","title":"<code>event_shape: Tuple</code>  <code>property</code>","text":"<p>Returns the event shape.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.__init__","title":"<code>__init__(loc: Optional[Float[Array, N]] = None, scale: Optional[LinearOperator] = None) -&gt; None</code>","text":"<p>Initialises the distribution.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.mean","title":"<code>mean() -&gt; Float[Array, N]</code>","text":"<p>Calculates the mean.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.median","title":"<code>median() -&gt; Float[Array, N]</code>","text":"<p>Calculates the median.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.mode","title":"<code>mode() -&gt; Float[Array, N]</code>","text":"<p>Calculates the mode.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.covariance","title":"<code>covariance() -&gt; Float[Array, N N]</code>","text":"<p>Calculates the covariance matrix.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.variance","title":"<code>variance() -&gt; Float[Array, N]</code>","text":"<p>Calculates the variance.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.stddev","title":"<code>stddev() -&gt; Float[Array, N]</code>","text":"<p>Calculates the standard deviation.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.entropy","title":"<code>entropy() -&gt; ScalarFloat</code>","text":"<p>Calculates the entropy of the distribution.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.log_prob","title":"<code>log_prob(y: Float[Array, N]) -&gt; ScalarFloat</code>","text":"<p>Calculates the log pdf of the multivariate Gaussian.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array,  N]</code> <p>The value to calculate the log probability of.</p> required"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.log_prob--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log probability of the value.\n</code></pre>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.sample","title":"<code>sample(seed: KeyArray, sample_shape: Tuple[int, ...])</code>","text":"<p>See <code>Distribution.sample</code>.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.kl_divergence","title":"<code>kl_divergence(other: GaussianDistribution) -&gt; ScalarFloat</code>","text":""},{"location":"api/gps/","title":"gps","text":""},{"location":"api/gps/#gpjax.gps","title":"<code>gpjax.gps</code>","text":""},{"location":"api/gps/#gpjax.gps.__all__","title":"<code>__all__ = ['AbstractPrior', 'Prior', 'AbstractPosterior', 'ConjugatePosterior', 'NonConjugatePosterior', 'construct_posterior']</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior","title":"<code>AbstractPrior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract Gaussian process prior.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.kernel","title":"<code>kernel: AbstractKernel</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.mean_function","title":"<code>mean_function: AbstractMeanFunction</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process at the given points. The output of this function is a TensorFlow probability distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to the predict method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to the predict method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior","title":"<code>Prior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractPrior</code></p> <p>A Gaussian process prior object. The GP is parameterised by a mean and kernel function.</p> <p>A Gaussian process prior parameterised by a mean function \\(m(\\cdot)\\) and a kernel function \\(k(\\cdot, \\cdot)\\) is given by \\(p(f(\\cdot)) = \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))\\).</p> <p>To invoke a <code>Prior</code> distribution, only a kernel function is required. By default, the mean function will be set to zero. In general, this assumption will be reasonable assuming the data being modelled has been centred.</p> <p>Example: <pre><code>    import gpjax as gpx\nkernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel = kernel)\n</code></pre></p>"},{"location":"api/gps/#gpjax.gps.Prior.__mul__","title":"<code>__mul__(other: AbstractLikelihood)</code>","text":"<p>The product of a prior and likelihood is proportional to the posterior distribution. By computing the product of a GP prior and a likelihood object, a posterior GP object will be returned. Mathematically, this can be described by: \\(p(f(\\cdot) \\mid y) \\propto p(y \\mid f(\\cdot)) p(f(\\cdot))\\), where \\(p(y | f(\\cdot))\\) is the likelihood and \\(p(f(\\cdot))\\) is the prior.</p> <p>Example: <pre><code>    import gpjax as gpx\n\n    meanf = gpx.mean_functions.Zero()\n    kernel = gpx.kernels.RBF()\n    prior = gpx.Prior(mean_function=meanf, kernel = kernel)\n    likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n\n    prior * likelihood\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Likelihood</code> <p>The likelihood distribution of the observed dataset.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.__mul__--returns","title":"Returns","text":"<pre><code>Posterior: The relevant GP posterior for the given prior and\n    likelihood. Special cases are accounted for where the model\n    is conjugate.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.__rmul__","title":"<code>__rmul__(other: AbstractLikelihood)</code>","text":"<p>Reimplement the multiplication operator to allow for order-invariant product of a likelihood and a prior i.e., likelihood * prior.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Likelihood</code> <p>The likelihood distribution of the observed dataset.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.__rmul__--returns","title":"Returns","text":"<pre><code>Posterior: The relevant GP posterior for the given prior and\n    likelihood. Special cases are accounted for where the model\n    is conjugate.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.predict","title":"<code>predict(test_inputs: Num[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive prior distribution for a given set of parameters. The output of this function is a function that computes a TFP distribution for a given set of inputs.</p> <p>In the following example, we compute the predictive prior distribution and then evaluate it on the interval :math:<code>[0, 1]</code>:</p> Example <p>import gpjax as gpx import jax.numpy as jnp</p> <p>kernel = gpx.kernels.RBF() prior = gpx.Prior(kernel = kernel)</p> <p>parameter_state = gpx.initialise(prior) prior_predictive = prior.predict(parameter_state.params) prior_predictive(jnp.linspace(0, 1, 100))</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The inputs at which to evaluate the prior distribution.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A mean\nfunction that accepts an input array for where the mean function\nshould be evaluated at. The mean function's value at these points is\nthen returned.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx","title":"<code>sample_approx(num_samples: int, key: KeyArray, num_features: Optional[int] = 100) -&gt; FunctionalSample</code>","text":"<p>Build an approximate sample from the Gaussian process prior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>In particular, we approximate the Gaussian processes' prior as the finite feature approximation</p> <p>.. math:: \\hat{f}(x) = \\sum_{i=1}^m \\phi_i(x)\\theta_i</p> <p>where :math:<code>\\phi_i</code> are m features sampled from the Fourier feature decomposition of the model's kernel and :math:<code>\\theta_i</code> are samples from a unit Gaussian.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>In the following example, we build 10 such samples and then evaluate them over the interval :math:<code>[0, 1]</code>:</p> Example <p>For a <code>prior</code> distribution, the following code snippet will build and evaluate an approximate sample.</p> <p>import gpjax as gpx import jax.numpy as jnp</p> <p>sample_fn = prior.sample_appox(10, key) sample_fn(jnp.linspace(0, 1, 100))</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The desired number of samples.</p> required <code>key</code> <code>KeyArray</code> <p>The random seed used for the sample(s).</p> required <code>num_features</code> <code>int</code> <p>The number of features used when approximating the kernel.</p> <code>100</code>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx--returns","title":"Returns","text":"<pre><code>FunctionalSample: A function representing an approximate sample from the Gaussian\nprocess prior.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior","title":"<code>AbstractPosterior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>The base GP posterior object conditioned on an observed dataset. All posterior objects should inherit from this class.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.prior","title":"<code>prior: AbstractPrior</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.likelihood","title":"<code>likelihood: AbstractLikelihood</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process at the given points. The output of this function is a <code>TFP distribution &lt;https://www.tensorflow.org/probability/api_docs/python/tfp/substrates/jax/distributions&gt;</code>_ from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to the predict method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to the predict method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior","title":"<code>ConjugatePosterior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractPosterior</code></p> <p>A Gaussian process posterior distribution when the constituent likelihood function is a Gaussian distribution. In such cases, the latent function values :math:<code>f</code> can be analytically integrated out of the posterior distribution. As such, many computational operations can be simplified; something we make use of in this object.</p> <p>For a Gaussian process prior :math:<code>p(\\mathbf{f})</code> and a Gaussian likelihood :math:<code>p(y | \\mathbf{f}) = \\mathcal{N}(y\\mid \\mathbf{f}, \\sigma^2))</code> where :math:<code>\\mathbf{f} = f(\\mathbf{x})</code>, the predictive posterior distribution at a set of inputs :math:<code>\\mathbf{x}</code> is given by</p> <p>.. math::</p> <pre><code>p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star} \\mathbf{f} \\mid \\mathbf{y})\\\\\n&amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}}\n</code></pre> <p>where</p> <p>.. math::</p> <pre><code>\\boldsymbol{\\mu}_{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\\\\n\\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}).\n</code></pre> Example <p>import gpjax as gpx import jax.numpy as jnp</p> <p>prior = gpx.Prior(kernel = gpx.kernels.RBF()) likelihood = gpx.likelihoods.Gaussian()</p> <p>posterior = prior * likelihood</p>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict","title":"<code>predict(test_inputs: Num[Array, N D], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Conditional on a training data set, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density.</p> <p>The predictive distribution of a conjugate GP is given by .. math::</p> <pre><code>p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star} \\mathbf{f} \\mid \\mathbf{y})\\\\\n&amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}}\n</code></pre> <p>where</p> <p>.. math::</p> <pre><code>\\boldsymbol{\\mu}_{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\\\\n\\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}).\n</code></pre> <p>The conditioning set is a GPJax <code>Dataset</code> object, whilst predictions are made on a regular Jax array.</p> Example <p>For a <code>posterior</code> distribution, the following code snippet will evaluate the predictive distribution.</p> <p>import gpjax as gpx</p> <p>xtrain = jnp.linspace(0, 1).reshape(-1, 1) ytrain = jnp.sin(xtrain) xtest = jnp.linspace(0, 1).reshape(-1, 1)</p> <p>params = gpx.initialise(posterior) predictive_dist = posterior.predict(params, gpx.Dataset(X=xtrain, y=ytrain)) predictive_dist(xtest)</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Num[Array, N D]</code> <p>A Jax array of test inputs at which the predictive distribution is evaluated.</p> required <code>train_data</code> <code>Dataset</code> <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> required"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A\n    function that accepts an input array and returns the predictive\n    distribution as a ``GaussianDistribution``.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx","title":"<code>sample_approx(num_samples: int, train_data: Dataset, key: KeyArray, num_features: Optional[int] = 100) -&gt; FunctionalSample</code>","text":"<p>Build an approximate sample from the Gaussian process posterior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>Unlike when building approximate samples from a Gaussian process prior, decompositions based on Fourier features alone rarely give accurate samples. Therefore, we must also include an additional set of features (known as canonical features) to better model the transition from Gaussian process prior to Gaussian process posterior. For more details see https://arxiv.org/pdf/2002.09309.pdf</p> <p>In particular, we approximate the Gaussian processes' posterior as the finite feature approximation</p> <p>.. math:: \\hat{f}(x) = \\sum_{i=1}^m \\phi_i(x)\\theta_i + \\sum{j=1}^N v_jk(.,x_j)</p> <p>where :math:<code>\\phi_i</code> are m features sampled from the Fourier feature decomposition of the model's kernel and :math:<code>k(., x_j)</code> are N canonical features. The Fourier weights :math:<code>\\theta_i</code> are samples from a unit Gaussian. See https://arxiv.org/pdf/2002.09309.pdf for expressions for the canonical weights :math:<code>v_j</code>.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The desired number of samples.</p> required <code>key</code> <code>KeyArray</code> <p>The random seed used for the sample(s).</p> required <code>num_features</code> <code>int</code> <p>The number of features used when approximating the kernel.</p> <code>100</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx--returns","title":"Returns","text":"<pre><code>FunctionalSample: A function representing an approximate sample from the Gaussian\nprocess prior.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior","title":"<code>NonConjugatePosterior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractPosterior</code></p> <p>A Gaussian process posterior object for models where the likelihood is non-Gaussian. Unlike the <code>ConjugatePosterior</code> object, the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.latent","title":"<code>latent: Float[Array, N 1] = param_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.key","title":"<code>key: KeyArray = static_field(PRNGKey(42))</code>  <code>class-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict","title":"<code>predict(test_inputs: Num[Array, N D], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Conditional on a set of training data, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density. Note, to gain predictions on the scale of the original data, the returned distribution will need to be transformed through the likelihood function's inverse link function.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> required"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts an\n    input array and returns the predictive distribution as\n    a ``dx.Distribution``.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.construct_posterior","title":"<code>construct_posterior(prior: Prior, likelihood: AbstractLikelihood) -&gt; AbstractPosterior</code>","text":"<p>Utility function for constructing a posterior object from a prior and likelihood. The function will automatically select the correct posterior object based on the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Prior</code> <p>The Prior distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood that represents our beliefs around the distribution of the data.</p> required"},{"location":"api/gps/#gpjax.gps.construct_posterior--returns","title":"Returns","text":"<pre><code>AbstractPosterior: A posterior distribution. If the likelihood is\n    Gaussian, then a ``ConjugatePosterior`` will be returned. Otherwise,\n    a ``NonConjugatePosterior`` will be returned.\n</code></pre>"},{"location":"api/likelihoods/","title":"likelihoods","text":""},{"location":"api/likelihoods/#gpjax.likelihoods","title":"<code>gpjax.likelihoods</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.tfb","title":"<code>tfb = tfp.bijectors</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.__all__","title":"<code>__all__ = ['AbstractLikelihood', 'Gaussian', 'Bernoulli', 'Poisson', 'inv_probit']</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood","title":"<code>AbstractLikelihood</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract base class for likelihoods.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.num_datapoints","title":"<code>num_datapoints: int = static_field()</code>  <code>class-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>  <code>abstractmethod</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>  <code>abstractmethod</code>","text":"<p>Return the link function of the likelihood function.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The distribution of observations, y, given values of the Gaussian process, f.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian","title":"<code>Gaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractLikelihood</code></p> <p>Gaussian likelihood object.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.obs_noise","title":"<code>obs_noise: Union[ScalarFloat, Float[Array, #N]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Normal</code>","text":"<p>The link function of the Gaussian likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ]</code> <p>Function values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function--returns","title":"Returns","text":"<pre><code>tfd.Normal: The likelihood function.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict","title":"<code>predict(dist: Union[tfd.MultivariateNormalTriL, GaussianDistribution]) -&gt; tfd.MultivariateNormalFullCovariance</code>","text":"<p>Evaluate the Gaussian likelihood function at a given predictive distribution. Computationally, this is equivalent to summing the observation noise term to the diagonal elements of the predictive distribution's covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>tfd.Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli","title":"<code>Bernoulli</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractLikelihood</code></p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>","text":"<p>The probit link function of the Bernoulli likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ]</code> <p>Function values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The likelihood function.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict","title":"<code>predict(dist: tfd.Distribution) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>tfd.Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The pointwise predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson","title":"<code>Poisson</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractLikelihood</code></p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>","text":"<p>The link function of the Poisson likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ]</code> <p>Function values.</p> required <p>Returns:</p> Type Description <code>tfd.Distribution</code> <p>tfd.Distribution: The likelihood function.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.predict","title":"<code>predict(dist: tfd.Distribution) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>tfd.Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required <p>Returns:</p> Type Description <code>tfd.Distribution</code> <p>tfd.Distribution: The pointwise predictive distribution.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit","title":"<code>inv_probit(x: Float[Array, *N]) -&gt; Float[Array, *N]</code>","text":"<p>Compute the inverse probit function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, *N]</code> <p>A vector of values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit--returns","title":"Returns","text":"<pre><code>Float[Array, \"*N\"]: The inverse probit of the input vector.\n</code></pre>"},{"location":"api/mean_functions/","title":"mean_functions","text":""},{"location":"api/mean_functions/#gpjax.mean_functions","title":"<code>gpjax.mean_functions</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.SumMeanFunction","title":"<code>SumMeanFunction = partial(CombinationMeanFunction, operator=partial(jnp.sum, axis=0))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.ProductMeanFunction","title":"<code>ProductMeanFunction = partial(CombinationMeanFunction, operator=partial(jnp.sum, axis=0))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Zero","title":"<code>Zero = partial(Constant, constant=jnp.array([0.0]))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction","title":"<code>AbstractMeanFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Mean function that is used to parameterise the Gaussian process.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__","title":"<code>__call__(x: Num[Array, N D]) -&gt; Float[Array, N 1]</code>  <code>abstractmethod</code>","text":"<p>Evaluate the mean function at the given points. This method is required for all subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__","title":"<code>__add__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__radd__","title":"<code>__radd__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__radd__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__","title":"<code>__mul__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__rmul__","title":"<code>__rmul__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__rmul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant","title":"<code>Constant</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractMeanFunction</code></p> <p>A constant mean function. This function returns a repeated scalar value for all inputs. The scalar value itself can be treated as a model hyperparameter and learned during training but defaults to 1.0.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.constant","title":"<code>constant: Float[Array, 1] = param_field(jnp.array([0.0]))</code>  <code>class-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__","title":"<code>__call__(x: Num[Array, N D]) -&gt; Float[Array, N 1]</code>","text":"<p>Evaluate the mean function at the given points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction","title":"<code>CombinationMeanFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractMeanFunction</code></p> <p>A base class for products or sums of AbstractMeanFunctions.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.means","title":"<code>means = items_list</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.operator","title":"<code>operator = operator</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__init__","title":"<code>__init__(means: List[AbstractMeanFunction], operator: Callable, **kwargs) -&gt; None</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__","title":"<code>__call__(x: Num[Array, N D]) -&gt; Float[Array, N 1]</code>","text":"<p>Evaluate combination kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \" Q\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/objectives/","title":"objectives","text":""},{"location":"api/objectives/#gpjax.objectives","title":"<code>gpjax.objectives</code>","text":""},{"location":"api/objectives/#gpjax.objectives.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.NonConjugateMLL","title":"<code>NonConjugateMLL = LogPosteriorDensity</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective","title":"<code>AbstractObjective</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract base class for objectives.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.constant","title":"<code>constant: float = static_field(init=False, repr=False)</code>  <code>class-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.step","title":"<code>step(*args, **kwargs) -&gt; ScalarFloat</code>  <code>abstractmethod</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL","title":"<code>ConjugateMLL</code>","text":"<p>         Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.step","title":"<code>step(posterior: gpjax.gps.ConjugatePosterior, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the marginal log-likelihood of the Gaussian process.</p> <p>Compute the marginal log-likelihood function of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here enables exact estimation of the Gaussian process' latent function values.</p> <p>For a training dataset \\(\\{x_n, y_n\\}_{n=1}^N\\), set of test inputs \\(\\mathbf{x}^{\\star}\\) the corresponding latent function evaluations are given by \\(\\mathbf{f}=f(\\mathbf{x})\\) and \\(\\mathbf{f}^{\\star}f(\\mathbf{x}^{\\star})\\), the marginal log-likelihood is given by: $$ \\begin{align}     \\log p(\\mathbf{y}) &amp; = \\int p(\\mathbf{y}\\mid\\mathbf{f})p(\\mathbf{f}, \\mathbf{f}^{\\star}\\mathrm{d}\\mathbf{f}^{\\star}\\     &amp;=0.5\\left(-\\mathbf{y}^{\\top}\\left(k(\\mathbf{x}, \\mathbf{x}') +\\sigma^2\\mathbf{I}_N  \\right)^{-1}\\mathbf{y}-\\log\\lvert k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_N\\rvert - n\\log 2\\pi \\right). \\end{align} $$</p> <p>Example:</p> <p>For a given <code>ConjugatePosterior</code> object, the following code snippet shows how the marginal log-likelihood can be evaluated.</p> <pre><code>    import gpjax as gpx\nxtrain = jnp.linspace(0, 1).reshape(-1, 1)\nytrain = jnp.sin(xtrain)\nD = gpx.Dataset(X=xtrain, y=ytrain)\nmll = gpx.ConjugateMLL(negative=True)\nmll(posterior, train_data = D)\n</code></pre> <p>Our goal is to maximise the marginal log-likelihood. Therefore, when optimising the model's parameters with respect to the parameters, we use the negative marginal log-likelihood. This can be realised through</p> <pre><code>    mll = gpx.ConjugateMLL(negative=True)\n</code></pre> <p>For optimal performance, the marginal log-likelihood should be <code>jax.jit</code> compiled. <pre><code>    mll = jit(gpx.ConjugateMLL(negative=True))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> required <code>train_data</code> <code>Dataset</code> <p>The training dataset used to compute the marginal log-likelihood.</p> required"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The marginal log-likelihood of the Gaussian process for the\n    current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity","title":"<code>LogPosteriorDensity</code>","text":"<p>         Bases: <code>AbstractObjective</code></p> <p>The log-posterior density of a non-conjugate Gaussian process. This is sometimes referred to as the marginal log-likelihood.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.step","title":"<code>step(posterior: gpjax.gps.NonConjugatePosterior, data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the log-posterior density of a Gaussian process.</p> <p>Compute the marginal log-likelihood, or log-posterior density of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here is general and will work for any likelihood support by GPJax.</p> <p>Unlike the marginal_log_likelihood function of the <code>ConjugatePosterior</code> object, the marginal_log_likelihood function of the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NonConjugatePosterior</code> <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> required <code>data</code> <code>Dataset</code> <p>The training dataset used to compute the marginal log-likelihood.</p> required"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log-posterior density of the Gaussian process for the\n    current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO","title":"<code>ELBO</code>","text":"<p>         Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.step","title":"<code>step(variational_family: gpjax.variational_families.AbstractVariationalFamily, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Compute the evidence lower bound of a variational approximation.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data for which we should maximise the ELBO with respect to.</p> required"},{"location":"api/objectives/#gpjax.objectives.ELBO.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation for\n    the current model parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO","title":"<code>CollapsedELBO</code>","text":"<p>         Bases: <code>AbstractObjective</code></p> <p>The collapsed evidence lower bound.</p> <p>Collapsed variational inference for a sparse Gaussian process regression model. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.step","title":"<code>step(variational_family: gpjax.variational_families.AbstractVariationalFamily, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Compute a single step of the collapsed evidence lower bound.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data for which we should maximise the ELBO with respect to.</p> required"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation for\n    the current model parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.variational_expectation","title":"<code>variational_expectation(variational_family: gpjax.variational_families.AbstractVariationalFamily, train_data: Dataset) -&gt; Float[Array, N]</code>","text":"<p>Compute the variational expectation.</p> <p>Compute the expectation of our model's log-likelihood under our variational distribution. Batching can be done here to speed up computation.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational family that we are using to approximate the posterior.</p> required <code>train_data</code> <code>Dataset</code> <p>The batch for which the expectation should be computed for.</p> required"},{"location":"api/objectives/#gpjax.objectives.variational_expectation--returns","title":"Returns","text":"<pre><code>Array: The expectation of the model's log-likelihood under our variational\n    distribution.\n</code></pre>"},{"location":"api/progress_bar/","title":"progress_bar","text":""},{"location":"api/progress_bar/#gpjax.progress_bar","title":"<code>gpjax.progress_bar</code>","text":""},{"location":"api/progress_bar/#gpjax.progress_bar.__all__","title":"<code>__all__ = ['progress_bar']</code>  <code>module-attribute</code>","text":""},{"location":"api/progress_bar/#gpjax.progress_bar.progress_bar","title":"<code>progress_bar(num_iters: int, log_rate: int) -&gt; Callable</code>","text":"<p>Progress bar decorator for the body function of a <code>jax.lax.scan</code>.</p> <p>Example</p> <pre><code>carry = jnp.array(0.0)\niteration_numbers = jnp.arange(100)\n@progress_bar(num_iters=x.shape[0], log_rate=10)\ndef body_func(carry, x):\nreturn carry, x\ncarry, _ = jax.lax.scan(body_func, carry, iteration_numbers)\n</code></pre> <p>Adapted from the excellent blog post: https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/.</p> <p>Might be nice in future to directly create a general purpose <code>verbose scan</code> inplace of a for a jax.lax.scan, that takes the same arguments as a jax.lax.scan, but prints a progress bar.</p>"},{"location":"api/quadrature/","title":"quadrature","text":""},{"location":"api/quadrature/#gpjax.quadrature","title":"<code>gpjax.quadrature</code>","text":""},{"location":"api/quadrature/#gpjax.quadrature.DEFAULT_NUM_GAUSS_HERMITE_POINTS","title":"<code>DEFAULT_NUM_GAUSS_HERMITE_POINTS = 20</code>  <code>module-attribute</code>","text":""},{"location":"api/quadrature/#gpjax.quadrature.__all__","title":"<code>__all__ = ['gauss_hermite_quadrature']</code>  <code>module-attribute</code>","text":""},{"location":"api/quadrature/#gpjax.quadrature.gauss_hermite_quadrature","title":"<code>gauss_hermite_quadrature(fun: Callable, mean: Float[Array, N D], sd: Float[Array, N D], deg: Optional[int] = DEFAULT_NUM_GAUSS_HERMITE_POINTS, *args, **kwargs) -&gt; Float[Array, N]</code>","text":"<p>Compute Gaussian-Hermite quadrature for a given function. The quadrature points are adjusted through the supplied mean and variance arrays.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function for which quadrature should be applied to.</p> required <code>mean</code> <code>Float[Array, N D]</code> <p>The mean of the Gaussian distribution that is used to shift quadrature points.</p> required <code>sd</code> <code>Float[Array, N D]</code> <p>The standard deviation of the Gaussian distribution that is used to scale quadrature points.</p> required <code>deg</code> <code>int</code> <p>The number of quadrature points that are to be used. Defaults to 20.</p> <code>DEFAULT_NUM_GAUSS_HERMITE_POINTS</code>"},{"location":"api/quadrature/#gpjax.quadrature.gauss_hermite_quadrature--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: The evaluated integrals value.\n</code></pre>"},{"location":"api/scan/","title":"scan","text":""},{"location":"api/scan/#gpjax.scan","title":"<code>gpjax.scan</code>","text":""},{"location":"api/scan/#gpjax.scan.Carry","title":"<code>Carry = TypeVar('Carry')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.X","title":"<code>X = TypeVar('X')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.Y","title":"<code>Y = TypeVar('Y')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.__all__","title":"<code>__all__ = ['vscan']</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.vscan","title":"<code>vscan(f: Callable[[Carry, X], Tuple[Carry, Y]], init: Carry, xs: X, length: Optional[int] = None, reverse: Optional[bool] = False, unroll: Optional[int] = 1, log_rate: Optional[int] = 10, log_value: Optional[bool] = True) -&gt; Tuple[Carry, Shaped[Array, ...]]</code>","text":"<p>Scan with verbose output.</p> <p>This is based on code from the excellent blog post: https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/.</p> Example <p>def f(carry, x): ...     return carry + x, carry + x init = 0 xs = jnp.arange(10) vscan(f, init, xs) (45, DeviceArray([ 0,  1,  3,  6, 10, 15, 21, 28, 36, 45], dtype=int32))</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Carry, X], Tuple[Carry, Y]]</code> <p>A function that takes in a carry and an input and returns a tuple of a new carry and an output.</p> required <code>init</code> <code>Carry</code> <p>The initial carry.</p> required <code>xs</code> <code>X</code> <p>The inputs.</p> required <code>length</code> <code>Optional[int]</code> <p>The length of the inputs. If None, then the length of the inputs is inferred.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>Whether to scan in reverse.</p> <code>False</code> <code>unroll</code> <code>int</code> <p>The number of iterations to unroll.</p> <code>1</code> <code>log_rate</code> <code>int</code> <p>The rate at which to log the progress bar.</p> <code>10</code> <code>log_value</code> <code>bool</code> <p>Whether to log the value of the objective function.</p> <code>True</code>"},{"location":"api/scan/#gpjax.scan.vscan--returns","title":"Returns","text":"<pre><code>Tuple[Carry, list[Y]]: A tuple of the final carry and the outputs.\n</code></pre>"},{"location":"api/typing/","title":"typing","text":""},{"location":"api/typing/#gpjax.typing","title":"<code>gpjax.typing</code>","text":""},{"location":"api/typing/#gpjax.typing.OldKeyArray","title":"<code>OldKeyArray = UInt32[JAXArray, '2']</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.KeyArray","title":"<code>KeyArray = Union[OldKeyArray, JAXKeyArray]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.Array","title":"<code>Array = Union[JAXArray, NumpyArray]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarBool","title":"<code>ScalarBool = Union[bool, Bool[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarInt","title":"<code>ScalarInt = Union[int, Int[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarFloat","title":"<code>ScalarFloat = Union[float, Float[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.VecNOrMatNM","title":"<code>VecNOrMatNM = Union[Float[Array, ' N'], Float[Array, 'N M']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.FunctionalSample","title":"<code>FunctionalSample = Callable[[Float[Array, 'N D']], Float[Array, 'N B']]</code>  <code>module-attribute</code>","text":"<p>Type alias for functions representing <code>B</code> samples from a model, to be evaluated on any set of <code>N</code> inputs (of dimension <code>D</code>) and returning the evaluations of each (potentially approximate) sample draw across these inputs.</p>"},{"location":"api/typing/#gpjax.typing.__all__","title":"<code>__all__ = ['KeyArray', 'ScalarBool', 'ScalarInt', 'ScalarFloat', 'FunctionalSample']</code>  <code>module-attribute</code>","text":""},{"location":"api/variational_families/","title":"variational_families","text":""},{"location":"api/variational_families/#gpjax.variational_families","title":"<code>gpjax.variational_families</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.__all__","title":"<code>__all__ = ['AbstractVariationalFamily', 'AbstractVariationalGaussian', 'VariationalGaussian', 'WhitenedVariationalGaussian', 'NaturalVariationalGaussian', 'ExpectationVariationalGaussian', 'CollapsedVariationalGaussian']</code>  <code>module-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily","title":"<code>AbstractVariationalFamily</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract base class used to represent families of distributions that can be used within variational inference.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Predict the GP's output given the input.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's ``predict`` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian","title":"<code>AbstractVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalFamily</code></p> <p>The variational Gaussian family of probability distributions.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, N D]</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian","title":"<code>VariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>The variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u00b7)) = \u222b p(f(\u00b7)|u) q(u) du, where \\(u = f(z)\\) are the function values at the inducing inputs z and the distribution over the inducing inputs is \\(q(u) = \\mathcal{N}(\\mu, S)\\).  We parameterise this over \\(\\mu\\) and sqrt with S = sqrt sqrt\u1d40.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.variational_mean","title":"<code>variational_mean: Float[Array, N 1] = param_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.variational_root_covariance","title":"<code>variational_root_covariance: Float[Array, N N] = param_field(None, bijector=tfb.FillTriangular())</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the prior KL divergence.</p> <p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have KL[q(f(\u00b7))||p(\u00b7)] = KL[q(u)||p(u)] = KL[ N(\u03bc, S) || N(\u03bcz, Kzz) ], where u = f(z) and z are the inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code> ScalarFloat: The KL-divergence between our variational\n    approximation and the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t)) = \u222b p(f(t)|u) q(u) du, which can be</p> computed in closed form as <p>N[f(t); \u03bct + Ktz Kzz\u207b\u00b9 (\u03bc - \u03bcz),  Ktt - Ktz Kzz\u207b\u00b9 Kzt + Ktz Kzz\u207b\u00b9 S Kzz\u207b\u00b9 Kzt ].</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The test inputs at which we wish to make a prediction.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the low-rank GP at\n    the test inputs.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian","title":"<code>WhitenedVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>VariationalGaussian</code></p> <p>The whitened variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u00b7)) = \u222b p(f(\u00b7)|u) q(u) du, where u = f(z) are the function values at the inducing inputs z and the distribution over the inducing inputs is q(u) = N(Lz \u03bc + mz, Lz S Lz\u1d40). We parameterise this over \u03bc and sqrt with S = sqrt sqrt\u1d40.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have KL[q(f(\u00b7))||p(\u00b7)] = KL[q(u)||p(u)] = KL[N(\u03bc, S)||N(0, I)].</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational\n    approximation and the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t)) = \u222b p(f(t)|u) q(u) du, which can be computed in closed form as</p> <pre><code>N[f(t); \u03bct  +  Ktz Lz\u207b\u1d40 \u03bc,  Ktt  -  Ktz Kzz\u207b\u00b9 Kzt  +  Ktz Lz\u207b\u1d40 S Lz\u207b\u00b9 Kzt].\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The test inputs at which we wish to make a prediction.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the low-rank GP at\n    the test inputs.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian","title":"<code>NaturalVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u00b7)) = \u222b p(f(\u00b7)|u) q(u) du, where u = f(z) are the function values at the inducing inputs z and the distribution over the inducing inputs is q(u) = N(\u03bc, S). Expressing the variational distribution, in the form of the exponential family, q(u) = exp(\u03b8\u1d40 T(u) - a(\u03b8)), gives rise to the natural parameterisation \u03b8 = (\u03b8\u2081, \u03b8\u2082) = (S\u207b\u00b9\u03bc, -S\u207b\u00b9/2), to perform model inference, where T(u) = [u, uu\u1d40] are the sufficient statistics.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.natural_vector","title":"<code>natural_vector: Float[Array, M 1] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.natural_matrix","title":"<code>natural_matrix: Float[Array, M M] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have KL[q(f(\u00b7))||p(\u00b7)] = KL[q(u)||p(u)] = KL[N(\u03bc, S)||N(mz, Kzz)],</p> <p>with \u03bc and S computed from the natural parameterisation \u03b8 = (S\u207b\u00b9\u03bc, -S\u207b\u00b9/2).</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational approximation and\n    the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs \\(t\\).</p> <p>This is the integral q(f(t)) = \u222b p(f(t)|u) q(u) du, which can be computed in closed form as</p> <pre><code> N[f(t); \u03bct + Ktz Kzz\u207b\u00b9 (\u03bc - \u03bcz),  Ktt - Ktz Kzz\u207b\u00b9 Kzt + Ktz Kzz\u207b\u00b9 S Kzz\u207b\u00b9 Kzt ],\n</code></pre> <p>with \u03bc and S computed from the natural parameterisation \u03b8 = (S\u207b\u00b9\u03bc, -S\u207b\u00b9/2).</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts a set of test points and will return the predictive distribution at those points.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian","title":"<code>ExpectationVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u00b7)) = \u222b p(f(\u00b7)|u) q(u) du, where u = f(z) are the function values at the inducing inputs z and the distribution over the inducing inputs is q(u) = N(\u03bc, S). Expressing the variational distribution, in the form of the exponential family, q(u) = exp(\u03b8\u1d40 T(u) - a(\u03b8)), gives rise to the natural parameterisation \u03b8 = (\u03b8\u2081, \u03b8\u2082) = (S\u207b\u00b9\u03bc, -S\u207b\u00b9/2) and sufficient statistics T(u) = [u, uu\u1d40]. The expectation parameters are given by \u03b7 = \u222b T(u) q(u) du. This gives a parameterisation, \u03b7 = (\u03b7\u2081, \u03b7\u2081) = (\u03bc, S + uu\u1d40) to perform model inference over.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.expectation_vector","title":"<code>expectation_vector: Float[Array, M 1] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.expectation_matrix","title":"<code>expectation_matrix: Float[Array, M M] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Evaluate the prior KL-divergence.</p> <p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have \\(\\operatorname{KL}(q(f(\u00b7))||p(\u00b7)) = \\operatorname{KL}(q(u)||p(u)) =\\operatorname{KL}(\\mathcal{N}(\\mu, S)\\mid\\mid \\mathcal{N}(m_z, K_{zz}))\\), where \\(\\mu\\) and \\(S\\) are the expectation parameters of the variational distribution and \\(m_z\\) and \\(K_{zz}\\) are the mean and covariance of the prior distribution.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational approximation and\n    the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the predictive distribution.</p> <p>Compute the predictive distribution of the GP at the test inputs \\(t\\).</p> <p>This is the integral \\(q(f(t)) = \\int p(f(t)\\mid u)q(u)\\mathrm{d}u\\), which can be computed in closed form as  which can be computed in closed form as \\(\\(\\mathcal{N}(f(t); \\mu_t + K_{tz}K_{zz}^{-1}(\\mu - \\mu_z), K_{tt} - K_{tz}K_{zz}^{-1}K_{zt} + K_{tz}K_{zz}^{-1}S K_{zz}^{-1}K_{zt}),\\)\\)</p> <p>with \\(\\mu\\) and \\(S\\) computed from the expectation parameterisation \\(\\eta = (\\mu, S + uu^\\top)\\).</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the GP at the\n    test inputs $t$.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian","title":"<code>CollapsedVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>Collapsed variational Gaussian.</p> <p>Collapsed variational Gaussian family of probability distributions. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The test inputs \\(t\\) at which to make predictions.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data that was used to fit the GP.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the collapsed\n    variational Gaussian process at the test inputs $t$.\n</code></pre>"},{"location":"api/base/module/","title":"module","text":""},{"location":"api/base/module/#gpjax.base.module","title":"<code>gpjax.base.module</code>","text":""},{"location":"api/base/module/#gpjax.base.module.__all__","title":"<code>__all__ = ['Module', 'meta_leaves', 'meta_flatten', 'meta_map', 'meta']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Self","title":"<code>Self = TypeVar('Self')</code>  <code>module-attribute</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Module","title":"<code>Module</code>","text":"<p>         Bases: <code>Pytree</code></p>"},{"location":"api/base/module/#gpjax.base.module.Module.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Module.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/base/module/#gpjax.base.module.meta_leaves","title":"<code>meta_leaves(pytree: Module, *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; List[Tuple[Optional[Dict[str, Any]], Any]]</code>","text":"<p>Returns the meta of the leaves of the pytree.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>pytree to get the meta of.</p> required <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_leaves--returns","title":"Returns","text":"<pre><code>List[Tuple[Dict[str, Any], Any]]: meta of the leaves of the pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta_flatten","title":"<code>meta_flatten(pytree: Union[Module, Any], *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Union[Module, Any]</code>","text":"<p>Returns the meta of the Module.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>Module to get the meta of.</p> required <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_flatten--returns","title":"Returns","text":"<pre><code>Module: meta of the Module.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta_map","title":"<code>meta_map(f: Callable[[Any, Dict[str, Any]], Any], pytree: Union[Module, Any], *rest: Any, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Union[Module, Any]</code>","text":"<p>Apply a function to a Module where the first argument are the pytree leaves, and the second argument are the Module metadata leaves.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Any, Dict[str, Any]], Any]</code> <p>The function to apply to the pytree.</p> required <code>pytree</code> <code>Module</code> <p>The pytree to apply the function to.</p> required <code>rest</code> <code>Any</code> <p>Additional pytrees to apply the function to. Defaults to None.</p> <code>()</code> <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_map--returns","title":"Returns","text":"<pre><code>Module: The transformed pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta","title":"<code>meta(pytree: Module, *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Module</code>","text":"<p>Returns the metadata of the Module as a pytree.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>pytree to get the metadata of.</p> required"},{"location":"api/base/module/#gpjax.base.module.meta--returns","title":"Returns","text":"<pre><code>Module: metadata of the pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.save_tree","title":"<code>save_tree(path: str, model: Module, overwrite: bool = False, iterate: int = None) -&gt; None</code>","text":""},{"location":"api/base/module/#gpjax.base.module.load_tree","title":"<code>load_tree(path: str, model: Module) -&gt; Module</code>","text":""},{"location":"api/base/param/","title":"param","text":""},{"location":"api/base/param/#gpjax.base.param","title":"<code>gpjax.base.param</code>","text":""},{"location":"api/base/param/#gpjax.base.param.__all__","title":"<code>__all__ = ['param_field']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/param/#gpjax.base.param.param_field","title":"<code>param_field(default: Any = dataclasses.MISSING, *, bijector: Optional[tfb.Bijector] = None, trainable: bool = True, default_factory: Any = dataclasses.MISSING, init: bool = True, repr: bool = True, hash: Optional[bool] = None, compare: bool = True, metadata: Optional[Mapping[str, Any]] = None)</code>","text":""},{"location":"api/kernels/base/","title":"base","text":""},{"location":"api/kernels/base/#gpjax.kernels.base","title":"<code>gpjax.kernels.base</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.SumKernel","title":"<code>SumKernel = partial(CombinationKernel, operator=jnp.sum)</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.ProductKernel","title":"<code>ProductKernel = partial(CombinationKernel, operator=jnp.prod)</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel","title":"<code>AbstractKernel</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Base kernel class.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.compute_engine","title":"<code>compute_engine: Type[AbstractKernelComputation] = static_field(DenseKernelComputation)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.cross_covariance","title":"<code>cross_covariance(x: Num[Array, N D], y: Num[Array, M D])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.gram","title":"<code>gram(x: Num[Array, N D])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input","title":"<code>slice_input(x: Float[Array, ... D]) -&gt; Float[Array, ... Q]</code>","text":"<p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>  <code>abstractmethod</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be added to the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be added to the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant","title":"<code>Constant</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>A constant kernel. This kernel evaluates to a constant for all inputs. The scalar value itself can be treated as a model hyperparameter and learned during training.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.constant","title":"<code>constant: ScalarFloat = param_field(jnp.array(0.0))</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel","title":"<code>CombinationKernel</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>A base class for products or sums of MeanFunctions.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.kernels","title":"<code>kernels: List[AbstractKernel] = None</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.operator","title":"<code>operator: Callable = static_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/approximations/rff/","title":"rff","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff","title":"<code>gpjax.kernels.approximations.rff</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF","title":"<code>RFF</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>Computes an approximation of the kernel using Random Fourier Features.</p> <p>All stationary kernels are equivalent to the Fourier transform of a probability distribution. We call the corresponding distribution the spectral density. Using a finite number of basis functions, we can compute the spectral density using a Monte-Carlo approximation. This is done by sampling from the spectral density and computing the Fourier transform of the samples. The kernel is then approximated by the inner product of the Fourier transform of the samples with the Fourier transform of the data.</p> <p>The key reference for this implementation is the following papers: - 'Random Features for Large-Scale Kernel Machines' by Rahimi and Recht (2008). - 'On the Error of Random Fourier Features' by Sutherland and Schneider (2015).</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.base_kernel","title":"<code>base_kernel: AbstractKernel = None</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.num_basis_fns","title":"<code>num_basis_fns: int = static_field(50)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.frequencies","title":"<code>frequencies: Float[Array, M 1] = param_field(None, bijector=tfb.Identity())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.key","title":"<code>key: KeyArray = static_field(PRNGKey(123))</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Post-initialisation function.</p> <p>This function is called after the initialisation of the kernel. It is used to set the computation engine to be the basis function computation engine.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__call__","title":"<code>__call__(x: Array, y: Array) -&gt; Array</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features","title":"<code>compute_features(x: Float[Array, N D]) -&gt; Float[Array, N L]</code>","text":"<p>Compute the features for the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>A \\(N \\times D\\) array of inputs.</p> required"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features--returns","title":"Returns","text":"<pre><code>Float[Array, \"N L\"]: A $N \\times L$ array of features where $L = 2M$.\n</code></pre>"},{"location":"api/kernels/computations/base/","title":"base","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base","title":"<code>gpjax.kernels.computations.base</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation","title":"<code>AbstractKernelComputation</code>  <code>dataclass</code>","text":"<p>Abstract class for kernel computations.</p>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.kernel","title":"<code>kernel: gpjax.kernels.base.AbstractKernel</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram","title":"<code>gram(x: Num[Array, N D]) -&gt; LinearOperator</code>","text":"<p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N N]</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: Gram covariance operator of the kernel function.\n</code></pre>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance","title":"<code>cross_covariance(x: Num[Array, N D], y: Num[Array, M D]) -&gt; Float[Array, N M]</code>  <code>abstractmethod</code>","text":"<p>For a given kernel, compute the NxM gram matrix on an a pair of input matrices with shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>The first input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The second input matrix.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal","title":"<code>diagonal(inputs: Num[Array, N D]) -&gt; DiagonalLinearOperator</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/","title":"basis_functions","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions","title":"<code>gpjax.kernels.computations.basis_functions</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation","title":"<code>BasisFunctionComputation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Compute engine class for finite basis function approximations to a kernel.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.num_basis_fns","title":"<code>num_basis_fns: int = None</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.cross_covariance","title":"<code>cross_covariance(x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute an approximate cross-covariance matrix.</p> <p>For a pair of inputs, compute the cross covariance matrix between the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>(Float[Array, \"N D\"]): A \\(N \\times D\\) array of inputs.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>(Float[Array, \"M D\"]): A \\(M \\times D\\) array of inputs.</p> required <p>Returns:</p> Type Description <code>Float[Array, N M]</code> <p>Float[Array, \"N M\"]: A \\(N \\times M\\) array of cross-covariances.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.gram","title":"<code>gram(inputs: Float[Array, N D]) -&gt; DenseLinearOperator</code>","text":"<p>Compute an approximate Gram matrix.</p> <p>For the Gram matrix, we can save computations by computing only one matrix multiplication between the inputs and the scaled frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Float[Array, N D]</code> <p>A N x D array of inputs.</p> required <p>Returns:</p> Name Type Description <code>DenseLinearOperator</code> <code>DenseLinearOperator</code> <p>A dense linear operator representing the \\(N \\times N\\) Gram matrix.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features","title":"<code>compute_features(x: Float[Array, N D]) -&gt; Float[Array, N L]</code>","text":"<p>Compute the features for the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>A \\(N \\times D\\) array of inputs.</p> required"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features--returns","title":"Returns","text":"<pre><code>Float[Array, \"N L\"]: A $N \\times L$ array of features where $L = 2M$.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/","title":"constant_diagonal","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal","title":"<code>gpjax.kernels.computations.constant_diagonal</code>","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation","title":"<code>ConstantDiagonalKernelComputation</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.gram","title":"<code>gram(x: Float[Array, N D]) -&gt; ConstantDiagonalLinearOperator</code>","text":"<p>Compute the Gram matrix.</p> <p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N N]</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal","title":"<code>diagonal(inputs: Float[Array, N D]) -&gt; DiagonalLinearOperator</code>","text":"<p>Compute the diagonal Gram matrix's entries.</p> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape \\(N\\times D\\).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance","title":"<code>cross_covariance(x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the NxM covariance matrix on a pair of input matrices of shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed square Gram matrix.\n</code></pre>"},{"location":"api/kernels/computations/dense/","title":"dense","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense","title":"<code>gpjax.kernels.computations.dense</code>","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation","title":"<code>DenseKernelComputation</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Dense kernel computation class. Operations with the kernel assume a dense gram matrix structure.</p>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance","title":"<code>cross_covariance(x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the NxM covariance matrix on a pair of input matrices of shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/","title":"diagonal","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal","title":"<code>gpjax.kernels.computations.diagonal</code>","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation","title":"<code>DiagonalKernelComputation</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Diagonal kernel computation class. Operations with the kernel assume a diagonal Gram matrix.</p>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.gram","title":"<code>gram(x: Float[Array, N D]) -&gt; DiagonalLinearOperator</code>","text":"<p>Compute the Gram matrix.</p> <p>For a kernel with diagonal structure, compute the \\(N\\times N\\) Gram matrix on an input matrix of shape \\(N\\times D\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.gram--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: The computed square Gram matrix.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance","title":"<code>cross_covariance(x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the \\(N\\times M\\) covariance matrix on a pair of input matrices of shape \\(N\\times D\\) and \\(M\\times D\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/eigen/","title":"eigen","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen","title":"<code>gpjax.kernels.computations.eigen</code>","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation","title":"<code>EigenKernelComputation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Eigen kernel computation class. Kernels who operate on an eigen-decomposed structure should use this computation object.</p>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.cross_covariance","title":"<code>cross_covariance(x: Num[Array, N D], y: Num[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For an \\(N\\times D\\) and \\(M\\times D\\) pair of matrices, evaluate the \\(N \\times M\\) cross-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>Float[Array, N M]</code> <p>description</p>"},{"location":"api/kernels/non_euclidean/graph/","title":"graph","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph","title":"<code>gpjax.kernels.non_euclidean.graph</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.tfb","title":"<code>tfb = tfp.bijectors</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel","title":"<code>GraphKernel</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn graph kernel defined on the vertex set of a graph.</p> <p>A Mat\u00e9rn graph kernel defined on the vertices of a graph. The key reference for this object is borovitskiy et. al., (2020).</p> <p>Parameters:</p> Name Type Description Default <code>laplacian</code> <code>Float[Array]</code> <p>An N x N matrix representing the Laplacian matrix of a graph.</p> <code>static_field(None)</code>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.laplacian","title":"<code>laplacian: Num[Array, N N] = static_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.lengthscale","title":"<code>lengthscale: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.smoothness","title":"<code>smoothness: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.eigenvalues","title":"<code>eigenvalues: Float[Array, N] = static_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.eigenvectors","title":"<code>eigenvectors: Float[Array, N N] = static_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.num_vertex","title":"<code>num_vertex: ScalarInt = static_field(None)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(EigenKernelComputation)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.name","title":"<code>name: str = 'Graph Mat\u00e9rn'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__call__","title":"<code>__call__(x: Int[Array, N 1], y: Int[Array, N 1], *, S, **kwargs)</code>","text":"<p>Compute the (co)variance between a vertex pair.</p> <p>For a graph \\(\\mathcal{G} = \\{V, E\\}\\) where $V = {v_1, v_2, \\ldots v_n }, evaluate the graph kernel on a pair of vertices \\((v_i, v_j)\\) for any \\(i,j&lt;n\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N 1]</code> <p>Index of the \\(i\\)th vertex.</p> required <code>y</code> <code>Float[Array, N 1]</code> <p>Index of the \\(j\\)th vertex.</p> required"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(v_i, v_j)$.\n</code></pre>"},{"location":"api/kernels/non_euclidean/utils/","title":"utils","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils","title":"<code>gpjax.kernels.non_euclidean.utils</code>","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd","title":"<code>jax_gather_nd(params: Num[Array, N *rest], indices: Int[Array, M 1]) -&gt; Num[Array, M *rest]</code>","text":"<p>Slice a <code>params</code> array at a set of <code>indices</code>.</p> <p>This is a reimplementation of TensorFlow's <code>gather_nd</code> function: link</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Num[Array]</code> <p>An arbitrary array with leading axes of length \\(N\\) upon which we shall slice.</p> required <code>indices</code> <code>Float[Int]</code> <p>An integer array of length \\(M\\) with values in the range \\([0, N)\\) whose value at index \\(i\\) will be used to slice <code>params</code> at index \\(i\\).</p> required"},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd--returns","title":"Returns","text":"<pre><code>Num[Array: An arbitrary array with leading axes of length $M$.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/","title":"arccosine","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine","title":"<code>gpjax.kernels.nonstationary.arccosine</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine","title":"<code>ArcCosine</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The ArCosine kernel. This kernel is non-stationary and resembles the behavior of neural networks. See Section 3.1 of https://arxiv.org/pdf/1112.3712.pdf for additional details.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.order","title":"<code>order: ScalarInt = static_field(0)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.weight_variance","title":"<code>weight_variance: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.bias_variance","title":"<code>bias_variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs :math:<code>(x, y)</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of :math:`k(x, y)`.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/","title":"linear","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear","title":"<code>gpjax.kernels.nonstationary.linear</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear","title":"<code>Linear</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The linear kernel.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.name","title":"<code>name: str = 'Linear'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the linear kernel between a pair of arrays.</p> <p>For a pair of inputs \\(x, y \\in \\mathbb{R}^{D}\\), let's evaluate the linear kernel \\(k(x, y)=\\sigma^2 x^{\\top}y\\) where \\(\\sigma^\\in \\mathbb{R}_{&gt;0}\\) is the kernel's variance parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function $k(x, y)$ at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/","title":"polynomial","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial","title":"<code>gpjax.kernels.nonstationary.polynomial</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial","title":"<code>Polynomial</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Polynomial kernel with variable degree.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.degree","title":"<code>degree: ScalarInt = static_field(2)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.shift","title":"<code>shift: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the polynomial kernel of degree \\(d\\) between a pair of arrays.</p> <p>For a pair of inputs \\(x, y \\in \\mathbb{R}^{D}\\), let's evaluate the polynomial kernel \\(k(x, y)=\\left( \\alpha + \\sigma^2 x y\\right)^{d}\\) where \\(\\sigma^\\in \\mathbb{R}_{&gt;0}\\) is the kernel's variance parameter, shift parameter \\(\\alpha\\) and integer degree \\(d\\).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/","title":"matern12","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12","title":"<code>gpjax.kernels.stationary.matern12</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12","title":"<code>Matern12</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 0.5.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.name","title":"<code>name: str = 'Mat\u00e9rn12'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 1/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\). \\(\\(k(x, y) = \\sigma^2\\exp\\Bigg(-\\frac{\\lvert x-y \\rvert}{2\\ell^2}\\Bigg)\\)\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of :math:<code>k(x, y)</code></p>"},{"location":"api/kernels/stationary/matern32/","title":"matern32","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32","title":"<code>gpjax.kernels.stationary.matern32</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32","title":"<code>Matern32</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 1.5.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.name","title":"<code>name: str = 'Mat\u00e9rn32'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 3/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\).</p> <p>.. math::     k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{3}\\lvert x-y \\rvert}{\\ell^2}  \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{3}\\lvert x-y\\rvert}{\\ell^2} \\Bigg)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/","title":"matern52","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52","title":"<code>gpjax.kernels.stationary.matern52</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52","title":"<code>Matern52</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 2.5.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.name","title":"<code>name: str = 'Mat\u00e9rn52'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 5/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\). $$ k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{5}\\lvert x-y \\rvert}{\\ell^2} + \\frac{5\\lvert x - y \\rvert^2}{3\\ell^2} \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{5}\\lvert x-y\\rvert}{\\ell^2} \\Bigg)$$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/","title":"periodic","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic","title":"<code>gpjax.kernels.stationary.periodic</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic","title":"<code>Periodic</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The periodic kernel.</p> <p>Key reference is MacKay 1998 - \"Introduction to Gaussian processes\".</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.period","title":"<code>period: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.name","title":"<code>name: str = 'Periodic'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Periodic kernel between a pair of arrays.</p> <p>TODO: update docstring Evaluate the kernel on a pair of inputs \\((x, y)\\) with length-scale parameter \\(\\ell\\) and variance \\(\\sigma\\). \\(\\(k(x, y) = \\sigma^2 \\exp \\Bigg( -0.5 \\sum_{i=1}^{d} \\Bigg)\\)\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of \\(k(x, y)\\).</p>"},{"location":"api/kernels/stationary/powered_exponential/","title":"powered_exponential","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential","title":"<code>gpjax.kernels.stationary.powered_exponential</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential","title":"<code>PoweredExponential</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The powered exponential family of kernels.</p> <p>Key reference is Diggle and Ribeiro (2007) - \"Model-based Geostatistics\".</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.power","title":"<code>power: ScalarFloat = param_field(jnp.array(1.0))</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.name","title":"<code>name: str = 'Powered Exponential'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Powered Exponential kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs \\((x, y)\\) with length-scale parameter \\(\\ell\\), \\(\\sigma\\) and power \\(\\kappa\\). \\(\\(k(x, y)=\\sigma^2\\exp\\Bigg(-\\Big(\\frac{\\lVert x-y\\rVert^2}{\\ell^2}\\Big)^\\kappa\\Bigg)\\)\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/","title":"rational_quadratic","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic","title":"<code>gpjax.kernels.stationary.rational_quadratic</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic","title":"<code>RationalQuadratic</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.alpha","title":"<code>alpha: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.name","title":"<code>name: str = 'Rational Quadratic'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Powered Exponential kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\). \\(\\(k(x,y)=\\sigma^2\\exp\\Bigg(1+\\frac{\\lVert x-y\\rVert^2_2}{2\\alpha\\ell^2}\\Bigg)\\)\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of :math:<code>k(x, y)</code></p>"},{"location":"api/kernels/stationary/rbf/","title":"rbf","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf","title":"<code>gpjax.kernels.stationary.rbf</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF","title":"<code>RBF</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Radial Basis Function (RBF) kernel.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.name","title":"<code>name: str = 'RBF'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.spectral_density","title":"<code>spectral_density: tfd.Normal</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the RBF kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs\\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\): \\(\\(k(x,y)=\\sigma^2\\exp\\Bigg(\\frac{\\lVert x - y \\rVert^2_2}{2 \\ell^2} \\Bigg)\\)\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of \\(k(x, y)\\).</p>"},{"location":"api/kernels/stationary/utils/","title":"utils","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils","title":"<code>gpjax.kernels.stationary.utils</code>","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution","title":"<code>build_student_t_distribution(nu: int) -&gt; tfd.Distribution</code>","text":"<p>For a fixed half-integer smoothness parameter, compute the spectral density of a Mat\u00e9rn kernel; a Student's t distribution.</p> <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>int</code> <p>The smoothness parameter of the Mat\u00e9rn kernel.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution--returns","title":"Returns","text":"<pre><code>tfp.Distribution: A Student's t distribution with the same smoothness parameter.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance","title":"<code>squared_distance(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the squared distance between a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>First input.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>Second input.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The squared distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance","title":"<code>euclidean_distance(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the euclidean distance between a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>First input.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>Second input.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The euclidean distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/white/","title":"white","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white","title":"<code>gpjax.kernels.stationary.white</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White","title":"<code>White</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(ConstantDiagonalKernelComputation)</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.name","title":"<code>name: str = 'White'</code>  <code>class-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the White noise kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs \\((x, y)\\) with variance \\(\\sigma^2\\): \\(\\(k(x, y) = \\sigma^2 \\delta(x-y)\\)\\)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/","title":"constant_diagonal_linear_operator","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator","title":"<code>gpjax.linops.constant_diagonal_linear_operator</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.__all__","title":"<code>__all__ = ['ConstantDiagonalLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator","title":"<code>ConstantDiagonalLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>DiagonalLinearOperator</code></p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.value","title":"<code>value = value</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.size","title":"<code>size = size</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.shape","title":"<code>shape = (size, size)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.dtype","title":"<code>dtype = value.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__init__","title":"<code>__init__(value: Float[Array, 1], size: int, dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Initialize the constant diagonal linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Float[Array, 1]</code> <p>Constant value of the diagonal.</p> required <code>size</code> <code>int</code> <p>Size of the diagonal.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__add__","title":"<code>__add__(other: Union[Float[Array, N N], LinearOperator]) -&gt; LinearOperator</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__mul__","title":"<code>__mul__(other: Union[ScalarFloat, Float[Array, 1]]) -&gt; LinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>LinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.to_root","title":"<code>to_root() -&gt; ConstantDiagonalLinearOperator</code>","text":"<p>Lower triangular.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower triangular matrix.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.inverse","title":"<code>inverse() -&gt; ConstantDiagonalLinearOperator</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.inverse--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Inverse of the covariance operator.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":"<p>Solve linear system.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; ConstantDiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_root","title":"<code>from_root(root: ConstantDiagonalLinearOperator) -&gt; ConstantDiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>ConstantDiagonalLinearOperator</code> <p>Root of the covariance operator.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>ConstantDiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/","title":"dense_linear_operator","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator","title":"<code>gpjax.linops.dense_linear_operator</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.__all__","title":"<code>__all__ = ['DenseLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator","title":"<code>DenseLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>LinearOperator</code></p> <p>Dense covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.matrix","title":"<code>matrix = matrix</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.shape","title":"<code>shape = matrix.shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.dtype","title":"<code>dtype = matrix.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__init__","title":"<code>__init__(matrix: Float[Array, N N], dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Initialize the covariance operator.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__add__","title":"<code>__add__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Add diagonal to another linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[LinearOperator, Float[Array, N N]]</code> <p>Other linear operator. Dimension of both operators must match. If the other linear operator is not a DiagonalLinearOperator, dense matrix addition is used.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__add__--returns","title":"Returns","text":"<pre><code>LinearOperator: linear operator plus the diagonal linear operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; LinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>LinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: The diagonal of the covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__matmul__","title":"<code>__matmul__(other: VecNOrMatNM) -&gt; VecNOrMatNM</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Float[Array, N M]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>","text":"<p>Construct dense Covariance matrix from the covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense covariance matrix.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_dense","title":"<code>from_dense(matrix: Float[Array, N N]) -&gt; DenseLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from dense covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>Float[Array, N N]</code> <p>Dense covariance matrix.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>DenseLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_root","title":"<code>from_root(root: LinearOperator) -&gt; DenseLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the root of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Float[Array, N N]</code> <p>Root of the covariance matrix.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>DenseLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator","title":"<code>DenseFromRootLinearOperator</code>","text":"<p>         Bases: <code>DenseLinearOperator</code></p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.root","title":"<code>root = root</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.shape","title":"<code>shape = root.shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.dtype","title":"<code>dtype = root.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.matrix","title":"<code>matrix: Float[Array, N N]</code>  <code>property</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.__init__","title":"<code>__init__(root: LinearOperator)</code>","text":"<p>Initialize the covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/diagonal_linear_operator/","title":"diagonal_linear_operator","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator","title":"<code>gpjax.linops.diagonal_linear_operator</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.__all__","title":"<code>__all__ = ['DiagonalLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator","title":"<code>DiagonalLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>LinearOperator</code></p> <p>Diagonal covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.diag","title":"<code>diag = diag</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.shape","title":"<code>shape = (dim, dim)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.dtype","title":"<code>dtype = diag.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__init__","title":"<code>__init__(diag: Float[Array, N], dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Initialize the covariance operator.</p> <p>Parameters:</p> Name Type Description Default <code>diag</code> <code>Float[Array,  N]</code> <p>Diagonal of the covariance operator.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: Diagonal of the covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__add__","title":"<code>__add__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Add diagonal to another linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[LinearOperator, Float[Array, N N]]</code> <p>Other linear operator. Dimension of both operators must match. If the other linear operator is not a DiagonalLinearOperator, dense matrix addition is used.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__add__--returns","title":"Returns","text":"<pre><code>LinearOperator: linear operator plus the diagonal linear operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; LinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>LinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>","text":"<p>Construct dense Covariance matrix from the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense covariance matrix.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__matmul__","title":"<code>__matmul__(other: VecNOrMatNM) -&gt; VecNOrMatNM</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Float[Array, N M]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_root","title":"<code>to_root() -&gt; DiagonalLinearOperator</code>","text":"<p>Lower triangular.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower triangular matrix.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.inverse","title":"<code>inverse() -&gt; DiagonalLinearOperator</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.inverse--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Inverse of the covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.solve","title":"<code>solve(rhs: VecNOrMatNM) -&gt; VecNOrMatNM</code>","text":"<p>Solve linear system.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_root","title":"<code>from_root(root: DiagonalLinearOperator) -&gt; DiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the lower triangular matrix.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; DiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from its dense matrix representation.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator","title":"<code>DiagonalFromRootLinearOperator</code>","text":"<p>         Bases: <code>DiagonalLinearOperator</code></p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.root","title":"<code>root = root</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.shape","title":"<code>shape = root.shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.dtype","title":"<code>dtype = root.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.diag","title":"<code>diag: Float[Array, N]</code>  <code>property</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.__init__","title":"<code>__init__(root: DiagonalLinearOperator)</code>","text":"<p>Initialize the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":""},{"location":"api/linops/identity_linear_operator/","title":"identity_linear_operator","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator","title":"<code>gpjax.linops.identity_linear_operator</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.__all__","title":"<code>__all__ = ['IdentityLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator","title":"<code>IdentityLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ConstantDiagonalLinearOperator</code></p> <p>Identity linear operator.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.value","title":"<code>value = jnp.array([1.0], dtype=dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.size","title":"<code>size = size</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.shape","title":"<code>shape = (size, size)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.dtype","title":"<code>dtype = dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.__init__","title":"<code>__init__(size: int, dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Identity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the identity matrix.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.__matmul__","title":"<code>__matmul__(other: Float[Array, N M]) -&gt; Float[Array, N M]</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Float[Array, N M]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.to_root","title":"<code>to_root() -&gt; IdentityLinearOperator</code>","text":"<p>Lower triangular.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower triangular matrix.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.inverse","title":"<code>inverse() -&gt; IdentityLinearOperator</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.inverse--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Inverse of the covariance operator.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":"<p>Solve linear system.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.from_root","title":"<code>from_root(root: IdentityLinearOperator) -&gt; IdentityLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct from root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>IdentityLinearOperator</code> <p>Root of the covariance operator.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>IdentityLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; IdentityLinearOperator</code>  <code>classmethod</code>","text":""},{"location":"api/linops/linear_operator/","title":"linear_operator","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator","title":"<code>gpjax.linops.linear_operator</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.NestedT","title":"<code>NestedT = Union[T, Iterable['NestedT'], Mapping[Any, 'NestedT']]</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.DTypes","title":"<code>DTypes = Union[Type[jnp.float32], Type[jnp.float64], Type[jnp.int32], Type[jnp.int64]]</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.ShapeT","title":"<code>ShapeT = TypeVar('ShapeT', bound=NestedT[Tuple[int, Ellipsis]])</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.DTypeT","title":"<code>DTypeT = TypeVar('DTypeT', bound=NestedT[DTypes])</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.__all__","title":"<code>__all__ = ['LinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator","title":"<code>LinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code>, <code>Generic[ShapeT, DTypeT]</code></p> <p>Linear operator base class.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.shape","title":"<code>shape: ShapeT = static_field()</code>  <code>class-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.dtype","title":"<code>dtype: DTypeT = static_field()</code>  <code>class-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Linear operator dimension.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.T","title":"<code>T: LinearOperator</code>  <code>property</code>","text":"<p>Transpose linear operator. Currently, we assume all linear operators are square and symmetric.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Linear operator representation.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__sub__","title":"<code>__sub__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Subtract linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__rsub__","title":"<code>__rsub__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Reimplimentation of subtract linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__add__","title":"<code>__add__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Add linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__radd__","title":"<code>__radd__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Reimplimentation of add linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; LinearOperator</code>  <code>abstractmethod</code>","text":"<p>Multiply linear operator by scalar.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__rmul__","title":"<code>__rmul__(other: ScalarFloat) -&gt; LinearOperator</code>","text":"<p>Reimplimentation of multiply linear operator by scalar.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__matmul__","title":"<code>__matmul__(other: Union[LinearOperator, Float[Array, N M]]) -&gt; Union[LinearOperator, Float[Array, N M]]</code>  <code>abstractmethod</code>","text":"<p>Matrix multiplication.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__rmatmul__","title":"<code>__rmatmul__(other: Union[LinearOperator, Float[Array, N M]]) -&gt; Union[LinearOperator, Float[Array, N M]]</code>","text":"<p>Reimplimentation of matrix multiplication.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>  <code>abstractmethod</code>","text":"<p>Diagonal of the linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: Diagonal of the linear operator.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.trace","title":"<code>trace() -&gt; ScalarFloat</code>","text":"<p>Trace of the linear matrix.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.trace--returns","title":"Returns","text":"<pre><code>ScalarFloat: Trace of the linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant of the linear matrix. Default implementation uses dense Cholesky decomposition.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":"<p>Compute the root of the linear operator via the Cholesky decomposition.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower Cholesky decomposition of the linear operator.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.inverse","title":"<code>inverse() -&gt; LinearOperator</code>","text":"<p>Inverse of the linear matrix. Default implementation uses dense Cholesky decomposition.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.inverse--returns","title":"Returns","text":"<pre><code>LinearOperator: Inverse of the linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":"<p>Solve linear system. Default implementation uses dense Cholesky decomposition.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>  <code>abstractmethod</code>","text":"<p>Construct dense matrix from the linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; LinearOperator</code>  <code>classmethod</code>","text":"<p>Construct linear operator from dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>LinearOperator: Linear operator.\n</code></pre>"},{"location":"api/linops/triangular_linear_operator/","title":"triangular_linear_operator","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator","title":"<code>gpjax.linops.triangular_linear_operator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.__all__","title":"<code>__all__ = ['LowerTriangularLinearOperator', 'UpperTriangularLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator","title":"<code>LowerTriangularLinearOperator</code>","text":"<p>         Bases: <code>DenseLinearOperator</code></p> <p>Current implementation of the following methods is inefficient. We assume a dense matrix representation of the operator. But take advantage of the solve structure.</p>"},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.T","title":"<code>T: UpperTriangularLinearOperator</code>  <code>property</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.inverse","title":"<code>inverse() -&gt; DenseLinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.from_root","title":"<code>from_root(root: LinearOperator) -&gt; None</code>  <code>classmethod</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; LowerTriangularLinearOperator</code>  <code>classmethod</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator","title":"<code>UpperTriangularLinearOperator</code>","text":"<p>         Bases: <code>DenseLinearOperator</code></p> <p>Current implementation of the following methods is inefficient. We assume a dense matrix representation of the operator. But take advantage of the solve structure.</p>"},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.T","title":"<code>T: LowerTriangularLinearOperator</code>  <code>property</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.inverse","title":"<code>inverse() -&gt; DenseLinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.from_root","title":"<code>from_root(root: LinearOperator) -&gt; None</code>  <code>classmethod</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; UpperTriangularLinearOperator</code>  <code>classmethod</code>","text":""},{"location":"api/linops/utils/","title":"utils","text":""},{"location":"api/linops/utils/#gpjax.linops.utils","title":"<code>gpjax.linops.utils</code>","text":""},{"location":"api/linops/utils/#gpjax.linops.utils.__all__","title":"<code>__all__ = ['identity', 'to_dense']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/utils/#gpjax.linops.utils.identity","title":"<code>identity(n: int) -&gt; gpjax.linops.identity_linear_operator.IdentityLinearOperator</code>","text":"<p>Identity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Size of the identity matrix.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.identity--returns","title":"Returns","text":"<pre><code>IdentityLinearOperator: Identity matrix of shape [n, n].\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.to_dense","title":"<code>to_dense(obj: Union[Float[Array, ...], LinearOperator])</code>","text":"<p>Ensure an object is a dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Float[Array, ], LinearOperator]</code> <p>Linear operator to convert.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"...\"]: Dense matrix.\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.to_linear_operator","title":"<code>to_linear_operator(obj: Union[Float[Array, ...], LinearOperator])</code>","text":"<p>Ensure an object is a linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Float[Array, ], LinearOperator]</code> <p>Linear operator to convert.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.to_linear_operator--returns","title":"Returns","text":"<pre><code>LinearOperator: Linear operator.\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.check_shapes_match","title":"<code>check_shapes_match(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -&gt; None</code>","text":"<p>Check shapes of two objects.</p> <p>Parameters:</p> Name Type Description Default <code>shape1</code> <code>Tuple[int, ]</code> <p>Shape of the first object.</p> required <code>shape2</code> <code>Tuple[int, ]</code> <p>Shape of the second object.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.check_shapes_match--raises","title":"Raises","text":"<pre><code>ValueError: Shapes of the two objects do not match.\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.default_dtype","title":"<code>default_dtype() -&gt; Union[Type[jnp.float64], Type[jnp.float32]]</code>","text":"<p>Get the default dtype for the linear operator.</p>"},{"location":"api/linops/utils/#gpjax.linops.utils.default_dtype--returns","title":"Returns","text":"<pre><code>jnp.dtype: Default dtype for the linear operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/","title":"zero_linear_operator","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator","title":"<code>gpjax.linops.zero_linear_operator</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.__all__","title":"<code>__all__ = ['ZeroLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator","title":"<code>ZeroLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>LinearOperator</code></p> <p>Zero linear operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.shape","title":"<code>shape = shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.dtype","title":"<code>dtype = dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__init__","title":"<code>__init__(shape: Tuple[int, ...], dtype: jnp.dtype = None) -&gt; None</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: The diagonal of the covariance operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__add__","title":"<code>__add__(other: Union[Float[Array, N N], LinearOperator]) -&gt; Union[Float[Array, N N], LinearOperator]</code>","text":"<p>Add covariance operator to another covariance operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[Float[Array, N N], LinearOperator]</code> <p>Covariance operator to add.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__add__--returns","title":"Returns","text":"<pre><code>Union[Float[Array, \"N N\"], LinearOperator]: Sum of the covariance operators.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; ZeroLinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ConstantDiagonalLinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__matmul__","title":"<code>__matmul__(other: Union[LinearOperator, Float[Array, N M]]) -&gt; ZeroLinearOperator</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[LinearOperator, Float[Array, N M]]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>","text":"<p>Construct dense Covariance matrix from the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense covariance matrix.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_root","title":"<code>to_root() -&gt; ZeroLinearOperator</code>","text":"<p>Root of the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Root of the covariance operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.inverse","title":"<code>inverse() -&gt; None</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.inverse--raises","title":"Raises","text":"<pre><code>RuntimeError: ZeroLinearOperator is not invertible.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; None</code>","text":"<p>Solve linear system.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.solve--raises","title":"Raises","text":"<pre><code>RuntimeError: ZeroLinearOperator is not invertible.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_root","title":"<code>from_root(root: ZeroLinearOperator) -&gt; ZeroLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>ZeroLinearOperator</code> <p>Root of the covariance operator.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; ZeroLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Covariance operator.\n</code></pre>"},{"location":"examples/","title":"Where to find the docs","text":"<p>The GPJax documentation can be found here: https://gpjax.readthedocs.io/en/latest/</p>"},{"location":"examples/#how-to-build-the-docs","title":"How to build the docs","text":"<ol> <li>Install the requirements using <code>pip install -r docs/requirements.txt</code></li> <li>Make sure <code>pandoc</code> is installed</li> <li>Run the make script <code>make html</code></li> </ol> <p>The corresponding HTML files can then be found in <code>docs/_build/html/</code>.</p>"},{"location":"examples/#how-to-write-code-documentation","title":"How to write code documentation","text":"<p>Our documentation it is written in ReStructuredText for Sphinx. This is a meta-language that is compiled into online documentation. For more details see Sphinx's documentation. As a result, our docstrings adhere to a specific syntax that has to be kept in mind. Below we provide some guidelines.</p>"},{"location":"examples/#how-much-information-to-put-in-a-docstring","title":"How much information to put in a docstring","text":"<p>A docstring should be informative. If in doubt, then it is best to add more information to a docstring than less. Many users will skim documentation, so please ensure the opening sentence or two of a docstring contains the core information. Adding examples and mathematical descriptions to documentation is highly desirable.</p> <p>We are making an active effort within GPJax to improve our documentation. If you spot any areas where there is missing information within the existing documentation, then please either raise an issue or create a pull request.</p>"},{"location":"examples/#an-example-docstring","title":"An example docstring","text":"<p>An example docstring that adheres the principles of GPJax is given below. The docstring contains a simple, snappy introduction with links to auxiliary components. More detail is then provided in the form of a mathematical description and a code example. The docstring is concluded with a description of the objects attributes with corresponding types.</p> <pre><code>class Prior(AbstractPrior):\n\"\"\"A Gaussian process prior object. The GP is parameterised by a\n    `mean &lt;https://gpjax.readthedocs.io/en/latest/api.html#module-gpjax.mean_functions&gt;`_\n    and `kernel &lt;https://gpjax.readthedocs.io/en/latest/api.html#module-gpjax.kernels&gt;`_ function.\n    A Gaussian process prior parameterised by a mean function :math:`m(\\\\cdot)` and a kernel\n    function :math:`k(\\\\cdot, \\\\cdot)` is given by\n    .. math::\n        p(f(\\\\cdot)) = \\mathcal{GP}(m(\\\\cdot), k(\\\\cdot, \\\\cdot)).\n    To invoke a ``Prior`` distribution, only a kernel function is required. By default,\n    the mean function will be set to zero. In general, this assumption will be reasonable\n    assuming the data being modelled has been centred.\n    Example:\n        &gt;&gt;&gt; import gpjax as gpx\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; kernel = gpx.kernels.RBF()\n        &gt;&gt;&gt; prior = gpx.Prior(kernel = kernel)\n    Attributes:\n        kernel (Kernel): The kernel function used to parameterise the prior.\n        mean_function (MeanFunction): The mean function used to parameterise the prior. Defaults to zero.\n        name (str): The name of the GP prior. Defaults to \"GP prior\".\n    \"\"\"\nkernel: Kernel\nmean_function: Optional[AbstractMeanFunction] = Zero()\nname: Optional[str] = \"GP prior\"\n</code></pre>"},{"location":"examples/#documentation-syntax","title":"Documentation syntax","text":"<p>A helpful cheatsheet for writing restructured text can be found here. In addition to that, we adopt the following convention when documenting `` objects.</p> <ul> <li>Class attributes should be specified using the <code>Attributes:</code> tag.</li> <li>Method argument should be specified using the <code>Args:</code> tags.</li> <li>All attributes and arguments should have types.</li> </ul>"},{"location":"examples/barycentres/","title":"Gaussian Processes Barycentres","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy.linalg as jsl\nfrom jaxtyping import install_import_hook\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax.distributions as tfd\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  import typing as tp  import jax import jax.numpy as jnp import jax.random as jr import jax.scipy.linalg as jsl from jaxtyping import install_import_hook import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax.distributions as tfd  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx   key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>n = 100\nn_test = 200\nn_datasets = 5\n\nx = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1)\nxtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1)\nf = lambda x, a, b: a + jnp.sin(b * x)\n\nys = []\nfor _i in range(n_datasets):\n    key, subkey = jr.split(key)\n    vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)\n    period = jr.uniform(subkey, minval=0.75, maxval=1.25)\n    noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)\n    noise = jr.normal(subkey, shape=x.shape) * noise_amount\n    ys.append(f(x, vertical_shift, period) + noise)\n\ny = jnp.hstack(ys)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\")\nplt.show()\n</pre> n = 100 n_test = 200 n_datasets = 5  x = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1) xtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1) f = lambda x, a, b: a + jnp.sin(b * x)  ys = [] for _i in range(n_datasets):     key, subkey = jr.split(key)     vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)     period = jr.uniform(subkey, minval=0.75, maxval=1.25)     noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)     noise = jr.normal(subkey, shape=x.shape) * noise_amount     ys.append(f(x, vertical_shift, period) + noise)  y = jnp.hstack(ys)  fig, ax = plt.subplots() ax.plot(x, y, \"x\") plt.show() In\u00a0[\u00a0]: Copied! <pre>def fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    D = gpx.Dataset(X=x, y=y)\n\n    likelihood = gpx.Gaussian(num_datapoints=n)\n    posterior = gpx.Prior(mean_function=gpx.Constant(), kernel=gpx.RBF()) * likelihood\n\n    opt_posterior, _ = gpx.fit(\n        model=posterior,\n        objective=jax.jit(gpx.ConjugateMLL(negative=True)),\n        train_data=D,\n        optim=ox.adamw(learning_rate=0.01),\n        num_iters=500,\n        key=key,\n    )\n    latent_dist = opt_posterior.predict(xtest, train_data=D)\n    return opt_posterior.likelihood(latent_dist)\n\n\nposterior_preds = [fit_gp(x, i) for i in ys]\n</pre> def fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:     if y.ndim == 1:         y = y.reshape(-1, 1)     D = gpx.Dataset(X=x, y=y)      likelihood = gpx.Gaussian(num_datapoints=n)     posterior = gpx.Prior(mean_function=gpx.Constant(), kernel=gpx.RBF()) * likelihood      opt_posterior, _ = gpx.fit(         model=posterior,         objective=jax.jit(gpx.ConjugateMLL(negative=True)),         train_data=D,         optim=ox.adamw(learning_rate=0.01),         num_iters=500,         key=key,     )     latent_dist = opt_posterior.predict(xtest, train_data=D)     return opt_posterior.likelihood(latent_dist)   posterior_preds = [fit_gp(x, i) for i in ys] In\u00a0[\u00a0]: Copied! <pre>def sqrtm(A: jax.Array):\n    return jnp.real(jsl.sqrtm(A))\n\n\ndef wasserstein_barycentres(\n    distributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array\n):\n    covariances = [d.covariance() for d in distributions]\n    cov_stack = jnp.stack(covariances)\n    stack_sqrt = jax.vmap(sqrtm)(cov_stack)\n\n    def step(covariance_candidate: jax.Array, idx: None):\n        inner_term = jax.vmap(sqrtm)(\n            jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)\n        )\n        fixed_point = jnp.tensordot(weights, inner_term, axes=1)\n        return fixed_point, fixed_point\n\n    return step\n</pre> def sqrtm(A: jax.Array):     return jnp.real(jsl.sqrtm(A))   def wasserstein_barycentres(     distributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array ):     covariances = [d.covariance() for d in distributions]     cov_stack = jnp.stack(covariances)     stack_sqrt = jax.vmap(sqrtm)(cov_stack)      def step(covariance_candidate: jax.Array, idx: None):         inner_term = jax.vmap(sqrtm)(             jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)         )         fixed_point = jnp.tensordot(weights, inner_term, axes=1)         return fixed_point, fixed_point      return step <p>With a function defined for learning a barycentre, we'll now compute it using the <code>lax.scan</code> operator that drastically speeds up for loops in Jax (see the Jax documentation). The iterative update will be executed 100 times, with convergence measured by the difference between the previous and current iteration that we can confirm by inspecting the <code>sequence</code> array in the following cell.</p> In\u00a0[\u00a0]: Copied! <pre>weights = jnp.ones((n_datasets,)) / n_datasets\n\nmeans = jnp.stack([d.mean() for d in posterior_preds])\nbarycentre_mean = jnp.tensordot(weights, means, axes=1)\n\nstep_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights))\ninitial_covariance = jnp.eye(n_test)\n\nbarycentre_covariance, sequence = jax.lax.scan(\n    step_fn, initial_covariance, jnp.arange(100)\n)\nL = jnp.linalg.cholesky(barycentre_covariance)\n\nbarycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L)\n</pre> weights = jnp.ones((n_datasets,)) / n_datasets  means = jnp.stack([d.mean() for d in posterior_preds]) barycentre_mean = jnp.tensordot(weights, means, axes=1)  step_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights)) initial_covariance = jnp.eye(n_test)  barycentre_covariance, sequence = jax.lax.scan(     step_fn, initial_covariance, jnp.arange(100) ) L = jnp.linalg.cholesky(barycentre_covariance)  barycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L) In\u00a0[\u00a0]: Copied! <pre>def plot(\n    dist: tfd.MultivariateNormalTriL,\n    ax,\n    color: str,\n    label: str = None,\n    ci_alpha: float = 0.2,\n    linewidth: float = 1.0,\n    zorder: int = 0,\n):\n    mu = dist.mean()\n    sigma = dist.stddev()\n    ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)\n    ax.fill_between(\n        xtest.squeeze(),\n        mu - sigma,\n        mu + sigma,\n        alpha=ci_alpha,\n        color=color,\n        zorder=zorder,\n    )\n\n\nfig, ax = plt.subplots()\n[plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds]\nplot(\n    barycentre_process,\n    ax,\n    color=cols[0],\n    label=\"Barycentre\",\n    ci_alpha=0.5,\n    linewidth=2,\n    zorder=1,\n)\nax.legend()\n</pre> def plot(     dist: tfd.MultivariateNormalTriL,     ax,     color: str,     label: str = None,     ci_alpha: float = 0.2,     linewidth: float = 1.0,     zorder: int = 0, ):     mu = dist.mean()     sigma = dist.stddev()     ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)     ax.fill_between(         xtest.squeeze(),         mu - sigma,         mu + sigma,         alpha=ci_alpha,         color=color,         zorder=zorder,     )   fig, ax = plt.subplots() [plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds] plot(     barycentre_process,     ax,     color=cols[0],     label=\"Barycentre\",     ci_alpha=0.5,     linewidth=2,     zorder=1, ) ax.legend() In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder'"},{"location":"examples/barycentres/#gaussian-processes-barycentres","title":"Gaussian Processes Barycentres\u00b6","text":"<p>In this notebook we'll give an implementation of . In this work, the existence of a Wasserstein barycentre between a collection of Gaussian processes is proven. When faced with trying to average a set of probability distributions, the Wasserstein barycentre is an attractive choice as it enables uncertainty amongst the individual distributions to be incorporated into the averaged distribution. When compared to a naive mean of means and mean of variances approach to computing the average probability distributions, it can be seen that Wasserstein barycentres offer significantly more favourable uncertainty estimation.</p>"},{"location":"examples/barycentres/#background","title":"Background\u00b6","text":""},{"location":"examples/barycentres/#wasserstein-distance","title":"Wasserstein distance\u00b6","text":"<p>The 2-Wasserstein distance metric between two probability measures $\\mu$ and $\\nu$ quantifies the minimal cost required to transport the unit mass from $\\mu$ to $\\nu$, or vice-versa. Typically, computing this metric requires solving a linear program. However, when $\\mu$ and $\\nu$ both belong to the family of multivariate Gaussian distributions, the solution is analytically given by $$W_2^2(\\mu, \\nu) = \\lVert m_1- m_2 \\rVert^2_2 + \\operatorname{Tr}(S_1 + S_2 - 2(S_1^{1/2}S_2S_1^{1/2})^{1/2}),$$ where $\\mu \\sim \\mathcal{N}(m_1, S_1)$ and $\\nu\\sim\\mathcal{N}(m_2, S_2)$.</p>"},{"location":"examples/barycentres/#wasserstein-barycentre","title":"Wasserstein barycentre\u00b6","text":"<p>For a collection of $T$ measures $\\lbrace\\mu_i\\rbrace_{t=1}^T \\in \\mathcal{P}_2(\\theta)$, the Wasserstein barycentre $\\bar{\\mu}$ is the measure that minimises the average Wasserstein distance to all other measures in the set. More formally, the Wasserstein barycentre is the Fr\u00e9chet mean on a Wasserstein space that we can write as $$\\bar{\\mu} = \\operatorname{argmin}_{\\mu\\in\\mathcal{P}_2(\\theta)}\\sum_{t=1}^T \\alpha_t W_2^2(\\mu, \\mu_t),$$ where $\\alpha\\in\\bbR^T$ is a weight vector that sums to 1.</p> <p>As with the Wasserstein distance, identifying the Wasserstein barycentre $\\bar{\\mu}$ is often an computationally demanding optimisation problem. However, when all the measures admit a multivariate Gaussian density, the barycentre $\\bar{\\mu} = \\mathcal{N}(\\bar{m}, \\bar{S})$ has analytical solutions $$\\bar{m} = \\sum_{t=1}^T \\alpha_t m_t\\,, \\quad \\bar{S}=\\sum_{t=1}^T\\alpha_t (\\bar{S}^{1/2}S_t\\bar{S}^{1/2})^{1/2}\\,. \\qquad (\\star)$$ Identifying $\\bar{S}$ is achieved through a fixed-point iterative update.</p>"},{"location":"examples/barycentres/#barycentre-of-gaussian-processes","title":"Barycentre of Gaussian processes\u00b6","text":"<p>It was shown in  that the barycentre $\\bar{f}$ of a collection of Gaussian processes $\\lbrace f_i\\rbrace_{i=1}^T$ such that $f_i \\sim \\mathcal{GP}(m_i, K_i)$ can be found using the same solutions as in $(\\star)$. For a full theoretical understanding, we recommend reading the original paper. However, the central argument to this result is that one can first show that the barycentre GP $\\bar{f}\\sim\\mathcal{GP}(\\bar{m}, \\bar{S})$ is non-degenerate for any finite set of GPs $\\lbrace f_t\\rbrace_{t=1}^T$ i.e., $T&lt;\\infty$. With this established, one can show that for a $n$-dimensional finite Gaussian distribution $f_{i,n}$, the Wasserstein metric between any two Gaussian distributions $f_{i, n}, f_{j, n}$ converges to the Wasserstein metric between GPs as $n\\to\\infty$.</p> <p>In this notebook, we will demonstrate how this can be achieved in GPJax.</p>"},{"location":"examples/barycentres/#dataset","title":"Dataset\u00b6","text":"<p>We'll simulate five datasets and develop a Gaussian process posterior before identifying the Gaussian process barycentre at a set of test points. Each dataset will be a sine function with a different vertical shift, periodicity, and quantity of noise.</p>"},{"location":"examples/barycentres/#learning-a-posterior-distribution","title":"Learning a posterior distribution\u00b6","text":"<p>We'll now independently learn Gaussian process posterior distributions for each dataset. We won't spend any time here discussing how GP hyperparameters are optimised. For advice on achieving this, see the Regression notebook for advice on optimisation and the Kernels notebook for advice on selecting an appropriate kernel.</p>"},{"location":"examples/barycentres/#computing-the-barycentre","title":"Computing the barycentre\u00b6","text":"<p>In GPJax, the predictive distribution of a GP is given by a TensorFlow Probability distribution, making it straightforward to extract the mean vector and covariance matrix of each GP for learning a barycentre. We implement the fixed point scheme given in (3) in the following cell by utilising Jax's <code>vmap</code> operator to speed up large matrix operations using broadcasting in <code>tensordot</code>.</p>"},{"location":"examples/barycentres/#plotting-the-result","title":"Plotting the result\u00b6","text":"<p>With a barycentre learned, we can visualise the result. We can see that the result looks reasonable as it follows the sinusoidal curve of all the inferred GPs, and the uncertainty bands are sensible.</p>"},{"location":"examples/barycentres/#displacement-interpolation","title":"Displacement interpolation\u00b6","text":"<p>In the above example, we assigned uniform weights to each of the posteriors within the barycentre. In practice, we may have prior knowledge of which posterior is most likely to be the correct one. Regardless of the weights chosen, the barycentre remains a Gaussian process. We can interpolate between a pair of posterior distributions $\\mu_1$ and $\\mu_2$ to visualise the corresponding barycentre $\\bar{\\mu}$.</p> <p></p>"},{"location":"examples/barycentres/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/classification/","title":"Classification","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom time import time\nimport blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nimport jax.tree_util as jtu\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom tqdm import trange\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\ntfd = tfp.distributions\nidentity_matrix = jnp.eye\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from time import time import blackjax import jax import jax.numpy as jnp import jax.random as jr import jax.scipy as jsp import jax.tree_util as jtu from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax as tfp from tqdm import trange  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  tfd = tfp.distributions identity_matrix = jnp.eye key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>key, subkey = jr.split(key)\nx = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0)\ny = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\n</pre> key, subkey = jr.split(key) x = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0) y = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)  fig, ax = plt.subplots() ax.scatter(x, y) In\u00a0[\u00a0]: Copied! <pre>kernel = gpx.RBF()\nmeanf = gpx.Constant()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Bernoulli(num_datapoints=D.n)\n</pre> kernel = gpx.RBF() meanf = gpx.Constant() prior = gpx.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.Bernoulli(num_datapoints=D.n) <p>We construct the posterior through the product of our prior and likelihood.</p> In\u00a0[\u00a0]: Copied! <pre>posterior = prior * likelihood\nprint(type(posterior))\n</pre> posterior = prior * likelihood print(type(posterior)) <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. We begin with maximum a posteriori (MAP) estimation, a fast inference procedure to obtain point estimates for the latent function and the kernel's hyperparameters by maximising the marginal log-likelihood.</p> <p>We can obtain a MAP estimate by optimising the log-posterior density with Optax's optimisers.</p> In\u00a0[\u00a0]: Copied! <pre>negative_lpd = jax.jit(gpx.LogPosteriorDensity(negative=True))\n\noptimiser = ox.adam(learning_rate=0.01)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_lpd,\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.01),\n    num_iters=1000,\n    key=key,\n)\n</pre> negative_lpd = jax.jit(gpx.LogPosteriorDensity(negative=True))  optimiser = ox.adam(learning_rate=0.01)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_lpd,     train_data=D,     optim=ox.adamw(learning_rate=0.01),     num_iters=1000,     key=key, ) <p>From which we can make predictions at novel inputs, as illustrated below.</p> In\u00a0[\u00a0]: Copied! <pre>map_latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(map_latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\n\nax.legend()\n</pre> map_latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(map_latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.2,     color=cols[1],     label=\"One sigma\", ) ax.plot(     xtest,     predictive_mean - predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, )  ax.legend() <p>Here we projected the map estimates $\\hat{\\boldsymbol{f}}$ for the function values $\\boldsymbol{f}$ at the data points $\\boldsymbol{x}$ to get predictions over the whole domain,</p> \\begin{align} p(f(\\cdot)| \\mathcal{D})  \\approx q_{map}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) \\delta(\\boldsymbol{f} - \\hat{\\boldsymbol{f}}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}  <p>However, as a point estimate, MAP estimation is severely limited for uncertainty quantification, providing only a single piece of information about the posterior.</p> In\u00a0[\u00a0]: Copied! <pre>gram, cross_covariance = (kernel.gram, kernel.cross_covariance)\njitter = 1e-6\n\n# Compute (latent) function value map estimates at training points:\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nLx = Kxx.to_root()\nf_hat = Lx @ opt_posterior.latent\n\n# Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f):\nH = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]\n\nL = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)\n\n# H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I\nL_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True)\nH_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\nLH = jnp.linalg.cholesky(H_inv)\nlaplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH)\n</pre> gram, cross_covariance = (kernel.gram, kernel.cross_covariance) jitter = 1e-6  # Compute (latent) function value map estimates at training points: Kxx = opt_posterior.prior.kernel.gram(x) Kxx += identity_matrix(D.n) * jitter Lx = Kxx.to_root() f_hat = Lx @ opt_posterior.latent  # Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f): H = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]  L = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)  # H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I L_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True) H_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False) LH = jnp.linalg.cholesky(H_inv) laplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH) <p>For novel inputs, we must project the above approximating distribution through the Gaussian conditional distribution $p(f(\\cdot)| \\boldsymbol{f})$,</p> \\begin{align} p(f(\\cdot)| \\mathcal{D}) \\approx q_{Laplace}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) q(\\boldsymbol{f}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} (\\mathbf{K}_{\\boldsymbol{xx}} - [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1}) \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}<p>This is the same approximate distribution $q_{map}(f(\\cdot))$, but we have perturbed the covariance by a curvature term of $\\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}$. We take the latent distribution computed in the previous section and add this term to the covariance to construct $q_{Laplace}(f(\\cdot))$.</p> In\u00a0[\u00a0]: Copied! <pre>def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:\n    map_latent_dist = opt_posterior.predict(xtest, train_data=D)\n\n    Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)\n    Kxx = opt_posterior.prior.kernel.gram(x)\n    Kxx += identity_matrix(D.n) * jitter\n    Lx = Kxx.to_root()\n\n    # Lx\u207b\u00b9 Kxt\n    Lx_inv_Ktx = Lx.solve(Kxt)\n\n    # Kxx\u207b\u00b9 Kxt\n    Kxx_inv_Ktx = Lx.T.solve(Lx_inv_Ktx)\n\n    # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt\n    laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Ktx.T, H_inv), Kxx_inv_Ktx)\n\n    mean = map_latent_dist.mean()\n    covariance = map_latent_dist.covariance() + laplace_cov_term\n    L = jnp.linalg.cholesky(covariance)\n    return tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L)\n</pre> def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:     map_latent_dist = opt_posterior.predict(xtest, train_data=D)      Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)     Kxx = opt_posterior.prior.kernel.gram(x)     Kxx += identity_matrix(D.n) * jitter     Lx = Kxx.to_root()      # Lx\u207b\u00b9 Kxt     Lx_inv_Ktx = Lx.solve(Kxt)      # Kxx\u207b\u00b9 Kxt     Kxx_inv_Ktx = Lx.T.solve(Lx_inv_Ktx)      # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt     laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Ktx.T, H_inv), Kxx_inv_Ktx)      mean = map_latent_dist.mean()     covariance = map_latent_dist.covariance() + laplace_cov_term     L = jnp.linalg.cholesky(covariance)     return tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L) <p>From this we can construct the predictive distribution at the test points.</p> In\u00a0[\u00a0]: Copied! <pre>laplace_latent_dist = construct_laplace(xtest)\npredictive_dist = opt_posterior.likelihood(laplace_latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> laplace_latent_dist = construct_laplace(xtest) predictive_dist = opt_posterior.likelihood(laplace_latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.2,     color=cols[1],     label=\"One sigma\", ) ax.plot(     xtest,     predictive_mean - predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() <p>However, the Laplace approximation is still limited by considering information about the posterior at a single location. On the other hand, through approximate sampling, MCMC methods allow us to learn all information about the posterior distribution.</p> In\u00a0[\u00a0]: Copied! <pre>num_adapt = 500\nnum_samples = 500\n\nlpd = jax.jit(gpx.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n\n# Initialise the chain\nstart = time()\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\nprint(f\"Adaption time taken: {time() - start: .1f} seconds\")\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Sample from the posterior distribution\nstart = time()\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nprint(f\"Sampling time taken: {time() - start: .1f} seconds\")\n</pre> num_adapt = 500 num_samples = 500  lpd = jax.jit(gpx.LogPosteriorDensity(negative=False)) unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))  adapt = blackjax.window_adaptation(     blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65 )  # Initialise the chain start = time() last_state, kernel, _ = adapt.run(key, posterior.unconstrain()) print(f\"Adaption time taken: {time() - start: .1f} seconds\")   def inference_loop(rng_key, kernel, initial_state, num_samples):     def one_step(state, rng_key):         state, info = kernel(rng_key, state)         return state, (state, info)      keys = jax.random.split(rng_key, num_samples)     _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)      return states, infos   # Sample from the posterior distribution start = time() states, infos = inference_loop(key, kernel, last_state, num_samples) print(f\"Sampling time taken: {time() - start: .1f} seconds\") In\u00a0[\u00a0]: Copied! <pre>acceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</pre> acceptance_rate = jnp.mean(infos.acceptance_probability) print(f\"Acceptance rate: {acceptance_rate:.2f}\") <p>Our acceptance rate is slightly too large, prompting an examination of the chain's trace plots. A well-mixing chain will have very few (if any) flat spots in its trace plot whilst also not having too many steps in the same direction. In addition to the model's hyperparameters, there will be 500 samples for each of the 100 latent function values in the <code>states.position</code> dictionary. We depict the chains that correspond to the model hyperparameters and the first value of the latent function for brevity.</p> In\u00a0[\u00a0]: Copied! <pre>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.prior.kernel.lengthscale)\nax1.plot(states.position.prior.kernel.variance)\nax2.plot(states.position.latent[:, 1, :])\nax0.set_title(\"Kernel Lengthscale\")\nax1.set_title(\"Kernel Variance\")\nax2.set_title(\"Latent Function (index = 1)\")\n</pre> fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3)) ax0.plot(states.position.prior.kernel.lengthscale) ax1.plot(states.position.prior.kernel.variance) ax2.plot(states.position.latent[:, 1, :]) ax0.set_title(\"Kernel Lengthscale\") ax1.set_title(\"Kernel Variance\") ax2.set_title(\"Latent Function (index = 1)\") In\u00a0[\u00a0]: Copied! <pre>thin_factor = 20\nposterior_samples = []\n\nfor i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):\n    sample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)\n    sample = sample.constrain()\n    latent_dist = sample.predict(xtest, train_data=D)\n    predictive_dist = sample.likelihood(latent_dist)\n    posterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n\nposterior_samples = jnp.vstack(posterior_samples)\nlower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(posterior_samples, axis=0)\n</pre> thin_factor = 20 posterior_samples = []  for i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):     sample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)     sample = sample.constrain()     latent_dist = sample.predict(xtest, train_data=D)     predictive_dist = sample.likelihood(latent_dist)     posterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))  posterior_samples = jnp.vstack(posterior_samples) lower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0) expected_val = jnp.mean(posterior_samples, axis=0) <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\nax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7)\nax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=cols[1],\n    label=\"95\\\\% CI\",\n)\nax.plot(\n    xtest,\n    lower_ci.flatten(),\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    upper_ci.flatten(),\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> fig, ax = plt.subplots() ax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7) ax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1) ax.fill_between(     xtest.flatten(),     lower_ci.flatten(),     upper_ci.flatten(),     alpha=0.2,     color=cols[1],     label=\"95\\\\% CI\", ) ax.plot(     xtest,     lower_ci.flatten(),     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     upper_ci.flatten(),     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() In\u00a0[\u00a0]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\"\n</pre> %load_ext watermark %watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\""},{"location":"examples/classification/#classification","title":"Classification\u00b6","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via maximum a posteriori (MAP) and Markov chain Monte Carlo (MCMC). We focus on a classification task here and use BlackJax for sampling.</p>"},{"location":"examples/classification/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-1., 1)$ and corresponding binary outputs</p> $$\\boldsymbol{y} = 0.5 * \\text{sign}(\\cos(2 *  + \\boldsymbol{\\epsilon})) + 0.5, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} * (0.05)^{2} \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs for later.</p>"},{"location":"examples/classification/#map-inference","title":"MAP inference\u00b6","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. Since our observations are binary, we choose a Bernoulli likelihood with a probit link function.</p>"},{"location":"examples/classification/#laplace-approximation","title":"Laplace approximation\u00b6","text":"<p>The Laplace approximation improves uncertainty quantification by incorporating curvature induced by the marginal log-likelihood's Hessian to construct an approximate Gaussian distribution centered on the MAP estimate. Writing $\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})$ as the unormalised posterior for function values $\\boldsymbol{f}$ at the datapoints $\\boldsymbol{x}$, we can expand the log of this about the posterior mode $\\hat{\\boldsymbol{f}}$ via a Taylor expansion. This gives:</p> \\begin{align} \\log\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) + \\left[\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})|_{\\hat{\\boldsymbol{f}}}\\right]^{T} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\mathcal{O}(\\lVert \\boldsymbol{f} - \\hat{\\boldsymbol{f}} \\rVert^3). \\end{align}<p>Since $\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})$ is zero at the mode, this suggests the following approximation \\begin{align} \\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) \\approx \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) \\exp\\left\\{ \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) \\right\\} \\end{align},</p> <p>that we identify as a Gaussian distribution, $p(\\boldsymbol{f}| \\mathcal{D}) \\approx q(\\boldsymbol{f}) := \\mathcal{N}(\\hat{\\boldsymbol{f}}, [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} )$. Since the negative Hessian is positive definite, we can use the Cholesky decomposition to obtain the covariance matrix of the Laplace approximation at the datapoints below.</p>"},{"location":"examples/classification/#mcmc-inference","title":"MCMC inference\u00b6","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"examples/classification/#mcmc-through-blackjax","title":"MCMC through BlackJax\u00b6","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We'll use the No U-Turn Sampler (NUTS) implementation given in BlackJax for sampling. For the interested reader, NUTS is a Hamiltonian Monte Carlo sampling scheme where the number of leapfrog integration steps is computed at each step of the change according to the NUTS algorithm. In general, samplers constructed under this framework are very efficient.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 500 values from our Markov chain. In practice, drawing more samples will be necessary.</p>"},{"location":"examples/classification/#sampler-efficiency","title":"Sampler efficiency\u00b6","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain). For NUTS and Hamiltonian Monte Carlo sampling, we typically seek an acceptance rate of 60-70% to strike the right balance between having a chain which is stuck and rarely moves versus a chain that is too jumpy with frequent small steps.</p>"},{"location":"examples/classification/#prediction","title":"Prediction\u00b6","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every $n$th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>"},{"location":"examples/classification/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/collapsed_vi/","title":"Sparse Gaussian Process Regression","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>n = 2500\nnoise = 0.5\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 2500 noise = 0.5  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise. We also plot an initial set of inducing points over the space.</p> In\u00a0[\u00a0]: Copied! <pre>n_inducing = 50\nz = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\n[\n    ax.axvline(x=z_i, alpha=0.3, linewidth=0.5, label=\"Inducing point\", color=cols[2])\n    for z_i in z\n]\nax.legend(loc=\"best\")\nax = clean_legend(ax)\nplt.show()\n</pre> n_inducing = 50 z = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1]) [     ax.axvline(x=z_i, alpha=0.3, linewidth=0.5, label=\"Inducing point\", color=cols[2])     for z_i in z ] ax.legend(loc=\"best\") ax = clean_legend(ax) plt.show() <p>Next we define the true posterior model for the data - note that whilst we can define this, it is intractable to evaluate.</p> In\u00a0[\u00a0]: Copied! <pre>meanf = gpx.Constant()\nkernel = gpx.RBF()\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior * likelihood\n</pre> meanf = gpx.Constant() kernel = gpx.RBF() likelihood = gpx.Gaussian(num_datapoints=D.n) prior = gpx.Prior(mean_function=meanf, kernel=kernel) posterior = prior * likelihood <p>We now define the SGPR model through <code>CollapsedVariationalGaussian</code>. Through a set of inducing points $\\boldsymbol{z}$ this object builds an approximation to the true posterior distribution. Consequently, we pass the true posterior and initial inducing points into the constructor as arguments.</p> In\u00a0[\u00a0]: Copied! <pre>q = gpx.CollapsedVariationalGaussian(posterior=posterior, inducing_inputs=z)\n</pre> q = gpx.CollapsedVariationalGaussian(posterior=posterior, inducing_inputs=z) <p>We define our variational inference algorithm through <code>CollapsedVI</code>. This defines the collapsed variational free energy bound considered in Titsias (2009).</p> In\u00a0[\u00a0]: Copied! <pre>elbo = jit(gpx.CollapsedELBO(negative=True))\n</pre> elbo = jit(gpx.CollapsedELBO(negative=True)) <p>We now train our model akin to a Gaussian process regression model via the <code>fit</code> abstraction. Unlike the regression example given in the conjugate regression notebook, the inducing locations that induce our variational posterior distribution are now part of the model's parameters. Using a gradient-based optimiser, we can then optimise their location such that the evidence lower bound is maximised.</p> In\u00a0[\u00a0]: Copied! <pre>opt_posterior, history = gpx.fit(\n    model=q,\n    objective=elbo,\n    train_data=D,\n    optim=ox.adamw(learning_rate=1e-2),\n    num_iters=500,\n    key=key,\n)\n</pre> opt_posterior, history = gpx.fit(     model=q,     objective=elbo,     train_data=D,     optim=ox.adamw(learning_rate=1e-2),     num_iters=500,     key=key, ) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")\n</pre> fig, ax = plt.subplots() ax.plot(history, color=cols[1]) ax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\") <p>We show predictions of our model with the learned inducing points overlaid in grey.</p> In\u00a0[\u00a0]: Copied! <pre>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\ninducing_points = opt_posterior.inducing_inputs\n\nsamples = latent_dist.sample(seed=key, sample_shape=(20,))\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\n\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\nax.plot(\n    xtest,\n    ytest,\n    label=\"Latent function\",\n    color=cols[1],\n    linestyle=\"-\",\n    linewidth=1,\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\n\n[\n    ax.axvline(x=z_i, alpha=0.3, linewidth=0.5, label=\"Inducing point\", color=cols[2])\n    for z_i in inducing_points\n]\nax.legend()\nax = clean_legend(ax)\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nplt.show()\n</pre> latent_dist = opt_posterior(xtest, train_data=D) predictive_dist = opt_posterior.posterior.likelihood(latent_dist)  inducing_points = opt_posterior.inducing_inputs  samples = latent_dist.sample(seed=key, sample_shape=(20,))  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots()  ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1) ax.plot(     xtest,     ytest,     label=\"Latent function\",     color=cols[1],     linestyle=\"-\",     linewidth=1, ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])  ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     color=cols[1],     label=\"Two sigma\", ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=0.5, ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=0.5, )  [     ax.axvline(x=z_i, alpha=0.3, linewidth=0.5, label=\"Inducing point\", color=cols[2])     for z_i in inducing_points ] ax.legend() ax = clean_legend(ax) ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\") plt.show() In\u00a0[\u00a0]: Copied! <pre>full_rank_model = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.RBF()) * gpx.Gaussian(\n    num_datapoints=D.n\n)\nnegative_mll = jit(gpx.ConjugateMLL(negative=True))\n%timeit negative_mll(full_rank_model, D).block_until_ready()\n</pre> full_rank_model = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.RBF()) * gpx.Gaussian(     num_datapoints=D.n ) negative_mll = jit(gpx.ConjugateMLL(negative=True)) %timeit negative_mll(full_rank_model, D).block_until_ready() In\u00a0[\u00a0]: Copied! <pre>negative_elbo = jit(gpx.CollapsedELBO(negative=True))\n%timeit negative_elbo(q, D).block_until_ready()\n</pre> negative_elbo = jit(gpx.CollapsedELBO(negative=True)) %timeit negative_elbo(q, D).block_until_ready() <p>As we can see, the sparse approximation given here is around 50 times faster when compared against a full-rank model.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Daniel Dodd'"},{"location":"examples/collapsed_vi/#sparse-gaussian-process-regression","title":"Sparse Gaussian Process Regression\u00b6","text":"<p>In this notebook we consider sparse Gaussian process regression (SGPR) Titsias (2009). This is a solution for medium to large-scale conjugate regression problems. In order to arrive at a computationally tractable method, the approximate posterior is parameterized via a set of $m$ pseudo-points $\\boldsymbol{z}$. Critically, the approach leads to $\\mathcal{O}(nm^2)$ complexity for approximate maximum likelihood learning and $O(m^2)$ per test point for prediction.</p>"},{"location":"examples/collapsed_vi/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{500}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(7\\boldsymbol{x}) + x \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.5^2 \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/collapsed_vi/#runtime-comparison","title":"Runtime comparison\u00b6","text":"<p>Given the size of the data being considered here, inference in a GP with a full-rank covariance matrix is possible, albeit quite slow. We can therefore compare the speedup that we get from using the above sparse approximation with corresponding bound on the marginal log-likelihood against the marginal log-likelihood in the full model.</p>"},{"location":"examples/collapsed_vi/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/deep_kernels/","title":"Deep Kernel Learning","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import (\n    dataclass,\n    field,\n)\nfrom typing import Any\n\nimport flax\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom scipy.signal import sawtooth\nfrom simple_pytree import static_field\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base import param_field\n    import gpjax.kernels as jk\n    from gpjax.kernels import DenseKernelComputation\n    from gpjax.kernels.base import AbstractKernel\n    from gpjax.kernels.computations import AbstractKernelComputation\n\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from dataclasses import (     dataclass,     field, ) from typing import Any  import flax from flax import linen as nn import jax import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from scipy.signal import sawtooth from simple_pytree import static_field  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base import param_field     import gpjax.kernels as jk     from gpjax.kernels import DenseKernelComputation     from gpjax.kernels.base import AbstractKernel     from gpjax.kernels.computations import AbstractKernelComputation  key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>n = 500\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nytest = f(xtest)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\nax.plot(xtest, ytest, label=\"True function\")\nax.legend(loc=\"best\")\n</pre> n = 500 noise = 0.2  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x)) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1) ytest = f(xtest)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5) ax.plot(xtest, ytest, label=\"True function\") ax.legend(loc=\"best\") In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass DeepKernelFunction(AbstractKernel):\n    base_kernel: AbstractKernel = None\n    network: nn.Module = static_field(None)\n    dummy_x: jax.Array = static_field(None)\n    key: jr.PRNGKeyArray = static_field(jr.PRNGKey(123))\n    nn_params: Any = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if self.base_kernel is None:\n            raise ValueError(\"base_kernel must be specified\")\n        if self.network is None:\n            raise ValueError(\"network must be specified\")\n        self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))\n\n    def __call__(\n        self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        state = self.network.init(self.key, x)\n        xt = self.network.apply(state, x)\n        yt = self.network.apply(state, y)\n        return self.base_kernel(xt, yt)\n</pre> @dataclass class DeepKernelFunction(AbstractKernel):     base_kernel: AbstractKernel = None     network: nn.Module = static_field(None)     dummy_x: jax.Array = static_field(None)     key: jr.PRNGKeyArray = static_field(jr.PRNGKey(123))     nn_params: Any = field(init=False, repr=False)      def __post_init__(self):         if self.base_kernel is None:             raise ValueError(\"base_kernel must be specified\")         if self.network is None:             raise ValueError(\"network must be specified\")         self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))      def __call__(         self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]     ) -&gt; Float[Array, \"1\"]:         state = self.network.init(self.key, x)         xt = self.network.apply(state, x)         yt = self.network.apply(state, y)         return self.base_kernel(xt, yt) In\u00a0[\u00a0]: Copied! <pre>feature_space_dim = 3\n\n\nclass Network(nn.Module):\n\"\"\"A simple MLP.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=32)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=64)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=feature_space_dim)(x)\n        return x\n\n\nforward_linear = Network()\n</pre> feature_space_dim = 3   class Network(nn.Module):     \"\"\"A simple MLP.\"\"\"      @nn.compact     def __call__(self, x):         x = nn.Dense(features=32)(x)         x = nn.relu(x)         x = nn.Dense(features=64)(x)         x = nn.relu(x)         x = nn.Dense(features=feature_space_dim)(x)         return x   forward_linear = Network() In\u00a0[\u00a0]: Copied! <pre>base_kernel = gpx.Matern52(active_dims=list(range(feature_space_dim)))\nkernel = DeepKernelFunction(\n    network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x\n)\nmeanf = gpx.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n</pre> base_kernel = gpx.Matern52(active_dims=list(range(feature_space_dim))) kernel = DeepKernelFunction(     network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x ) meanf = gpx.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.Gaussian(num_datapoints=D.n) posterior = prior * likelihood In\u00a0[\u00a0]: Copied! <pre>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.01,\n    warmup_steps=75,\n    decay_steps=700,\n    end_value=0.0,\n)\n\noptimiser = ox.chain(\n    ox.clip(1.0),\n    ox.adamw(learning_rate=schedule),\n)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=jax.jit(gpx.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=optimiser,\n    num_iters=800,\n    key=key,\n)\n</pre> schedule = ox.warmup_cosine_decay_schedule(     init_value=0.0,     peak_value=0.01,     warmup_steps=75,     decay_steps=700,     end_value=0.0, )  optimiser = ox.chain(     ox.clip(1.0),     ox.adamw(learning_rate=schedule), )  opt_posterior, history = gpx.fit(     model=posterior,     objective=jax.jit(gpx.ConjugateMLL(negative=True)),     train_data=D,     optim=optimiser,     num_iters=800,     key=key, ) In\u00a0[\u00a0]: Copied! <pre>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> latent_dist = opt_posterior(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     color=cols[1],     label=\"Two sigma\", ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder'"},{"location":"examples/deep_kernels/#deep-kernel-learning","title":"Deep Kernel Learning\u00b6","text":"<p>In this notebook we demonstrate how GPJax can be used in conjunction with Flax to build deep kernel Gaussian processes. Modelling data with discontinuities is a challenging task for regular Gaussian process models. However, as shown in , transforming the inputs to our Gaussian process model's kernel through a neural network can offer a solution to this.</p>"},{"location":"examples/deep_kernels/#dataset","title":"Dataset\u00b6","text":"<p>As previously mentioned, deep kernels are particularly useful when the data has discontinuities. To highlight this, we will use a sawtooth function as our data.</p>"},{"location":"examples/deep_kernels/#deep-kernels","title":"Deep kernels\u00b6","text":""},{"location":"examples/deep_kernels/#details","title":"Details\u00b6","text":"<p>Instead of applying a kernel $k(\\cdot, \\cdot')$ directly on some data, we seek to apply a feature map $\\phi(\\cdot)$ that projects the data to learn more meaningful representations beforehand. In deep kernel learning, $\\phi$ is a neural network whose parameters are learned jointly with the GP model's hyperparameters. The corresponding kernel is then computed by $k(\\phi(\\cdot), \\phi(\\cdot'))$. Here $k(\\cdot,\\cdot')$ is referred to as the base kernel.</p>"},{"location":"examples/deep_kernels/#implementation","title":"Implementation\u00b6","text":"<p>Although deep kernels are not currently supported natively in GPJax, defining one is straightforward as we now demonstrate. Inheriting from the base <code>AbstractKernel</code> in GPJax, we create the <code>DeepKernelFunction</code> object that allows the user to supply the neural network and base kernel of their choice. Kernel matrices are then computed using the regular <code>gram</code> and <code>cross_covariance</code> functions.</p>"},{"location":"examples/deep_kernels/#defining-a-network","title":"Defining a network\u00b6","text":"<p>With a deep kernel object created, we proceed to define a neural network. Here we consider a small multi-layer perceptron with two linear hidden layers and ReLU activation functions between the layers. The first hidden layer contains 64 units, while the second layer contains 32 units. Finally, we'll make the output of our network a three units wide. The corresponding kernel that we define will then be of ARD form to allow for different lengthscales in each dimension of the feature space. Users may wish to design more intricate network structures for more complex tasks, which functionality is supported well in Haiku.</p>"},{"location":"examples/deep_kernels/#defining-a-model","title":"Defining a model\u00b6","text":"<p>Having characterised the feature extraction network, we move to define a Gaussian process parameterised by this deep kernel. We consider a third-order Mat\u00e9rn base kernel and assume a Gaussian likelihood.</p>"},{"location":"examples/deep_kernels/#optimisation","title":"Optimisation\u00b6","text":"<p>We train our model via maximum likelihood estimation of the marginal log-likelihood. The parameters of our neural network are learned jointly with the model's hyperparameter set.</p> <p>With the inclusion of a neural network, we take this opportunity to highlight the additional benefits gleaned from using Optax for optimisation. In particular, we showcase the ability to use a learning rate scheduler that decays the optimiser's learning rate throughout the inference. We decrease the learning rate according to a half-cosine curve over 700 iterations, providing us with large step sizes early in the optimisation procedure before approaching more conservative values, ensuring we do not step too far. We also consider a linear warmup, where the learning rate is increased from 0 to 1 over 50 steps to get a reasonable initial learning rate value.</p>"},{"location":"examples/deep_kernels/#prediction","title":"Prediction\u00b6","text":"<p>With a set of learned parameters, the only remaining task is to predict the output of the model. We can do this by simply applying the model to a test data set.</p>"},{"location":"examples/deep_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/graph_kernels/","title":"Graph Kernels","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport random\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport optax as ox\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  import random  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import networkx as nx import optax as ox  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>vertex_per_side = 20\nn_edges_to_remove = 30\np = 0.8\n\nG = nx.barbell_graph(vertex_per_side, 0)\n\nrandom.seed(123)\n[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\n\npos = nx.spring_layout(G, seed=123)  # positions for all nodes\n\nnx.draw(\n    G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n)\n</pre> vertex_per_side = 20 n_edges_to_remove = 30 p = 0.8  G = nx.barbell_graph(vertex_per_side, 0)  random.seed(123) [G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]  pos = nx.spring_layout(G, seed=123)  # positions for all nodes  nx.draw(     G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False ) In\u00a0[\u00a0]: Copied! <pre>L = nx.laplacian_matrix(G).toarray()\n</pre> L = nx.laplacian_matrix(G).toarray() In\u00a0[\u00a0]: Copied! <pre>x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\n\ntrue_kernel = gpx.GraphKernel(\n    laplacian=L,\n    lengthscale=2.3,\n    variance=3.2,\n    smoothness=6.1,\n)\nprior = gpx.Prior(mean_function=gpx.Zero(), kernel=true_kernel)\n\nfx = prior(x)\ny = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)\n\nD = gpx.Dataset(X=x, y=y)\n</pre> x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)  true_kernel = gpx.GraphKernel(     laplacian=L,     lengthscale=2.3,     variance=3.2,     smoothness=6.1, ) prior = gpx.Prior(mean_function=gpx.Zero(), kernel=true_kernel)  fx = prior(x) y = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)  D = gpx.Dataset(X=x, y=y) <p>We can visualise this signal in the following cell.</p> In\u00a0[\u00a0]: Copied! <pre>nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\n\nvmin, vmax = y.min(), y.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\ncbar = plt.colorbar(sm)\n</pre> nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)  vmin, vmax = y.min(), y.max() sm = plt.cm.ScalarMappable(     cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax) ) sm.set_array([]) cbar = plt.colorbar(sm) In\u00a0[\u00a0]: Copied! <pre>likelihood = gpx.Gaussian(num_datapoints=D.n)\nprior = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.GraphKernel(laplacian=L))\nposterior = prior * likelihood\n\nopt_posterior, training_history = gpx.fit(\n    model=posterior,\n    objective=jit(gpx.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.01),\n    num_iters=1000,\n    key=key,\n)\n</pre> likelihood = gpx.Gaussian(num_datapoints=D.n) prior = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.GraphKernel(laplacian=L)) posterior = prior * likelihood  opt_posterior, training_history = gpx.fit(     model=posterior,     objective=jit(gpx.ConjugateMLL(negative=True)),     train_data=D,     optim=ox.adamw(learning_rate=0.01),     num_iters=1000,     key=key, ) In\u00a0[\u00a0]: Copied! <pre>initial_dist = likelihood(posterior(x, D))\npredictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\n\ninitial_mean = initial_dist.mean()\nlearned_mean = predictive_dist.mean()\n\nrmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\n\ninitial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\nlearned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\nprint(\n    f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\n    f\" {learned_rmse: .2f}\"\n)\n</pre> initial_dist = likelihood(posterior(x, D)) predictive_dist = opt_posterior.likelihood(opt_posterior(x, D))  initial_mean = initial_dist.mean() learned_mean = predictive_dist.mean()  rmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))  initial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean))) learned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean))) print(     f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"     f\" {learned_rmse: .2f}\" ) <p>We can also plot the source of error in our model's predictions on the graph by the following.</p> In\u00a0[\u00a0]: Copied! <pre>error = jnp.abs(learned_mean - y.squeeze())\n\nnx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\n\nvmin, vmax = error.min(), error.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\ncbar = plt.colorbar(sm)\n</pre> error = jnp.abs(learned_mean - y.squeeze())  nx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)  vmin, vmax = error.min(), error.max() sm = plt.cm.ScalarMappable(     cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax) ) sm.set_array([]) cbar = plt.colorbar(sm) <p>Reassuringly, our model seems to provide equally good predictions in each cluster.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder'"},{"location":"examples/graph_kernels/#graph-kernels","title":"Graph Kernels\u00b6","text":"<p>This notebook demonstrates how regression models can be constructed on the vertices of a graph using a Gaussian process with a Mat\u00e9rn kernel presented in . For a general discussion of the kernels supported within GPJax, see the kernels notebook.</p>"},{"location":"examples/graph_kernels/#graph-construction","title":"Graph construction\u00b6","text":"<p>Our graph $\\mathcal{G}=\\lbrace V, E \\rbrace$ comprises a set of vertices $V = \\lbrace v_1, v_2, \\ldots, v_n\\rbrace$ and edges $E=\\lbrace (v_i, v_j)\\in V \\ : \\ i \\neq j\\rbrace$. In particular, we will consider a barbell graph that is an undirected graph containing two clusters of vertices with a single shared edge between the two clusters.</p> <p>Contrary to the typical barbell graph, we'll randomly remove a subset of 30 edges within each of the two clusters. Given the 40 vertices within the graph, this results in 351 edges as shown below.</p>"},{"location":"examples/graph_kernels/#computing-the-graph-laplacian","title":"Computing the graph Laplacian\u00b6","text":"<p>Graph kernels use the Laplacian matrix $L$ to quantify the smoothness of a signal (or function) on a graph $$L=D-A,$$ where $D$ is the diagonal degree matrix containing each vertices' degree and $A$ is the adjacency matrix that has an $(i,j)^{\\text{th}}$ entry of 1 if $v_i, v_j$ are connected and 0 otherwise. Networkx gives us an easy way to compute this.</p>"},{"location":"examples/graph_kernels/#simulating-a-signal-on-the-graph","title":"Simulating a signal on the graph\u00b6","text":"<p>Our task is to construct a Gaussian process $f(\\cdot)$ that maps from the graph's vertex set $V$ onto the real line. To that end, we begin by simulating a signal on the graph's vertices that we will go on to try and predict. We use a single draw from a Gaussian process prior to draw our response values $\\boldsymbol{y}$ where we hardcode parameter values. The corresponding input value set for this model, denoted $\\boldsymbol{x}$, is the index set of the graph's vertices.</p>"},{"location":"examples/graph_kernels/#constructing-a-graph-gaussian-process","title":"Constructing a graph Gaussian process\u00b6","text":"<p>With our dataset created, we proceed to define our posterior Gaussian process and optimise the model's hyperparameters. Whilst our underlying space is the graph's vertex set and is therefore non-Euclidean, our likelihood is still Gaussian and the model is still conjugate. For this reason, we simply perform gradient descent on the GP's marginal log-likelihood term as in the regression notebook. We do this using the Adam optimiser provided in <code>optax</code>.</p>"},{"location":"examples/graph_kernels/#making-predictions","title":"Making predictions\u00b6","text":"<p>Having optimised our hyperparameters, we can now make predictions on the graph. Though we haven't defined a training and testing dataset here, we'll simply query the predictive posterior for the full graph to compare the root-mean-squared error (RMSE) of the model for the initialised parameters vs the optimised set.</p>"},{"location":"examples/graph_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/intro_to_gps/","title":"New to Gaussian Processes?","text":"<p>A Gaussian random variable is uniquely defined in distribution by its mean $\\mu$ and variance $\\sigma^2$ and we therefore write $y\\sim\\mathcal{N}(\\mu, \\sigma^2)$ when describing a Gaussian random variable. We can compute these two quantities by $$ \\begin{align}     \\mathbb{E}[y] = \\mu\\,, \\quad \\quad \\mathbb{E}\\left[(y-\\mu)^2\\right] =\\sigma^2\\,. \\end{align} $$ Extending this concept to vector-valued random variables reveals the multivariate Gaussian random variables which brings us closer to the full definition of a GP.</p> <p>Let $\\mathbf{y}$ be a $D$-dimensional random variable, $\\boldsymbol{\\mu}$ be a $D$-dimensional mean vector and $\\boldsymbol{\\Sigma}$ be a $D\\times D$ covariance matrix. If $\\mathbf{y}$ is a Gaussian random variable, then the density of $\\mathbf{y}$ is $$ \\begin{align}     \\mathcal{N}(\\mathbf{y}\\,|\\, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2\\pi}^{D/2} \\lvert\\boldsymbol{\\Sigma}\\rvert^{1/2}} \\exp\\left(-\\frac{1}{2} \\left(\\mathbf{y} - \\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{y}-\\boldsymbol{\\mu}\\right) \\right) \\,. \\end{align} $$ Three example parameterisations of this can be visualised below where $\\rho$ determines the correlation of the multivariate Gaussian.</p> In\u00a0[\u00a0]: Copied! <pre>key = jr.PRNGKey(123)\n\nd1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2))\nd2 = tfd.MultivariateNormalTriL(\n    jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]]))\n)\nd3 = tfd.MultivariateNormalTriL(\n    jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]]))\n)\n\ndists = [d1, d2, d3]\n\nxvals = jnp.linspace(-5.0, 5.0, 500)\nyvals = jnp.linspace(-5.0, 5.0, 500)\n\nxx, yy = jnp.meshgrid(xvals, yvals)\n\npos = jnp.empty(xx.shape + (2,))\npos.at[:, :, 0].set(xx)\npos.at[:, :, 1].set(yy)\n\nfig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\ntitles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\n\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)\n\nfor a, t, d in zip([ax0, ax1, ax2], titles, dists):\n    d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(\n        xx.shape\n    )\n    cntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)\n    for c in cntf.collections:\n        c.set_edgecolor(\"face\")\n    a.set_xlim(-2.75, 2.75)\n    a.set_ylim(-2.75, 2.75)\n    samples = d.sample(seed=key, sample_shape=(5000,))\n    xsample, ysample = samples[:, 0], samples[:, 1]\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n    )\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"\n    )\n    a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)\n    a.set(xlabel=\"x\", ylabel=\"y\", title=t)\n</pre> key = jr.PRNGKey(123)  d1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2)) d2 = tfd.MultivariateNormalTriL(     jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]])) ) d3 = tfd.MultivariateNormalTriL(     jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]])) )  dists = [d1, d2, d3]  xvals = jnp.linspace(-5.0, 5.0, 500) yvals = jnp.linspace(-5.0, 5.0, 500)  xx, yy = jnp.meshgrid(xvals, yvals)  pos = jnp.empty(xx.shape + (2,)) pos.at[:, :, 0].set(xx) pos.at[:, :, 1].set(yy)  fig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True) titles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]  cmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)  for a, t, d in zip([ax0, ax1, ax2], titles, dists):     d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(         xx.shape     )     cntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)     for c in cntf.collections:         c.set_edgecolor(\"face\")     a.set_xlim(-2.75, 2.75)     a.set_ylim(-2.75, 2.75)     samples = d.sample(seed=key, sample_shape=(5000,))     xsample, ysample = samples[:, 0], samples[:, 1]     confidence_ellipse(         xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8     )     confidence_ellipse(         xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"     )     a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)     a.set(xlabel=\"x\", ylabel=\"y\", title=t) <p>Extending the intuition given for the moments of a univariate Gaussian random variables, we can obtain the mean and covariance by $$ \\begin{align}     &amp; = \\boldsymbol{\\Sigma}\\,. \\end{align} $$ The covariance matrix is a symmetric positive definite matrix that generalises the notion of variance to multiple dimensions. The matrix's diagonal entries contain the variance of each element, whilst the off-diagonal entries quantify the degree to which the respective pair of random variables are linearly related; this quantity is called the covariance.</p> <p>Assuming a Gaussian likelihood function in a Bayesian model is attractive as the mean and variance parameters are highly interpretable. This makes prior elicitation straightforward as the parameters' value can be intuitively contextualised within the scope of the problem at hand. Further, in models where the posterior distribution is Gaussian, we again use the distribution's mean and variance to describe our prediction and corresponding uncertainty around a given event occurring.</p> <p>Not only are Gaussian random variables highly interpretable, but linear operations involving them lead to analytical solutions. An example of this that will be useful in the sequel is the marginalisation and conditioning property of sets of Gaussian random variables. We will present these two results now for a pair of Gaussian random variables, but it should be stressed that these results hold for any finite set of Gaussian random variables.</p> <p>For a pair of random variables $\\mathbf{x}$ and $\\mathbf{y}$ defined on the same support, the distribution over them both is known as the joint distribution. The joint distribution $p(\\mathbf{x}, \\mathbf{y})$ quantifies the probability of two events, one from $p(\\mathbf{x})$ and another from $p(\\mathbf{y})$, occurring at the same time. We visualise this idea below.</p> In\u00a0[\u00a0]: Copied! <pre>n = 1000\nx = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,))\nkey, subkey = jr.split(key)\ny = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,))\nkey, subkey = jr.split(subkey)\nxfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\nyfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\ndf = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    g = sns.jointplot(\n        data=df,\n        x=\"x\",\n        y=\"y\",\n        hue=\"idx\",\n        marker=\".\",\n        space=0.0,\n        xlim=(-4.0, 4.0),\n        ylim=(-4.0, 4.0),\n        height=4,\n        marginal_ticks=False,\n        legend=False,\n        palette=\"inferno\",\n        marginal_kws={\n            \"fill\": True,\n            \"linewidth\": 1,\n            \"color\": cols[1],\n            \"alpha\": 0.3,\n            \"bw_adjust\": 2,\n            \"cmap\": cmap,\n        },\n        joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},\n    )\n    g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\n    g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\n    g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=1.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=2.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=3.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n</pre> n = 1000 x = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,)) key, subkey = jr.split(key) y = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,)) key, subkey = jr.split(subkey) xfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,)) key, subkey = jr.split(subkey) yfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,)) key, subkey = jr.split(subkey) df = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})  with warnings.catch_warnings():     warnings.simplefilter(\"ignore\")     g = sns.jointplot(         data=df,         x=\"x\",         y=\"y\",         hue=\"idx\",         marker=\".\",         space=0.0,         xlim=(-4.0, 4.0),         ylim=(-4.0, 4.0),         height=4,         marginal_ticks=False,         legend=False,         palette=\"inferno\",         marginal_kws={             \"fill\": True,             \"linewidth\": 1,             \"color\": cols[1],             \"alpha\": 0.3,             \"bw_adjust\": 2,             \"cmap\": cmap,         },         joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},     )     g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))     g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))     g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=1.0,         linestyle=\"--\",         linewidth=0.5,     )     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=2.0,         linestyle=\"--\",         linewidth=0.5,     )     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=3.0,         linestyle=\"--\",         linewidth=0.5,     ) <p>Formmally, we can define this by letting $p(\\mathbf{x}, \\mathbf{y})$ be the joint probability distribution defined over $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}}, \\boldsymbol{\\Sigma}_{\\mathbf{xx}})$ and $\\mathbf{y}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}})$. We define the joint distribution as $$ \\begin{align}     p\\left(\\begin{bmatrix}         \\mathbf{x} \\\\ \\mathbf{y}     \\end{bmatrix}\\right) = \\mathcal{N}\\left(\\begin{bmatrix}         \\boldsymbol{\\mu}_{\\mathbf{x}} \\\\ \\boldsymbol{\\mu}_{\\mathbf{y}}     \\end{bmatrix}, \\begin{bmatrix}         \\boldsymbol{\\Sigma}_{\\mathbf{yx}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}}     \\end{bmatrix} \\right)\\,, \\end{align} $$ where $\\boldsymbol{\\Sigma}_{\\mathbf{x}\\mathbf{y}}$ is the cross-covariance matrix of $\\mathbf{x}$ and $\\mathbf{y}$.</p> <p>When presented with a joint distribution, two tasks that we may wish to perform are marginalisation and conditioning. For a joint distribution $p(\\mathbf{x}, \\mathbf{y})$ where we are interested only in $p(\\mathbf{x})$, we must integrate over all possible values of $\\mathbf{y}$ to obtain $p(\\mathbf{x})$. This process is marginalisation. Conditioning allows us to evaluate the probability of one random variable, given that the other random variable is fixed. For a joint Gaussian distribution, marginalisation and conditioning have analytical expressions where the resulting distribution is also a Gaussian random variable.</p> <p>For a joint Gaussian random variable, the marginalisation of $\\mathbf{x}$ or $\\mathbf{y}$ is given by $$ \\begin{alignat}{3}     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{y} &amp;&amp; = p(\\mathbf{x})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}},     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{x} &amp;&amp; = p(\\mathbf{y})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}},     \\boldsymbol{\\Sigma}_{\\mathbf{yy}})\\,. \\end{alignat} $$ The conditional distributions are given by $$ \\begin{align}     p(\\mathbf{y}\\,|\\, \\mathbf{x}) &amp; = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{y}} + \\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{\\mathbf{x}}), \\boldsymbol{\\Sigma}_{\\mathbf{yy}}-\\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\right)\\,. \\end{align} $$</p> <p>Within this section, we have introduced the idea of multivariate Gaussian random variables and presented some key results concerning their properties. In the following section, we will lift our presentation of Gaussian random variables to GPs.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder'"},{"location":"examples/intro_to_gps/#new-to-gaussian-processes","title":"New to Gaussian Processes?\u00b6","text":"<p>Fantastic that you're here! This notebook is designed to be a gentle introduction to the mathematics of Gaussian processes (GPs). No prior knowledge of Bayesian inference or GPs is assumed, and this notebook is self-contained. At a high level, we begin by introducing Bayes' theorem and its implications within probabilistic modelling. We then proceed to introduce the Gaussian random variable along with its multivariate form. We conclude by showing how this notion can be extended to GPs.</p>"},{"location":"examples/intro_to_gps/#bayes-theorem","title":"Bayes' Theorem\u00b6","text":"<p>A probabilistic modelling task is comprised of an observed dataset $\\mathbf{y}$ for which we construct a model. The parameters $\\theta$ of our model are unknown, and our goal is to conduct inference to determine their range of likely values. To achieve this, we apply Bayes' theorem $$ \\begin{align}     \\label{eq:BayesTheorem}     p(\\theta\\,|\\, \\mathbf{y}) = \\frac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{p(\\mathbf{y})} = \\frac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{\\int_{\\theta}p(\\mathbf{y}, \\theta)\\mathrm{d}\\theta}\\,, \\end{align} $$ where $p(\\mathbf{y}\\,|\\,\\theta)$ denotes the likelihood, or model, and quantifies how likely the observed dataset $\\mathbf{y}$ is, given the parameter estimate $\\theta$. The prior distribution $p(\\theta)$ reflects our initial beliefs about the value of $\\theta$ before observing data, whilst the posterior $p(\\theta\\,|\\, \\mathbf{y})$ gives an updated estimate of the parameters' value, after observing $\\mathbf{y}$. The marginal likelihood, or Bayesian model evidence, $p(\\mathbf{y})$ is the probability of the observed data under all possible hypotheses that our prior model can generate. Within Bayesian model selection, this property makes the marginal log-likelihood an indispensable tool. Selecting models under this criterion places a higher emphasis on models that can generalise better to new data points.</p> <p>When the posterior distribution belongs to the same family of probability distributions as the prior, we describe the prior and the likelihood as conjugate to each other. Such a scenario is convenient in Bayesian inference as it allows us to derive closed-form expressions for the posterior distribution. When the likelihood function is a member of the exponential family, then there exists a conjugate prior. However, the conjugate prior may not have a form that precisely reflects the practitioner's belief surrounding the parameter. For this reason, conjugate models seldom appear; one exception to this is GP regression that we present fully in our Regression notebook.</p> <p>For models that do not contain a conjugate prior, the marginal log-likelihood must be calculated to normalise the posterior distribution and ensure it integrates to 1. For models with a single, 1-dimensional parameter, it may be possible to compute this integral analytically or through a quadrature scheme, such as Gauss-Hermite. However, in machine learning, the dimensionality of $\\theta$ is often large and the corresponding integral required to compute $p(\\mathbf{y})$ quickly becomes intractable as the dimension grows. Techniques such as Markov Chain Monte Carlo and variational inference allow us to approximate integrals such as the one seen in $p(\\mathbf{y})$.</p> <p>Once a posterior distribution has been obtained, we can make predictions at new points $\\mathbf{y}^{\\star}$ through the posterior predictive distribution. This is achieved by integrating out the parameter set $\\theta$ from our posterior distribution through $$ \\begin{align}     &amp; = \\int p(\\mathbf{y}^{\\star} \\,|\\, \\theta, \\mathbf{y} ) p(\\theta\\,|\\, \\mathbf{y})\\mathrm{d}\\theta\\,. \\end{align} $$ As with the marginal log-likelihood, evaluating this quantity requires computing an integral which may not be tractable, particularly when $\\theta$ is high-dimensional.</p> <p>It is difficult to communicate statistics directly through a posterior distribution, so we often compute and report moments of the posterior distribution. Most commonly, we report the first moment and the centred second moment $$ \\begin{alignat}{2}     \\mu  = \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]  &amp; = \\int \\theta     \\sigma^2  = \\mathbb{V}[\\theta\\,|\\,\\mathbf{y}] &amp; = \\int \\left(\\theta -     \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]\\right)^2p(\\theta\\,|\\,\\mathbf{y})\\mathrm{d}\\theta&amp;\\,. \\end{alignat} $$ Through this pair of statistics, we can communicate our beliefs about the most likely value of $\\theta$ i.e., $\\mu$, and the uncertainty $\\sigma$ around the expected value. However, as with the marginal log-likelihood and predictive posterior distribution, computing these statistics again requires a potentially intractable integral.</p>"},{"location":"examples/intro_to_gps/#gaussian-random-variables","title":"Gaussian random variables\u00b6","text":"<p>We begin our review with the simplest case; a univariate Gaussian random variable. For a random variable $y$, let $\\mu\\in\\mathbb{R}$ be a mean scalar and $\\sigma^2\\in\\mathbb{R}_{&gt;0}$ a variance scalar. If $y$ is a Gaussian random variable, then the density of $y$ is $$ \\begin{align}     \\mathcal{N}(y\\,|\\, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^{2}}\\right)\\,. \\end{align} $$ We can plot three different parameterisations of this density.</p> <p>import warnings</p> <p>import jax.numpy as jnp import jax.random as jr import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import tensorflow_probability.substrates.jax as tfp from utils import confidence_ellipse</p> <p>plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] tfd = tfp.distributions</p> <p>ud1 = tfd.Normal(0.0, 1.0) ud2 = tfd.Normal(-1.0, 0.5) ud3 = tfd.Normal(0.25, 1.5)</p> <p>xs = jnp.linspace(-5.0, 5.0, 500)</p> <p>fig, ax = plt.subplots() for d in [ud1, ud2, ud3]:     ax.plot(         xs,         jnp.exp(d.log_prob(xs)),         label=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",     )     ax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2) ax.legend(loc=\"best\")</p>"},{"location":"examples/intro_to_gps/#gaussian-processes","title":"Gaussian processes\u00b6","text":"<p>When transitioning from Gaussian random variables to GP there is a shift in thought required to parse the forthcoming material. Firstly, to be consistent with the general literature, we hereon use $\\mathbf{X}$ to denote an observed vector of data points, not a random variable as has been true up until now. To distinguish between matrices and vectors, we use bold upper case characters e.g., $\\mathbf{X}$ for matrices, and bold lower case characters for vectors e.g., $\\mathbf{x}$.</p> <p>We are interested in modelling supervised learning problems, where we have $n$ observations $\\mathbf{y}=\\{y_1, y_2,\\ldots ,y_n\\}\\subset\\mathcal{Y}$ at corresponding inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\subset\\mathcal{X}$. We aim to capture the relationship between $\\mathbf{X}$ and $\\mathbf{y}$ using a model $f$ with which we may make predictions at an unseen set of test points $\\mathbf{X}^{\\star}\\subset\\mathcal{X}$. We formalise this by $$ \\begin{align}     y = f(\\mathbf{X}) + \\varepsilon\\,, \\end{align} $$ where $\\varepsilon$ is an observational noise term. We collectively refer to $(\\mathbf{X}, \\mathbf{y})$ as the training data and $\\mathbf{X}^{\\star}$ as the set of test points. This process is visualised below</p> <p></p> <p>As we shall go on to see, GPs offer an appealing workflow for scenarios such as this, all under a Bayesian framework.</p> <p>We write a GP $f(\\cdot) \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot))$ with mean function $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ and $\\boldsymbol{\\theta}$-parameterised kernel $k: \\mathcal{X} \\times \\mathcal{X}\\rightarrow \\mathbb{R}$. When evaluating the GP on a finite set of points $\\mathbf{X}\\subset\\mathcal{X}$, $k$ gives rise to the Gram matrix $\\mathbf{K}_{ff}$ such that the $(i, j)^{\\text{th}}$ entry of the matrix is given by $[\\mathbf{K}_{ff}]_{i, j} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. As is conventional within the literature, we centre our training data and assume $\\mu(\\mathbf{X}):= 0$ for all $\\mathbf{X}\\in\\mathbf{X}$. We further drop dependency on $\\boldsymbol{\\theta}$ and $\\mathbf{X}$ for notational convenience in the remainder of this article.</p> <p>We define a joint GP prior over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}) = \\mathcal{N}\\left(\\mathbf{0}, \\begin{bmatrix}         \\mathbf{K}_{xf} &amp; \\mathbf{K}_{xx}     \\end{bmatrix}\\right)\\,, \\end{align} $$ where $\\mathbf{f}^{\\star} = f(\\mathbf{X}^{\\star})$. Conditional on the GP's latent function $f$, we assume a factorising likelihood generates our observations $$ \\begin{align}     p(\\mathbf{y}\\,|\\,\\mathbf{f}) = \\prod_{i=1}^n p(y_i\\,|\\, f_i)\\,. \\end{align} $$ Strictly speaking, the likelihood function is $p(\\mathbf{y}\\,|\\,\\phi(\\mathbf{f}))$ where $\\phi$ is the likelihood function's associated link function. Example link functions include the probit or logistic functions for a Bernoulli likelihood and the identity function for a Gaussian likelihood. We eschew this notation for now as this section primarily considers Gaussian likelihood functions where the role of $\\phi$ is superfluous. However, this intuition will be helpful for models with a non-Gaussian likelihood, such as those encountered in classification.</p> <p>Applying Bayes' theorem \\eqref{eq:BayesTheorem} yields the joint posterior distribution over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}\\,|\\,\\mathbf{y}) = \\frac{p(\\mathbf{y}\\,|\\,\\mathbf{f})p(\\mathbf{f},\\mathbf{f}^{\\star})}{p(\\mathbf{y})}\\,. \\end{align} $$</p> <p>The choice of kernel function that we use to parameterise our GP is an important modelling decision as the choice of kernel dictates properties such as differentiability, variance and characteristic lengthscale of the functions that are admissible under the GP prior. A kernel is a positive-definite function with parameters $\\boldsymbol{\\theta}$ that maps pairs of inputs $\\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}$ onto the real line. We dedicate the entirety of the Kernel Guide notebook to exploring the different GPs each kernel can yield.</p>"},{"location":"examples/intro_to_gps/#gaussian-process-regression","title":"Gaussian process regression\u00b6","text":"<p>When the likelihood function is a Gaussian distribution $p(y_i\\,|\\, f_i) = \\mathcal{N}(y_i\\,|\\, f_i, \\sigma_n^2)$, marginalising $\\mathbf{f}$ from the joint posterior to obtain the posterior predictive distribution is exact $$ \\begin{align}     &amp; = \\mathcal{N}(\\mathbf{f}^{\\star}\\,|\\,\\boldsymbol{\\mu}_{\\,|\\,\\mathbf{y}}, \\Sigma_{\\,|\\,\\mathbf{y}})\\,, \\end{align} $$ where $$ \\begin{align}     \\Sigma_{\\,|\\,\\mathbf{y}} &amp; = \\mathbf{K}_{\\star\\star} - \\mathbf{K}_{xf}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n\\right)^{-1}\\mathbf{K}_{fx} \\,. \\end{align} $$ Further, the log of the  marginal likelihood of the GP can be analytically expressed as $$ \\begin{align}         &amp; = 0.5\\left(-\\underbrace{\\mathbf{y}^{\\top}\\left(\\mathbf{K}_{ff} - \\sigma_n^2\\mathbf{I}_n \\right)^{-1}\\mathbf{y}}_{\\text{Data fit}} -\\underbrace{\\log\\lvert \\mathbf{K}_{ff} + \\sigma^2_n\\rvert}_{\\text{Complexity}} -\\underbrace{n\\log 2\\pi}_{\\text{Constant}} \\right)\\,. \\end{align} $$</p> <p>Model selection can be performed for a GP through gradient-based optimisation of $\\log p(\\mathbf{y})$ with respect to the kernel's parameters $\\boldsymbol{\\theta}$ and the observational noise $\\sigma^2_n$. Collectively, we call these terms the model hyperparameters $\\boldsymbol{\\xi} = \\{\\boldsymbol{\\theta},\\sigma_n^2\\}$ from which the maximum likelihood estimate is given by $$ \\begin{align*}     \\boldsymbol{\\xi}^{\\star} = \\operatorname{argmax}_{\\boldsymbol{\\xi} \\in \\Xi} \\log p(\\mathbf{y})\\,. \\end{align*} $$</p> <p>Observing the individual terms in the marginal log-likelihood can help understand exactly why optimising the marginal log-likelihood gives reasonable solutions. The data fit term is the only component of the marginal log-likelihood that includes the observed response $\\mathbf{y}$ and will therefore encourage solutions that model the data well. Conversely, the complexity term contains a determinant operator and therefore measures the volume of the function space covered by the GP. Whilst a more complex function has a better chance of modelling the observed data well, this is only true to a point and functions that are overly complex will overfit the data. Optimising with respect to the marginal log-likelihood balances these two objectives when identifying the optimal solution, as visualised below.</p> <p></p>"},{"location":"examples/intro_to_gps/#conclusions","title":"Conclusions\u00b6","text":"<p>Within this notebook we have built up the concept of a GP, starting from Bayes' theorem and the definition of a Gaussian random variable. Using the ideas presented in this notebook, the user should be in a position to dive into our Regression notebook and start getting their hands on some code. For those looking to learn more about the underling theory of GPs, an excellent starting point is the Gaussian Processes for Machine Learning textbook. Alternatively, the thesis of Alexander Terenin provides a rigorous exposition of GPs that served as the inspiration for this notebook.</p>"},{"location":"examples/intro_to_gps/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/kernels/","title":"Kernel Guide","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax as ox\nfrom simple_pytree import static_field\nimport tensorflow_probability.substrates.jax as tfp\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base.param import param_field\n\nkey = jr.PRNGKey(123)\ntfb = tfp.bijectors\nplt.style.use(\"./gpjax.mplstyle\")\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from dataclasses import dataclass from typing import Dict  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import numpy as np import optax as ox from simple_pytree import static_field import tensorflow_probability.substrates.jax as tfp  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base.param import param_field  key = jr.PRNGKey(123) tfb = tfp.bijectors plt.style.use(\"./gpjax.mplstyle\") cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>kernels = [\n    gpx.kernels.Matern12(),\n    gpx.kernels.Matern32(),\n    gpx.kernels.Matern52(),\n    gpx.kernels.RBF(),\n    gpx.kernels.Polynomial(),\n    gpx.kernels.Polynomial(degree=2),\n]\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n\nmeanf = gpx.mean_functions.Zero()\n\nfor k, ax in zip(kernels, axes.ravel()):\n    prior = gpx.Prior(mean_function=meanf, kernel=k)\n    rv = prior(x)\n    y = rv.sample(seed=key, sample_shape=(10,))\n    ax.plot(x, y.T, alpha=0.7)\n    ax.set_title(k.name)\n</pre> kernels = [     gpx.kernels.Matern12(),     gpx.kernels.Matern32(),     gpx.kernels.Matern52(),     gpx.kernels.RBF(),     gpx.kernels.Polynomial(),     gpx.kernels.Polynomial(degree=2), ] fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)  meanf = gpx.mean_functions.Zero()  for k, ax in zip(kernels, axes.ravel()):     prior = gpx.Prior(mean_function=meanf, kernel=k)     rv = prior(x)     y = rv.sample(seed=key, sample_shape=(10,))     ax.plot(x, y.T, alpha=0.7)     ax.set_title(k.name) In\u00a0[\u00a0]: Copied! <pre>slice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3])\n</pre> slice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3]) <p>The resulting kernel has one length-scale parameter per input dimension --- an ARD kernel.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Lengthscales: {slice_kernel.lengthscale}\")\n</pre> print(f\"Lengthscales: {slice_kernel.lengthscale}\") <p>We'll now simulate some data and evaluate the kernel on the previously selected input dimensions.</p> In\u00a0[\u00a0]: Copied! <pre># Inputs\nx_matrix = jr.normal(key, shape=(50, 5))\n\n# Compute the Gram matrix\nK = slice_kernel.gram(x_matrix)\nprint(K.shape)\n</pre> # Inputs x_matrix = jr.normal(key, shape=(50, 5))  # Compute the Gram matrix K = slice_kernel.gram(x_matrix) print(K.shape) In\u00a0[\u00a0]: Copied! <pre>k1 = gpx.kernels.RBF()\nk2 = gpx.kernels.Polynomial()\nsum_k = gpx.kernels.SumKernel(kernels=[k1, k2])\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(sum_k.gram(x).to_dense())\n\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\n</pre> k1 = gpx.kernels.RBF() k2 = gpx.kernels.Polynomial() sum_k = gpx.kernels.SumKernel(kernels=[k1, k2])  fig, ax = plt.subplots(ncols=3, figsize=(9, 3)) im0 = ax[0].matshow(k1.gram(x).to_dense()) im1 = ax[1].matshow(k2.gram(x).to_dense()) im2 = ax[2].matshow(sum_k.gram(x).to_dense())  fig.colorbar(im0, ax=ax[0], fraction=0.05) fig.colorbar(im1, ax=ax[1], fraction=0.05) fig.colorbar(im2, ax=ax[2], fraction=0.05) <p>Similarly, products of kernels can be created through the <code>ProductKernel</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>k3 = gpx.kernels.Matern32()\n\nprod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])\n\nfig, ax = plt.subplots(ncols=4, figsize=(12, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(k3.gram(x).to_dense())\nim3 = ax[3].matshow(prod_k.gram(x).to_dense())\n\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nfig.colorbar(im3, ax=ax[3], fraction=0.05)\n</pre> k3 = gpx.kernels.Matern32()  prod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])  fig, ax = plt.subplots(ncols=4, figsize=(12, 3)) im0 = ax[0].matshow(k1.gram(x).to_dense()) im1 = ax[1].matshow(k2.gram(x).to_dense()) im2 = ax[2].matshow(k3.gram(x).to_dense()) im3 = ax[3].matshow(prod_k.gram(x).to_dense())  fig.colorbar(im0, ax=ax[0], fraction=0.05) fig.colorbar(im1, ax=ax[1], fraction=0.05) fig.colorbar(im2, ax=ax[2], fraction=0.05) fig.colorbar(im3, ax=ax[3], fraction=0.05) In\u00a0[\u00a0]: Copied! <pre>def angular_distance(x, y, c):\n    return jnp.abs((x - y + c) % (c * 2) - c)\n\n\nbij = tfb.Chain([tfb.Softplus(), tfb.Shift(np.array(4.0).astype(np.float64))])\n\n\n@dataclass\nclass Polar(gpx.kernels.AbstractKernel):\n    period: float = static_field(2 * jnp.pi)\n    tau: float = param_field(jnp.array([4.0]), bijector=bij)\n\n    def __post_init__(self):\n        self.c = self.period / 2.0\n\n    def __call__(\n        self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        t = angular_distance(x, y, self.c)\n        K = (1 + self.tau * t / self.c) * jnp.clip(\n            1 - t / self.c, 0, jnp.inf\n        ) ** self.tau\n        return K.squeeze()\n</pre> def angular_distance(x, y, c):     return jnp.abs((x - y + c) % (c * 2) - c)   bij = tfb.Chain([tfb.Softplus(), tfb.Shift(np.array(4.0).astype(np.float64))])   @dataclass class Polar(gpx.kernels.AbstractKernel):     period: float = static_field(2 * jnp.pi)     tau: float = param_field(jnp.array([4.0]), bijector=bij)      def __post_init__(self):         self.c = self.period / 2.0      def __call__(         self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]     ) -&gt; Float[Array, \"1\"]:         t = angular_distance(x, y, self.c)         K = (1 + self.tau * t / self.c) * jnp.clip(             1 - t / self.c, 0, jnp.inf         ) ** self.tau         return K.squeeze() <p>We unpack this now to make better sense of it. In the kernel's initialiser we specify the length of a single period. As the underlying domain is a circle, this is $2\\pi$. Next, we define the Kernel's half-period parameter. As the kernel is a <code>dataclass</code> and <code>c</code> is function of <code>period</code>, we must define it in the <code>__post_init__</code> method. Finally, we define the kernel's <code>__call__</code> function which is a direct implementation of Equation (1).</p> <p>To constrain $\\tau$ to be greater than 4, we use a <code>Softplus</code> bijector with a clipped lower bound of 4.0. This is done by specifying the <code>bijector</code> argument when we define the parameter field.</p> In\u00a0[\u00a0]: Copied! <pre># Simulate data\nangles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1)\nn = 20\nnoise = 0.2\n\nX = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0)\ny = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise\n\nD = gpx.Dataset(X=X, y=y)\n\n# Define polar Gaussian process\nPKern = Polar()\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.Gaussian(num_datapoints=n)\ncirclular_posterior = gpx.Prior(mean_function=meanf, kernel=PKern) * likelihood\n\n# Optimise GP's marginal log-likelihood using Adam\nopt_posterior, history = gpx.fit(\n    model=circlular_posterior,\n    objective=jit(gpx.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.05),\n    num_iters=500,\n    key=key,\n)\n</pre> # Simulate data angles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1) n = 20 noise = 0.2  X = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0) y = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise  D = gpx.Dataset(X=X, y=y)  # Define polar Gaussian process PKern = Polar() meanf = gpx.mean_functions.Zero() likelihood = gpx.Gaussian(num_datapoints=n) circlular_posterior = gpx.Prior(mean_function=meanf, kernel=PKern) * likelihood  # Optimise GP's marginal log-likelihood using Adam opt_posterior, history = gpx.fit(     model=circlular_posterior,     objective=jit(gpx.ConjugateMLL(negative=True)),     train_data=D,     optim=ox.adamw(learning_rate=0.05),     num_iters=500,     key=key, ) In\u00a0[\u00a0]: Copied! <pre>posterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D))\nmu = posterior_rv.mean()\none_sigma = posterior_rv.stddev()\n</pre> posterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D)) mu = posterior_rv.mean() one_sigma = posterior_rv.stddev() In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(7, 3.5))\ngridspec = fig.add_gridspec(1, 1)\nax = plt.subplot(gridspec[0], polar=True)\n\nax.fill_between(\n    angles.squeeze(),\n    mu - one_sigma,\n    mu + one_sigma,\n    alpha=0.3,\n    label=r\"1 Posterior s.d.\",\n    color=cols[1],\n)\nax.fill_between(\n    angles.squeeze(),\n    mu - 3 * one_sigma,\n    mu + 3 * one_sigma,\n    alpha=0.15,\n    label=r\"3 Posterior s.d.\",\n    color=cols[1],\n)\nax.plot(angles, mu, label=\"Posterior mean\")\nax.scatter(D.X, D.y, alpha=1, label=\"Observations\")\nax.legend()\n</pre> fig = plt.figure(figsize=(7, 3.5)) gridspec = fig.add_gridspec(1, 1) ax = plt.subplot(gridspec[0], polar=True)  ax.fill_between(     angles.squeeze(),     mu - one_sigma,     mu + one_sigma,     alpha=0.3,     label=r\"1 Posterior s.d.\",     color=cols[1], ) ax.fill_between(     angles.squeeze(),     mu - 3 * one_sigma,     mu + 3 * one_sigma,     alpha=0.15,     label=r\"3 Posterior s.d.\",     color=cols[1], ) ax.plot(angles, mu, label=\"Posterior mean\") ax.scatter(D.X, D.y, alpha=1, label=\"Observations\") ax.legend() In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder'"},{"location":"examples/kernels/#kernel-guide","title":"Kernel Guide\u00b6","text":"<p>In this guide, we introduce the kernels available in GPJax and demonstrate how to create custom kernels.</p>"},{"location":"examples/kernels/#supported-kernels","title":"Supported Kernels\u00b6","text":"<p>The following kernels are natively supported in GPJax.</p> <ul> <li>Mat\u00e9rn 1/2, 3/2 and 5/2.</li> <li>RBF (or squared exponential).</li> <li>Rational quadratic.</li> <li>Powered exponential.</li> <li>Polynomial.</li> <li>White noise</li> <li>Linear.</li> <li>Polynomial.</li> <li>Graph kernels.</li> </ul> <p>While the syntax is consistent, each kernel's type influences the characteristics of the sample paths drawn. We visualise this below with 10 function draws per kernel.</p>"},{"location":"examples/kernels/#active-dimensions","title":"Active dimensions\u00b6","text":"<p>By default, kernels operate over every dimension of the supplied inputs. In some use cases, it is desirable to restrict kernels to specific dimensions of the input data. We can achieve this by the <code>active dims</code> argument, which determines which input index values the kernel evaluates.</p> <p>To see this, consider the following 5-dimensional dataset for which we would like our RBF kernel to act on the first, second and fourth dimensions.</p>"},{"location":"examples/kernels/#kernel-combinations","title":"Kernel combinations\u00b6","text":"<p>The product or sum of two positive definite matrices yields a positive definite matrix. Consequently, summing or multiplying sets of kernels is a valid operation that can give rich kernel functions. In GPJax, functionality for a sum kernel is provided by the <code>SumKernel</code> class.</p>"},{"location":"examples/kernels/#custom-kernel","title":"Custom kernel\u00b6","text":"<p>GPJax makes the process of implementing kernels of your choice straightforward with two key steps:</p> <ol> <li>Listing the kernel's parameters.</li> <li>Defining the kernel's pairwise operation.</li> </ol> <p>We'll demonstrate this process now for a circular kernel --- an adaption of the excellent guide given in the PYMC3 documentation. We encourage curious readers to visit their notebook here.</p>"},{"location":"examples/kernels/#circular-kernel","title":"Circular kernel\u00b6","text":"<p>When the underlying space is polar, typical Euclidean kernels such as Mat\u00e9rn kernels are insufficient at the boundary where discontinuities will present themselves. This is due to the fact that for a polar space $\\lvert 0, 2\\pi\\rvert=0$ i.e., the space wraps. Euclidean kernels have no mechanism in them to represent this logic and will instead treat $0$ and $2\\pi$ and elements far apart. Circular kernels do not exhibit this behaviour and instead wrap around the boundary points to create a smooth function. Such a kernel was given in Padonou &amp; Roustant (2015) where any two angles $\\theta$ and $\\theta'$ are written as $$W_c(\\theta, \\theta') = \\left\\lvert \\left(1 + \\tau \\frac{d(\\theta, \\theta')}{c} \\right) \\left(1 - \\frac{d(\\theta, \\theta')}{c} \\right)^{\\tau} \\right\\rvert \\quad \\tau \\geq 4 \\tag{1}.$$</p> <p>Here the hyperparameter $\\tau$ is analogous to a lengthscale for Euclidean stationary kernels, controlling the correlation between pairs of observations. While $d$ is an angular distance metric</p> $$d(\\theta, \\theta') = \\lvert (\\theta-\\theta'+c) \\operatorname{mod} 2c - c \\rvert.$$<p>To implement this, one must write the following class.</p>"},{"location":"examples/kernels/#using-our-polar-kernel","title":"Using our polar kernel\u00b6","text":"<p>We proceed to fit a GP with our custom circular kernel to a random sequence of points on a circle (see the Regression notebook for further details on this process).</p>"},{"location":"examples/kernels/#prediction","title":"Prediction\u00b6","text":"<p>We'll now query the GP's predictive posterior at linearly spaced novel inputs and illustrate the results.</p>"},{"location":"examples/kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/poisson/","title":"Count data regression","text":"In\u00a0[\u00a0]: Copied! <pre>import blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\nfrom jax.config import config\nfrom jaxtyping import install_import_hook\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\ntfd = tfp.distributions\nkey = jr.PRNGKey(123)\n</pre> import blackjax import jax import jax.numpy as jnp import jax.random as jr import jax.tree_util as jtu import matplotlib.pyplot as plt import tensorflow_probability.substrates.jax as tfp from jax.config import config from jaxtyping import install_import_hook  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. config.update(\"jax_enable_x64\", True) tfd = tfp.distributions key = jr.PRNGKey(123) In\u00a0[\u00a0]: Copied! <pre>key, subkey = jr.split(key)\nn = 50\nx = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\nf = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\ny = jr.poisson(key, jnp.exp(f(x)))\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nplt.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\nplt.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\nplt.legend()\n</pre> key, subkey = jr.split(key) n = 50 x = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0) f = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function y = jr.poisson(key, jnp.exp(f(x)))  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1) plt.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\") plt.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\") plt.legend() In\u00a0[\u00a0]: Copied! <pre>kernel = gpx.RBF()\nmeanf = gpx.Constant()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Poisson(num_datapoints=D.n)\n</pre> kernel = gpx.RBF() meanf = gpx.Constant() prior = gpx.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.Poisson(num_datapoints=D.n) <p>We construct the posterior through the product of our prior and likelihood.</p> In\u00a0[\u00a0]: Copied! <pre>posterior = prior * likelihood\nprint(type(posterior))\n</pre> posterior = prior * likelihood print(type(posterior)) <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. Here, we show how to use MCMC methods.</p> In\u00a0[\u00a0]: Copied! <pre># Adapted from BlackJax's introduction notebook.\nnum_adapt = 100\nnum_samples = 200\n\nlpd = jax.jit(gpx.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n\n# Initialise the chain\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Sample from the posterior distribution\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\n</pre> # Adapted from BlackJax's introduction notebook. num_adapt = 100 num_samples = 200  lpd = jax.jit(gpx.LogPosteriorDensity(negative=False)) unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))  adapt = blackjax.window_adaptation(     blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65 )  # Initialise the chain last_state, kernel, _ = adapt.run(key, posterior.unconstrain())   def inference_loop(rng_key, kernel, initial_state, num_samples):     def one_step(state, rng_key):         state, info = kernel(rng_key, state)         return state, (state, info)      keys = jax.random.split(rng_key, num_samples)     _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)      return states, infos   # Sample from the posterior distribution states, infos = inference_loop(key, kernel, last_state, num_samples) In\u00a0[\u00a0]: Copied! <pre>acceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</pre> acceptance_rate = jnp.mean(infos.acceptance_probability) print(f\"Acceptance rate: {acceptance_rate:.2f}\") In\u00a0[\u00a0]: Copied! <pre>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5), tight_layout=True)\nax0.plot(states.position.constrain().prior.kernel.variance)\nax1.plot(states.position.constrain().prior.kernel.lengthscale)\nax2.plot(states.position.constrain().prior.mean_function.constant)\nax0.set_title(\"Kernel variance\")\nax1.set_title(\"Kernel lengthscale\")\nax2.set_title(\"Mean function constant\")\n</pre> fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5), tight_layout=True) ax0.plot(states.position.constrain().prior.kernel.variance) ax1.plot(states.position.constrain().prior.kernel.lengthscale) ax2.plot(states.position.constrain().prior.mean_function.constant) ax0.set_title(\"Kernel variance\") ax1.set_title(\"Kernel lengthscale\") ax2.set_title(\"Mean function constant\") In\u00a0[\u00a0]: Copied! <pre>thin_factor = 10\nsamples = []\n\nfor i in range(num_adapt, num_samples + num_adapt, thin_factor):\n    sample = jtu.tree_map(lambda samples: samples[i], states.position)\n    sample = sample.constrain()\n    latent_dist = sample.predict(xtest, train_data=D)\n    predictive_dist = sample.likelihood(latent_dist)\n    samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n\nsamples = jnp.vstack(samples)\n\nlower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(samples, axis=0)\n</pre> thin_factor = 10 samples = []  for i in range(num_adapt, num_samples + num_adapt, thin_factor):     sample = jtu.tree_map(lambda samples: samples[i], states.position)     sample = sample.constrain()     latent_dist = sample.predict(xtest, train_data=D)     predictive_dist = sample.likelihood(latent_dist)     samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))  samples = jnp.vstack(samples)  lower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0) expected_val = jnp.mean(samples, axis=0) <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(16, 5), tight_layout=True)\nax.plot(\n    x, y, \"o\", markersize=5, color=\"tab:red\", label=\"Observations\", zorder=2, alpha=0.7\n)\nax.plot(\n    xtest, expected_val, linewidth=2, color=\"tab:blue\", label=\"Predicted mean\", zorder=1\n)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=\"tab:blue\",\n    label=\"95% CI\",\n)\n</pre> fig, ax = plt.subplots(figsize=(16, 5), tight_layout=True) ax.plot(     x, y, \"o\", markersize=5, color=\"tab:red\", label=\"Observations\", zorder=2, alpha=0.7 ) ax.plot(     xtest, expected_val, linewidth=2, color=\"tab:blue\", label=\"Predicted mean\", zorder=1 ) ax.fill_between(     xtest.flatten(),     lower_ci.flatten(),     upper_ci.flatten(),     alpha=0.2,     color=\"tab:blue\",     label=\"95% CI\", ) In\u00a0[\u00a0]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Francesco Zanetta\"\n</pre> %load_ext watermark %watermark -n -u -v -iv -w -a \"Francesco Zanetta\" In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/poisson/#count-data-regression","title":"Count data regression\u00b6","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via Markov chain Monte Carlo (MCMC). We focus on a count data regression task here and use BlackJax for sampling.</p>"},{"location":"examples/poisson/#dataset","title":"Dataset\u00b6","text":"<p>For count data regression, the Poisson distribution is a natural choice for the likelihood function. The probability mass function of the Poisson distribution is given by</p> $$ p(y \\,|\\, \\lambda) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!},$$<p>where $y$ is the count and the parameter $\\lambda \\in \\mathbb{R}_{&gt;0}$ is the rate of the Poisson distribution.</p> <p>We than set $\\lambda = \\exp(f)$ where $f$ is the latent Gaussian process. The exponential function is the link function for the Poisson distribution: it maps the output of a GP to the positive real line, which is suitable for modeling count data.</p> <p>We simulate a dataset $\\mathcal{D} = \\{(\\mathbf{X}, \\mathbf{y})\\}$ with inputs $\\mathbf{X} \\in \\mathbb{R}^d$ and corresponding count outputs $\\mathbf{y}$. We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code>.</p>"},{"location":"examples/poisson/#gaussian-process-definition","title":"Gaussian Process definition\u00b6","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. We adopt the Poisson likelihood available in GPJax.</p>"},{"location":"examples/poisson/#mcmc-inference","title":"MCMC inference\u00b6","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"examples/poisson/#mcmc-through-blackjax","title":"MCMC through BlackJax\u00b6","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 200 values from our Markov chain. In practice, drawing more samples will be necessary.</p>"},{"location":"examples/poisson/#sampler-efficiency","title":"Sampler efficiency\u00b6","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain).</p>"},{"location":"examples/poisson/#prediction","title":"Prediction\u00b6","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every $n$-th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>"},{"location":"examples/poisson/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/pytrees/","title":"\ud83c\udf33 GPJax PyTrees","text":"In\u00a0[\u00a0]: Copied! <pre>import jax\nimport jax.numpy as jnp\n\n\nclass RBF:\n    def __init__(self, lengthscale: float, variance: float) -&gt; None:\n        self.lengthscale = lengthscale\n        self.variance = variance\n\n    def covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\n        l_squared = self.lengthscale\n        sigma_sqaured = self.variance\n\n        return sigma_sqaured * jnp.exp(\n            -((jnp.linalg.norm(x, y) / 2.0 * l_squared) ** 2)\n        )\n</pre> import jax import jax.numpy as jnp   class RBF:     def __init__(self, lengthscale: float, variance: float) -&gt; None:         self.lengthscale = lengthscale         self.variance = variance      def covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:         l_squared = self.lengthscale         sigma_sqaured = self.variance          return sigma_sqaured * jnp.exp(             -((jnp.linalg.norm(x, y) / 2.0 * l_squared) ** 2)         ) <p>However, asserting equivalence between two class instances with exactly the same parameters</p> In\u00a0[\u00a0]: Copied! <pre>kernel_1 = RBF(1.0, 1.0)\nkernel_2 = RBF(1.0, 1.0)\n\nprint(kernel_1 == kernel_2)\n</pre> kernel_1 = RBF(1.0, 1.0) kernel_2 = RBF(1.0, 1.0)  print(kernel_1 == kernel_2) <p>The assertion is <code>False</code>. Yet the lengthscale and variance are certainly the same.</p> In\u00a0[\u00a0]: Copied! <pre>print(kernel_1.lengthscale == kernel_2.lengthscale)\nprint(kernel_1.variance == kernel_2.variance)\n</pre> print(kernel_1.lengthscale == kernel_2.lengthscale) print(kernel_1.variance == kernel_2.variance) In\u00a0[\u00a0]: Copied! <pre>from dataclasses import dataclass\n\n\n@dataclass\nclass RBF:\n    lengthscale: float\n    variance: float\n\n    def covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\n        return self.variance * jnp.exp(\n            -0.5 * (jnp.linalg.norm(x, y) / self.lengthscale) ** 2\n        )\n</pre> from dataclasses import dataclass   @dataclass class RBF:     lengthscale: float     variance: float      def covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:         return self.variance * jnp.exp(             -0.5 * (jnp.linalg.norm(x, y) / self.lengthscale) ** 2         ) <p>This time we now have equality between instances.</p> In\u00a0[\u00a0]: Copied! <pre>kernel_1 = RBF(1.0, 1.0)\nkernel_2 = RBF(1.0, 1.0)\n\nprint(kernel_1 == kernel_2)\n</pre> kernel_1 = RBF(1.0, 1.0) kernel_2 = RBF(1.0, 1.0)  print(kernel_1 == kernel_2) <p>To establish some terminology, within the above RBF <code>dataclass</code>, we refer to the lengthscale and variance as fields. Further, the <code>RBF.covariance()</code> is a method.</p> <p>However, the object we have defined are not yet compatible with JAX, for this we must consider PyTree's.</p> In\u00a0[\u00a0]: Copied! <pre>import tensorflow_probability.substrates.jax.bijectors as tfb\nfrom gpjax import base\n\n\n@dataclass\nclass RBF(base.Module):\n    lengthscale: float = base.param_field(1.0, bijector=tfb.Softplus(), trainable=True)\n    variance: float = base.param_field(1.0, bijector=tfb.Softplus(), trainable=True)\n\n    def covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\n        return self.variance * jnp.exp(\n            -0.5 * (jnp.linalg.norm(x, y) / self.lengthscale) ** 2\n        )\n</pre> import tensorflow_probability.substrates.jax.bijectors as tfb from gpjax import base   @dataclass class RBF(base.Module):     lengthscale: float = base.param_field(1.0, bijector=tfb.Softplus(), trainable=True)     variance: float = base.param_field(1.0, bijector=tfb.Softplus(), trainable=True)      def covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:         return self.variance * jnp.exp(             -0.5 * (jnp.linalg.norm(x, y) / self.lengthscale) ** 2         ) In\u00a0[\u00a0]: Copied! <pre>kernel = RBF()\nkernel = kernel.replace(lengthscale=3.14)  # Update e.g., the lengthscale.\nprint(kernel)\n</pre> kernel = RBF() kernel = kernel.replace(lengthscale=3.14)  # Update e.g., the lengthscale. print(kernel) In\u00a0[\u00a0]: Copied! <pre># Transform kernel to unconstrained space\nunconstrained_kernel = kernel.unconstrain()\nprint(unconstrained_kernel)\n\n# Transform kernel back to constrained space\nkernel = unconstrained_kernel.constrain()\nprint(kernel)\n</pre> # Transform kernel to unconstrained space unconstrained_kernel = kernel.unconstrain() print(unconstrained_kernel)  # Transform kernel back to constrained space kernel = unconstrained_kernel.constrain() print(kernel) In\u00a0[\u00a0]: Copied! <pre>new_kernel = kernel.replace_bijector(lengthscale=tfb.Identity())\n\n# Transform kernel to unconstrained space\nunconstrained_kernel = new_kernel.unconstrain()\nprint(unconstrained_kernel)\n\n# Transform kernel back to constrained space\nnew_kernel = unconstrained_kernel.constrain()\nprint(new_kernel)\n</pre> new_kernel = kernel.replace_bijector(lengthscale=tfb.Identity())  # Transform kernel to unconstrained space unconstrained_kernel = new_kernel.unconstrain() print(unconstrained_kernel)  # Transform kernel back to constrained space new_kernel = unconstrained_kernel.constrain() print(new_kernel)"},{"location":"examples/pytrees/#gpjax-pytrees","title":"\ud83c\udf33 GPJax PyTrees\u00b6","text":"<p><code>GPJax</code> represents all objects as JAX PyTrees, giving</p> <ul> <li>A simple API with a TensorFlow / PyTorch feel \u2026</li> <li>\u2026 whilst fully compatible with JAX's functional paradigm.</li> </ul>"},{"location":"examples/pytrees/#gaussian-process-objects-as-data","title":"Gaussian process objects as data:\u00b6","text":"<p>Our abstraction is based on the Equinox library and aims to offer a Bayesian/GP equivalent to their neural network abstractions. However, we take it a step further by enabling users to create standard Python classes and easily define and modify parameter domains and training statuses for optimisation within a single model object. This object is fully compatible with JAX autogradients without the need for filtering.</p> <p>The core idea is to represent all mathematical objects as immutable tree's...</p> <p>In the following we will consider an academic example, but which should be enough to understand the mechanics of how to write custom objects in GPJax.</p>"},{"location":"examples/pytrees/#the-rbf-kernel","title":"The RBF kernel\u00b6","text":"<p>The kernel in Gaussian process modeling is a mathematical function that defines the covariance structure between data points, allowing us to model complex relationships and make predictions based on the observed data. The radial basis function (RBF, or squared exponential) kernel is a popular choice. For a pair of vectors $x, y \\in \\mathbb{R}^d$, its form can be mathematically given by $$ k(x, y) = \\sigma^2\\exp\\left(\\frac{\\lVert x-y\\rVert_{2}^2}{2\\ell^2} \\right) $$ where $\\sigma^2\\in\\mathbb{R}_{&gt;0}$ is a variance parameter and $\\ell^2\\in\\mathbb{R}_{&gt;0}$ a lengthscale parameter. Terming the evaluation of $k(x, y)$ the covariance, we can crudely represent this object in Python as follows:</p>"},{"location":"examples/pytrees/#dataclasses","title":"Dataclasses\u00b6","text":"<p>A <code>dataclass</code> in Python can simplify the creation of classes for storing data and make the code more readable and maintainable. They offer several benefits over a regular class, including:</p> <ol> <li><p>Conciseness: Dataclasses automatically generate default implementations for several common methods, such as init(), repr(), and eq(), which means less boilerplate code needs to be written.</p> </li> <li><p>Type hinting: Dataclasses provide native support for type annotations, which can help catch errors at compile-time and improve code readability.</p> </li> </ol> <p>For the RBF kernel, we use a <code>dataclass</code> to represent this object as follows</p>"},{"location":"examples/pytrees/#pytrees","title":"PyTree\u2019s\u00b6","text":"<p>To efficiently represent data JAX provides a PyTree abstraction. PyTree\u2019s as such, are immutable tree-like structure built out of \u2018node\u2019 types \u2014 container-like Python objects. For instance,</p> <pre>[3.14, {\"Monte\": object(), \"Carlo\": False}]\n</pre> <p>is a PyTree with structure <code>[*, {\"Monte\": *, \"Carlo\": *}]</code> and leaves <code>3.14</code>, <code>object()</code>, <code>False</code>. As such, most JAX functions operate over pytrees, e.g., <code>jax.lax.scan</code>, accepts as input and produces as output a pytrees of JAX arrays.</p> <p>While the default set of \u2018node\u2019 types that are regarded internal pytree nodes is limited to objects such as lists, tuples, and dicts, JAX permits custom types to be readily registered through a global registry, with the values of such traversed recursively (i.e., as a tree!). This is the functionality that we exploit, whereby we construct all Gaussian process models via a tree-structure through our <code>Module</code> object.</p>"},{"location":"examples/pytrees/#module","title":"Module\u00b6","text":"<p>Our design, first and foremost, minimises additional abstractions on top of standard JAX: everything is just PyTrees and transformations on PyTrees, and secondly, provides full compatibility with the main JAX library itself, enhancing integrability with the broader ecosystem of third-party JAX libraries. To achieve this, our core idea is represent all model objects via an immutable tree-structure.</p>"},{"location":"examples/pytrees/#defining-a-module","title":"Defining a Module\u00b6","text":"<p>There are two main considerations for model parameters, their:</p> <ul> <li>Trainability status.</li> <li>Domain.</li> <li>Explain why normalising flows don\u2019t break the convention.</li> <li>Mark leaf attributes with <code>param_field</code> to set a default bijector and     trainable status.</li> <li>Unmarked leaf attributes default to an <code>Identity</code> bijector and trainablility     set to <code>True</code>.</li> <li>Fully compatible with Distrax and     TensorFlow Probability bijectors,     so feel free to use these!</li> </ul>"},{"location":"examples/pytrees/#replacing-values","title":"Replacing values\u00b6","text":"<p>For consistency with JAX\u2019s functional programming principles, <code>Module</code> instances are immutable. Parameter updates can be achieved out-of-place via <code>replace</code>.</p>"},{"location":"examples/pytrees/#transformations","title":"Transformations \ud83e\udd16\u00b6","text":""},{"location":"examples/pytrees/#applying-transformations","title":"Applying transformations\u00b6","text":"<p>Use <code>constrain</code> / <code>unconstrain</code> to return a <code>Mytree</code> with each parameter's bijector <code>forward</code> / <code>inverse</code> operation applied!</p>"},{"location":"examples/pytrees/#replacing-transformations","title":"Replacing transformations\u00b6","text":"<p>Default transformations can be replaced on an instance via the <code>replace_bijector</code> method.</p>"},{"location":"examples/pytrees/#trainability","title":"Trainability \ud83d\ude82\u00b6","text":""},{"location":"examples/pytrees/#applying-trainability","title":"Applying trainability\u00b6","text":"<p>Applying <code>stop_gradient</code> within the loss function, prevents the flow of gradients during forward or reverse-mode automatic differentiation.</p> <pre>import jax\n\n# Create simulated data.\nn = 100\nkey = jax.random.PRNGKey(123)\nx = jax.random.uniform(key, (n, ))\ny = 3.0 * x + 2.0 + 1e-3 * jax.random.normal(key, (n, ))\n\n\n# Define a mean-squared-error loss.\ndef loss(model: SimpleModel) -&gt; float:\n   model = model.stop_gradient() # \ud83d\uded1 Stop gradients!\n   return jax.numpy.sum((y - model(x))**2)\n\njax.grad(loss)(model)\n</pre> <pre><code>SimpleModel(weight=0.0, bias=-188.37418)\n</code></pre> <p>As <code>weight</code> trainability was set to <code>False</code>, it's gradient is zero as expected!</p>"},{"location":"examples/pytrees/#replacing-trainability","title":"Replacing trainability\u00b6","text":"<p>Default trainability status can be replaced via the <code>replace_trainable</code> method.</p> <pre>new = model.replace_trainable(weight=True)\njax.grad(loss)(model)\n</pre> <pre><code>SimpleModel(weight=-121.42676, bias=-188.37418)\n</code></pre> <p>And we see that <code>weight</code>'s gradient is no longer zero.</p>"},{"location":"examples/pytrees/#metadata","title":"Metadata\u00b6","text":""},{"location":"examples/pytrees/#viewing-field-metadata","title":"Viewing <code>field</code> metadata\u00b6","text":"<p>View field metadata pytree via <code>meta</code>.</p> <pre>from mytree import meta\nmeta(model)\n</pre> <pre><code>SimpleModel(weight=({'bijector': Bijector(forward=&lt;function &lt;lambda&gt; at 0x17a024e50&gt;, inverse=&lt;function &lt;lambda&gt; at 0x17a024430&gt;), 1.0), 'trainable': False, 'pytree_node': True}, bias=({}, 2.0))\n</code></pre> <p>Or the metadata pytree leaves via <code>meta_leaves</code>.</p> <pre>from mytree import meta_leaves\nmeta_leaves(model)\n</pre> <pre><code>[({}, 2.0),\n ({'bijector': Bijector(forward=&lt;function &lt;lambda&gt; at 0x17a024e50&gt;, inverse=&lt;function &lt;lambda&gt; at 0x17a024430&gt;),\n  'trainable': False,\n  'pytree_node': True}, 1.0)]\n</code></pre> <p>Note this shows any metadata defined via a <code>dataclasses.field</code> for the pytree leaves. So feel free to define your own.</p>"},{"location":"examples/pytrees/#applying-field-metadata","title":"Applying <code>field</code> metadata\u00b6","text":"<p>Leaf metadata can be applied via the <code>meta_map</code> function.</p> <pre>from mytree import meta_map\n\n# Function passed to `meta_map` has its argument as a `(meta, leaf)` tuple!\ndef if_trainable_then_10(meta_leaf):\n    meta, leaf = meta_leaf\n    if meta.get(\"trainable\", True):\n        return 10.0\n    else:\n        return leaf\n\nmeta_map(if_trainable_then_10, model)\n</pre> <pre><code>SimpleModel(weight=1.0, bias=10.0)\n</code></pre> <p>It is possible to define your own custom metadata and therefore your own metadata transformations in this vein.</p>"},{"location":"examples/pytrees/#static-fields","title":"Static fields\u00b6","text":"<p>Fields can be marked as static via simple_pytree's <code>static_field</code>.</p> <pre>import jax.tree_util as jtu\nfrom simple_pytree import static_field\n\nclass StaticExample(Mytree):\n    b: float = static_field\n\n    def __init__(self, a=1.0, b=2.0):\n        self.a=a\n        self.b=b\n\njtu.tree_leaves(StaticExample())\n</pre> <pre><code>[1.0]\n</code></pre>"},{"location":"examples/regression/","title":"Regression","text":"In\u00a0[49]: Copied! <pre>from jax import jit\nfrom jax.config import config\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom utils import clean_legend\n\nimport gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> from jax import jit from jax.config import config import jax.numpy as jnp import jax.random as jr import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from utils import clean_legend  import gpjax as gpx  # Enable Float64 for more stable matrix inversions. config.update(\"jax_enable_x64\", True) key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[50]: Copied! <pre>n = 100\nnoise = 0.3\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 100 noise = 0.3  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise.</p> In\u00a0[51]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\n</pre> fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1]) ax.legend(loc=\"best\") Out[51]: <pre>&lt;matplotlib.legend.Legend at 0x17f719d90&gt;</pre> <p>Our aim in this tutorial will be to reconstruct the latent function from our noisy observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a Gaussian process prior in the next section.</p> In\u00a0[52]: Copied! <pre>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\n</pre> kernel = gpx.kernels.RBF() meanf = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel) <p>The above construction forms the foundation for GPJax's models. Moreover, the GP prior we have just defined can be represented by a TensorFlow Probability multivariate Gaussian distribution. Such functionality enables trivial sampling, and the evaluation of the GP's mean and covariance .</p> In\u00a0[53]: Copied! <pre>prior_dist = prior.predict(xtest)\n\nprior_mean = prior_dist.mean()\nprior_std = prior_dist.variance()\nsamples = prior_dist.sample(seed=key, sample_shape=(20,))\n\n\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\n    xtest.flatten(),\n    prior_mean - prior_std,\n    prior_mean + prior_std,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\n</pre> prior_dist = prior.predict(xtest)  prior_mean = prior_dist.mean() prior_std = prior_dist.variance() samples = prior_dist.sample(seed=key, sample_shape=(20,))   fig, ax = plt.subplots() ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\") ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\") ax.fill_between(     xtest.flatten(),     prior_mean - prior_std,     prior_mean + prior_std,     alpha=0.3,     color=cols[1],     label=\"Prior variance\", ) ax.legend(loc=\"best\") ax = clean_legend(ax) In\u00a0[54]: Copied! <pre>likelihood = gpx.Gaussian(num_datapoints=D.n)\n</pre> likelihood = gpx.Gaussian(num_datapoints=D.n) <p>The posterior is proportional to the prior multiplied by the likelihood, written as</p> <p>$$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$</p> <p>Mimicking this construct, the posterior is established in GPJax through the <code>*</code> operator.</p> In\u00a0[55]: Copied! <pre>posterior = prior * likelihood\n</pre> posterior = prior * likelihood In\u00a0[56]: Copied! <pre>negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))\nnegative_mll(posterior, train_data=D)\n</pre> negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True)) negative_mll(posterior, train_data=D) Out[56]: <pre>Array(124.80517341, dtype=float64)</pre> In\u00a0[59]: Copied! <pre>kernel = gpx.kernels.RBF(variance=1.)\nkernel = kernel.replace_trainable(variance=False)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior*likelihood\n</pre> kernel = gpx.kernels.RBF(variance=1.) kernel = kernel.replace_trainable(variance=False) meanf = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel) posterior = prior*likelihood In\u00a0[63]: Copied! <pre>import jax \n\nstatic_tree = jax.tree_map(lambda x: not(x), posterior.trainables())\nstatic_tree\n</pre> import jax   static_tree = jax.tree_map(lambda x: not(x), posterior.trainables()) static_tree Out[63]: <pre>ConjugatePosterior(prior=Prior(kernel=RBF(compute_engine=&lt;class 'gpjax.kernels.computations.dense.DenseKernelComputation'&gt;, active_dims=None, name='RBF', lengthscale=False, variance=True), mean_function=Constant(constant=False), jitter=1e-06), likelihood=Gaussian(num_datapoints=100, obs_noise=False), jitter=1e-06)</pre> In\u00a0[34]: Copied! <pre>static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\noptim = ox.chain(\n    ox.adam(learning_rate=0.01), \n    ox.masked(ox.set_to_zero(), static_tree)\n    )\n</pre> static_tree = jax.tree_map(lambda x: not(x), posterior.trainables) optim = ox.chain(     ox.adam(learning_rate=0.01),      ox.masked(ox.set_to_zero(), static_tree)     ) Out[34]: <pre>ConjugatePosterior(prior=Prior(kernel=RBF(compute_engine=&lt;class 'gpjax.kernels.computations.dense.DenseKernelComputation'&gt;, active_dims=None, name='RBF', lengthscale=Array([0.54132485], dtype=float32), variance=Array(0.54132485, dtype=float32)), mean_function=Constant(constant=Array([0.], dtype=float32)), jitter=1e-06), likelihood=Gaussian(num_datapoints=100, obs_noise=Array([0.54132485], dtype=float32)), jitter=1e-06)</pre> <p>Since most optimisers (including here) minimise a given function, we have realised the negative marginal log-likelihood and just-in-time (JIT) compiled this to accelerate training.</p> <p>We can now define an optimiser with <code>optax</code>. For this example we'll use the <code>adam</code> optimiser.</p> In\u00a0[9]: Copied! <pre>opt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=500,\n    safe=True,\n)\n</pre> opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=500,     safe=True, ) <pre>/Users/tompinder/Development/JaxGPDevs/GPJax/gpjax/fit.py:203: UserWarning: Objective is jit-compiled. Please ensure that the objective is of type gpjax.Objective.\n  warn(\n</pre> <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <p>The calling of <code>fit</code> returns two objects: the optimised posterior and a history of training losses. We can plot the training loss to see how the optimisation has progressed.</p> In\u00a0[10]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\")\n</pre> fig, ax = plt.subplots() ax.plot(history, color=cols[1]) ax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\") Out[10]: <pre>[Text(0.5, 0, 'Training iteration'),\n Text(0, 0.5, 'Negative marginal log likelihood')]</pre> In\u00a0[11]: Copied! <pre>latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev() <p>With the predictions and their uncertainty acquired, we illustrate the GP's performance at explaining the data $\\mathcal{D}$ and recovering the underlying latent function of interest.</p> In\u00a0[12]: Copied! <pre>fig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(7.5, 2.5)) ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[12]: <pre>&lt;matplotlib.legend.Legend at 0x2a2272670&gt;</pre> In\u00a0[13]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd' <pre>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Tue Apr 25 2023\n\nPython implementation: CPython\nPython version       : 3.8.16\nIPython version      : 8.11.0\n\nmatplotlib: 3.7.1\noptax     : 0.1.4\ngpjax     : 0.5.9.post33.dev0+4853ebf\njax       : 0.4.8\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/regression/#regression","title":"Regression\u00b6","text":"<p>In this notebook we demonstate how to fit a Gaussian process regression model.</p>"},{"location":"examples/regression/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/regression/#defining-the-prior","title":"Defining the prior\u00b6","text":"<p>A zero-mean Gaussian process (GP) places a prior distribution over real-valued functions $f(\\cdot)$ where $f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$ for any finite collection of inputs $\\boldsymbol{x}$.</p> <p>Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$ with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$. The choice of kernel function is critical as, among other things, it governs the smoothness of the outputs that our GP can generate.</p> <p>For simplicity, we consider a radial basis function (RBF) kernel: $$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$</p> <p>On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$, we can reciprocate this process in GPJax via defining a <code>Prior</code> with our chosen <code>RBF</code> kernel.</p>"},{"location":"examples/regression/#constructing-the-posterior","title":"Constructing the posterior\u00b6","text":"<p>Having defined our GP, we proceed to define a description of our data $\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of likelihood is a critical in Bayesian modelling, for simplicity we consider a Gaussian with noise parameter $\\alpha$ $$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$ This is defined in GPJax through calling a <code>Gaussian</code> instance.</p>"},{"location":"examples/regression/#parameter-state","title":"Parameter state\u00b6","text":"<p>As outlined in the PyTrees documentation, parameters are contained within the model and for the leaves of the PyTree. Consequently, in this particular model, we have three parameters: the kernel lengthscale, kernel variance and the observation noise variance. Whilst we have initialised each of these to 1, we can learn Type 2 MLEs for each of these parameters by optimising the marginal log-likelihood (MLL).</p>"},{"location":"examples/regression/#prediction","title":"Prediction\u00b6","text":"<p>Equipped with the posterior and a set of optimised hyperparameter values, we are now in a position to query our GP's predictive distribution at novel test inputs. To do this, we use our defined <code>posterior</code> and <code>likelihood</code> at our test inputs to obtain the predictive distribution as a <code>Distrax</code> multivariate Gaussian upon which <code>mean</code> and <code>stddev</code> can be used to extract the predictive mean and standard deviatation.</p>"},{"location":"examples/regression/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/regression/","title":"Regression","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>n = 100\nnoise = 0.3\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 100 noise = 0.3  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\n</pre> fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1]) ax.legend(loc=\"best\") <p>Our aim in this tutorial will be to reconstruct the latent function from our noisy observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a Gaussian process prior in the next section.</p> In\u00a0[\u00a0]: Copied! <pre>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\n</pre> kernel = gpx.kernels.RBF() meanf = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel) <p>The above construction forms the foundation for GPJax's models. Moreover, the GP prior we have just defined can be represented by a TensorFlow Probability multivariate Gaussian distribution. Such functionality enables trivial sampling, and the evaluation of the GP's mean and covariance .</p> In\u00a0[\u00a0]: Copied! <pre>prior_dist = prior.predict(xtest)\n\nprior_mean = prior_dist.mean()\nprior_std = prior_dist.variance()\nsamples = prior_dist.sample(seed=key, sample_shape=(20,))\n\n\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\n    xtest.flatten(),\n    prior_mean - prior_std,\n    prior_mean + prior_std,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\n</pre> prior_dist = prior.predict(xtest)  prior_mean = prior_dist.mean() prior_std = prior_dist.variance() samples = prior_dist.sample(seed=key, sample_shape=(20,))   fig, ax = plt.subplots() ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\") ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\") ax.fill_between(     xtest.flatten(),     prior_mean - prior_std,     prior_mean + prior_std,     alpha=0.3,     color=cols[1],     label=\"Prior variance\", ) ax.legend(loc=\"best\") ax = clean_legend(ax) In\u00a0[\u00a0]: Copied! <pre>likelihood = gpx.Gaussian(num_datapoints=D.n)\n</pre> likelihood = gpx.Gaussian(num_datapoints=D.n) <p>The posterior is proportional to the prior multiplied by the likelihood, written as</p> <p>$$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$</p> <p>Mimicking this construct, the posterior is established in GPJax through the <code>*</code> operator.</p> In\u00a0[\u00a0]: Copied! <pre>posterior = prior * likelihood\n</pre> posterior = prior * likelihood In\u00a0[\u00a0]: Copied! <pre>negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))\nnegative_mll(posterior, train_data=D)\n\n\n# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n# optim = ox.chain(\n#     ox.adam(learning_rate=0.01),\n#     ox.masked(ox.set_to_zero(), static_tree)\n#     )\n</pre> negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True)) negative_mll(posterior, train_data=D)   # static_tree = jax.tree_map(lambda x: not(x), posterior.trainables) # optim = ox.chain( #     ox.adam(learning_rate=0.01), #     ox.masked(ox.set_to_zero(), static_tree) #     ) <p>Since most optimisers (including here) minimise a given function, we have realised the negative marginal log-likelihood and just-in-time (JIT) compiled this to accelerate training.</p> <p>We can now define an optimiser with <code>optax</code>. For this example we'll use the <code>adam</code> optimiser.</p> In\u00a0[\u00a0]: Copied! <pre>opt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=500,\n    safe=True,\n    key=key,\n)\n</pre> opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=500,     safe=True,     key=key, ) <p>The calling of <code>fit</code> returns two objects: the optimised posterior and a history of training losses. We can plot the training loss to see how the optimisation has progressed.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\")\n</pre> fig, ax = plt.subplots() ax.plot(history, color=cols[1]) ax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\") In\u00a0[\u00a0]: Copied! <pre>latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev() <p>With the predictions and their uncertainty acquired, we illustrate the GP's performance at explaining the data $\\mathcal{D}$ and recovering the underlying latent function of interest.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(7.5, 2.5)) ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'"},{"location":"examples/regression/#regression","title":"Regression\u00b6","text":"<p>In this notebook we demonstate how to fit a Gaussian process regression model.</p>"},{"location":"examples/regression/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/regression/#defining-the-prior","title":"Defining the prior\u00b6","text":"<p>A zero-mean Gaussian process (GP) places a prior distribution over real-valued functions $f(\\cdot)$ where $f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$ for any finite collection of inputs $\\boldsymbol{x}$.</p> <p>Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$ with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$. The choice of kernel function is critical as, among other things, it governs the smoothness of the outputs that our GP can generate.</p> <p>For simplicity, we consider a radial basis function (RBF) kernel: $$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$</p> <p>On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$, we can reciprocate this process in GPJax via defining a <code>Prior</code> with our chosen <code>RBF</code> kernel.</p>"},{"location":"examples/regression/#constructing-the-posterior","title":"Constructing the posterior\u00b6","text":"<p>Having defined our GP, we proceed to define a description of our data $\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of likelihood is a critical in Bayesian modelling, for simplicity we consider a Gaussian with noise parameter $\\alpha$ $$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$ This is defined in GPJax through calling a <code>Gaussian</code> instance.</p>"},{"location":"examples/regression/#parameter-state","title":"Parameter state\u00b6","text":"<p>As outlined in the PyTrees documentation, parameters are contained within the model and for the leaves of the PyTree. Consequently, in this particular model, we have three parameters: the kernel lengthscale, kernel variance and the observation noise variance. Whilst we have initialised each of these to 1, we can learn Type 2 MLEs for each of these parameters by optimising the marginal log-likelihood (MLL).</p>"},{"location":"examples/regression/#prediction","title":"Prediction\u00b6","text":"<p>Equipped with the posterior and a set of optimised hyperparameter values, we are now in a position to query our GP's predictive distribution at novel test inputs. To do this, we use our defined <code>posterior</code> and <code>likelihood</code> at our test inputs to obtain the predictive distribution as a <code>Distrax</code> multivariate Gaussian upon which <code>mean</code> and <code>stddev</code> can be used to extract the predictive mean and standard deviatation.</p>"},{"location":"examples/regression/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/spatial.pct/","title":"Spatial modelling: efficient sampling via pathwise conditioning","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import dataclass\n\nimport fsspec\nimport geopandas as gpd\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nimport planetary_computer\nimport pystac_client\nimport rioxarray as rio\nfrom rioxarray.merge import merge_arrays\nimport xarray as xr\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base import param_field\n    from gpjax.dataset import Dataset\n\n\nkey = jr.PRNGKey(123)\n\n# Observed temperature data\ntemperature = pd.read_csv(\"data/max_tempeature_switzerland.csv\")\ntemperature = gpd.GeoDataFrame(\n    temperature,\n    geometry=gpd.points_from_xy(temperature.longitude, temperature.latitude),\n).dropna(how=\"any\")\n\n# Country borders shapefile\npath = \"simplecache::https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\"\nwith fsspec.open(path) as file:\n    ch_shp = gpd.read_file(file).query(\"ADMIN == 'Switzerland'\")\n\n\n# Read DEM data and clip it to switzerland\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\nsearch = catalog.search(collections=[\"cop-dem-glo-90\"], bbox=[5.5, 45.5, 10.0, 48.5])\nitems = list(search.get_all_items())\ntiles = [rio.open_rasterio(i.assets[\"data\"].href).squeeze().drop(\"band\") for i in items]\ndem = merge_arrays(tiles).coarsen(x=10, y=10).mean().rio.clip(ch_shp[\"geometry\"])\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from dataclasses import dataclass  import fsspec import geopandas as gpd import jax import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import optax as ox import pandas as pd import planetary_computer import pystac_client import rioxarray as rio from rioxarray.merge import merge_arrays import xarray as xr  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base import param_field     from gpjax.dataset import Dataset   key = jr.PRNGKey(123)  # Observed temperature data temperature = pd.read_csv(\"data/max_tempeature_switzerland.csv\") temperature = gpd.GeoDataFrame(     temperature,     geometry=gpd.points_from_xy(temperature.longitude, temperature.latitude), ).dropna(how=\"any\")  # Country borders shapefile path = \"simplecache::https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\" with fsspec.open(path) as file:     ch_shp = gpd.read_file(file).query(\"ADMIN == 'Switzerland'\")   # Read DEM data and clip it to switzerland catalog = pystac_client.Client.open(     \"https://planetarycomputer.microsoft.com/api/stac/v1\",     modifier=planetary_computer.sign_inplace, ) search = catalog.search(collections=[\"cop-dem-glo-90\"], bbox=[5.5, 45.5, 10.0, 48.5]) items = list(search.get_all_items()) tiles = [rio.open_rasterio(i.assets[\"data\"].href).squeeze().drop(\"band\") for i in items] dem = merge_arrays(tiles).coarsen(x=10, y=10).mean().rio.clip(ch_shp[\"geometry\"]) <p>Let us take a look at the data. The topography of Switzerland is quite complex, and there are sometimes very large height differences over short distances. This measuring network is fairly dense, and you may already notice that there's a dependency between maximum daily temperature and elevation.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 5), layout=\"constrained\")\ndem.plot(\n    cmap=\"terrain\", cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Elevation [m]\"}\n)\ntemperature.plot(\"t_max\", ax=ax, cmap=\"RdBu_r\", vmin=-15, vmax=15, edgecolor=\"k\", s=50)\nax.set(title=\"Switzerland's topography and SwissMetNet stations\", aspect=\"auto\")\ncb = fig.colorbar(ax.collections[-1], aspect=50, pad=0.02)\ncb.set_label(\"Max. daily temperature [\u00b0C]\", labelpad=-2)\n</pre>  fig, ax = plt.subplots(figsize=(8, 5), layout=\"constrained\") dem.plot(     cmap=\"terrain\", cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Elevation [m]\"} ) temperature.plot(\"t_max\", ax=ax, cmap=\"RdBu_r\", vmin=-15, vmax=15, edgecolor=\"k\", s=50) ax.set(title=\"Switzerland's topography and SwissMetNet stations\", aspect=\"auto\") cb = fig.colorbar(ax.collections[-1], aspect=50, pad=0.02) cb.set_label(\"Max. daily temperature [\u00b0C]\", labelpad=-2) <p>As always, we store our training data in a <code>Dataset</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>x = temperature[[\"latitude\", \"longitude\", \"elevation\"]].values\ny = temperature[[\"t_max\"]].values\nD = Dataset(\n    X=jnp.array(x),\n    y=jnp.array(y),\n)\n</pre>   x = temperature[[\"latitude\", \"longitude\", \"elevation\"]].values y = temperature[[\"t_max\"]].values D = Dataset(     X=jnp.array(x),     y=jnp.array(y), ) In\u00a0[\u00a0]: Copied! <pre>kernel = gpx.kernels.RBF(\n    active_dims=[0, 1, 2],\n    lengthscale=jnp.array([0.1, 0.1, 100.0]),\n)\n</pre> kernel = gpx.kernels.RBF(     active_dims=[0, 1, 2],     lengthscale=jnp.array([0.1, 0.1, 100.0]), ) In\u00a0[\u00a0]: Copied! <pre>@dataclass\nclass MeanFunction(gpx.gps.AbstractMeanFunction):\n    w: Float[Array, \"1\"] = param_field(jnp.array([0.0]))\n    b: Float[Array, \"1\"] = param_field(jnp.array([0.0]))\n\n    def __call__(self, x: Float[Array, \"N D\"]) -&gt; Float[Array, \"N 1\"]:\n        elevation = x[:, 2:3]\n        out = elevation * self.w + self.b\n        return out\n</pre> @dataclass class MeanFunction(gpx.gps.AbstractMeanFunction):     w: Float[Array, \"1\"] = param_field(jnp.array([0.0]))     b: Float[Array, \"1\"] = param_field(jnp.array([0.0]))      def __call__(self, x: Float[Array, \"N D\"]) -&gt; Float[Array, \"N 1\"]:         elevation = x[:, 2:3]         out = elevation * self.w + self.b         return out <p>Now we can define our prior. We'll also choose a Gaussian likelihood.</p> In\u00a0[\u00a0]: Copied! <pre>mean_function = MeanFunction()\nprior = gpx.Prior(kernel=kernel, mean_function=mean_function)\nlikelihood = gpx.Gaussian(D.n)\n</pre> mean_function = MeanFunction() prior = gpx.Prior(kernel=kernel, mean_function=mean_function) likelihood = gpx.Gaussian(D.n) <p>Finally, we construct the posterior.</p> In\u00a0[\u00a0]: Copied! <pre>posterior = prior * likelihood\n</pre> posterior = prior * likelihood In\u00a0[\u00a0]: Copied! <pre>negative_mll = jax.jit(gpx.objectives.ConjugateMLL(negative=True))\nnegative_mll(posterior, train_data=D)\n</pre>  negative_mll = jax.jit(gpx.objectives.ConjugateMLL(negative=True)) negative_mll(posterior, train_data=D) In\u00a0[\u00a0]: Copied! <pre>optim = ox.chain(ox.adam(learning_rate=0.1), ox.clip(1.0))\nposterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=optim,\n    num_iters=3000,\n    safe=True,\n    key=key,\n)\nposterior: gpx.gps.ConjugatePosterior\n</pre>   optim = ox.chain(ox.adam(learning_rate=0.1), ox.clip(1.0)) posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=optim,     num_iters=3000,     safe=True,     key=key, ) posterior: gpx.gps.ConjugatePosterior In\u00a0[\u00a0]: Copied! <pre># select the target pixels and exclude nans\nxtest = dem.drop(\"spatial_ref\").stack(p=[\"y\", \"x\"]).to_dataframe(name=\"dem\")\nmask = jnp.any(jnp.isnan(xtest.values), axis=-1)\n\n# generate 50 samples\nytest = posterior.sample_approx(50, D, key, num_features=200)(\n    jnp.array(xtest.values[~mask])\n)\n</pre> # select the target pixels and exclude nans xtest = dem.drop(\"spatial_ref\").stack(p=[\"y\", \"x\"]).to_dataframe(name=\"dem\") mask = jnp.any(jnp.isnan(xtest.values), axis=-1)  # generate 50 samples ytest = posterior.sample_approx(50, D, key, num_features=200)(     jnp.array(xtest.values[~mask]) ) <p>Let's take a look at the results. We start with the mean and standard deviation.</p> In\u00a0[\u00a0]: Copied! <pre>predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan\npredtest[~mask] = ytest.mean(axis=-1)\npredtest = predtest.unstack()\n\npredtest.plot(\n    vmin=-15.0,\n    vmax=15.0,\n    cmap=\"RdBu_r\",\n    cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"},\n)\nplt.gca().set_title(\"Interpolated maximum daily temperature\")\n</pre> predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan predtest[~mask] = ytest.mean(axis=-1) predtest = predtest.unstack()  predtest.plot(     vmin=-15.0,     vmax=15.0,     cmap=\"RdBu_r\",     cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"}, ) plt.gca().set_title(\"Interpolated maximum daily temperature\") In\u00a0[\u00a0]: Copied! <pre>predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan\npredtest[~mask] = ytest.std(axis=-1)\npredtest = predtest.unstack()\n\n# plot\npredtest.plot(\n    cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Standard deviation [\u00b0C]\"},\n)\nplt.gca().set_title(\"Standard deviation\")\n</pre> predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan predtest[~mask] = ytest.std(axis=-1) predtest = predtest.unstack()  # plot predtest.plot(     cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Standard deviation [\u00b0C]\"}, ) plt.gca().set_title(\"Standard deviation\") <p>And now some individual realizations of our GP posterior.</p> In\u00a0[\u00a0]: Copied! <pre>predtest = (\n    xr.zeros_like(dem.stack(p=[\"y\", \"x\"]))\n    .expand_dims(realization=range(9))\n    .transpose(\"p\", \"realization\")\n    .copy()\n)\npredtest[~mask] = ytest[:, :9]\npredtest = predtest.unstack()\npredtest.plot(\n    col=\"realization\",\n    col_wrap=3,\n    cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"},\n)\n</pre> predtest = (     xr.zeros_like(dem.stack(p=[\"y\", \"x\"]))     .expand_dims(realization=range(9))     .transpose(\"p\", \"realization\")     .copy() ) predtest[~mask] = ytest[:, :9] predtest = predtest.unstack() predtest.plot(     col=\"realization\",     col_wrap=3,     cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"}, ) <p>Remember when we said that on average the temperature decreases with height at a rate of approximately -6.5\u00b0C/km? That's -0.0065\u00b0C/m. The <code>w</code> parameter of our mean function is very close: we have learned the environmental lapse rate!</p> In\u00a0[\u00a0]: Copied! <pre>print(posterior.prior.mean_function)\n</pre> print(posterior.prior.mean_function) <p>That's it! We've successfully interpolated an observed meteorological parameter on a grid. We have used several components of GPJax and adapted them to our needs: a custom mean function that modelled the average temperature lapse rate; an ARD kernel that learned to give more relevance to elevation rather than horizontal distance; an efficient sampling technique to produce probabilistic realizations of our posterior on a large number of test points, which is important for many spatiotemporal modelling applications. If you're interested in a more elaborate work on temperature interpolation for the same domain used here, refer to Frei 2014.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Francesco Zanetta'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Francesco Zanetta' In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/spatial.pct/#spatial-modelling-efficient-sampling-via-pathwise-conditioning","title":"Spatial modelling: efficient sampling via pathwise conditioning\u00b6","text":"<p>In this notebook, we demonstrate an application of Gaussian Processes to a spatial interpolation problem. We will show how to efficiently sample from a GP posterior as shown in .</p>"},{"location":"examples/spatial.pct/#data-loading","title":"Data loading\u00b6","text":"<p>We'll use open-source data from SwissMetNet, the surface weather monitoring network of the Swiss national weather service, and digital elevation model (DEM) data from Copernicus, accessible here via the Planetary Computer data catalog. We will coarsen this data by a factor of 10 (going from 90m to 900m resolution), but feel free to change this.</p> <p>Our variable of interest is the maximum daily temperature, observed on the 4th of April 2023 at 150 weather stations, and we'll try to interpolate it on a spatial grid using geographical coordinates (latitude and longitude) and elevation as input variables.</p>"},{"location":"examples/spatial.pct/#ard-kernel","title":"ARD Kernel\u00b6","text":"<p>As temperature decreases with height (at a rate of approximately -6.5 \u00b0C/km in average conditions), we can expect that using the geographical distance alone isn't enough to to a decent job at interpolating this data. Therefore, we can also use elevation and optimize the parameters of our kernel such that more relevance should be given to elevation. This is possible by using a kernel that has one length-scale parameter per input dimension: an automatic relevance determination (ARD) kernel. See our kernel notebook for more an introduction to kernels in GPJax.</p>"},{"location":"examples/spatial.pct/#mean-function","title":"Mean function\u00b6","text":"<p>As stated before, we already know that temperature strongly depends on elevation. So why not use it for our mean function? GPJax lets you define custom mean functions; simply subclass <code>AbstractMeanFunction</code>.</p>"},{"location":"examples/spatial.pct/#model-fitting","title":"Model fitting\u00b6","text":"<p>We proceed to train our model. Because we used a Gaussian likelihood, the resulting posterior is a <code>ConjugatePosterior</code>, which allows us to optimize the analytically expressed marginal loglikelihood.</p> <p>As always, we can jit-compile the objective function to speed things up.</p>"},{"location":"examples/spatial.pct/#sampling-on-a-grid","title":"Sampling on a grid\u00b6","text":"<p>Now comes the cool part. In a standard GP implementation, for n test points, we have a $\\mathcal{O}(n^2)$ computational complexity and $\\mathcal{O}(n^2)$ memory requirement. We want to make predictions on a total of roughly 70'000 pixels, and that would require us to compute a covariance matrix of <code>70000 ** 2 = 4900000000</code> elements. If these are <code>float64</code>s, as it is often the case in GPJax, it would be equivalent to more than 36 Gigabytes of memory. And that's for a fairly coarse and tiny grid. If we were to make predictions on a 1000x1000 grid, the total memory required would be 8 Terabytes of memory, which is intractable. Fortunately, the pathwise conditioning method allows us to sample from our posterior in linear complexity, $\\mathcal{O}(n)$, with the number of pixels.</p> <p>GPJax provides the <code>sample_approx</code> method to generate random conditioned samples from our posterior.</p>"},{"location":"examples/spatial.pct/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/uncollapsed_vi/","title":"Sparse Stochastic Variational Inference","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    import gpjax.kernels as jk\n\nkey = jr.PRNGKey(123)\ntfb = tfp.bijectors\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax as tfp from utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     import gpjax.kernels as jk  key = jr.PRNGKey(123) tfb = tfp.bijectors plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>n = 50000\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)\n</pre> n = 50000 noise = 0.2  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1) In\u00a0[\u00a0]: Copied! <pre>z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\n\nfig, ax = plt.subplots()\n[\n    ax.axvline(x=z_i, color=cols[2], alpha=0.3, linewidth=1, label=\"Inducing point\")\n    for z_i in z\n]\nax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\nax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\nax.legend()\nax = clean_legend(ax)\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\n</pre> z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)  fig, ax = plt.subplots() [     ax.axvline(x=z_i, color=cols[2], alpha=0.3, linewidth=1, label=\"Inducing point\")     for z_i in z ] ax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\") ax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\") ax.legend() ax = clean_legend(ax) ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\") <p>The inducing inputs will summarise our dataset, and since they are treated as variational parameters, their locations will be optimised. The next step to SVGP is to define a variational family.</p> In\u00a0[\u00a0]: Copied! <pre>meanf = gpx.mean_functions.Zero()\nlikelihood = gpx.Gaussian(num_datapoints=n)\nprior = gpx.Prior(mean_function=meanf, kernel=jk.RBF())\np = prior * likelihood\nq = gpx.VariationalGaussian(posterior=p, inducing_inputs=z)\n</pre> meanf = gpx.mean_functions.Zero() likelihood = gpx.Gaussian(num_datapoints=n) prior = gpx.Prior(mean_function=meanf, kernel=jk.RBF()) p = prior * likelihood q = gpx.VariationalGaussian(posterior=p, inducing_inputs=z) <p>Here, the variational process $q(\\cdot)$ depends on the prior through $p(f(\\cdot)|f(\\boldsymbol{z}))$ in $(\\times)$.</p> In\u00a0[\u00a0]: Copied! <pre>negative_elbo = jit(gpx.ELBO(negative=True))\n</pre> negative_elbo = jit(gpx.ELBO(negative=True)) In\u00a0[\u00a0]: Copied! <pre>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.1,\n    warmup_steps=75,\n    decay_steps=1500,\n    end_value=0.001,\n)\n\nopt_posterior, history = gpx.fit(\n    model=q,\n    objective=negative_elbo,\n    train_data=D,\n    optim=ox.adam(learning_rate=schedule),\n    num_iters=3000,\n    key=jr.PRNGKey(42),\n    batch_size=128,\n)\n</pre> schedule = ox.warmup_cosine_decay_schedule(     init_value=0.0,     peak_value=0.1,     warmup_steps=75,     decay_steps=1500,     end_value=0.001, )  opt_posterior, history = gpx.fit(     model=q,     objective=negative_elbo,     train_data=D,     optim=ox.adam(learning_rate=schedule),     num_iters=3000,     key=jr.PRNGKey(42),     batch_size=128, ) In\u00a0[\u00a0]: Copied! <pre>latent_dist = opt_posterior(xtest)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\n[\n    ax.axvline(x=z_i, color=cols[2], alpha=0.3, linewidth=1, label=\"Inducing point\")\n    for z_i in opt_posterior.inducing_inputs\n]\nax.legend()\nax = clean_legend(ax)\n</pre> latent_dist = opt_posterior(xtest) predictive_dist = opt_posterior.posterior.likelihood(latent_dist)  meanf = predictive_dist.mean() sigma = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0]) ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1]) ax.fill_between(     xtest.flatten(),     meanf - 2 * sigma,     meanf + 2 * sigma,     alpha=0.3,     color=cols[1],     label=\"Two sigma\", ) [     ax.axvline(x=z_i, color=cols[2], alpha=0.3, linewidth=1, label=\"Inducing point\")     for z_i in opt_posterior.inducing_inputs ] ax.legend() ax = clean_legend(ax) In\u00a0[\u00a0]: Copied! <pre>triangular_transform = tfb.FillScaleTriL(\n    diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter)\n)\nreparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform)\n</pre> triangular_transform = tfb.FillScaleTriL(     diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter) ) reparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform) In\u00a0[\u00a0]: Copied! <pre>opt_rep, history = gpx.fit(\n    model=reparameterised_q,\n    objective=negative_elbo,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=3000,\n    key=jr.PRNGKey(42),\n    batch_size=128,\n)\n</pre> opt_rep, history = gpx.fit(     model=reparameterised_q,     objective=negative_elbo,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=3000,     key=jr.PRNGKey(42),     batch_size=128, ) In\u00a0[\u00a0]: Copied! <pre>latent_dist = opt_rep(xtest)\npredictive_dist = opt_rep.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\n[\n    ax.axvline(x=z_i, color=cols[2], alpha=0.3, linewidth=1, label=\"Inducing point\")\n    for z_i in opt_posterior.inducing_inputs\n]\nax.legend()\nax = clean_legend(ax)\n</pre> latent_dist = opt_rep(xtest) predictive_dist = opt_rep.posterior.likelihood(latent_dist)  meanf = predictive_dist.mean() sigma = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0]) ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1]) ax.fill_between(     xtest.flatten(),     meanf - 2 * sigma,     meanf + 2 * sigma,     alpha=0.3,     color=cols[1],     label=\"Two sigma\", ) [     ax.axvline(x=z_i, color=cols[2], alpha=0.3, linewidth=1, label=\"Inducing point\")     for z_i in opt_posterior.inducing_inputs ] ax.legend() ax = clean_legend(ax) <p>We can see that <code>Square</code> transformation is able to get relatively better fit compared to <code>Softplus</code> with the same number of iterations, but <code>Softplus</code> is recommended over <code>Square</code> for stability of optimisation.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'"},{"location":"examples/uncollapsed_vi/#sparse-stochastic-variational-inference","title":"Sparse Stochastic Variational Inference\u00b6","text":"<p>In this notebook we demonstrate how to implement sparse variational Gaussian processes (SVGPs) of Hensman et al. (2015). In particular, this approximation framework provides a tractable option for working with non-conjugate Gaussian processes with more than ~5000 data points. However, for conjugate models of less than 5000 data points, we recommend using the marginal log-likelihood approach presented in the regression notebook. Though we illustrate SVGPs here with a conjugate regression example, the same GPJax code works for general likelihoods, such as a Bernoulli for classification.</p>"},{"location":"examples/uncollapsed_vi/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{5000}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-5, 5)$ and corresponding binary outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4 * \\boldsymbol{x}) + \\sin(2 * \\boldsymbol{x}), \\textbf{I} * (0.2)^{2} \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs for later.</p>"},{"location":"examples/uncollapsed_vi/#sparse-gps-via-inducing-inputs","title":"Sparse GPs via inducing inputs\u00b6","text":"<p>Despite their endowment with elegant theoretical properties, GPs are burdened with prohibitive $\\mathcal{O}(n^3)$ inference and $\\mathcal{O}(n^2)$ memory costs in the number of data points $n$ due to the necessity of computing inverses and determinants of the kernel Gram matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ during inference and hyperparameter learning. Sparse GPs seek to resolve tractability through low-rank approximations.</p> <p>Their name originates with the idea of using subsets of the data to approximate the kernel matrix, with sparseness occurring through the selection of the data points. Given inputs $\\boldsymbol{x}$ and outputs $\\boldsymbol{y}$ the task was to select an $m&lt;n$ lower-dimensional dataset $(\\boldsymbol{z},\\boldsymbol{\\tilde{y}}) \\subset (\\boldsymbol{x}, \\boldsymbol{y})$ to train a Gaussian process on instead. By generalising the set of selected points $\\boldsymbol{z}$, known as inducing inputs, to remove the restriction of being part of the dataset, we can arrive at a flexible low-rank approximation framework of the model using functions of $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$ to replace the true covariance matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ at significantly lower costs. For example,  review many popular approximation schemes in this vein. However, because the model and the approximation are intertwined, assigning performance and faults to one or the other becomes tricky.</p> <p>On the other hand, sparse variational Gaussian processes (SVGPs) approximate the posterior, not the model. These provide a low-rank approximation scheme via variational inference. Here we posit a family of densities parameterised by \u201cvariational parameters\u201d. We then seek to find the closest family member to the posterior by minimising the Kullback-Leibler divergence over the variational parameters. The fitted variational density then serves as a proxy for the exact posterior. This procedure makes variational methods efficiently solvable via off-the-shelf optimisation techniques whilst retaining the true-underlying model. Furthermore, SVGPs offer further cost reductions with mini-batch stochastic gradient descent   and address non-conjugacy . We show a cost comparison between the approaches below, where $b$ is the mini-batch size.</p> GPs sparse GPs SVGP Inference cost $\\mathcal{O}(n^3)$ $\\mathcal{O}(n m^2)$ $\\mathcal{O}(b m^2 + m^3)$ Memory cost $\\mathcal{O}(n^2)$ $\\mathcal{O}(n m)$ $\\mathcal{O}(b m + m^2)$ <p>To apply SVGP inference to our dataset, we begin by initialising $m = 50$ equally spaced inducing inputs $\\boldsymbol{z}$ across our observed data's support. These are depicted below via horizontal black lines.</p>"},{"location":"examples/uncollapsed_vi/#defining-the-variational-process","title":"Defining the variational process\u00b6","text":"<p>We begin by considering the form of the posterior distribution for all function values $f(\\cdot)$</p> \\begin{align} p(f(\\cdot) | \\mathcal{D}) = \\int p(f(\\cdot)|f(\\boldsymbol{x})) p(f(\\boldsymbol{x})|\\mathcal{D}) \\text{d}f(\\boldsymbol{x}). \\qquad (\\dagger) \\end{align}<p>To arrive at an approximation framework, we assume some redundancy in the data. Instead of predicting $f(\\cdot)$ with function values at the datapoints $f(\\boldsymbol{x})$, we assume this can be achieved with only function values at $m$ inducing inputs $\\boldsymbol{z}$</p> $$ p(f(\\cdot) | \\mathcal{D}) \\approx \\int p(f(\\cdot)|f(\\boldsymbol{z})) p(f(\\boldsymbol{z})|\\mathcal{D}) \\text{d}f(\\boldsymbol{z}). \\qquad (\\star) $$<p>This lower dimensional integral results in computational savings in the model's predictive component from $p(f(\\cdot)|f(\\boldsymbol{x}))$ to $p(f(\\cdot)|f(\\boldsymbol{z}))$ where inverting $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is replaced with inverting $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$. However, since we did not observe our data $\\mathcal{D}$ at $\\boldsymbol{z}$ we ask, what exactly is the posterior $p(f(\\boldsymbol{z})|\\mathcal{D})$?</p> <p>Notice this is simply obtained by substituting $\\boldsymbol{z}$ into $(\\dagger)$, but we arrive back at square one with computing the expensive integral. To side-step this, we consider replacing $p(f(\\boldsymbol{z})|\\mathcal{D})$ in $(\\star)$ with a cheap-to-compute approximate distribution $q(f(\\boldsymbol{z}))$</p> <p>$$ q(f(\\cdot)) = \\int p(f(\\cdot)|f(\\boldsymbol{z})) q(f(\\boldsymbol{z})) \\text{d}f(\\boldsymbol{z}). \\qquad (\\times) $$</p> <p>To measure the quality of the approximation, we consider the Kullback-Leibler divergence $\\operatorname{KL}(\\cdot || \\cdot)$ from our approximate process $q(f(\\cdot))$ to the true process $p(f(\\cdot)|\\mathcal{D})$. By parametrising $q(f(\\boldsymbol{z}))$ over a variational family of distributions, we can optimise Kullback-Leibler divergence with respect to the variational parameters. Moreover, since inducing input locations $\\boldsymbol{z}$ augment the model, they themselves can be treated as variational parameters without altering the true underlying model $p(f(\\boldsymbol{z})|\\mathcal{D})$. This is exactly what gives SVGPs great flexibility whilst retaining robustness to overfitting.</p> <p>It is popular to elect a Gaussian variational distribution $q(f(\\boldsymbol{z})) = \\mathcal{N}(f(\\boldsymbol{z}); \\mathbf{m}, \\mathbf{S})$ with parameters $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S}\\}$, since conjugacy is provided between $q(f(\\boldsymbol{z}))$ and $p(f(\\cdot)|f(\\boldsymbol{z}))$ so that the resulting variational process $q(f(\\cdot))$ is a GP. We can implement this in GPJax by the following.</p>"},{"location":"examples/uncollapsed_vi/#inference","title":"Inference\u00b6","text":""},{"location":"examples/uncollapsed_vi/#evidence-lower-bound","title":"Evidence lower bound\u00b6","text":"<p>With our model defined, we seek to infer the optimal inducing inputs $\\boldsymbol{z}$, variational mean $\\mathbf{m}$ and covariance $\\mathbf{S}$ that define our approximate posterior. To achieve this, we maximise the evidence lower bound (ELBO) with respect to $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S} \\}$, a proxy for minimising the Kullback-Leibler divergence. Moreover, as hinted by its name, the ELBO is a lower bound to the marginal log-likelihood, providing a tractable objective to optimise the model's hyperparameters akin to the conjugate setting. For further details on this, see Sections 3.1 and 4.1 of the excellent review paper .</p> <p>Since Optax's optimisers work to minimise functions, to maximise the ELBO we return its negative.</p>"},{"location":"examples/uncollapsed_vi/#mini-batching","title":"Mini-batching\u00b6","text":"<p>Despite introducing inducing inputs into our model, inference can still be intractable with large datasets. To circumvent this, optimisation can be done using stochastic mini-batches.</p>"},{"location":"examples/uncollapsed_vi/#predictions","title":"Predictions\u00b6","text":"<p>With optimisation complete, we can use our inferred parameter set to make predictions at novel inputs akin to all other models within GPJax on our variational process object $q(\\cdot)$ (for example, see the regression notebook).</p>"},{"location":"examples/uncollapsed_vi/#custom-transformations","title":"Custom transformations\u00b6","text":"<p>To train a covariance matrix, GPJax uses <code>tfb.FillScaleTriL</code> transformation by default. <code>tfb.FillScaleTriL</code> fills a 1d vector into a lower triangular matrix and then applies <code>Softplus</code> transformation on the diagonal to satisfy the necessary conditions for a valid Cholesky matrix. Users can change this default transformation with another valid transformation of their choice. For example, <code>Square</code> transformation on the diagonal can also serve the purpose.</p>"},{"location":"examples/uncollapsed_vi/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>from matplotlib import transforms\nfrom matplotlib.patches import Ellipse\nimport numpy as np\n</pre> from matplotlib import transforms from matplotlib.patches import Ellipse import numpy as np In\u00a0[\u00a0]: Copied! <pre>def confidence_ellipse(x, y, ax, n_std=3.0, facecolor=\"none\", **kwargs):\n\"\"\"\n    Create a plot of the covariance confidence ellipse of *x* and *y*.\n\n    Parameters\n    ----------\n    x, y : array-like, shape (n, )\n        Input data.\n\n    ax : matplotlib.axes.Axes\n        The axes object to draw the ellipse into.\n\n    n_std : float\n        The number of standard deviations to determine the ellipse's radiuses.\n\n    **kwargs\n        Forwarded to `~matplotlib.patches.Ellipse`\n\n    Returns\n    -------\n    matplotlib.patches.Ellipse\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    if x.size != y.size:\n        raise ValueError(\"x and y must be the same size\")\n\n    cov = np.cov(x, y)\n    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n    # Using a special case to obtain the eigenvalues of this\n    # two-dimensionl dataset.\n    ell_radius_x = np.sqrt(1 + pearson)\n    ell_radius_y = np.sqrt(1 - pearson)\n    ellipse = Ellipse(\n        (0, 0),\n        width=ell_radius_x * 2,\n        height=ell_radius_y * 2,\n        facecolor=facecolor,\n        **kwargs,\n    )\n\n    # Calculating the stdandard deviation of x from\n    # the squareroot of the variance and multiplying\n    # with the given number of standard deviations.\n    scale_x = np.sqrt(cov[0, 0]) * n_std\n    mean_x = np.mean(x)\n\n    # calculating the stdandard deviation of y ...\n    scale_y = np.sqrt(cov[1, 1]) * n_std\n    mean_y = np.mean(y)\n\n    transf = (\n        transforms.Affine2D()\n        .rotate_deg(45)\n        .scale(scale_x, scale_y)\n        .translate(mean_x, mean_y)\n    )\n\n    ellipse.set_transform(transf + ax.transData)\n    return ax.add_patch(ellipse)\n</pre> def confidence_ellipse(x, y, ax, n_std=3.0, facecolor=\"none\", **kwargs):     \"\"\"     Create a plot of the covariance confidence ellipse of *x* and *y*.      Parameters     ----------     x, y : array-like, shape (n, )         Input data.      ax : matplotlib.axes.Axes         The axes object to draw the ellipse into.      n_std : float         The number of standard deviations to determine the ellipse's radiuses.      **kwargs         Forwarded to `~matplotlib.patches.Ellipse`      Returns     -------     matplotlib.patches.Ellipse     \"\"\"     x = np.array(x)     y = np.array(y)     if x.size != y.size:         raise ValueError(\"x and y must be the same size\")      cov = np.cov(x, y)     pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])     # Using a special case to obtain the eigenvalues of this     # two-dimensionl dataset.     ell_radius_x = np.sqrt(1 + pearson)     ell_radius_y = np.sqrt(1 - pearson)     ellipse = Ellipse(         (0, 0),         width=ell_radius_x * 2,         height=ell_radius_y * 2,         facecolor=facecolor,         **kwargs,     )      # Calculating the stdandard deviation of x from     # the squareroot of the variance and multiplying     # with the given number of standard deviations.     scale_x = np.sqrt(cov[0, 0]) * n_std     mean_x = np.mean(x)      # calculating the stdandard deviation of y ...     scale_y = np.sqrt(cov[1, 1]) * n_std     mean_y = np.mean(y)      transf = (         transforms.Affine2D()         .rotate_deg(45)         .scale(scale_x, scale_y)         .translate(mean_x, mean_y)     )      ellipse.set_transform(transf + ax.transData)     return ax.add_patch(ellipse) In\u00a0[\u00a0]: Copied! <pre>def clean_legend(ax):\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    ax.legend(by_label.values(), by_label.keys())\n    return ax\n</pre> def clean_legend(ax):     handles, labels = ax.get_legend_handles_labels()     by_label = dict(zip(labels, handles))     ax.legend(by_label.values(), by_label.keys())     return ax"},{"location":"examples/yacht/","title":"UCI Data Benchmarking","text":"In\u00a0[\u00a0]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax as ox\nimport pandas as pd\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nkey = jr.PRNGKey(123)\nplt.style.use(\"./gpjax.mplstyle\")\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import optax as ox import pandas as pd from sklearn.metrics import (     mean_squared_error,     r2_score, ) from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. key = jr.PRNGKey(123) plt.style.use(\"./gpjax.mplstyle\") cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[\u00a0]: Copied! <pre>yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\nX = yacht[:, :-1]\ny = yacht[:, -1].reshape(-1, 1)\n</pre> yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :] X = yacht[:, :-1] y = yacht[:, -1].reshape(-1, 1) In\u00a0[\u00a0]: Copied! <pre>Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\n</pre> Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) In\u00a0[\u00a0]: Copied! <pre>log_ytr = np.log(ytr)\nlog_yte = np.log(yte)\n\ny_scaler = StandardScaler().fit(log_ytr)\nscaled_ytr = y_scaler.transform(log_ytr)\nscaled_yte = y_scaler.transform(log_yte)\n</pre> log_ytr = np.log(ytr) log_yte = np.log(yte)  y_scaler = StandardScaler().fit(log_ytr) scaled_ytr = y_scaler.transform(log_ytr) scaled_yte = y_scaler.transform(log_yte) <p>We can see the effect of these transformations in the below three panels.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\nax[0].hist(ytr, bins=30, color=cols[1])\nax[0].set_title(\"y\")\nax[1].hist(log_ytr, bins=30, color=cols[1])\nax[1].set_title(\"log(y)\")\nax[2].hist(scaled_ytr, bins=30, color=cols[1])\nax[2].set_title(\"scaled log(y)\")\n</pre> fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5)) ax[0].hist(ytr, bins=30, color=cols[1]) ax[0].set_title(\"y\") ax[1].hist(log_ytr, bins=30, color=cols[1]) ax[1].set_title(\"log(y)\") ax[2].hist(scaled_ytr, bins=30, color=cols[1]) ax[2].set_title(\"scaled log(y)\") In\u00a0[\u00a0]: Copied! <pre>x_scaler = StandardScaler().fit(Xtr)\nscaled_Xtr = x_scaler.transform(Xtr)\nscaled_Xte = x_scaler.transform(Xte)\n</pre> x_scaler = StandardScaler().fit(Xtr) scaled_Xtr = x_scaler.transform(Xtr) scaled_Xte = x_scaler.transform(Xte) In\u00a0[\u00a0]: Copied! <pre>n_train, n_covariates = scaled_Xtr.shape\nkernel = gpx.RBF(active_dims=list(range(n_covariates)))\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\n\nlikelihood = gpx.Gaussian(num_datapoints=n_train)\n\nposterior = prior * likelihood\n</pre> n_train, n_covariates = scaled_Xtr.shape kernel = gpx.RBF(active_dims=list(range(n_covariates))) meanf = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel)  likelihood = gpx.Gaussian(num_datapoints=n_train)  posterior = prior * likelihood In\u00a0[\u00a0]: Copied! <pre>training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\n\nnegative_mll = jit(gpx.ConjugateMLL(negative=True))\noptimiser = ox.adamw(0.05)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=training_data,\n    optim=ox.adamw(learning_rate=0.05),\n    num_iters=500,\n    key=key,\n)\n</pre> training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)  negative_mll = jit(gpx.ConjugateMLL(negative=True)) optimiser = ox.adamw(0.05)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=training_data,     optim=ox.adamw(learning_rate=0.05),     num_iters=500,     key=key, ) In\u00a0[\u00a0]: Copied! <pre>latent_dist = opt_posterior(scaled_Xte, training_data)\npredictive_dist = likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_stddev = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior(scaled_Xte, training_data) predictive_dist = likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_stddev = predictive_dist.stddev() In\u00a0[\u00a0]: Copied! <pre>rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nr2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nprint(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")\n</pre> rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean) r2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean) print(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\") <p>Both of these metrics seem very promising, so, based off these, we can be quite happy that our first attempt at modelling the Yacht data is promising.</p> In\u00a0[\u00a0]: Copied! <pre>residuals = scaled_yte.squeeze() - predictive_mean\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\n\nax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\nax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\nax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\n\nax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\nax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\nax[1].set_ylim([-1.0, 1.0])\nax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n\nax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\nax[2].set_title(\"Residuals\")\n</pre> residuals = scaled_yte.squeeze() - predictive_mean  fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)  ax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1]) ax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes) ax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")  ax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1]) ax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes) ax[1].set_ylim([-1.0, 1.0]) ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")  ax[2].hist(np.asarray(residuals), bins=30, color=cols[1]) ax[2].set_title(\"Residuals\") <p>From this, we can see that our model is struggling to predict the smallest values of the Yacht's hydrodynamic and performs increasingly well as the Yacht's hydrodynamic performance increases. This is likely due to the original data's heavy right-skew, and successive modelling attempts may wish to introduce a heteroscedastic likelihood function that would enable more flexible modelling of the smaller response values.</p> In\u00a0[\u00a0]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder'"},{"location":"examples/yacht/#uci-data-benchmarking","title":"UCI Data Benchmarking\u00b6","text":"<p>In this notebook, we will show how to apply GPJax on a benchmark UCI regression problem. These kind of tasks are often used in the research community to benchmark and assess new techniques against those already in the literature. Much of the code contained in this notebook can be adapted to applied problems concerning datasets other than the one presented here.</p>"},{"location":"examples/yacht/#data-loading","title":"Data Loading\u00b6","text":"<p>We'll be using the Yacht dataset from the UCI machine learning data repository. Each observation describes the hydrodynamic performance of a yacht through its resistance. The dataset contains 6 covariates and a single positive, real valued response variable. There are 308 observations in the dataset, so we can comfortably use a conjugate regression Gaussian process here (for more more details, checkout the Regression notebook).</p>"},{"location":"examples/yacht/#preprocessing","title":"Preprocessing\u00b6","text":"<p>With a dataset loaded, we'll now preprocess it such that it is more amenable to modelling with a Gaussian process.</p>"},{"location":"examples/yacht/#data-partitioning","title":"Data Partitioning\u00b6","text":"<p>We'll first partition our data into a training and testing split. We'll fit our Gaussian process to the training data and evaluate its performance on the test data. This allows us to investigate how effectively our Gaussian process generalises to out-of-sample datapoints and ensure that we are not overfitting. We'll hold 30% of our data back for testing purposes.</p>"},{"location":"examples/yacht/#response-variable","title":"Response Variable\u00b6","text":"<p>We'll now process our response variable $\\mathbf{y}$. As the below plots show, the data has a very long tail and is certainly not Gaussian. However, we would like to model a Gaussian response variable so that we can adopt a Gaussian likelihood function and leverage the model's conjugacy. To achieve this, we'll first log-scale the data, to bring the long right tail in closer to the data's mean. We'll then standardise the data such that is distributed according to a unit normal distribution. Both of these transformations are invertible through the log-normal expectation and variance formulae and the the inverse standardisation identity, should we ever need our model's predictions to be back on the scale of the original dataset.</p> <p>For transforming both the input and response variable, all transformations will be done with respect to the training data where relevant.</p>"},{"location":"examples/yacht/#input-variable","title":"Input Variable\u00b6","text":"<p>We'll now transform our input variable $\\mathbf{X}$ to be distributed according to a unit Gaussian.</p>"},{"location":"examples/yacht/#model-fitting","title":"Model fitting\u00b6","text":"<p>With data now loaded and preprocessed, we'll proceed to defining a Gaussian process model and optimising its parameters. This notebook purposefully does not go into great detail on this process, so please see notebooks such as the Regression notebook and Classification notebook for further information.</p>"},{"location":"examples/yacht/#model-specification","title":"Model specification\u00b6","text":"<p>We'll use a radial basis function kernel to parameterise the Gaussian process in this notebook. As we have 5 covariates, we'll assign each covariate its own lengthscale parameter. This form of kernel is commonly known as an automatic relevance determination (ARD) kernel.</p> <p>In practice, the exact form of kernel used should be selected such that it represents your understanding of the data. For example, if you were to model temperature; a process that we know to be periodic, then you would likely wish to select a periodic kernel. Having Gaussian-ised our data somewhat, we'll also adopt a Gaussian likelihood function.</p>"},{"location":"examples/yacht/#model-optimisation","title":"Model Optimisation\u00b6","text":"<p>With a model now defined, we can proceed to optimise the hyperparameters of our model using Optax.</p>"},{"location":"examples/yacht/#prediction","title":"Prediction\u00b6","text":"<p>With an optimal set of parameters learned, we can make predictions on the set of data that we held back right at the start. We'll do this in the usual way by first computing the latent function's distribution before computing the predictive posterior distribution.</p>"},{"location":"examples/yacht/#evaluation","title":"Evaluation\u00b6","text":"<p>We'll now show how the performance of our Gaussian process can be evaluated by numerically and visually.</p>"},{"location":"examples/yacht/#metrics","title":"Metrics\u00b6","text":"<p>To numerically assess the performance of our model, two commonly used metrics are root mean squared error (RMSE) and the R2 coefficient. RMSE is simply the square root of the squared difference between predictions and actuals. A value of 0 for this metric implies that our model has 0 generalisation error on the test set. R2 measures the amount of variation within the data that is explained by the model. This can be useful when designing variance reduction methods such as control variates as it allows you to understand what proportion of the data's variance will be soaked up. A perfect model here would score 1 for R2 score, whereas predicting the data's mean would score 0 and models doing worse than simple mean predictions can score less than 0.</p>"},{"location":"examples/yacht/#diagnostic-plots","title":"Diagnostic plots\u00b6","text":"<p>To accompany the above metrics, we can also produce residual plots to explore exactly where our model's shortcomings lie. If we define a residual as the true value minus the prediction, then we can produce three plots:</p> <ol> <li>Predictions vs. actuals.</li> <li>Predictions vs. residuals.</li> <li>Residual density.</li> </ol> <p>The first plot allows us to explore if our model struggles to predict well for larger or smaller values by observing where the model deviates more from the line $y=x$. In the second plot we can inspect whether or not there were outliers or structure within the errors of our model. A well-performing model would have predictions close to and symmetrically distributed either side of $y=0$. Such a plot can be useful for diagnosing heteroscedasticity. Finally, by plotting a histogram of our residuals we can observe whether or not there is any skew to our residuals.</p>"},{"location":"examples/yacht/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"scripts/gen_pages/","title":"Gen pages","text":"In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\"gpjax\").rglob(\"*.py\")):\n    module_path = path.relative_to(\".\").with_suffix(\"\")\n    doc_path = path.relative_to(\"gpjax\").with_suffix(\".md\")\n    full_doc_path = Path(\"api\", doc_path)\n\n    parts = list(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n        continue\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()\n    # breakpoint()\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        identifier = \".\".join(parts)\n        print(\"::: \" + identifier, file=fd)  #\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\"gpjax\").rglob(\"*.py\")):     module_path = path.relative_to(\".\").with_suffix(\"\")     doc_path = path.relative_to(\"gpjax\").with_suffix(\".md\")     full_doc_path = Path(\"api\", doc_path)      parts = list(module_path.parts)      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")         continue     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()     # breakpoint()     with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         identifier = \".\".join(parts)         print(\"::: \" + identifier, file=fd)  #      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"api/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"api/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"}]}