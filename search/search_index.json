{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe1 Home","text":""},{"location":"#welcome-to-gpjax","title":"Welcome to GPJax","text":"<p>GPJax is a didactic Gaussian process (GP) library in JAX, supporting GPU acceleration and just-in-time compilation. We seek to provide a flexible API to enable researchers to rapidly prototype and develop new ideas.</p> <p></p>"},{"location":"#hello-gp","title":"\"Hello, GP!\"","text":"<p>Typing GP models is as simple as the maths we would write on paper, as shown below.</p> PythonMath <pre><code>import gpjax as gpx\n\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.RBF()\nprior = gpx.gps.Prior(mean_function = mean, kernel = kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints = 123)\n\nposterior = prior * likelihood\n</code></pre> <p>k(\u22c5,\u22c5\u2032)=\u03c32exp\u2061(\u2212\u2225\u22c5\u2212\u22c5\u2032\u2225222\u21132)p(f(\u22c5))=GP(0,k(\u22c5,\u22c5\u2032))p(y \u2223 f(\u22c5))=N(y \u2223 f(\u22c5),\u03c3n2)p(f(\u22c5) \u2223 y)\u221dp(f(\u22c5))p(y \u2223 f(\u22c5)) . \\begin{align} k(\\cdot, \\cdot') &amp; = \\sigma^2\\exp\\left(-\\frac{\\lVert \\cdot- \\cdot'\\rVert_2^2}{2\\ell^2}\\right)\\\\ p(f(\\cdot)) &amp; = \\mathcal{GP}(\\mathbf{0}, k(\\cdot, \\cdot')) \\\\ p(y\\,|\\, f(\\cdot)) &amp; = \\mathcal{N}(y\\,|\\, f(\\cdot), \\sigma_n^2) \\\\ \\\\ p(f(\\cdot) \\,|\\, y) &amp; \\propto p(f(\\cdot))p(y\\,|\\, f(\\cdot))\\,. \\end{align} k(\u22c5,\u22c5\u2032)p(f(\u22c5))p(y\u2223f(\u22c5))p(f(\u22c5)\u2223y)\u200b=\u03c32exp(\u22122\u21132\u2225\u22c5\u2212\u22c5\u2032\u222522\u200b\u200b)=GP(0,k(\u22c5,\u22c5\u2032))=N(y\u2223f(\u22c5),\u03c3n2\u200b)\u221dp(f(\u22c5))p(y\u2223f(\u22c5)).\u200b\u200b</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Install</p> <p>GPJax can be installed via pip. See our installation guide for further details.</p> <pre><code>pip install gpjax\n</code></pre> <p>New</p> <p>New to GPs? Then why not check out our introductory notebook that starts from Bayes' theorem and univariate Gaussian distributions.</p> <p>Begin</p> <p>Looking for a good place to start? Then why not begin with our regression notebook.</p>"},{"location":"#citing-gpjax","title":"Citing GPJax","text":"<p>If you use GPJax in your research, please cite our JOSS paper.</p> <pre><code>@article{Pinder2022,\n  doi = {10.21105/joss.04455},\n  url = {https://doi.org/10.21105/joss.04455},\n  year = {2022},\n  publisher = {The Open Journal},\n  volume = {7},\n  number = {75},\n  pages = {4455},\n  author = {Thomas Pinder and Daniel Dodd},\n  title = {GPJax: A Gaussian Process Framework in JAX},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"\ud83d\udc65 Code of conduct","text":"<p>Like the technical community as a whole, the GPJax team and community is made up of a mixture of professionals and volunteers from all over the world, working on every aspect of the mission - including mentorship, teaching and connecting people.</p> <p>Diversity is one of our huge strengths, but it can also lead to communication issues and unhappiness. To that end, we have a few ground rules that we ask people to adhere to when they're participating within this community and project. These rules apply equally to founders, mentors, and those seeking help and guidance.</p> <p>This isn't an exhaustive list of things that you can't do. Rather, take it in the spirit in which it's intended - a guide to make it easier to enrich all of us, the technical community and the conferences and usergroups we hope to guide new speakers to.</p> <p>This code of conduct applies to all communication: this includes IRC, the mailing list, and other forums such as Skype, Google+ Hangouts, etc.</p> <ul> <li>Be welcoming, friendly, and patient.</li> <li>Be considerate. Your work will be used by other people, and you in turn will depend on the work of others. Any decision you make will affect users and colleagues, and you should take those consequences into account when making decisions.</li> <li>Be respectful. Not all of us will agree all the time, but disagreement is no excuse for poor behaviour and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It's important to remember that a community where people feel uncomfortable or threatened is not a productive one. Members of the GPJax community should be respectful when dealing with other members as well as with people outside the GPJax community and with user groups/conferences, usergroup/conference organizers.</li> <li>Be careful in the words that you choose. Remember that sexist, racist, and other exclusionary jokes can be offensive to those around you. Be kind and welcoming to others. Do not insult or put down other participants. Behave professionally. The harassment and exclusionary behaviour towards others is unacceptable. This includes, but is not limited to:</li> <li>Violent threats or language directed against another person.</li> <li>Discriminatory jokes and language.</li> <li>Posting sexually explicit or violent material.</li> <li>Posting (or threatening to post) other people's personally identifying information (\"doxing\").</li> <li>Personal insults, especially those using racist or sexist terms.</li> <li>Unwelcome sexual attention.</li> <li>Advocating for, or encouraging, any of the above behaviour.</li> <li>Repeated harassment of others. In general, if someone asks you to stop, then stop.</li> <li>When we disagree, we try to understand why. Disagreements, both social and technical, happen all the time and GPJax is no exception. It is important that we resolve disagreements and differing views constructively. Remember that we're different. The strength of GPJax comes from its varied community, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn't mean that they're wrong. Don't forget that it is human to err and blaming each other doesn't get us anywhere, rather offer to help resolving issues and to help learn from mistakes.</li> </ul> <p>If you have experienced or witnessed any behaviour that violates anything in this document, then please report it through our contact form.</p> <p>Text adapted from the Speak Up! project and Django</p>"},{"location":"GOVERNANCE/","title":"GPJax Governance Document","text":""},{"location":"GOVERNANCE/#the-project","title":"The Project","text":"<p>GPJax is an open-source library that supports Gaussian process modelling in the JAX scientific computation ecosystem. The abstractions provided in GPJax are designed to mimic the underlying maths through, making the library easy to use for both researchers and practitioners alike.</p> <p>GPJax was created by Thomas Pinder as a Single-Maintainer Houseplant project following the BDFL model of governance. We have since moved to the governance model of Specialty Library and benefited from a community of contributors. This document outlines the governance structure for the current status.</p>"},{"location":"GOVERNANCE/#roles","title":"Roles","text":"<ul> <li>Contributors: Anyone who contributes to GPJAx is considered a contributor. This   includes submitting code, filing issues, reviewing pull requests, and participating in   discussions. They are listed under:</li> <li>https://github.com/JaxGaussianProcesses/GPJax/graphs/contributors</li> <li>Core contributors: Core contributors are contributors who have made significant   contributions to the GPJax project, for example large modules or functionality.</li> <li>GPJax gardeners: Gardeners are core contributors who are responsible for maintaining   the project and making decisions about its future direction. GPJax gardeners have the   ability to merge pull requests into the GPJax repository. GPJax gardeners also take on   administrative tasks such as website maintenance.</li> <li>Currently daniel-dodd@,      henrymoss@, st--@, and      thomaspinder@ are the gardeners of GPJax.</li> </ul>"},{"location":"GOVERNANCE/#responsibility","title":"Responsibility","text":"<p>We cannot hold anyone responsible really since we are all doing free work here, but some general expectations are: * Contributors are responsible for following the project's code of conduct and   contributing to the project in a positive and constructive manner. Contributors are   also responsible for testing their code and ensuring that it meets the project's   standards. * Core contributors are expected to review pull requests and provide feedback to   contributors. They also make decisions about the architecture and implementation of   the module/functionality they contributed to. Also the \u201cif you broke something please   fix it\u201d applies. * Maintainers are responsible for monitoring the benchmark, the documentation and the   website are up to date and built passed, update dependency and apply best practice</p> <p>In addition to these specific responsibilities, all contributors are encouraged to participate in discussions about the project and to help out in any way they can.</p>"},{"location":"GOVERNANCE/#decision-making","title":"Decision-making","text":"<p>Decisions about the GPJax project are made by consensus among the GPJax gardeners. This means that all GPJax gardeners must agree on a decision before it can be implemented. If a consensus cannot be reached, we will flip a (virtual) coin.</p>"},{"location":"GOVERNANCE/#communication","title":"Communication","text":"<p>Communication about the GPJax project takes place in the following channels:</p> <p>GitHub issues: Issues are used to track bugs, feature requests, and other tasks. GitHub discussion: Discussions are used to answer user questions, scope for features, and discuss solutions to bugs. GitHub pull requests: Pull requests propose changes to the GPJax codebase. Slack: The GPJax Slack Channel is used for internal communication about the project for core contributors.</p>"},{"location":"GOVERNANCE/#contributing","title":"Contributing","text":"<p>Anyone is welcome to contribute to the GPJax project. Contributions can be made in the form of code, documentation, or other forms of support. To learn more about how to contribute, please see the contributing guide.</p>"},{"location":"GOVERNANCE/#code-of-conduct","title":"Code of conduct","text":"<p>All contributors to the GPJax project are expected to follow the project's code of conduct. The code of conduct outlines the expected behavior of contributors and helps to ensure a welcoming and productive environment for all.</p> <p>Any breaches of the code of conduct should be reported using our contact form.</p>"},{"location":"GOVERNANCE/#governance-changes","title":"Governance changes","text":"<p>This governance document is subject to change. Changes to the governance document must be approved by consensus among the core contributors.</p>"},{"location":"GOVERNANCE/#contact","title":"Contact","text":"<p>If you have any questions about the GPJax project, please feel free to contact the maintainers or reach out over Slack.</p> <p>This file was adapted from BlackJAX.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#how-can-i-contribute","title":"How can I contribute?","text":"<p>GPJax welcomes contributions from interested individuals or groups. There are many ways to contribute, including:</p> <ul> <li>Answering questions on our discussions   page.</li> <li>Raising issues related to bugs   or desired enhancements.</li> <li>Contributing or improving the   docs or   examples.</li> <li>Fixing outstanding issues   (bugs).</li> <li>Extending or improving our codebase.</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of conduct","text":"<p>As a contributor to GPJax, you can help us keep the community open and inclusive. Please read and follow our Code of Conduct.</p>"},{"location":"contributing/#opening-issues-and-getting-support","title":"Opening issues and getting support","text":"<p>Please open issues on Github Issue Tracker. Here you can mention</p> <p>You can ask a question or start a discussion in the Discussion section on Github.</p>"},{"location":"contributing/#contributing-to-the-source-code","title":"Contributing to the source code","text":"<p>Submitting code contributions to GPJax is done via a GitHub pull request. Our preferred workflow is to first fork the GitHub repository, clone it to your local machine, and develop on a feature branch. Once you're happy with your changes, install our <code>pre-commit hooks</code>, <code>commit</code> and <code>push</code> your code.</p> <p>New to this? Don't panic, our guide below will walk you through every detail!</p> <p>Note</p> <p>Before opening a pull request we recommend you check our pull request checklist.</p>"},{"location":"contributing/#step-by-step-guide","title":"Step-by-step guide:","text":"<ol> <li> <p>Click here to Fork GPJax's   codebase (alternatively, click the 'Fork' button towards the top right of   the main repository page). This   adds a copy of the codebase to your GitHub user account.</p> </li> <li> <p>Clone your GPJax fork from your GitHub account to your local disk, and add   the base repository as a remote:   <pre><code>$ git clone git@github.com:&lt;your GitHub handle&gt;/GPJax.git\n$ cd GPJax\n$ git remote add upstream git@github.com:GPJax.git\n</code></pre></p> </li> <li> <p>Create a <code>feature</code> branch to hold your development changes:</p> </li> </ol> <p><pre><code>$ git checkout -b my-feature\n</code></pre>   Always use a <code>feature</code> branch. It's good practice to avoid   work on the <code>main</code> branch of any repository.</p> <ol> <li>We use Hatch for packaging and dependency management. Project requirements are in <code>pyproject.toml</code>. To install GPJax into a Hatch virtual environment, run:</li> </ol> <pre><code>$ hatch env create\n</code></pre> <p>At this point we recommend you check your installation passes the supplied unit tests:</p> <pre><code>$ hatch run dev:all-tests\n</code></pre> <ol> <li>Add changed files using <code>git add</code> and then <code>git commit</code> files to record your   changes locally:</li> </ol> <p><pre><code>$ git add modified_files\n$ git commit\n</code></pre>   After committing, it is a good idea to sync with the base repository in case   there have been any changes:</p> <pre><code>$ git fetch upstream\n$ git rebase upstream/main\n</code></pre> <p>Then push the changes to your GitHub account with:</p> <pre><code>$ git push -u origin my-feature\n</code></pre> <ol> <li>Go to the GitHub web page of your fork of the GPJax repo. Click the 'Pull   request' button to send your changes to the project's maintainers for   review.</li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<p>We welcome both complete or \"work in progress\" pull requests. Before opening one, we recommended you check the following guidelines to ensure a smooth review process.</p> <p>My contribution is a \"work in progress\":</p> <p>Please prefix the title of incomplete contributions with <code>[WIP]</code> (to indicate a work in progress). WIPs are useful to:</p> <ol> <li>Indicate you are working on something to avoid duplicated work.</li> <li>Request broad review of functionality or API.</li> <li>Seek collaborators.</li> </ol> <p>In the description of the pull request, we recommend you outline where work needs doing. For example, do some tests need writing?</p> <p>My contribution is complete:</p> <p>If addressing an issue, please use the pull request title to describe the issue and mention the issue number in the pull request description. This will make sure a link back to the original issue is created. Then before making your pull request, we recommend you check the following:</p> <ul> <li>Do all public methods have informative docstrings that describe their   function, input(s) and output(s)?</li> <li>Do the pre-commit hooks pass?</li> <li>Do the tests pass when everything is rebuilt from scratch?</li> <li> <p>Documentation and high-coverage tests are necessary for enhancements to be   accepted. Test coverage can be checked with:</p> <pre><code>$ hatch run dev:coverage\n</code></pre> </li> </ul> <p>Navigate to the newly created folder <code>htmlcov</code> and open <code>index.html</code> to view   the coverage report.</p> <p>This guide was derived from PyMC's guide to contributing.</p>"},{"location":"design/","title":"Design Principles","text":"<p><code>GPJax</code> is designed to be a Gaussian process package that provides an accurate representation of the underlying maths. Variable names are chosen to closely match the notation in [@rasmussen2006gaussian]. We here list the notation used in <code>GPJax</code> with its corresponding mathematical quantity.</p>"},{"location":"design/#gaussian-process-notation","title":"Gaussian process notation","text":"On paper GPJax code Description \\(n\\) n Number of train inputs \\(\\boldsymbol{x} = (x_1,\\dotsc,x_{n})\\) x Train inputs \\(\\boldsymbol{y} = (y_1,\\dotsc,y_{n})\\) y Train labels \\(\\boldsymbol{t}\\) t Test inputs \\(f(\\cdot)\\) f Latent function modelled as a GP \\(f({\\boldsymbol{x}})\\) fx Latent function at inputs \\(\\boldsymbol{x}\\) \\(\\boldsymbol{\\mu}_{\\boldsymbol{x}}\\) mux Prior mean at inputs \\(\\boldsymbol{x}\\) \\(\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) Kxx Kernel Gram matrix at inputs \\(\\boldsymbol{x}\\) \\(\\mathbf{L}_{\\boldsymbol{x}}\\) Lx Lower Cholesky decomposition of \\(\\boldsymbol{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) \\(\\mathbf{K}_{\\boldsymbol{t}\\boldsymbol{x}}\\) Ktx Cross-covariance between inputs \\(\\boldsymbol{t}\\) and \\(\\boldsymbol{x}\\)"},{"location":"design/#sparse-gaussian-process-notation","title":"Sparse Gaussian process notation","text":"On paper GPJax code Description \\(m\\) m Number of inducing inputs \\(\\boldsymbol{z} = (z_1,\\dotsc,z_{m})\\) z Inducing inputs \\(\\boldsymbol{u} = (u_1,\\dotsc,u_{m})\\) u Inducing outputs"},{"location":"design/#package-style","title":"Package style","text":"<p>Prior to building GPJax, the developers of GPJax have benefited greatly from the GPFlow and GPyTorch packages. As such, many of the design principles in GPJax are inspired by the excellent precursory packages. Documentation designs have been greatly inspired by the exceptional Flax docs.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-version","title":"Stable version","text":"<p>The latest stable release of <code>GPJax</code> can be installed via <code>pip</code>:</p> <pre><code>pip install gpjax\n</code></pre> <p>Check your installation</p> <p>We recommend you check your installation version: <pre><code>python -c 'import gpjax; print(gpjax.__version__)'\n</code></pre></p>"},{"location":"installation/#gputpu-support","title":"GPU/TPU support","text":"<p>Fancy using GPJax on GPU/TPU? Then you'll need to install JAX with the relevant hardware acceleration support as detailed in the JAX installation guide.</p>"},{"location":"installation/#development-version","title":"Development version","text":"<p>Warning</p> <p>This version is possibly unstable and may contain bugs.</p> <p>The latest development version of <code>GPJax</code> can be installed via running following:</p> <pre><code>git clone https://github.com/thomaspinder/GPJax.git\ncd GPJax\nhatch shell create\n</code></pre> <p>Tip</p> <p>We advise you create virtual environment before installing:</p> <pre><code>conda create -n gpjax_experimental python=3.10.0\nconda activate gpjax_experimental\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>hatch run dev:all-tests\n</code></pre>"},{"location":"sharp_bits/","title":"\ud83d\udd2a Sharp bits","text":""},{"location":"sharp_bits/#the-sharp-bits","title":"\ud83d\udd2a The sharp bits","text":""},{"location":"sharp_bits/#pseudo-randomness","title":"Pseudo-randomness","text":"<p>Libraries like NumPy and Scipy use stateful pseudorandom number generators (PRNGs). However, the PRNG in JAX is stateless. This means that for a given function, the return always returns the same result unless the seed is changed. This is a good thing, but it means that we need to be careful when using JAX's PRNGs.</p> <p>To examine what it means for a PRNG to be stateful, consider the following example:</p> <p><pre><code>import numpy as np\nimport jax.random as jr\nkey = jr.key(123)\n\n# NumPy\nprint('NumPy:')\nprint(np.random.random())\nprint(np.random.random())\n\nprint('\\nJAX:')\nprint(jr.uniform(key))\nprint(jr.uniform(key))\n\nprint('\\nSplitting key')\nkey, subkey = jr.split(key)\nprint(jr.uniform(subkey))\n</code></pre> <pre><code>NumPy:\n0.5194454541172852\n0.9815886617924413\n\nJAX:\n0.95821166\n0.95821166\n\nSplitting key\n0.23886406\n</code></pre> We can see that, in libraries like NumPy, the PRNG key's state is incremented whenever a pseudorandom call is made. This can make debugging difficult to manage as it is not always clear when a PRNG is being used. In JAX, the PRNG key is not incremented, so the same key will always return the same result. This has further positive benefits for reproducibility.</p> <p>GPJax relies on JAX's PRNGs for all random number generation. Whilst we try wherever possible to handle the PRNG key's state for you, care must be taken when defining your own models and inference schemes to ensure that the PRNG key is handled correctly. The JAX documentation has an excellent section on this.</p>"},{"location":"sharp_bits/#bijectors","title":"Bijectors","text":"<p>Parameters such as the kernel's lengthscale or variance have their support defined on a constrained subset of the real-line. During gradient-based optimisation, as we approach the set's boundary, it becomes possible that we could step outside of the set's support and introduce a numerical and mathematical error into our model. For example, consider the lengthscale parameter \\(\\ell\\), which we know must be strictly positive. If at \\(t^{\\text{th}}\\) iterate, our current estimate of \\(\\ell\\) was 0.02 and our derivative informed us that \\(\\ell\\) should decrease, then if our learning rate is greater is than 0.03, we would end up with a negative variance term. We visualise this issue below where the red cross denotes the invalid lengthscale value that would be obtained, were we to optimise in the unconstrained parameter space.</p> <p></p> <p>A simple but impractical solution would be to use a tiny learning rate which would reduce the possibility of stepping outside of the parameter's support. However, this would be incredibly costly and does not eradicate the problem. An alternative solution is to apply a functional mapping to the parameter that projects it from a constrained subspace of the real-line onto the entire real-line. Here, gradient updates are applied in the unconstrained parameter space before transforming the value back to the original support of the parameters. Such a transformation is known as a bijection.</p> <p></p> <p>To help understand this, we show the effect of using a log-exp bijector in the above figure. We have six points on the positive real line that range from 0.1 to 3 depicted by a blue cross. We then apply the bijector by log-transforming the constrained value. This gives us the points' unconstrained value which we depict by a red circle. It is this value that we apply gradient updates to. When we wish to recover the constrained value, we apply the inverse of the bijector, which is the exponential function in this case. This gives us back the blue cross.</p> <p>In GPJax, we supply bijective functions using Numpyro.</p>"},{"location":"sharp_bits/#positive-definiteness","title":"Positive-definiteness","text":"<p>\"Symmetric positive definiteness is one of the highest accolades to which a matrix can aspire\" - Nicholas Highman, Accuracy and stability of numerical algorithms [@higham2022accuracy]</p>"},{"location":"sharp_bits/#why-is-positive-definiteness-important","title":"Why is positive-definiteness important?","text":"<p>The Gram matrix of a kernel, a concept that we explore more in our kernels notebook. As such, we have a range of tools at our disposal to make subsequent operations on the covariance matrix faster. One of these tools is the Cholesky factorisation that uniquely decomposes any symmetric positive-definite matrix \\(\\mathbf{\\Sigma}\\) by</p> <p><p>\u03a3=LL\u22a4 , \\begin{align}     \\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\top}\\,, \\end{align} \u03a3=LL\u22a4,\u200b\u200b</p> where \\(\\mathbf{L}\\) is a lower triangular matrix.</p> <p>We make use of this result in GPJax when solving linear systems of equations of the form \\(\\mathbf{A}\\boldsymbol{x} = \\boldsymbol{b}\\). Whilst seemingly abstract at first, such problems are frequently encountered when constructing Gaussian process models. One such example is frequently encountered in the regression setting for learning Gaussian process kernel hyperparameters. Here we have labels \\(\\boldsymbol{y} \\sim \\mathcal{N}(f(\\boldsymbol{x}), \\sigma^2\\mathbf{I})\\) with \\(f(\\boldsymbol{x}) \\sim \\mathcal{N}(\\boldsymbol{0}, \\mathbf{K}_{\\boldsymbol{xx}})\\) arising from zero-mean Gaussian process prior and Gram matrix \\(\\mathbf{K}_{\\boldsymbol{xx}}\\) at the inputs \\(\\boldsymbol{x}\\). Here the marginal log-likelihood comprises the following form</p> <p>log\u2061p(y)=0.5(\u2212y\u22a4(Kxx+\u03c32I)\u22121y\u2212log\u2061\u2223Kxx+\u03c32I\u2223\u2212nlog\u2061(2\u03c0)), \\begin{align}     \\log p(\\boldsymbol{y}) = 0.5\\left(-\\boldsymbol{y}^{\\top}\\left(\\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I} \\right)^{-1}\\boldsymbol{y} -\\log\\lvert \\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I}\\rvert -n\\log(2\\pi)\\right) , \\end{align} logp(y)=0.5(\u2212y\u22a4(Kxx\u200b+\u03c32I)\u22121y\u2212log\u2223Kxx\u200b+\u03c32I\u2223\u2212nlog(2\u03c0)),\u200b\u200b</p> <p>and the goal of inference is to maximise kernel hyperparameters (contained in the Gram matrix \\(\\mathbf{K}_{\\boldsymbol{xx}}\\)) and likelihood hyperparameters (contained in the noise covariance \\(\\sigma^2\\mathbf{I}\\)). Computing the marginal log-likelihood (and its gradients), draws our attention to the term</p> <p>(Kxx+\u03c32I)\u22121\u23dfAy, \\begin{align}     \\underbrace{\\left(\\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I} \\right)^{-1}}_{\\mathbf{A}}\\boldsymbol{y}, \\end{align} A(Kxx\u200b+\u03c32I)\u22121\u200b\u200by,\u200b\u200b</p> <p>then we can see a solution can be obtained by solving the corresponding system of equations. By working with \\(\\mathbf{L} = \\operatorname{chol}{\\mathbf{A}}\\) instead of \\(\\mathbf{A}\\), we save a significant amount of floating-point operations (flops) by solving two triangular systems of equations (one for \\(\\mathbf{L}\\) and another for \\(\\mathbf{L}^{\\top}\\)) instead of one dense system of equations. Solving two triangular systems of equations has complexity \\(\\mathcal{O}(n^3/6)\\); a vast improvement compared to regular solvers that have \\(\\mathcal{O}(n^3)\\) complexity in the number of datapoints \\(n\\).</p>"},{"location":"sharp_bits/#the-cholesky-drawback","title":"The Cholesky drawback","text":"<p>While the computational acceleration provided by using Cholesky factors instead of dense matrices is hopefully now apparent, an awkward numerical instability gotcha can arise due to floating-point rounding errors. When we evaluate a covariance function on a set of points that are very close to one another, eigenvalues of the corresponding Gram matrix can get very small. While not mathematically less than zero, the smallest eigenvalues can become negative-valued due to finite-precision numerical errors. This becomes a problem when we want to compute a Cholesky factor since this requires that the input matrix is numerically positive-definite. If there are negative eigenvalues, this violates the requirements and results in a \"Cholesky failure\".</p> <p>To resolve this, we apply some numerical jitter to the diagonals of any Gram matrix. Typically this is very small, with \\(10^{-6}\\) being the system default. However, for some problems, this amount may need to be increased.</p>"},{"location":"sharp_bits/#slow-to-evaluate","title":"Slow-to-evaluate","text":"<p>Famously, a regular Gaussian process model (as detailed in our regression notebook) will scale cubically in the number of data points. Consequently, if you try to fit your Gaussian process model to a data set containing more than several thousand data points, then you will likely incur a significant computational overhead. In such cases, we recommend using Sparse Gaussian processes to alleviate this issue.</p> <p>When the data contains less than around 50000 data points, we recommend using the collapsed evidence lower bound objective [@titsias2009] to optimise the parameters of your sparse Gaussian process model. Such a model will scale linearly in the number of data points and quadratically in the number of inducing points. We demonstrate its use in our sparse regression notebook.</p> <p>For data sets exceeding 50000 data points, even the sparse Gaussian process outlined above will become computationally infeasible. In such cases, we recommend using the uncollapsed evidence lower bound objective [@hensman2013gaussian] that allows stochastic mini-batch optimisation of the parameters of your sparse Gaussian process model. Such a model will scale linearly in the batch size and quadratically in the number of inducing points. We demonstrate its use in our sparse stochastic variational inference notebook.</p>"},{"location":"sharp_bits/#jit-compilation","title":"JIT compilation","text":"<p>There are a subset of operations in GPJax that are not JIT compatible by default. This is because we have assertions in place to check the properties of the parameters. For example, we check that the lengthscale parameter that a user provides is positive. This makes for a better user experience as we can provide more informative error messages; however, JIT compiling functions wherein these assertions are made will break the code. As an example, consider the following code:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport gpjax as gpx\n\nx = jnp.linspace(0, 1, 10)[:, None]\n\ndef compute_gram(lengthscale):\n    k = gpx.kernels.RBF(active_dims=[0], lengthscale=lengthscale, variance=jnp.array(1.0))\n    return k.gram(x)\n\ncompute_gram(1.0)\n</code></pre> <p>so far so good. However, if we try to JIT compile this function, we will get an error:</p> <pre><code>jit_compute_gram = jax.jit(compute_gram)\ntry:\n    jit_compute_gram(1.0)\nexcept Exception as e:\n    print(e)\n</code></pre> <p>This error is due to the fact that the <code>RBF</code> kernel contains an assertion that checks that the lengthscale is positive. It does not matter that the assertion is satisfied; the very presence of the assertion will break JIT compilation.</p> <p>To resolve this, we can use the <code>checkify</code> decorator to remove the assertion. This will allow the function to be JIT compiled.</p> <p><pre><code>from jax.experimental import checkify\n\njit_compute_gram = jax.jit(checkify.checkify(compute_gram))\nerror, value = jit_compute_gram(1.0)\n</code></pre> By virtue of the <code>checkify.checkify</code>, a tuple is returned where the first element is the output of the assertion, and the second element is the value of the function. </p> <p>This design is not perfect, and in an ideal world we would not enforce the user to wrap their code in <code>checkify.checkify</code>. We are actively looking into cleaner ways to provide guardrails in a less intrusive manner. However, for now, should you try to JIT compile a component of GPJax wherein there is an assertion, you will need to wrap the function in <code>checkify.checkify</code> as shown above.</p> <p>For more on <code>checkify</code>, please see the JAX Checkify Doc.</p>"},{"location":"_examples/backend/","title":"Backend Module Design","text":"<p>Since v0.9, GPJax is built upon Flax's NNX module. This transition allows for more efficient parameter handling, improved integration with Flax and Flax-based libraries, and enhanced flexibility in model design. This notebook provides a high-level overview of the backend module design in GPJax. For an introduction to NNX, please refer to the official documentation.</p> <pre><code>import typing as tp\n\nfrom flax import nnx\n\n# Enable Float64 for more stable matrix inversions.\nfrom jax import (\n    config,\n    grad,\n)\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nfrom jaxtyping import (\n    Float,\n    Num,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom examples.utils import use_mpl_style\nfrom gpjax.mean_functions import (\n    AbstractMeanFunction,\n    Constant,\n)\nfrom gpjax.parameters import (\n    DEFAULT_BIJECTION,\n    Parameter,\n    PositiveReal,\n    Real,\n    transform,\n)\nfrom gpjax.typing import (\n    Array,\n    ScalarFloat,\n)\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\n# set the default style for plotting\nuse_mpl_style()\n\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/backend/#parameters","title":"Parameters","text":"<p>The biggest change bought about by the transition to an NNX backend is the increased support we now provide for handling parameters. As discussed in our Sharp Bits - Bijectors Doc, GPJax uses bijectors to transform constrained parameters to unconstrained parameters during optimisation. You may now register the support of a parameter using our <code>Parameter</code> class. To see this, consider the constant mean function who contains a single constant parameter whose value ordinarily exists on the real line. We can register this parameter as follows:</p> <pre><code>constant_param = Parameter(value=1.0, tag=None)\nmeanf = Constant(constant_param)\nprint(meanf)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mConstant\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Parameter: 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mconstant\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mParameter\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n    \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'None'\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\n</code></pre> <p>However, suppose you wish your mean function's constant parameter to be strictly positive. This is easy to achieve by using the correct Parameter type which, in this case, will be the <code>PositiveReal</code>. However, any Parameter that subclasses from <code>Parameter</code> will be transformed by GPJax.</p> <pre><code>issubclass(PositiveReal, Parameter)\n</code></pre> <pre><code>True\n</code></pre> <p>Injecting this newly constrained parameter into our mean function is then identical to before.</p> <pre><code>constant_param = PositiveReal(value=1.0)\nmeanf = Constant(constant_param)\nprint(meanf)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mConstant\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # PositiveReal: 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mconstant\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n    \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n    \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'positive'\"\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\n</code></pre> <p>Were we to try and instantiate the <code>PositiveReal</code> class with a negative value, then an explicit error would be raised.</p> <pre><code>try:\n    PositiveReal(value=-1.0)\nexcept ValueError as e:\n    print(e)\n</code></pre> <pre><code>value needs to be positive, got -1.0 (`check` failed)\n</code></pre>"},{"location":"_examples/backend/#parameter-transforms","title":"Parameter Transforms","text":"<p>With a parameter instantiated, you likely wish to transform the parameter's value from its constrained support onto the entire real line. To do this, you can apply the <code>transform</code> function to the parameter. To control the bijector used to transform the parameter, you may pass a set of bijectors into the transform function. Under-the-hood, the <code>transform</code> function is looking up the bijector of a parameter using it's <code>_tag</code> field in the bijector dictionary, and then applying the bijector to the parameter's value using a tree map operation.</p> <pre><code>print(constant_param._tag)\n</code></pre> <pre><code>positive\n</code></pre> <p>For most users, you will not need to worry about this as we provide a set of default bijectors that are defined for all the parameter types we support. However, see our Kernel Guide Notebook to see how you can define your own bijectors and parameter types.</p> <pre><code>print(DEFAULT_BIJECTION[constant_param._tag])\n</code></pre> <pre><code>&lt;numpyro.distributions.transforms.SoftplusTransform object at 0x7f89af847940&gt;\n</code></pre> <p>We see here that the Softplus bijector is specified as the default for strictly positive parameters. To apply this, we must first realise the state of our model. This is achieved using the <code>split</code> function provided by <code>nnx</code>.</p> <pre><code>_, _params = nnx.split(meanf, Parameter)\n\ntranformed_params = transform(_params, DEFAULT_BIJECTION, inverse=True)\n</code></pre> <p>The parameter's value was changed here from 1. to 0.54132485. This is the result of applying the Softplus bijector to the parameter's value and projecting its value onto the real line. Were the parameter's value to be closer to 0, then the transformation would be more pronounced.</p> <pre><code>_, _close_to_zero_state = nnx.split(Constant(PositiveReal(value=1e-6)), Parameter)\n\ntransform(_close_to_zero_state, DEFAULT_BIJECTION, inverse=True)\n</code></pre> <pre><code>State({\n  'constant': VariableState( # 1 (8 B)\n    type=PositiveReal,\n    value=Array(-13.81551006, dtype=float64, weak_type=True),\n    _tag='positive'\n  )\n})\n</code></pre>"},{"location":"_examples/backend/#transforming-multiple-parameters","title":"Transforming Multiple Parameters","text":"<p>In the above, we transformed a single parameter. However, in practice your parameters may be nested within several functions e.g., a kernel function within a GP model. Fortunately, transforming several parameters is a simple operation that we here demonstrate for a conjugate GP posterior (see our Regression Notebook for detailed explanation of this model.).</p> <pre><code>kernel = gpx.kernels.Matern32()\nmeanf = gpx.mean_functions.Constant()\n\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n\nlikelihood = gpx.likelihoods.Gaussian(100)\nposterior = likelihood * prior\nprint(posterior)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mConjugatePosterior\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # NonNegativeReal: 2 (16 B), PositiveReal: 1 (8 B), Real: 1 (8 B), Total: 4 (32 B)\u001b[0m\n  \u001b[38;2;156;220;254mprior\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPrior\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # PositiveReal: 1 (8 B), NonNegativeReal: 1 (8 B), Real: 1 (8 B), Total: 3 (24 B)\u001b[0m\n    \u001b[38;2;156;220;254mkernel\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mMatern32\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # PositiveReal: 1 (8 B), NonNegativeReal: 1 (8 B), Total: 2 (16 B)\u001b[0m\n      \u001b[38;2;156;220;254mactive_dims\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mslice(None, None, None),\n      \u001b[38;2;156;220;254mn_dims\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n      \u001b[38;2;156;220;254mcompute_engine\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m&lt;gpjax.kernels.computations.dense.DenseKernelComputation object at 0x7f89af7045e0&gt;,\n      \u001b[38;2;156;220;254mlengthscale\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'positive'\"\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m,\n      \u001b[38;2;156;220;254mvariance\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'non_negative'\"\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m,\n    \u001b[38;2;156;220;254mmean_function\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mConstant\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Real: 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mconstant\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'real'\"\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m,\n    \u001b[38;2;156;220;254mjitter\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m1e-06\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m,\n  \u001b[38;2;156;220;254mlikelihood\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mGaussian\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # NonNegativeReal: 1 (8 B)\u001b[0m\n    \u001b[38;2;156;220;254mobs_stddev\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n      \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'non_negative'\"\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m,\n    \u001b[38;2;156;220;254mnum_datapoints\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m100\u001b[0m,\n    \u001b[38;2;156;220;254mintegrator\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m&lt;gpjax.integrators.AnalyticalGaussianIntegrator object at 0x7f89ac5bb670&gt;\n  \u001b[38;2;255;213;3m)\u001b[0m,\n  \u001b[38;2;156;220;254mjitter\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m1e-06\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\n</code></pre> <p>Now contained within the posterior PyGraph here there are four parameters: the kernel's lengthscale and variance, the noise variance of the likelihood, and the constant of the mean function. Using NNX, we may realise these parameters through the <code>nnx.split</code> function. The <code>split</code> function deomposes a PyGraph into a <code>GraphDef</code> and <code>State</code> object. As the name suggests, <code>State</code> contains information on the parameters' state, whilst <code>GraphDef</code> contains the information required to reconstruct a PyGraph from a give <code>State</code>.</p> <pre><code>graphdef, state = nnx.split(posterior)\nprint(state)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mState\u001b[0m\u001b[38;2;255;213;3m({\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n  \u001b[38;2;156;220;254m'likelihood'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'obs_stddev'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m,\n      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n      \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'non_negative'\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m,\n  \u001b[38;2;156;220;254m'prior'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'kernel'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'lengthscale'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'positive'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m,\n      \u001b[38;2;156;220;254m'variance'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'non_negative'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m,\n    \u001b[38;2;156;220;254m'mean_function'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'constant'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'real'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m\n\u001b[38;2;255;213;3m})\u001b[0m\n</code></pre> <p>The <code>State</code> object behaves just like a PyTree and, consequently, we may use JAX's <code>tree_map</code> function to alter the values of the <code>State</code>. The updated <code>State</code> can then be used to reconstruct our posterior. In the below, we simply increment each parameter's value by 1.</p> <pre><code>updated_state = jtu.tree_map(lambda x: x + 1, state)\nprint(updated_state)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mState\u001b[0m\u001b[38;2;255;213;3m({\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n  \u001b[38;2;156;220;254m'likelihood'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'obs_stddev'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m,\n      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(2., dtype=float64, weak_type=True),\n      \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'non_negative'\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m,\n  \u001b[38;2;156;220;254m'prior'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'kernel'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'lengthscale'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(2., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'positive'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m,\n      \u001b[38;2;156;220;254m'variance'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(2., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'non_negative'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m,\n    \u001b[38;2;156;220;254m'mean_function'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'constant'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'real'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m\n\u001b[38;2;255;213;3m})\u001b[0m\n</code></pre> <p>Let us now use NNX's <code>merge</code> function to reconstruct the posterior distribution using the updated state.</p> <pre><code>updated_posterior = nnx.merge(graphdef, updated_state)\nprint(updated_posterior)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mConjugatePosterior\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # NonNegativeReal: 2 (16 B), PositiveReal: 1 (8 B), Real: 1 (8 B), Total: 4 (32 B)\u001b[0m\n  \u001b[38;2;156;220;254mjitter\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m1e-06\u001b[0m,\n  \u001b[38;2;156;220;254mlikelihood\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mGaussian\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # NonNegativeReal: 1 (8 B)\u001b[0m\n    \u001b[38;2;156;220;254mintegrator\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m&lt;gpjax.integrators.AnalyticalGaussianIntegrator object at 0x7f89ac5bb670&gt;,\n    \u001b[38;2;156;220;254mnum_datapoints\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m100\u001b[0m,\n    \u001b[38;2;156;220;254mobs_stddev\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(2., dtype=float64, weak_type=True),\n      \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'non_negative'\"\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m,\n  \u001b[38;2;156;220;254mprior\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPrior\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # PositiveReal: 1 (8 B), NonNegativeReal: 1 (8 B), Real: 1 (8 B), Total: 3 (24 B)\u001b[0m\n    \u001b[38;2;156;220;254mjitter\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;182;207;169m1e-06\u001b[0m,\n    \u001b[38;2;156;220;254mkernel\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mMatern32\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # PositiveReal: 1 (8 B), NonNegativeReal: 1 (8 B), Total: 2 (16 B)\u001b[0m\n      \u001b[38;2;156;220;254mactive_dims\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mslice(None, None, None),\n      \u001b[38;2;156;220;254mcompute_engine\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m&lt;gpjax.kernels.computations.dense.DenseKernelComputation object at 0x7f89af7045e0&gt;,\n      \u001b[38;2;156;220;254mlengthscale\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(2., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'positive'\"\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m,\n      \u001b[38;2;156;220;254mn_dims\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;86;156;214mNone\u001b[0m,\n      \u001b[38;2;156;220;254mvariance\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(2., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'non_negative'\"\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m,\n    \u001b[38;2;156;220;254mmean_function\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mConstant\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # Real: 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mconstant\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'real'\"\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m\n  \u001b[38;2;255;213;3m)\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\n</code></pre> <p>However, we begun this point of conversation with bijectors in mind, so let us now see how bijectors may be applied to a collection of parameters in GPJax. Fortunately, this is very straightforward, and we may simply use the <code>transform</code> function as before.</p> <pre><code>transformed_state = transform(state, DEFAULT_BIJECTION, inverse=True)\nprint(transformed_state)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mState\u001b[0m\u001b[38;2;255;213;3m({\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n  \u001b[38;2;156;220;254m'likelihood'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'obs_stddev'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n      \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m,\n      \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0.54132485, dtype=float64, weak_type=True),\n      \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'non_negative'\u001b[0m\n    \u001b[38;2;255;213;3m)\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m,\n  \u001b[38;2;156;220;254m'prior'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'kernel'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'lengthscale'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0.54132485, dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'positive'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m,\n      \u001b[38;2;156;220;254m'variance'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mNonNegativeReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0.54132485, dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'non_negative'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m,\n    \u001b[38;2;156;220;254m'mean_function'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'constant'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'real'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m\n\u001b[38;2;255;213;3m})\u001b[0m\n</code></pre> <p>We may also (re-)constrain the parameters' values by setting the <code>inverse</code> argument of <code>transform</code> to False.</p> <pre><code>retransformed_state = transform(transformed_state, DEFAULT_BIJECTION, inverse=False)\n</code></pre>"},{"location":"_examples/backend/#fine-scale-control","title":"Fine-Scale Control","text":"<p>One of the advantages of being able to split and re-merge the PyGraph is that we are able to gain fine-scale control over the parameters' whose state we wish to realise. This is by virtue of the fact that each of our parameters now inherit from <code>gpjax.parameters.Parameter</code>. In the former, we were simply extracting any <code>Parameter</code>subclass from the posterior. However, suppose we only wish to extract those parameters whose support is the positive real line. This is easily achieved by altering the way in which we invoke <code>nnx.split</code>.</p> <pre><code>graphdef, positive_reals, other_params = nnx.split(posterior, PositiveReal, ...)\nprint(positive_reals)\n</code></pre> <pre><code>\u001b[38;2;79;201;177mState\u001b[0m\u001b[38;2;255;213;3m({\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n  \u001b[38;2;156;220;254m'prior'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n    \u001b[38;2;156;220;254m'kernel'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;255;213;3m{\u001b[0m\u001b[38;2;105;105;105m\u001b[0m\n      \u001b[38;2;156;220;254m'lengthscale'\u001b[0m\u001b[38;2;212;212;212m: \u001b[0m\u001b[38;2;79;201;177mVariableState\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n        \u001b[38;2;156;220;254mtype\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;79;201;177mPositiveReal\u001b[0m,\n        \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n        \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m'positive'\u001b[0m\n      \u001b[38;2;255;213;3m)\u001b[0m\n    \u001b[38;2;255;213;3m}\u001b[0m\n  \u001b[38;2;255;213;3m}\u001b[0m\n\u001b[38;2;255;213;3m})\u001b[0m\n</code></pre> <p>Now we see that we have two state objects: one containing the positive real parameters and the other containing the remaining parameters. This functionality is exceptionally useful as it allows us to efficiently operate on a subset of the parameters whilst leaving the others untouched. Looking forward, we hope to use this functionality in our Variational Inference Approximations to perform more efficient updates of the variational parameters and then the model's hyperparameters.</p>"},{"location":"_examples/backend/#nnx-modules","title":"NNX Modules","text":"<p>To conclude this notebook, we will now demonstrate the ease of use and flexibility offered by NNX modules. To do this, we will implement a linear mean function using the existing abstractions in GPJax.</p> <p>For inputs \\(x_n \\in \\mathbb{R}^d\\), the linear mean function \\(m(x): \\mathbb{R}^d \\to \\mathbb{R}\\) is defined as: $$ m(x) = \\alpha + \\sum_{i=1}^d \\beta_i x_i $$ where \\(\\alpha \\in \\mathbb{R}\\) and \\(\\beta_i \\in \\mathbb{R}\\) are the parameters of the mean function. Let's now implement that using the new NNX backend.</p> <pre><code>class LinearMeanFunction(AbstractMeanFunction):\n    def __init__(\n        self,\n        intercept: tp.Union[ScalarFloat, Float[Array, \" O\"], Parameter] = 0.0,\n        slope: tp.Union[ScalarFloat, Float[Array, \" D O\"], Parameter] = 0.0,\n    ):\n        if isinstance(intercept, Parameter):\n            self.intercept = intercept\n        else:\n            self.intercept = Real(jnp.array(intercept))\n\n        if isinstance(slope, Parameter):\n            self.slope = slope\n        else:\n            self.slope = Real(jnp.array(slope))\n\n    def __call__(self, x: Num[Array, \"N D\"]) -&gt; Float[Array, \"N O\"]:\n        return self.intercept.value + jnp.dot(x, self.slope.value)\n</code></pre> <p>As we can see, the implementation is straightforward and concise. The <code>AbstractMeanFunction</code> module is a subclass of <code>nnx.Module</code> and may, therefore, be used in any <code>split</code> or <code>merge</code> call. Further, we have registered the intercept and slope parameters as <code>Real</code> parameter types. This registers their value in the PyGraph and means that they will be part of any operation applied to the PyGraph e.g., transforming and differentiation.</p> <p>To check our implementation worked, let's now plot the value of our mean function for a linearly spaced set of inputs.</p> <pre><code>N = 100\nX = jnp.linspace(-5.0, 5.0, N)[:, None]\n\nmeanf = LinearMeanFunction(intercept=1.0, slope=2.0)\nplt.plot(X, meanf(X))\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7f899d2a0610&gt;]\n</code></pre> <p></p> <p>Looks good! To conclude this section, let's now parameterise a GP with our new mean function and see how gradients may be computed.</p> <pre><code>y = jnp.sin(X)\nD = gpx.Dataset(X, y)\n\nprior = gpx.gps.Prior(mean_function=meanf, kernel=gpx.kernels.Matern32())\nlikelihood = gpx.likelihoods.Gaussian(D.n)\nposterior = likelihood * prior\n</code></pre> <p>We'll compute derivatives of the conjugate marginal log-likelihood, with respect to the unconstrained state of the kernel, mean function, and likelihood parameters.</p> <pre><code>graphdef, params, others = nnx.split(posterior, Parameter, ...)\nparams = transform(params, DEFAULT_BIJECTION, inverse=True)\n\n\ndef loss_fn(params: nnx.State, data: gpx.Dataset) -&gt; ScalarFloat:\n    params = transform(params, DEFAULT_BIJECTION)\n    model = nnx.merge(graphdef, params, *others)\n    return -gpx.objectives.conjugate_mll(model, data)\n\n\nparam_grads = grad(loss_fn)(params, D)\n</code></pre> <p>In practice, you would wish to perform multiple iterations of gradient descent to learn the optimal parameter values. However, for the purposes of illustration, we use another <code>tree_map</code> in the below to update the parameters' state using their previously computed gradients. As you can see, the really beauty in having access to the model's state is that we have full control over the operations that we perform to the state.</p> <pre><code>LEARNING_RATE = 0.01\noptimised_params = jtu.tree_map(\n    lambda _params, _grads: _params + LEARNING_RATE * _grads, params, param_grads\n)\n</code></pre> <p>Now we will plot the updated mean function alongside its initial form. To achieve this, we first merge the state back into the model using <code>merge</code>, and we then simply invoke the model as normal.</p> <pre><code>optimised_posterior = nnx.merge(graphdef, optimised_params, *others)\n\nfig, ax = plt.subplots()\nax.plot(X, optimised_posterior.prior.mean_function(X), label=\"Updated mean function\")\nax.plot(X, meanf(X), label=\"Initial mean function\")\nax.legend()\nax.set(xlabel=\"x\", ylabel=\"m(x)\")\n</code></pre> <pre><code>[Text(0.5, 0, 'x'), Text(0, 0.5, 'm(x)')]\n</code></pre> <p></p>"},{"location":"_examples/backend/#conclusions","title":"Conclusions","text":"<p>In this notebook we have explored how GPJax's Flax-based backend may be easily manipulated and extended. For a more applied look at this, see how we construct a kernel on polar coordinates in our Kernel Guide notebook.</p>"},{"location":"_examples/backend/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njaxtyping : 0.3.2\nflax      : 0.10.6\nmatplotlib: 3.10.3\njax       : 0.6.0\ngpjax     : 0.11.1\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/barycentres/","title":"Gaussian Processes Barycentres","text":"<p>In this notebook we'll give an implementation of . In this work, the existence of a Wasserstein barycentre between a collection of Gaussian processes is proven. When faced with trying to average a set of probability distributions, the Wasserstein barycentre is an attractive choice as it enables uncertainty amongst the individual distributions to be incorporated into the averaged distribution. When compared to a naive mean of means and mean of variances approach to computing the average probability distributions, it can be seen that Wasserstein barycentres offer significantly more favourable uncertainty estimation.</p> <pre><code>import typing as tp\n\nimport jax\n\n# Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy.linalg as jsl\nfrom jaxtyping import install_import_hook\nimport matplotlib.pyplot as plt\nimport numpyro.distributions as npd\n\nfrom examples.utils import use_mpl_style\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nkey = jr.key(123)\n\n# set the default style for plotting\nuse_mpl_style()\n\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/barycentres/#background","title":"Background","text":""},{"location":"_examples/barycentres/#wasserstein-distance","title":"Wasserstein distance","text":"<p>The 2-Wasserstein distance metric between two probability measures \\(\\mu\\) and \\(\\nu\\) quantifies the minimal cost required to transport the unit mass from \\(\\mu\\) to \\(\\nu\\), or vice-versa. Typically, computing this metric requires solving a linear program. However, when \\(\\mu\\) and \\(\\nu\\) both belong to the family of multivariate Gaussian distributions, the solution is analytically given by</p> \\[ W_2^2(\\mu, \\nu) = \\lVert m_1- m_2 \\rVert^2_2 + \\operatorname{Tr}(S_1 + S_2 - 2(S_1^{1/2}S_2S_1^{1/2})^{1/2}), \\] <p>where \\(\\mu \\sim \\mathcal{N}(m_1, S_1)\\) and \\(\\nu\\sim\\mathcal{N}(m_2, S_2)\\).</p>"},{"location":"_examples/barycentres/#wasserstein-barycentre","title":"Wasserstein barycentre","text":"<p>For a collection of \\(T\\) measures \\(\\lbrace\\mu_i\\rbrace_{t=1}^T \\in \\mathcal{P}_2(\\theta)\\), the Wasserstein barycentre \\(\\bar{\\mu}\\) is the measure that minimises the average Wasserstein distance to all other measures in the set. More formally, the Wasserstein barycentre is the Fr\u00e9chet mean on a Wasserstein space that we can write as</p> \\[ \\bar{\\mu} = \\operatorname{argmin}_{\\mu\\in\\mathcal{P}_2(\\theta)}\\sum_{t=1}^T \\alpha_t W_2^2(\\mu, \\mu_t), \\] <p>where \\(\\alpha\\in\\mathbb{R}^T\\) is a weight vector that sums to 1.</p> <p>As with the Wasserstein distance, identifying the Wasserstein barycentre \\(\\bar{\\mu}\\) is often an computationally demanding optimisation problem. However, when all the measures admit a multivariate Gaussian density, the barycentre \\(\\bar{\\mu} = \\mathcal{N}(\\bar{m}, \\bar{S})\\) has analytical solutions</p> \\[ \\bar{m} = \\sum_{t=1}^T \\alpha_t m_t\\,, \\quad \\bar{S}=\\sum_{t=1}^T\\alpha_t (\\bar{S}^{1/2}S_t\\bar{S}^{1/2})^{1/2}\\,. \\qquad (\\star) \\] <p>Identifying \\(\\bar{S}\\) is achieved through a fixed-point iterative update.</p>"},{"location":"_examples/barycentres/#barycentre-of-gaussian-processes","title":"Barycentre of Gaussian processes","text":"<p>It was shown in  that the barycentre \\(\\bar{f}\\) of a collection of Gaussian processes \\(\\lbrace f_i\\rbrace_{i=1}^T\\) such that \\(f_i \\sim \\mathcal{GP}(m_i, K_i)\\) can be found using the same solutions as in \\((\\star)\\). For a full theoretical understanding, we recommend reading the original paper. However, the central argument to this result is that one can first show that the barycentre GP \\(\\bar{f}\\sim\\mathcal{GP}(\\bar{m}, \\bar{S})\\) is non-degenerate for any finite set of GPs \\(\\lbrace f_t\\rbrace_{t=1}^T\\) i.e., \\(T&lt;\\infty\\). With this established, one can show that for a \\(n\\)-dimensional finite Gaussian distribution \\(f_{i,n}\\), the Wasserstein metric between any two Gaussian distributions \\(f_{i, n}, f_{j, n}\\) converges to the Wasserstein metric between GPs as \\(n\\to\\infty\\).</p> <p>In this notebook, we will demonstrate how this can be achieved in GPJax.</p>"},{"location":"_examples/barycentres/#dataset","title":"Dataset","text":"<p>We'll simulate five datasets and develop a Gaussian process posterior before identifying the Gaussian process barycentre at a set of test points. Each dataset will be a sine function with a different vertical shift, periodicity, and quantity of noise.</p> <pre><code>n = 100\nn_test = 200\nn_datasets = 5\n\nx = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1)\nxtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1)\nf = lambda x, a, b: a + jnp.sin(b * x)\n\nys = []\nfor _ in range(n_datasets):\n    key, subkey = jr.split(key)\n    vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)\n    period = jr.uniform(subkey, minval=0.75, maxval=1.25)\n    noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)\n    noise = jr.normal(subkey, shape=x.shape) * noise_amount\n    ys.append(f(x, vertical_shift, period) + noise)\n\ny = jnp.hstack(ys)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\")\nplt.show()\n</code></pre> <p></p>"},{"location":"_examples/barycentres/#learning-a-posterior-distribution","title":"Learning a posterior distribution","text":"<p>We'll now independently learn Gaussian process posterior distributions for each dataset. We won't spend any time here discussing how GP hyperparameters are optimised. For advice on achieving this, see the Regression notebook for advice on optimisation and the Kernels notebook for advice on selecting an appropriate kernel.</p> <pre><code>def fit_gp(x: jax.Array, y: jax.Array) -&gt; npd.MultivariateNormal:\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    D = gpx.Dataset(X=x, y=y)\n\n    likelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\n    posterior = (\n        gpx.gps.Prior(\n            mean_function=gpx.mean_functions.Constant(), kernel=gpx.kernels.RBF()\n        )\n        * likelihood\n    )\n\n    nmll = lambda p, d: -gpx.objectives.conjugate_mll(p, d)\n    opt_posterior, _ = gpx.fit_scipy(\n        model=posterior,\n        objective=nmll,\n        train_data=D,\n    )\n    latent_dist = opt_posterior.predict(xtest, train_data=D)\n    return opt_posterior.likelihood(latent_dist)\n\n\nposterior_preds = [fit_gp(x, i) for i in ys]\n</code></pre> <pre><code>Optimization terminated successfully.\n         Current function value: -31.899723\n         Iterations: 12\n         Function evaluations: 18\n         Gradient evaluations: 18\n\n\nOptimization terminated successfully.\n         Current function value: 28.634760\n         Iterations: 11\n         Function evaluations: 20\n         Gradient evaluations: 20\n\n\nOptimization terminated successfully.\n         Current function value: -102.556037\n         Iterations: 10\n         Function evaluations: 19\n         Gradient evaluations: 19\n\n\nOptimization terminated successfully.\n         Current function value: -143.067418\n         Iterations: 13\n         Function evaluations: 23\n         Gradient evaluations: 23\n\n\n         Current function value: -271.634988\n         Iterations: 11\n         Function evaluations: 72\n         Gradient evaluations: 60\n\n\n/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n</code></pre>"},{"location":"_examples/barycentres/#computing-the-barycentre","title":"Computing the barycentre","text":"<p>In GPJax, the predictive distribution of a GP is given by a TensorFlow Probability distribution, making it straightforward to extract the mean vector and covariance matrix of each GP for learning a barycentre. We implement the fixed point scheme given in (3) in the following cell by utilising Jax's <code>vmap</code> operator to speed up large matrix operations using broadcasting in <code>tensordot</code>.</p> <pre><code>def sqrtm(A: jax.Array):\n    return jnp.real(jsl.sqrtm(A))\n\n\ndef wasserstein_barycentres(\n    distributions: tp.List[npd.MultivariateNormal], weights: jax.Array\n):\n    covariances = [d.covariance_matrix for d in distributions]\n    cov_stack = jnp.stack(covariances)\n    stack_sqrt = jax.vmap(sqrtm)(cov_stack)\n\n    def step(covariance_candidate: jax.Array, idx: None):\n        inner_term = jax.vmap(sqrtm)(\n            jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)\n        )\n        fixed_point = jnp.tensordot(weights, inner_term, axes=1)\n        return fixed_point, fixed_point\n\n    return step\n</code></pre> <p>With a function defined for learning a barycentre, we'll now compute it using the <code>lax.scan</code> operator that drastically speeds up for loops in Jax (see the Jax documentation). The iterative update will be executed 100 times, with convergence measured by the difference between the previous and current iteration that we can confirm by inspecting the <code>sequence</code> array in the following cell.</p> <pre><code>weights = jnp.ones((n_datasets,)) / n_datasets\n\nmeans = jnp.stack([d.mean for d in posterior_preds])\nbarycentre_mean = jnp.tensordot(weights, means, axes=1)\n\nstep_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights))\ninitial_covariance = jnp.eye(n_test)\n\nbarycentre_covariance, sequence = jax.lax.scan(\n    step_fn, initial_covariance, jnp.arange(50)\n)\nL = jnp.linalg.cholesky(barycentre_covariance)\n\nbarycentre_process = npd.MultivariateNormal(barycentre_mean, scale_tril=L)\n</code></pre>"},{"location":"_examples/barycentres/#plotting-the-result","title":"Plotting the result","text":"<p>With a barycentre learned, we can visualise the result. We can see that the result looks reasonable as it follows the sinusoidal curve of all the inferred GPs, and the uncertainty bands are sensible.</p> <pre><code>def plot(\n    dist: npd.MultivariateNormal,\n    ax,\n    color: str,\n    label: str = None,\n    ci_alpha: float = 0.2,\n    linewidth: float = 1.0,\n    zorder: int = 0,\n):\n    mu = dist.mean\n    sigma = jnp.sqrt(dist.variance)\n    ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)\n    ax.fill_between(\n        xtest.squeeze(),\n        mu - sigma,\n        mu + sigma,\n        alpha=ci_alpha,\n        color=color,\n        zorder=zorder,\n    )\n\n\nfig, ax = plt.subplots()\n[plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds]\nplot(\n    barycentre_process,\n    ax,\n    color=cols[0],\n    label=\"Barycentre\",\n    ci_alpha=0.5,\n    linewidth=2,\n    zorder=1,\n)\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f8420c27460&gt;\n</code></pre> <p></p>"},{"location":"_examples/barycentres/#displacement-interpolation","title":"Displacement interpolation","text":"<p>In the above example, we assigned uniform weights to each of the posteriors within the barycentre. In practice, we may have prior knowledge of which posterior is most likely to be the correct one. Regardless of the weights chosen, the barycentre remains a Gaussian process. We can interpolate between a pair of posterior distributions \\(\\mu_1\\) and \\(\\mu_2\\) to visualise the corresponding barycentre \\(\\bar{\\mu}\\).</p> <p></p>"},{"location":"_examples/barycentres/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njaxtyping : 0.3.2\nnumpyro   : 0.18.0\nmatplotlib: 3.10.3\ngpjax     : 0.11.1\njax       : 0.6.0\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/classification/","title":"Classification","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via maximum a posteriori (MAP). We focus on a classification task here.</p> <pre><code>import cola\nfrom flax import nnx\nimport jax\n\n# Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport numpyro.distributions as npd\nimport optax as ox\n\nfrom examples.utils import use_mpl_style\nfrom gpjax.lower_cholesky import lower_cholesky\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nidentity_matrix = jnp.eye\n\n# set the default style for plotting\nuse_mpl_style()\n\nkey = jr.key(42)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/classification/#dataset","title":"Dataset","text":"<p>With the necessary modules imported, we simulate a dataset \\(\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}\\) with inputs \\(\\boldsymbol{x}\\) sampled uniformly on \\((-1., 1)\\) and corresponding binary outputs</p> \\[ \\boldsymbol{y} = 0.5 * \\text{sign}(\\cos(2 *  + \\boldsymbol{\\epsilon})) + 0.5, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} * (0.05)^{2} \\right). \\] <p>We store our data \\(\\mathcal{D}\\) as a GPJax <code>Dataset</code> and create test inputs for later.</p> <pre><code>key, subkey = jr.split(key)\nx = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0)\ny = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x7fccaa1280a0&gt;\n</code></pre> <p></p>"},{"location":"_examples/classification/#map-inference","title":"MAP inference","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. Since our observations are binary, we choose a Bernoulli likelihood with a probit link function.</p> <pre><code>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Constant()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\n</code></pre> <p>We construct the posterior through the product of our prior and likelihood.</p> <pre><code>posterior = prior * likelihood\nprint(type(posterior))\n</code></pre> <pre><code>&lt;class 'gpjax.gps.NonConjugatePosterior'&gt;\n</code></pre> <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. We begin with maximum a posteriori (MAP) estimation, a fast inference procedure to obtain point estimates for the latent function and the kernel's hyperparameters by maximising the marginal log-likelihood.</p> <p>We can obtain a MAP estimate by optimising the log-posterior density with Optax's optimisers.</p> <pre><code>optimiser = ox.adam(learning_rate=0.01)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    # we use the negative lpd as we are minimising\n    objective=lambda p, d: -gpx.objectives.log_posterior_density(p, d),\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.01),\n    num_iters=1000,\n    key=key,\n)\n</code></pre> <pre><code>  0%|          | 0/1000 [00:00&lt;?, ?it/s]\n</code></pre> <p>From which we can make predictions at novel inputs, as illustrated below.</p> <pre><code>map_latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(map_latent_dist)\n\npredictive_mean = predictive_dist.mean\npredictive_std = jnp.sqrt(predictive_dist.variance)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\n\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7fcc78a12b00&gt;\n</code></pre> <p></p> <p>Here we projected the map estimates \\(\\hat{\\boldsymbol{f}}\\) for the function values \\(\\boldsymbol{f}\\) at the data points \\(\\boldsymbol{x}\\) to get predictions over the whole domain,</p> \\[\\begin{align} p(f(\\cdot)| \\mathcal{D})  \\approx q_{map}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) \\delta(\\boldsymbol{f} - \\hat{\\boldsymbol{f}}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}\\] <p>However, as a point estimate, MAP estimation is severely limited for uncertainty quantification, providing only a single piece of information about the posterior.</p>"},{"location":"_examples/classification/#laplace-approximation","title":"Laplace approximation","text":"<p>The Laplace approximation improves uncertainty quantification by incorporating curvature induced by the marginal log-likelihood's Hessian to construct an approximate Gaussian distribution centered on the MAP estimate. Writing \\(\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})\\) as the unormalised posterior for function values \\(\\boldsymbol{f}\\) at the datapoints \\(\\boldsymbol{x}\\), we can expand the log of this about the posterior mode \\(\\hat{\\boldsymbol{f}}\\) via a Taylor expansion. This gives:</p> \\[ \\begin{align} \\log\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) + \\left[\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})|_{\\hat{\\boldsymbol{f}}}\\right]^{T} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\mathcal{O}(\\lVert \\boldsymbol{f} - \\hat{\\boldsymbol{f}} \\rVert^3). \\end{align} \\] <p>Since \\(\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})\\) is zero at the mode, this suggests the following approximation</p> \\[ \\begin{align} \\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) \\approx \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) \\exp\\left\\{ \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) \\right\\} \\end{align}, \\] <p>that we identify as a Gaussian distribution, \\(p(\\boldsymbol{f}| \\mathcal{D}) \\approx q(\\boldsymbol{f}) := \\mathcal{N}(\\hat{\\boldsymbol{f}}, [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} )\\). Since the negative Hessian is positive definite, we can use the Cholesky decomposition to obtain the covariance matrix of the Laplace approximation at the datapoints below.</p> <pre><code>gram, cross_covariance = (kernel.gram, kernel.cross_covariance)\njitter = 1e-6\n\n# Compute (latent) function value map estimates at training points:\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nKxx = cola.PSD(Kxx)\nLx = lower_cholesky(Kxx)\nf_hat = Lx @ opt_posterior.latent.value\n\n# Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f):\ngraphdef, params, *static_state = nnx.split(\n    opt_posterior, gpx.parameters.Parameter, ...\n)\n\n\ndef loss(params, D):\n    model = nnx.merge(graphdef, params, *static_state)\n    return -gpx.objectives.log_posterior_density(model, D)\n\n\njacobian = jax.jacfwd(jax.jacrev(loss))(params, D)\nH = jacobian[\"latent\"].value[\"latent\"].value[:, 0, :, 0]\nL = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)\n\n# H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I\nL_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True)\nH_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\nLH = jnp.linalg.cholesky(H_inv)\nlaplace_approximation = npd.MultivariateNormal(f_hat.squeeze(), scale_tril=LH)\n</code></pre> <p>For novel inputs, we must project the above approximating distribution through the Gaussian conditional distribution \\(p(f(\\cdot)| \\boldsymbol{f})\\),</p> \\[\\begin{align} p(f(\\cdot)| \\mathcal{D}) \\approx q_{Laplace}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) q(\\boldsymbol{f}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} (\\mathbf{K}_{\\boldsymbol{xx}} - [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1}) \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}\\] <p>This is the same approximate distribution \\(q_{map}(f(\\cdot))\\), but we have perturbed the covariance by a curvature term of \\(\\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}\\). We take the latent distribution computed in the previous section and add this term to the covariance to construct \\(q_{Laplace}(f(\\cdot))\\).</p> <pre><code>def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; npd.MultivariateNormal:\n    map_latent_dist = opt_posterior.predict(xtest, train_data=D)\n\n    Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)\n    Kxx = opt_posterior.prior.kernel.gram(x)\n    Kxx += identity_matrix(D.n) * jitter\n    Kxx = cola.PSD(Kxx)\n\n    # Kxx\u207b\u00b9 Kxt\n    Kxx_inv_Kxt = cola.solve(Kxx, Kxt)\n\n    # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt\n    laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Kxt.T, H_inv), Kxx_inv_Kxt)\n\n    mean = map_latent_dist.mean\n    covariance = map_latent_dist.covariance_matrix + laplace_cov_term\n    L = jnp.linalg.cholesky(covariance)\n    return npd.MultivariateNormal(jnp.atleast_1d(mean.squeeze()), scale_tril=L)\n</code></pre> <p>From this we can construct the predictive distribution at the test points.</p> <pre><code>laplace_latent_dist = construct_laplace(xtest)\npredictive_dist = opt_posterior.likelihood(laplace_latent_dist)\n\npredictive_mean = predictive_dist.mean\npredictive_std = jnp.sqrt(predictive_dist.variance)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7fcc64c6d0c0&gt;\n</code></pre> <p></p>"},{"location":"_examples/classification/#system-configuration","title":"System configuration","text":"<pre><code>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\"\n</code></pre> <pre><code>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\ngpjax     : 0.11.1\nmatplotlib: 3.10.3\njaxtyping : 0.3.2\nflax      : 0.10.6\noptax     : 0.2.4\nnumpyro   : 0.18.0\ncola      : 0.0.7\njax       : 0.6.0\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/collapsed_vi/","title":"Sparse Gaussian Process Regression","text":"<p>In this notebook we consider sparse Gaussian process regression (SGPR) Titsias (2009). This is a solution for medium to large-scale conjugate regression problems. In order to arrive at a computationally tractable method, the approximate posterior is parameterized via a set of \\(m\\) pseudo-points \\(\\boldsymbol{z}\\). Critically, the approach leads to \\(\\mathcal{O}(nm^2)\\) complexity for approximate maximum likelihood learning and \\(O(m^2)\\) per test point for prediction.</p> <pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import (\n    config,\n    jit,\n)\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\n\nfrom examples.utils import use_mpl_style\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\n# set the default style for plotting\nuse_mpl_style()\n\nkey = jr.key(42)\n\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/collapsed_vi/#dataset","title":"Dataset","text":"<p>With the necessary modules imported, we simulate a dataset \\(\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{500}\\) with inputs \\(\\boldsymbol{x}\\) sampled uniformly on \\((-3., 3)\\) and corresponding independent noisy outputs</p> \\[\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(7\\boldsymbol{x}) + x \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.5^2 \\right).\\] <p>We store our data \\(\\mathcal{D}\\) as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p> <pre><code>n = 2500\nnoise = 0.5\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\nytest = f(xtest)\n</code></pre> <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise. We also plot an initial set of inducing points over the space.</p> <pre><code>n_inducing = 50\nz = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\nax.vlines(\n    x=z,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=0.5,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend(loc=\"best\")\nplt.show()\n</code></pre> <p></p> <p>Next we define the true posterior model for the data - note that whilst we can define this, it is intractable to evaluate.</p> <pre><code>meanf = gpx.mean_functions.Constant()\nkernel = gpx.kernels.RBF()  # 1-dimensional inputs\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior * likelihood\n</code></pre> <p>We now define the SGPR model through <code>CollapsedVariationalGaussian</code>. Through a set of inducing points \\(\\boldsymbol{z}\\) this object builds an approximation to the true posterior distribution. Consequently, we pass the true posterior and initial inducing points into the constructor as arguments.</p> <pre><code>q = gpx.variational_families.CollapsedVariationalGaussian(\n    posterior=posterior, inducing_inputs=z\n)\n</code></pre> <p>We now train our model akin to a Gaussian process regression model via the <code>fit</code> abstraction. Unlike the regression example given in the conjugate regression notebook, the inducing locations that induce our variational posterior distribution are now part of the model's parameters. Using a gradient-based optimiser, we can then optimise their location such that the evidence lower bound is maximised.</p> <pre><code>opt_posterior, history = gpx.fit(\n    model=q,\n    # we want want to minimize the *negative* ELBO\n    objective=lambda p, d: -gpx.objectives.collapsed_elbo(p, d),\n    train_data=D,\n    optim=ox.adamw(learning_rate=1e-2),\n    num_iters=500,\n    key=key,\n)\n</code></pre> <pre><code>  0%|          | 0/500 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")\n</code></pre> <pre><code>[Text(0.5, 0, 'Training iterate'), Text(0, 0.5, 'ELBO')]\n</code></pre> <p></p> <p>We show predictions of our model with the learned inducing points overlaid in grey.</p> <pre><code>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\ninducing_points = opt_posterior.inducing_inputs.value\n\nsamples = latent_dist.sample(key=key, sample_shape=(20,))\n\npredictive_mean = predictive_dist.mean\npredictive_std = jnp.sqrt(predictive_dist.variance)\n\nfig, ax = plt.subplots()\n\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\nax.plot(\n    xtest,\n    ytest,\n    label=\"Latent function\",\n    color=cols[1],\n    linestyle=\"-\",\n    linewidth=1,\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\n\n\nax.vlines(\n    x=inducing_points,\n    ymin=ytest.min(),\n    ymax=ytest.max(),\n    alpha=0.3,\n    linewidth=0.5,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nplt.show()\n</code></pre> <p></p>"},{"location":"_examples/collapsed_vi/#runtime-comparison","title":"Runtime comparison","text":"<p>Given the size of the data being considered here, inference in a GP with a full-rank covariance matrix is possible, albeit quite slow. We can therefore compare the speedup that we get from using the above sparse approximation with corresponding bound on the marginal log-likelihood against the marginal log-likelihood in the full model.</p> <pre><code>full_rank_model = gpx.gps.Prior(\n    mean_function=gpx.mean_functions.Zero(), kernel=gpx.kernels.RBF()\n) * gpx.likelihoods.Gaussian(num_datapoints=D.n)\nnmll = jit(lambda: -gpx.objectives.conjugate_mll(full_rank_model, D))\n%timeit nmll().block_until_ready()\n</code></pre> <pre><code>671 ms \u00b1 6.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre> <pre><code>nelbo = jit(lambda: -gpx.objectives.collapsed_elbo(q, D))\n%timeit nelbo().block_until_ready()\n</code></pre> <pre><code>2.25 ms \u00b1 89.5 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> <p>As we can see, the sparse approximation given here is much faster when compared against a full-rank model.</p>"},{"location":"_examples/collapsed_vi/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Daniel Dodd'\n</code></pre> <pre><code>Author: Daniel Dodd\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njax       : 0.6.0\noptax     : 0.2.4\njaxtyping : 0.3.2\ngpjax     : 0.11.1\nmatplotlib: 3.10.3\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/deep_kernels/","title":"Deep Kernel Learning","text":"<p>In this notebook we demonstrate how GPJax can be used in conjunction with Flax to build deep kernel Gaussian processes. Modelling data with discontinuities is a challenging task for regular Gaussian process models. However, as shown in , transforming the inputs to our Gaussian process model's kernel through a neural network can offer a solution to this.</p> <pre><code>from dataclasses import (\n    dataclass,\n    field,\n)\n\nfrom flax import nnx\nimport jax\n\n# Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom scipy.signal import sawtooth\n\nfrom examples.utils import use_mpl_style\nfrom gpjax.kernels.computations import (\n    AbstractKernelComputation,\n    DenseKernelComputation,\n)\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.kernels.base import AbstractKernel\n\n\n# set the default style for plotting\nuse_mpl_style()\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nkey = jr.key(42)\n</code></pre>"},{"location":"_examples/deep_kernels/#dataset","title":"Dataset","text":"<p>As previously mentioned, deep kernels are particularly useful when the data has discontinuities. To highlight this, we will use a sawtooth function as our data.</p> <pre><code>n = 500\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nytest = f(xtest)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\nax.plot(xtest, ytest, label=\"True function\")\nax.legend(loc=\"best\")\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f98742a0d30&gt;\n</code></pre> <p></p>"},{"location":"_examples/deep_kernels/#deep-kernels","title":"Deep kernels","text":""},{"location":"_examples/deep_kernels/#details","title":"Details","text":"<p>Instead of applying a kernel \\(k(\\cdot, \\cdot')\\) directly on some data, we seek to apply a feature map \\(\\phi(\\cdot)\\) that projects the data to learn more meaningful representations beforehand. In deep kernel learning, \\(\\phi\\) is a neural network whose parameters are learned jointly with the GP model's hyperparameters. The corresponding kernel is then computed by \\(k(\\phi(\\cdot), \\phi(\\cdot'))\\). Here \\(k(\\cdot,\\cdot')\\) is referred to as the base kernel.</p>"},{"location":"_examples/deep_kernels/#implementation","title":"Implementation","text":"<p>Although deep kernels are not currently supported natively in GPJax, defining one is straightforward as we now demonstrate. Inheriting from the base <code>AbstractKernel</code> in GPJax, we create the <code>DeepKernelFunction</code> object that allows the user to supply the neural network and base kernel of their choice. Kernel matrices are then computed using the regular <code>gram</code> and <code>cross_covariance</code> functions.</p> <pre><code>@dataclass\nclass DeepKernelFunction(AbstractKernel):\n    base_kernel: AbstractKernel\n    network: nnx.Module\n    compute_engine: AbstractKernelComputation = field(\n        default_factory=lambda: DenseKernelComputation()\n    )\n\n    def __call__(\n        self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        xt = self.network(x)\n        yt = self.network(y)\n        return self.base_kernel(xt, yt)\n</code></pre>"},{"location":"_examples/deep_kernels/#defining-a-network","title":"Defining a network","text":"<p>With a deep kernel object created, we proceed to define a neural network. Here we consider a small multi-layer perceptron with two linear hidden layers and ReLU activation functions between the layers. The first hidden layer contains 64 units, while the second layer contains 32 units. Finally, we'll make the output of our network a three units wide. The corresponding kernel that we define will then be of ARD form to allow for different lengthscales in each dimension of the feature space. Users may wish to design more intricate network structures for more complex tasks, which functionality is supported well in Haiku.</p> <pre><code>feature_space_dim = 3\n\n\nclass Network(nnx.Module):\n    def __init__(\n        self, rngs: nnx.Rngs, *, input_dim: int, inner_dim: int, feature_space_dim: int\n    ) -&gt; None:\n        self.layer1 = nnx.Linear(input_dim, inner_dim, rngs=rngs)\n        self.output_layer = nnx.Linear(inner_dim, feature_space_dim, rngs=rngs)\n        self.rngs = rngs\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        x = x.reshape((x.shape[0], -1))\n        x = self.layer1(x)\n        x = jax.nn.relu(x)\n        x = self.output_layer(x).squeeze()\n        return x\n\n\nforward_linear = Network(\n    nnx.Rngs(123), feature_space_dim=feature_space_dim, inner_dim=32, input_dim=1\n)\n</code></pre>"},{"location":"_examples/deep_kernels/#defining-a-model","title":"Defining a model","text":"<p>Having characterised the feature extraction network, we move to define a Gaussian process parameterised by this deep kernel. We consider a third-order Mat\u00e9rn base kernel and assume a Gaussian likelihood.</p> <pre><code>base_kernel = gpx.kernels.Matern52(\n    active_dims=list(range(feature_space_dim)),\n    lengthscale=jnp.ones((feature_space_dim,)),\n)\nkernel = DeepKernelFunction(network=forward_linear, base_kernel=base_kernel)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n</code></pre>"},{"location":"_examples/deep_kernels/#optimisation","title":"Optimisation","text":"<p>We train our model via maximum likelihood estimation of the marginal log-likelihood. The parameters of our neural network are learned jointly with the model's hyperparameter set.</p> <p>With the inclusion of a neural network, we take this opportunity to highlight the additional benefits gleaned from using Optax for optimisation. In particular, we showcase the ability to use a learning rate scheduler that decays the optimiser's learning rate throughout the inference. We decrease the learning rate according to a half-cosine curve over 700 iterations, providing us with large step sizes early in the optimisation procedure before approaching more conservative values, ensuring we do not step too far. We also consider a linear warmup, where the learning rate is increased from 0 to 1 over 50 steps to get a reasonable initial learning rate value.</p> <pre><code>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.01,\n    warmup_steps=75,\n    decay_steps=700,\n    end_value=0.0,\n)\n\noptimiser = ox.chain(\n    ox.clip(1.0),\n    ox.adamw(learning_rate=schedule),\n)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=lambda p, d: -gpx.objectives.conjugate_mll(p, d),\n    train_data=D,\n    optim=optimiser,\n    num_iters=800,\n    key=key,\n)\n</code></pre> <pre><code>  0%|          | 0/800 [00:00&lt;?, ?it/s]\n</code></pre>"},{"location":"_examples/deep_kernels/#prediction","title":"Prediction","text":"<p>With a set of learned parameters, the only remaining task is to predict the output of the model. We can do this by simply applying the model to a test data set.</p> <pre><code>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean\npredictive_std = jnp.sqrt(predictive_dist.variance)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f9830903df0&gt;\n</code></pre> <p></p>"},{"location":"_examples/deep_kernels/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\nflax      : 0.10.6\njax       : 0.6.0\noptax     : 0.2.4\nmatplotlib: 3.10.3\ngpjax     : 0.11.1\njaxtyping : 0.3.2\nscipy     : 1.15.3\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/graph_kernels/","title":"Graph Kernels","text":"<p>This notebook demonstrates how regression models can be constructed on the vertices of a graph using a Gaussian process with a Mat\u00e9rn kernel presented in . For a general discussion of the kernels supported within GPJax, see the kernels notebook.</p> <pre><code>import random\n\n# Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\nfrom examples.utils import use_mpl_style\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\n# set the default style for plotting\nuse_mpl_style()\n\nkey = jr.key(42)\n\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/graph_kernels/#graph-construction","title":"Graph construction","text":"<p>Our graph \\(\\mathcal{G}=\\lbrace V, E \\rbrace\\) comprises a set of vertices \\(V = \\lbrace v_1, v_2, \\ldots, v_n\\rbrace\\) and edges \\(E=\\lbrace (v_i, v_j)\\in V \\ : \\ i \\neq j\\rbrace\\). In particular, we will consider a barbell graph that is an undirected graph containing two clusters of vertices with a single shared edge between the two clusters.</p> <p>Contrary to the typical barbell graph, we'll randomly remove a subset of 30 edges within each of the two clusters. Given the 40 vertices within the graph, this results in 351 edges as shown below.</p> <pre><code>vertex_per_side = 20\nn_edges_to_remove = 30\np = 0.8\n\nG = nx.barbell_graph(vertex_per_side, 0)\n\nrandom.seed(123)\n[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\n\npos = nx.spring_layout(G, seed=123)  # positions for all nodes\n\nnx.draw(\n    G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n)\n</code></pre> <pre><code>/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/IPython/core/events.py:82: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</code></pre> <p></p>"},{"location":"_examples/graph_kernels/#computing-the-graph-laplacian","title":"Computing the graph Laplacian","text":"<p>Graph kernels use the Laplacian matrix \\(L\\) to quantify the smoothness of a signal (or function) on a graph</p> \\[L=D-A,\\] <p>where \\(D\\) is the diagonal degree matrix containing each vertices' degree and \\(A\\) is the adjacency matrix that has an \\((i,j)^{\\text{th}}\\) entry of 1 if \\(v_i, v_j\\) are connected and 0 otherwise. Networkx gives us an easy way to compute this.</p> <pre><code>L = nx.laplacian_matrix(G).toarray()\n</code></pre>"},{"location":"_examples/graph_kernels/#simulating-a-signal-on-the-graph","title":"Simulating a signal on the graph","text":"<p>Our task is to construct a Gaussian process \\(f(\\cdot)\\) that maps from the graph's vertex set \\(V\\) onto the real line. To that end, we begin by simulating a signal on the graph's vertices that we will go on to try and predict. We use a single draw from a Gaussian process prior to draw our response values \\(\\boldsymbol{y}\\) where we hardcode parameter values. The corresponding input value set for this model, denoted \\(\\boldsymbol{x}\\), is the index set of the graph's vertices.</p> <pre><code>x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\n\ntrue_kernel = gpx.kernels.GraphKernel(\n    laplacian=L,\n    lengthscale=2.3,\n    variance=3.2,\n    smoothness=6.1,\n)\nprior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=true_kernel)\n\nfx = prior(x)\ny = fx.sample(key=key, sample_shape=(1,)).reshape(-1, 1)\n\nD = gpx.Dataset(X=x, y=y)\n</code></pre> <pre><code>/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/jaxtyping/_decorator.py:473: UserWarning: X is not of type float64. Got X.dtype=int64. This may lead to numerical instability. \n  out = fn(*args, **kwargs)\n</code></pre> <p>We can visualise this signal in the following cell.</p> <pre><code>nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\n\nvmin, vmax = y.min(), y.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\nax = plt.gca()\ncbar = plt.colorbar(sm, ax=ax)\n</code></pre> <pre><code>/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/IPython/core/events.py:82: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</code></pre> <p></p>"},{"location":"_examples/graph_kernels/#constructing-a-graph-gaussian-process","title":"Constructing a graph Gaussian process","text":"<p>With our dataset created, we proceed to define our posterior Gaussian process and optimise the model's hyperparameters. Whilst our underlying space is the graph's vertex set and is therefore non-Euclidean, our likelihood is still Gaussian and the model is still conjugate. For this reason, we simply perform gradient descent on the GP's marginal log-likelihood term as in the regression notebook. We do this using the BFGS optimiser.</p> <pre><code>likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nkernel = gpx.kernels.GraphKernel(laplacian=L)\nprior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=kernel)\nposterior = prior * likelihood\n</code></pre> <p>For researchers and the curious reader, GPJax provides the ability to print the bibtex citation for objects such as the graph kernel through the <code>cite()</code> function.</p> <pre><code>print(gpx.cite(kernel))\n</code></pre> <pre><code>@inproceedings{borovitskiy2021matern,\nauthors = {Borovitskiy, Viacheslav and Azangulov, Iskander and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc and Durrande, Nicolas},\ntitle = {Mat\u00e9rn Gaussian Processes on Graphs},\nyear = {2021},\nbooktitle = {International Conference on Artificial Intelligence and Statistics},\n}\n</code></pre> <p>With a posterior defined, we can now optimise the model's hyperparameters.</p> <pre><code>opt_posterior, training_history = gpx.fit_scipy(\n    model=posterior,\n    objective=lambda p, d: -gpx.objectives.conjugate_mll(p, d),\n    train_data=D,\n)\n</code></pre> <pre><code>         Current function value: -113.647577\n         Iterations: 91\n         Function evaluations: 134\n         Gradient evaluations: 120\n\n\n/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n</code></pre>"},{"location":"_examples/graph_kernels/#making-predictions","title":"Making predictions","text":"<p>Having optimised our hyperparameters, we can now make predictions on the graph. Though we haven't defined a training and testing dataset here, we'll simply query the predictive posterior for the full graph to compare the root-mean-squared error (RMSE) of the model for the initialised parameters vs the optimised set.</p> <pre><code>initial_dist = likelihood(posterior(x, D))\npredictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\n\ninitial_mean = initial_dist.mean\nlearned_mean = predictive_dist.mean\n\nrmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\n\ninitial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\nlearned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\nprint(\n    f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\n    f\" {learned_rmse: .2f}\"\n)\n</code></pre> <pre><code>RMSE with initial parameters:  7.53\nRMSE with learned parameters:  0.20\n</code></pre> <p>We can also plot the source of error in our model's predictions on the graph by the following.</p> <pre><code>error = jnp.abs(learned_mean - y.squeeze())\n\nnx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\n\nvmin, vmax = error.min(), error.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nax = plt.gca()\ncbar = plt.colorbar(sm, ax=ax)\n</code></pre> <pre><code>/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/IPython/core/events.py:82: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</code></pre> <p></p> <p>Reassuringly, our model seems to provide equally good predictions in each cluster.</p>"},{"location":"_examples/graph_kernels/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\nmatplotlib: 3.10.3\njaxtyping : 0.3.2\ngpjax     : 0.11.1\nnetworkx  : 3.4.2\njax       : 0.6.0\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/intro_to_gps/","title":"New to Gaussian Processes?","text":"<p>Fantastic that you're here! This notebook is designed to be a gentle introduction to the mathematics of Gaussian processes (GPs). No prior knowledge of Bayesian inference or GPs is assumed, and this notebook is self-contained. At a high level, we begin by introducing Bayes' theorem and its implications within probabilistic modelling. We then proceed to introduce the Gaussian random variable along with its multivariate form. We conclude by showing how this notion can be extended to GPs.</p>"},{"location":"_examples/intro_to_gps/#bayes-theorem","title":"Bayes' Theorem","text":"<p>A probabilistic modelling task is comprised of an observed dataset \\(\\mathbf{y}\\) for which we construct a model. The parameters \\(\\theta\\) of our model are unknown, and our goal is to conduct inference to determine their range of likely values. To achieve this, we apply Bayes' theorem</p> \\[ \\begin{align}     p(\\theta\\mid\\mathbf{y}) = \\frac{p(\\theta)p(\\mathbf{y}\\mid\\theta)}{p(\\mathbf{y})} = \\frac{p(\\theta)p(\\mathbf{y}\\mid\\theta)}{\\int_{\\theta}p(\\mathbf{y}, \\theta)\\mathrm{d}\\theta}, \\end{align} \\] <p>where \\(p(\\mathbf{y}\\,|\\,\\theta)\\) denotes the likelihood, or model, and quantifies how likely the observed dataset \\(\\mathbf{y}\\) is, given the parameter estimate \\(\\theta\\). The prior distribution \\(p(\\theta)\\) reflects our initial beliefs about the value of \\(\\theta\\) before observing data, whilst the posterior \\(p(\\theta\\,|\\, \\mathbf{y})\\) gives an updated estimate of the parameters' value, after observing \\(\\mathbf{y}\\). The marginal likelihood, or Bayesian model evidence, \\(p(\\mathbf{y})\\) is the probability of the observed data under all possible hypotheses that our prior model can generate. Within Bayesian model selection, this property makes the marginal log-likelihood an indispensable tool. Selecting models under this criterion places a higher emphasis on models that can generalise better to new data points.</p> <p>When the posterior distribution belongs to the same family of probability distributions as the prior, we describe the prior and the likelihood as conjugate to each other. Such a scenario is convenient in Bayesian inference as it allows us to derive closed-form expressions for the posterior distribution. When the likelihood function is a member of the exponential family, then there exists a conjugate prior. However, the conjugate prior may not have a form that precisely reflects the practitioner's belief surrounding the parameter. For this reason, conjugate models seldom appear; one exception to this is GP regression that we present fully in our Regression notebook.</p> <p>For models that do not contain a conjugate prior, the marginal log-likelihood must be calculated to normalise the posterior distribution and ensure it integrates to 1. For models with a single, 1-dimensional parameter, it may be possible to compute this integral analytically or through a quadrature scheme, such as Gauss-Hermite. However, in machine learning, the dimensionality of \\(\\theta\\) is often large and the corresponding integral required to compute \\(p(\\mathbf{y})\\) quickly becomes intractable as the dimension grows. Techniques such as Markov Chain Monte Carlo and variational inference allow us to approximate integrals such as the one seen in \\(p(\\mathbf{y})\\).</p> <p>Once a posterior distribution has been obtained, we can make predictions at new points \\(\\mathbf{y}^{\\star}\\) through the posterior predictive distribution. This is achieved by integrating out the parameter set \\(\\theta\\) from our posterior distribution through</p> \\[ \\begin{align}     p(\\mathbf{y}^{\\star}\\mid \\mathbf{y}) = \\int p(\\mathbf{y}^{\\star} \\,|\\, \\theta, \\mathbf{y} ) p(\\theta\\,|\\, \\mathbf{y})\\mathrm{d}\\theta\\,. \\end{align} \\] <p>As with the marginal log-likelihood, evaluating this quantity requires computing an integral which may not be tractable, particularly when \\(\\theta\\) is high-dimensional.</p> <p>It is difficult to communicate statistics directly through a posterior distribution, so we often compute and report moments of the posterior distribution. Most commonly, we report the first moment and the centred second moment</p> \\[ \\begin{alignat}{2}     \\mu  = \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]  &amp; = \\int \\theta     p(\\theta\\mid\\mathbf{y})\\mathrm{d}\\theta \\quad \\\\     \\sigma^2  = \\mathbb{V}[\\theta\\,|\\,\\mathbf{y}] &amp; = \\int \\left(\\theta -     \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]\\right)^2p(\\theta\\,|\\,\\mathbf{y})\\mathrm{d}\\theta&amp;\\,. \\end{alignat} \\] <p>Through this pair of statistics, we can communicate our beliefs about the most likely value of \\(\\theta\\) i.e., \\(\\mu\\), and the uncertainty \\(\\sigma\\) around the expected value. However, as with the marginal log-likelihood and predictive posterior distribution, computing these statistics again requires a potentially intractable integral.</p>"},{"location":"_examples/intro_to_gps/#gaussian-random-variables","title":"Gaussian random variables","text":"<p>We begin our review with the simplest case; a univariate Gaussian random variable. For a random variable \\(y\\), let \\(\\mu\\in\\mathbb{R}\\) be a mean scalar and \\(\\sigma^2\\in\\mathbb{R}_{&gt;0}\\) a variance scalar. If \\(y\\) is a Gaussian random variable, then the density of \\(y\\) is $$ \\begin{align}     \\mathcal{N}(y\\,|\\, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^{2}}\\right)\\,. \\end{align} $$ We can plot three different parameterisations of this density.</p> <pre><code>import warnings\n\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpyro.distributions as npd\nimport pandas as pd\nimport seaborn as sns\n\nfrom examples.utils import (\n    confidence_ellipse,\n    use_mpl_style,\n)\n\n# set the default style for plotting\nuse_mpl_style()\n\nkey = jr.key(42)\n\n\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nud1 = npd.Normal(0.0, 1.0)\nud2 = npd.Normal(-1.0, 0.5)\nud3 = npd.Normal(0.25, 1.5)\n\nxs = jnp.linspace(-5.0, 5.0, 500)\n\nfig, ax = plt.subplots()\nfor d in [ud1, ud2, ud3]:\n    ax.plot(\n        xs,\n        jnp.exp(d.log_prob(xs)),\n        label=f\"$\\\\mathcal{{N}}({{{float(d.mean)}}},\\\\  {{{float(jnp.sqrt(d.variance))}}}^2)$\",\n    )\n    ax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2)\nax.legend(loc=\"best\")\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7fb8287a7c40&gt;\n</code></pre> <p></p> <p>A Gaussian random variable is uniquely defined in distribution by its mean \\(\\mu\\) and variance \\(\\sigma^2\\) and we therefore write \\(y\\sim\\mathcal{N}(\\mu, \\sigma^2)\\) when describing a Gaussian random variable. We can compute these two quantities by $$ \\begin{align}     \\mathbb{E}[y] = \\mu\\,, \\quad \\quad \\mathbb{E}\\left[(y-\\mu)^2\\right] =\\sigma^2\\,. \\end{align} $$ Extending this concept to vector-valued random variables reveals the multivariate Gaussian random variables which brings us closer to the full definition of a GP.</p> <p>Let \\(\\mathbf{y}\\) be a \\(D\\)-dimensional random variable, \\(\\boldsymbol{\\mu}\\) be a \\(D\\)-dimensional mean vector and \\(\\boldsymbol{\\Sigma}\\) be a \\(D\\times D\\) covariance matrix. If \\(\\mathbf{y}\\) is a Gaussian random variable, then the density of \\(\\mathbf{y}\\) is $$ \\begin{align}     \\mathcal{N}(\\mathbf{y}\\,|\\, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2\\pi}^{D/2} \\lvert\\boldsymbol{\\Sigma}\\rvert^{1/2}} \\exp\\left(-\\frac{1}{2} \\left(\\mathbf{y} - \\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{y}-\\boldsymbol{\\mu}\\right) \\right) \\,. \\end{align} $$ Three example parameterisations of this can be visualised below where \\(\\rho\\) determines the correlation of the multivariate Gaussian.</p> <pre><code>key = jr.key(123)\n\nd1 = npd.MultivariateNormal(loc=jnp.zeros(2), covariance_matrix=jnp.diag(jnp.ones(2)))\nd2 = npd.MultivariateNormal(\n    jnp.zeros(2), scale_tril=jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]]))\n)\nd3 = npd.MultivariateNormal(\n    jnp.zeros(2), scale_tril=jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]]))\n)\n\ndists = [d1, d2, d3]\n\nxvals = jnp.linspace(-5.0, 5.0, 500)\nyvals = jnp.linspace(-5.0, 5.0, 500)\n\nxx, yy = jnp.meshgrid(xvals, yvals)\n\npos = jnp.empty(xx.shape + (2,))\npos.at[:, :, 0].set(xx)\npos.at[:, :, 1].set(yy)\n\nfig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\ntitles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\n\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)\n\nfor a, t, d in zip([ax0, ax1, ax2], titles, dists, strict=False):\n    d_prob = jnp.exp(\n        d.log_prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)]))\n    ).reshape(xx.shape)\n    cntf = a.contourf(\n        xx,\n        yy,\n        jnp.exp(d_prob),\n        levels=20,\n        antialiased=True,\n        cmap=cmap,\n        edgecolor=\"face\",\n    )\n    a.set_xlim(-2.75, 2.75)\n    a.set_ylim(-2.75, 2.75)\n    samples = d.sample(key=key, sample_shape=(5000,))\n    xsample, ysample = samples[:, 0], samples[:, 1]\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n    )\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"\n    )\n    a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)\n    a.set(xlabel=\"x\", ylabel=\"y\", title=t)\n</code></pre> <pre><code>/tmp/ipykernel_3730/413864837.py:31: UserWarning: The following kwargs were not used by contour: 'edgecolor'\n  cntf = a.contourf(\n</code></pre> <p></p> <p>Extending the intuition given for the moments of a univariate Gaussian random variables, we can obtain the mean and covariance by</p> \\[ \\begin{align}     \\mathbb{E}[\\mathbf{y}] &amp; = \\mathbf{\\mu}, \\\\     \\operatorname{Cov}(\\mathbf{y}) &amp; = \\mathbf{E}\\left[(\\mathbf{y} - \\mathbf{\\mu})(\\mathbf{y} - \\mathbf{\\mu})^{\\top} \\right] \\\\       &amp; =\\mathbb{E}[\\mathbf{y}\\mathbf{y}^{\\top}] - \\mathbb{E}[\\mathbf{y}]\\mathbb{E}[\\mathbf{y}]^{\\top} \\\\       &amp; =\\mathbf{\\Sigma}\\,. \\end{align} \\] <p>The covariance matrix is a symmetric positive definite matrix that generalises the notion of variance to multiple dimensions. The matrix's diagonal entries contain the variance of each element, whilst the off-diagonal entries quantify the degree to which the respective pair of random variables are linearly related; this quantity is called the covariance.</p> <p>Assuming a Gaussian likelihood function in a Bayesian model is attractive as the mean and variance parameters are highly interpretable. This makes prior elicitation straightforward as the parameters' value can be intuitively contextualised within the scope of the problem at hand. Further, in models where the posterior distribution is Gaussian, we again use the distribution's mean and variance to describe our prediction and corresponding uncertainty around a given event occurring.</p> <p>Not only are Gaussian random variables highly interpretable, but linear operations involving them lead to analytical solutions. An example of this that will be useful in the sequel is the marginalisation and conditioning property of sets of Gaussian random variables. We will present these two results now for a pair of Gaussian random variables, but it should be stressed that these results hold for any finite set of Gaussian random variables.</p> <p>For a pair of random variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) defined on the same support, the distribution over them both is known as the joint distribution. The joint distribution \\(p(\\mathbf{x}, \\mathbf{y})\\) quantifies the probability of two events, one from \\(p(\\mathbf{x})\\) and another from \\(p(\\mathbf{y})\\), occurring at the same time. We visualise this idea below.</p> <pre><code>n = 1000\nx = npd.Normal(loc=0.0, scale=1.0).sample(key, sample_shape=(n,))\nkey, subkey = jr.split(key)\ny = npd.Normal(loc=0.25, scale=0.5).sample(subkey, sample_shape=(n,))\nkey, subkey = jr.split(subkey)\nxfull = npd.Normal(loc=0.0, scale=1.0).sample(subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\nyfull = npd.Normal(loc=0.25, scale=0.5).sample(subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\ndf = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    g = sns.jointplot(\n        data=df,\n        x=\"x\",\n        y=\"y\",\n        hue=\"idx\",\n        marker=\".\",\n        space=0.0,\n        xlim=(-4.0, 4.0),\n        ylim=(-4.0, 4.0),\n        height=4,\n        marginal_ticks=False,\n        legend=False,\n        palette=\"inferno\",\n        marginal_kws={\n            \"fill\": True,\n            \"linewidth\": 1,\n            \"color\": cols[1],\n            \"alpha\": 0.3,\n            \"bw_adjust\": 2,\n            \"cmap\": cmap,\n        },\n        joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},\n    )\n    g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\n    g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\n    g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=1.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=2.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=3.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n</code></pre> <p></p> <p>Formally, we can define this by letting \\(p(\\mathbf{x}, \\mathbf{y})\\) be the joint probability distribution defined over \\(\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}}, \\boldsymbol{\\Sigma}_{\\mathbf{xx}})\\) and \\(\\mathbf{y}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}})\\). We define the joint distribution as</p> \\[ \\begin{align}     p\\left(\\begin{bmatrix}         \\mathbf{x} \\\\ \\mathbf{y}     \\end{bmatrix}\\right) = \\mathcal{N}\\left(\\begin{bmatrix}         \\boldsymbol{\\mu}_{\\mathbf{x}} \\\\ \\boldsymbol{\\mu}_{\\mathbf{y}}     \\end{bmatrix}, \\begin{bmatrix}         \\boldsymbol{\\Sigma}_{\\mathbf{xx}} &amp; \\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\\\         \\boldsymbol{\\Sigma}_{\\mathbf{yx}} &amp; \\boldsymbol{\\Sigma}_{\\mathbf{yy}}     \\end{bmatrix} \\right)\\,, \\end{align} \\] <p>where \\(\\boldsymbol{\\Sigma}_{\\mathbf{x}\\mathbf{y}}\\) is the cross-covariance matrix of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</p> <p>When presented with a joint distribution, two tasks that we may wish to perform are marginalisation and conditioning. For a joint distribution \\(p(\\mathbf{x}, \\mathbf{y})\\) where we are interested only in \\(p(\\mathbf{x})\\), we must integrate over all possible values of \\(\\mathbf{y}\\) to obtain \\(p(\\mathbf{x})\\). This process is marginalisation. Conditioning allows us to evaluate the probability of one random variable, given that the other random variable is fixed. For a joint Gaussian distribution, marginalisation and conditioning have analytical expressions where the resulting distribution is also a Gaussian random variable.</p> <p>For a joint Gaussian random variable, the marginalisation of \\(\\mathbf{x}\\) or \\(\\mathbf{y}\\) is given by</p> \\[ \\begin{alignat}{3}     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{y} &amp;&amp; = p(\\mathbf{x})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}},\\boldsymbol{\\Sigma}_{\\mathbf{xx}})\\\\     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{x} &amp;&amp; = p(\\mathbf{y})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}},     \\boldsymbol{\\Sigma}_{\\mathbf{yy}})\\,. \\end{alignat} \\] <p>The conditional distributions are given by</p> \\[ \\begin{align}     p(\\mathbf{y}\\,|\\, \\mathbf{x}) &amp; = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{y}} + \\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{\\mathbf{x}}), \\boldsymbol{\\Sigma}_{\\mathbf{yy}}-\\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\right)\\,. \\end{align} \\] <p>Within this section, we have introduced the idea of multivariate Gaussian random variables and presented some key results concerning their properties. In the following section, we will lift our presentation of Gaussian random variables to GPs.</p>"},{"location":"_examples/intro_to_gps/#gaussian-processes","title":"Gaussian processes","text":"<p>When transitioning from Gaussian random variables to GP there is a shift in thought required to parse the forthcoming material. Firstly, to be consistent with the general literature, we hereon use \\(\\mathbf{X}\\) to denote an observed vector of data points, not a random variable as has been true up until now. To distinguish between matrices and vectors, we use bold upper case characters e.g., \\(\\mathbf{X}\\) for matrices, and bold lower case characters for vectors e.g., \\(\\mathbf{x}\\).</p> <p>We are interested in modelling supervised learning problems, where we have \\(n\\) observations \\(\\mathbf{y}=\\{y_1, y_2,\\ldots ,y_n\\}\\subset\\mathcal{Y}\\) at corresponding inputs \\(\\mathbf{X}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\subset\\mathcal{X}\\). We aim to capture the relationship between \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) using a model \\(f\\) with which we may make predictions at an unseen set of test points \\(\\mathbf{X}^{\\star}\\subset\\mathcal{X}\\). We formalise this by</p> <p>$$ \\begin{align}     y = f(\\mathbf{X}) + \\varepsilon\\,, \\end{align} $$ where \\(\\varepsilon\\) is an observational noise term. We collectively refer to \\((\\mathbf{X}, \\mathbf{y})\\) as the training data and \\(\\mathbf{X}^{\\star}\\) as the set of test points. This process is visualised below</p> <p></p> <p>As we shall go on to see, GPs offer an appealing workflow for scenarios such as this, all under a Bayesian framework.</p> <p>We write a GP \\(f(\\cdot) \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot))\\) with mean function \\(\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}\\) and \\(\\boldsymbol{\\theta}\\)-parameterised kernel \\(k: \\mathcal{X} \\times \\mathcal{X}\\rightarrow \\mathbb{R}\\). When evaluating the GP on a finite set of points \\(\\mathbf{X}\\subset\\mathcal{X}\\), \\(k\\) gives rise to the Gram matrix \\(\\mathbf{K}_{ff}\\) such that the \\((i, j)^{\\text{th}}\\) entry of the matrix is given by \\([\\mathbf{K}_{ff}]_{i, j} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\). As is conventional within the literature, we centre our training data and assume \\(\\mu(\\mathbf{X}):= 0\\) for all \\(\\mathbf{X}\\in\\mathcal{X}\\). We further drop dependency on \\(\\boldsymbol{\\theta}\\) and \\(\\mathbf{X}\\) for notational convenience in the remainder of this article.</p> <p>We define a joint GP prior over the latent function</p> \\[ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}) = \\mathcal{N}\\left(\\mathbf{0}, \\begin{bmatrix}         \\mathbf{K}_{xf} &amp; \\mathbf{K}_{xx}     \\end{bmatrix}\\right)\\,, \\end{align} \\] <p>where \\(\\mathbf{f}^{\\star} = f(\\mathbf{X}^{\\star})\\). Conditional on the GP's latent function \\(f\\), we assume a factorising likelihood generates our observations</p> \\[ \\begin{align}     p(\\mathbf{y}\\,|\\,\\mathbf{f}) = \\prod_{i=1}^n p(y_i\\,|\\, f_i)\\,. \\end{align} \\] <p>Strictly speaking, the likelihood function is \\(p(\\mathbf{y}\\,|\\,\\phi(\\mathbf{f}))\\) where \\(\\phi\\) is the likelihood function's associated link function. Example link functions include the probit or logistic functions for a Bernoulli likelihood and the identity function for a Gaussian likelihood. We eschew this notation for now as this section primarily considers Gaussian likelihood functions where the role of \\(\\phi\\) is superfluous. However, this intuition will be helpful for models with a non-Gaussian likelihood, such as those encountered in classification.</p> <p>Applying Bayes' theorem \\eqref{eq:BayesTheorem} yields the joint posterior distribution over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}\\,|\\,\\mathbf{y}) = \\frac{p(\\mathbf{y}\\,|\\,\\mathbf{f})p(\\mathbf{f},\\mathbf{f}^{\\star})}{p(\\mathbf{y})}\\,. \\end{align} $$</p> <p>The choice of kernel function that we use to parameterise our GP is an important modelling decision as the choice of kernel dictates properties such as differentiability, variance and characteristic lengthscale of the functions that are admissible under the GP prior. A kernel is a positive-definite function with parameters \\(\\boldsymbol{\\theta}\\) that maps pairs of inputs \\(\\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}\\) onto the real line. We dedicate the entirety of the Introduction to Kernels notebook to exploring the different GPs each kernel can yield.</p>"},{"location":"_examples/intro_to_gps/#gaussian-process-regression","title":"Gaussian process regression","text":"<p>When the likelihood function is a Gaussian distribution \\(p(y_i\\,|\\, f_i) = \\mathcal{N}(y_i\\,|\\, f_i, \\sigma_n^2)\\), marginalising \\(\\mathbf{f}\\) from the joint posterior to obtain the posterior predictive distribution is exact</p> \\[ \\begin{align}     p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) = \\mathcal{N}(\\mathbf{f}^{\\star}\\,|\\,\\boldsymbol{\\mu}_{\\,|\\,\\mathbf{y}}, \\Sigma_{\\,|\\,\\mathbf{y}})\\,, \\end{align} \\] <p>where</p> \\[ \\begin{align}     \\mathbf{\\mu}_{\\mid \\mathbf{y}} &amp; = \\mathbf{K}_{\\star f}\\left( \\mathbf{K}_{ff}+\\sigma^2_n\\mathbf{I}_n\\right)^{-1}\\mathbf{y} \\\\     \\Sigma_{\\,|\\,\\mathbf{y}} &amp; = \\mathbf{K}_{\\star\\star} - \\mathbf{K}_{xf}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n\\right)^{-1}\\mathbf{K}_{fx} \\,. \\end{align} \\] <p>Further, the log of the  marginal likelihood of the GP can be analytically expressed as</p> \\[ \\begin{align}         &amp; = 0.5\\left(-\\underbrace{\\mathbf{y}^{\\top}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n \\right)^{-1}\\mathbf{y}}_{\\text{Data fit}} -\\underbrace{\\log\\lvert \\mathbf{K}_{ff} + \\sigma^2_n\\rvert}_{\\text{Complexity}} -\\underbrace{n\\log 2\\pi}_{\\text{Constant}} \\right)\\,. \\end{align} \\] <p>Model selection can be performed for a GP through gradient-based optimisation of \\(\\log p(\\mathbf{y})\\) with respect to the kernel's parameters \\(\\boldsymbol{\\theta}\\) and the observational noise \\(\\sigma^2_n\\). Collectively, we call these terms the model hyperparameters \\(\\boldsymbol{\\xi} = \\{\\boldsymbol{\\theta},\\sigma_n^2\\}\\) from which the maximum likelihood estimate is given by</p> \\[ \\begin{align*}     \\boldsymbol{\\xi}^{\\star} = \\operatorname{argmax}_{\\boldsymbol{\\xi} \\in \\Xi} \\log p(\\mathbf{y})\\,. \\end{align*} \\] <p>Observing the individual terms in the marginal log-likelihood can help understand exactly why optimising the marginal log-likelihood gives reasonable solutions. The data fit term is the only component of the marginal log-likelihood that includes the observed response \\(\\mathbf{y}\\) and will therefore encourage solutions that model the data well. Conversely, the complexity term contains a determinant operator and therefore measures the volume of the function space covered by the GP. Whilst a more complex function has a better chance of modelling the observed data well, this is only true to a point and functions that are overly complex will overfit the data. Optimising with respect to the marginal log-likelihood balances these two objectives when identifying the optimal solution, as visualised below.</p> <p></p>"},{"location":"_examples/intro_to_gps/#conclusions","title":"Conclusions","text":"<p>Within this notebook we have built up the concept of a GP, starting from Bayes' theorem and the definition of a Gaussian random variable. Using the ideas presented in this notebook, the user should be in a position to dive into our Regression notebook and start getting their hands on some code. For those looking to learn more about the underling theory of GPs, an excellent starting point is the Gaussian Processes for Machine Learning textbook. Alternatively, the thesis of Alexander Terenin provides a rigorous exposition of GPs that served as the inspiration for this notebook.</p>"},{"location":"_examples/intro_to_gps/#system-configuration","title":"System Configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njax       : 0.6.0\nmatplotlib: 3.10.3\nseaborn   : 0.13.2\nnumpyro   : 0.18.0\npandas    : 2.2.3\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/intro_to_kernels/","title":"Introduction to Kernels","text":"<p>In this guide we provide an introduction to kernels, and the role they play in Gaussian process models.</p> <pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Float,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nfrom examples.utils import use_mpl_style\nfrom gpjax.parameters import Static\nfrom gpjax.typing import Array\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nkey = jr.key(42)\n\n# set the default style for plotting\nuse_mpl_style()\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre> <p>Using Gaussian Processes (GPs) to model functions can offer several advantages over alternative methods, such as deep neural networks. One key advantage is their rich quantification of uncertainty; not only do they provide point estimates for the values taken by a function throughout its domain, but they provide a full predictive posterior distribution over the range of values the function may take. This rich quantification of uncertainty is useful in many applications, such as Bayesian optimisation, which relies on being able to make uncertainty-aware decisions.</p> <p>However, another advantage of GPs is the ability for one to place priors on the functions being modelled. For instance, one may know that the underlying function being modelled observes certain characteristics, such as being periodic or having a certain level of smoothness. The kernel, or covariance function, is the primary means through which one is able to encode such prior knowledge about the function being modelled. This enables one to equip the GP with inductive biases which enable it to learn from data more efficiently, whilst generalising to unseen data more effectively.</p> <p>In this notebook we'll develop some intuition for what kinds of priors are encoded through the use of different kernels, and how this can be useful when modelling different types of functions.</p>"},{"location":"_examples/intro_to_kernels/#what-is-a-kernel","title":"What is a Kernel?","text":"<p>Intuitively, for a function \\(f\\), the kernel defines the notion of similarity between the value of the function at two points, \\(f(\\mathbf{x})\\) and \\(f(\\mathbf{x}')\\), and will be denoted as \\(k(\\mathbf{x}, \\mathbf{x}')\\):</p> \\[ \\begin{aligned}   k(\\mathbf{x}, \\mathbf{x}') &amp;= \\text{Cov}[f(\\mathbf{x}), f(\\mathbf{x}')] \\\\   &amp;= \\mathbb{E}[(f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{x})])(f(\\mathbf{x}') - \\mathbb{E}[f(\\mathbf{x}')])] \\end{aligned} \\] <p>One would expect that, given a previously unobserved test point \\(\\mathbf{x}^*\\), the training points which are closest to this unobserved point will be most similar to it. As such, the kernel is used to define this notion of similarity within the GP framework. It is up to the user to select a kernel function which is appropriate for the function being modelled. In this notebook we are going to give some examples of commonly used kernels, and try to develop an understanding of when one may wish to use one kernel over another. However, before we do this, it is worth discussing the necessary conditions for a function to be a valid kernel/covariance function. This requires a little bit of maths, so for those of you who just wish to obtain an intuitive understanding, feel free to skip to the section introducing the Mat\u00e9rn  family of kernels.</p>"},{"location":"_examples/intro_to_kernels/#what-are-the-necessary-conditions-for-a-function-to-be-a-valid-kernel","title":"What are the necessary conditions for a function to be a valid kernel?","text":"<p>Whilst intuitively the kernel function is used to define the notion of similarity within the GP framework, it is important to note that there are two necessary conditions that a kernel function must satisfy in order to be a valid covariance function. For clarity, we will refer to any function mapping two inputs to a scalar output as a kernel function, and we will refer to a valid kernel function satisfying the two necessary conditions as a covariance function. However, it is worth noting that the GP community often uses the terms kernel function and covariance function interchangeably.</p> <p>The first necessary condition is that the covariance function must be symmetric, i.e. \\(k(\\mathbf{x}, \\mathbf{x}') = k(\\mathbf{x}', \\mathbf{x})\\). This is because the covariance between two random variables \\(X\\) and \\(X'\\) is symmetric; if one looks at the definition of covariance given above, it is clear that it is invariant to swapping the order of the inputs \\(\\mathbf{x}\\) and \\(\\mathbf{x}'\\).</p> <p>The second necessary condition is that the covariance function must be positive semi-definite (PSD). In order to understand this condition, it is useful to first introduce the concept of a Gram matrix. We'll use the same notation as the GP introduction notebook, and denote \\(n\\) input points as \\(\\mathbf{X} = \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}\\). Given these input points and a kernel function \\(k\\) the Gram matrix stores the pairwise kernel evaluations between all input points. Mathematically, this leads to the Gram matrix being defined as:</p> \\[K(\\mathbf{X}, \\mathbf{X}) = \\begin{bmatrix} k(\\mathbf{x}_1, \\mathbf{x}_1) &amp; \\cdots &amp; k(\\mathbf{x}_1, \\mathbf{x}_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ k(\\mathbf{x}_n, \\mathbf{x}_1) &amp; \\cdots &amp; k(\\mathbf{x}_n, \\mathbf{x}_n) \\end{bmatrix}\\] <p>such that \\(K(\\mathbf{X}, \\mathbf{X})_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).</p> <p>In order for \\(k\\) to be a valid covariance function, the corresponding Gram matrix must be positive semi-definite. In this case the Gram matrix is referred to as a covariance matrix. A real \\(n \\times n\\) matrix \\(K\\) is positive semi-definite if and only if for all vectors \\(\\mathbf{z} \\in \\mathbb{R}^n\\):</p> \\[\\mathbf{z}^\\top K \\mathbf{z} \\geq 0\\] <p>Alternatively, a real \\(n \\times n\\) matrix \\(K\\) is positive semi-definite if and only if all of its eigenvalues are non-negative.</p> <p>Therefore, the two necessary conditions for a function to be a valid covariance function are that it must be symmetric and positive semi-definite. In this section we have referred to any function from two inputs to a scalar output as a kernel function, with its corresponding matrix of pairwise evaluations referred to as the Gram matrix, and a function satisfying the two necessary conditions as a covariance function, with its corresponding matrix of pairwise evaluations referred to as the covariance matrix. This enabled us to easily define the necessary conditions for a function to be a valid covariance function. However, as noted previously, the GP community often uses these terms interchangeably, and so we will for the remainder of this notebook.</p>"},{"location":"_examples/intro_to_kernels/#introducing-a-common-family-of-kernels-the-matern-family","title":"Introducing a Common Family of Kernels - The Mat\u00e9rn Family","text":"<p>One of the most widely used families of kernels is the Mat\u00e9rn family (Mat\u00e9rn, 1960). These kernels take on the following form:</p> \\[k_{\\nu}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2\\nu} \\frac{|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right)^{\\nu} K_{\\nu} \\left(\\sqrt{2\\nu} \\frac{|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right)\\] <p>where \\(K_{\\nu}\\) is a modified Bessel function, \\(\\nu\\), \\(\\kappa\\) and \\(\\sigma^2\\) are hyperparameters specifying the mean-square differentiability, lengthscale and variability respectively, and \\(|\\cdot|\\) is used to denote the Euclidean norm. Note that for those of you less interested in the mathematical underpinnings of kernels, it isn't necessary to understand the exact functional form of the Mat\u00e9rn kernels to gain an understanding of how they behave. The key takeaway is that they are parameterised by several hyperparameters, and that these hyperparameters dictate the behaviour of functions sampled from the corresponding GP. The plots below will provide some more intuition for how these hyperparameters affect the behaviour of functions sampled from the corresponding GP.</p> <p>Some commonly used Mat\u00e9rn kernels use half-integer values of \\(\\nu\\), such as \\(\\nu = 1/2\\) or \\(\\nu = 3/2\\). The fraction is sometimes omitted when naming the kernel, so that \\(\\nu = 1/2\\) is referred to as the Mat\u00e9rn12 kernel, and \\(\\nu = 3/2\\) is referred to as the Mat\u00e9rn32 kernel. When \\(\\nu\\) takes in a half-integer value, \\(\\nu = k + 1/2\\), the kernel can be expressed as the product of a polynomial of order \\(k\\) and an exponential:</p> \\[k_{k + 1/2}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\exp\\left(-\\frac{\\sqrt{2\\nu}|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right) \\frac{\\Gamma(k+1)}{\\Gamma(2k+1)} \\times \\sum_{i= 0}^k \\frac{(k+i)!}{i!(k-i)!} \\left(\\frac{(\\sqrt{8\\nu}|\\mathbf{x} - \\mathbf{x'}|)}{\\kappa}\\right)^{k-i}\\] <p>In the limit of \\(\\nu \\to \\infty\\) this yields the squared-exponential, or radial basis function (RBF), kernel, which is infinitely mean-square differentiable:</p> \\[k_{\\infty}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\exp\\left(-\\frac{|\\mathbf{x} - \\mathbf{x'}|^2}{2\\kappa^2}\\right)\\] <p>But what kind of functions does this kernel encode prior knowledge about? Let's take a look at some samples from GP priors defined used Mat\u00e9rn kernels with different values of \\(\\nu\\):</p> <pre><code>kernels = [\n    gpx.kernels.Matern12(),\n    gpx.kernels.Matern32(),\n    gpx.kernels.Matern52(),\n    gpx.kernels.RBF(),\n]\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n\nmeanf = gpx.mean_functions.Zero()\n\nfor k, ax in zip(kernels, axes.ravel(), strict=False):\n    prior = gpx.gps.Prior(mean_function=meanf, kernel=k)\n    rv = prior(x)\n    y = rv.sample(key=key, sample_shape=(10,))\n    ax.plot(x, y.T, alpha=0.7)\n    ax.set_title(k.name)\n</code></pre> <p></p> <p>The plots above clearly show that the choice of \\(\\nu\\) has a large impact on the smoothness of the functions being modelled by the GP, with functions drawn from GPs defined with the Mat\u00e9rn kernel becoming increasingly smooth as \\(\\nu \\to \\infty\\). More formally, this notion of smoothness is captured through the mean-square differentiability of the function being modelled. Functions sampled from GPs using a Mat\u00e9rn kernel are \\(k\\)-times mean-square differentiable, if and only if \\(\\nu &gt; k\\). For instance, functions sampled from a GP using a Mat\u00e9rn12 kernel are zero times mean-square differentiable, and functions sampled from a GP using the RBF kernel are infinitely mean-square differentiable.</p> <p>As an important aside, a general property of the Mat\u00e9rn family of kernels is that they are examples of stationary kernels. This means that they only depend on the displacement of the two points being compared, \\(\\mathbf{x} - \\mathbf{x}'\\), and not on their absolute values. This is a useful property to have, as it means that the kernel is invariant to translations in the input space. They also go beyond this, as they only depend on the Euclidean distance between the two points being compared, \\(|\\mathbf{x} - \\mathbf{x}'|\\). Kernels which satisfy this property are known as isotropic kernels. This makes the function invariant to all rigid motions in the input space, such as rotations.</p>"},{"location":"_examples/intro_to_kernels/#inferring-kernel-hyperparameters","title":"Inferring Kernel Hyperparameters","text":"<p>Most kernels have several hyperparameters, which we denote \\(\\mathbf{\\theta}\\), which encode different assumptions about the underlying function being modelled. For the Mat\u00e9rn family described above, \\(\\mathbf{\\theta} = \\{\\nu, \\kappa, \\sigma\\}\\). A fully Bayesian approach to dealing with hyperparameters would be to place a prior over them, and marginalise over the posterior derived from the data in order to perform predictions. However, this is often computationally very expensive, and so a common approach is to instead optimise the hyperparameters by maximising the log marginal likelihood of the data. Given training data \\(\\mathbf{D} = (\\mathbf{X}, \\mathbf{y})\\), assumed to contain some additive Gaussian noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), the log marginal likelihood of the dataset is defined as:</p> \\[ \\begin{aligned} \\log(p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\theta})) &amp;= \\log\\left(\\int p(\\mathbf{y} | \\mathbf{f}, \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{f} | \\mathbf{X}, \\boldsymbol{\\theta}) d\\mathbf{f}\\right) \\nonumber \\\\ &amp;= - \\frac{1}{2} \\mathbf{y} ^ \\top \\left(K(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I} \\right)^{-1} \\mathbf{y} - \\frac{1}{2} \\log |K(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}| - \\frac{n}{2} \\log 2 \\pi \\end{aligned}\\] <p>This expression can then be maximised with respect to the hyperparameters using a gradient-based approach such as Adam or L-BFGS. Note that we may choose to fix some hyperparameters, and in GPJax the parameter \\(\\nu\\) is set by the user, and not inferred though optimisation. For more details on using the log marginal likelihood to optimise kernel hyperparameters, see our GP introduction notebook.</p> <p>We'll demonstrate the advantages of being able to infer kernel parameters from the training data by fitting a GP to the widely used Forrester function:</p> \\[f(x) = (6x - 2)^2 \\sin(12x - 4)\\] <pre><code># Forrester function\ndef forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:  # noqa: F821\n    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\n\n\nn = 13\n\ntraining_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1)\ntraining_y = forrester(training_x)\nD = gpx.Dataset(X=training_x, y=training_y)\n\ntest_x = jnp.linspace(0, 1, 100).reshape(-1, 1)\ntest_y = forrester(test_x)\n</code></pre> <p>First we define our model, using the Mat\u00e9rn52 kernel, and construct our posterior without optimising the kernel hyperparameters:</p> <pre><code>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\n    lengthscale=jnp.array(0.1)\n)  # Initialise our kernel lengthscale to 0.1\n\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n\nlikelihood = gpx.likelihoods.Gaussian(\n    num_datapoints=D.n, obs_stddev=Static(jnp.array(1e-3))\n)  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value\n\nno_opt_posterior = prior * likelihood\n</code></pre> <p>We can then optimise the hyperparameters by minimising the negative log marginal likelihood of the data:</p> <pre><code>gpx.objectives.conjugate_mll(no_opt_posterior, data=D)\n</code></pre> <pre><code>Array(-19.18644709, dtype=float64)\n</code></pre> <pre><code>opt_posterior, history = gpx.fit_scipy(\n    model=no_opt_posterior,\n    objective=lambda p, d: -gpx.objectives.conjugate_mll(p, d),\n    train_data=D,\n)\n</code></pre> <pre><code>Optimization terminated successfully.\n         Current function value: 2.545694\n         Iterations: 18\n         Function evaluations: 21\n         Gradient evaluations: 21\n</code></pre> <p>Having optimised the hyperparameters, we can now make predictions using the posterior with the optimised hyperparameters, and compare them to the predictions made using the posterior with the default hyperparameters:</p> <pre><code>def plot_ribbon(ax, x, dist, color):\n    mean = dist.mean\n    std = jnp.sqrt(dist.variance)\n    ax.plot(x, mean, label=\"Predictive mean\", color=color)\n    ax.fill_between(\n        x.squeeze(),\n        mean - 2 * std,\n        mean + 2 * std,\n        alpha=0.2,\n        label=\"Two sigma\",\n        color=color,\n    )\n    ax.plot(x, mean - 2 * std, linestyle=\"--\", linewidth=1, color=color)\n    ax.plot(x, mean + 2 * std, linestyle=\"--\", linewidth=1, color=color)\n</code></pre> <pre><code>opt_latent_dist = opt_posterior.predict(test_x, train_data=D)\nopt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)\n\nopt_predictive_mean = opt_predictive_dist.mean\nopt_predictive_std = jnp.sqrt(opt_predictive_dist.variance)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6))\nax1.plot(\n    test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5)\nplot_ribbon(ax1, test_x, opt_predictive_dist, color=cols[1])\nax1.set_title(\"Posterior with Hyperparameter Optimisation\")\nax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n\nno_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D)\nno_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)\n\nax2.plot(\n    test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5)\nplot_ribbon(ax2, test_x, no_opt_predictive_dist, color=cols[1])\nax2.set_title(\"Posterior without Hyperparameter Optimisation\")\nax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f75708d8430&gt;\n</code></pre> <p></p> <p>We can see that optimising the hyperparameters by minimising the negative log marginal likelihood of the data results in a more faithful fit of the GP to the data. In particular, we can observe that the GP using optimised hyperparameters is more accurately able to reflect uncertainty in its predictions, as opposed to the GP using the default parameters, which is overconfident in its predictions.</p> <p>The lengthscale, \\(\\kappa\\), and variance, \\(\\sigma^2\\), are shown below, both before and after optimisation:</p> <pre><code>no_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale\nno_opt_variance = no_opt_posterior.prior.kernel.variance\nopt_lengthscale = opt_posterior.prior.kernel.lengthscale\nopt_variance = opt_posterior.prior.kernel.variance\n\nprint(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\")\nprint(\n    f\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\"\n)\n</code></pre> <pre><code>Optimised Lengthscale: \u001b[38;2;79;201;177mPositiveReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0.27113679, dtype=float64),\n  \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'positive'\"\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m and Variance: \u001b[38;2;79;201;177mNonNegativeReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(24.79013317, dtype=float64),\n  \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'non_negative'\"\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\nNon-Optimised Lengthscale: \u001b[38;2;79;201;177mPositiveReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0.1, dtype=float64, weak_type=True),\n  \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'positive'\"\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m and Variance: \u001b[38;2;79;201;177mNonNegativeReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(1., dtype=float64, weak_type=True),\n  \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'non_negative'\"\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\n</code></pre>"},{"location":"_examples/intro_to_kernels/#expressing-other-priors-with-different-kernels","title":"Expressing Other Priors with Different Kernels","text":"<p>Whilst the Mat\u00e9rn kernels are often used as a first choice of kernel, and they often perform well due to their smoothing properties often being well-aligned with the properties of the underlying function being modelled, sometimes more prior knowledge is known about the function being modelled. For instance, it may be known that the function being modelled is periodic. In this case, a suitable kernel choice would be the periodic kernel:</p> \\[k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\left(\\frac{\\sin (\\pi (\\mathbf{x}_i - \\mathbf{x}_i')/p)}{\\ell}\\right)^2 \\right)\\] <p>with \\(D\\) being the dimensionality of the inputs.</p> <p>Below we show \\(10\\) samples drawn from a GP prior using the periodic kernel:</p> <pre><code>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Periodic()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(key=key, sample_shape=(10,))\n\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Periodic Kernel\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Samples from the Periodic Kernel')\n</code></pre> <p></p> <p>In other scenarios, it may be known that the underlying function is linear, in which case the linear kernel would be a suitable choice:</p> \\[k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\mathbf{x}^\\top \\mathbf{x}'\\] <p>Unlike the kernels shown above, the linear kernel is not stationary, and so it is not invariant to translations in the input space.</p> <p>Below we show \\(10\\) samples drawn from a GP prior using the linear kernel:</p> <pre><code>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Linear()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(key=key, sample_shape=(10,))\n\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Linear Kernel\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Samples from the Linear Kernel')\n</code></pre> <p></p>"},{"location":"_examples/intro_to_kernels/#composing-kernels","title":"Composing Kernels","text":"<p>It is also mathematically valid to compose kernels through operations such as addition and multiplication in order to produce more expressive kernels. For the mathematically interested amongst you, this is valid as the resulting kernel functions still satisfy the necessary conditions introduced at the start of this notebook. Adding or multiplying kernel functions is equivalent to performing elementwise addition or multiplication of the corresponding covariance matrices, and fortunately symmetric, positive semi-definite kernels are closed under these operations. This means that kernels produced by adding or multiplying other kernels will also be symmetric and positive semi-definite, and so will also be valid kernels. GPJax provides the functionality required to easily compose kernels via addition and multiplication, which we'll demonstrate below.</p> <p>First, we'll take a look at some samples drawn from a GP prior using a kernel which is composed of the sum of a linear kernel and a periodic kernel:</p> <pre><code>kernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(key=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Samples from a GP Prior with Kernel = Linear + Periodic')\n</code></pre> <p></p> <p>We can see that the samples drawn behave as one would naturally expect through adding the two kernels together. In particular, the samples are still periodic, as with the periodic kernel, but their mean also linearly increases/decreases as they move away from the origin, as seen with the linear kernel.</p> <p>Below we take a look at some samples drawn from a GP prior using a kernel which is composed of the same two kernels, but this time multiplied together:</p> <pre><code>kernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(key=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Samples from a GP with Kernel = Linear x Periodic')\n</code></pre> <p></p> <p>Once again, the samples drawn behave as one would naturally expect through multiplying the two kernels together. In particular, the samples are still periodic but their mean linearly increases/decreases as they move away from the origin, and the amplitude of the oscillations also linearly increases with increasing distance from the origin.</p>"},{"location":"_examples/intro_to_kernels/#putting-it-all-together-on-a-real-world-dataset","title":"Putting it All Together on a Real-World Dataset","text":""},{"location":"_examples/intro_to_kernels/#mauna-loa-co2-dataset","title":"Mauna Loa CO2 Dataset","text":"<p>We'll put together some of the ideas we've discussed in this notebook by fitting a GP to the Mauna Loa CO2 dataset. This dataset measures atmospheric CO2 concentration at the Mauna Loa Observatory in Hawaii, and is widely used in the GP literature. It contains monthly CO2 readings starting in March 1958. Interestingly, there was an eruption at the Mauna Loa volcano in November 2022, so readings from December 2022 have changed to a site roughly 21 miles North of the Mauna Loa Observatory. We'll use the data from March 1958 to November 2022, and see how our GP extrapolates to 8 years before and after the data in the training set.</p> <p>First we'll load the data and plot it:</p> <pre><code>co2_data = pd.read_csv(\n    \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\"\n)\nco2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12]\ntrain_x = co2_data[\"decimal date\"].values[:, None]\ntrain_y = co2_data[\"average\"].values[:, None]\n\nfig, ax = plt.subplots()\nax.plot(train_x, train_y)\nax.set_title(\"CO2 Concentration in the Atmosphere\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO2 Concentration (ppm)\")\n</code></pre> <pre><code>Text(0, 0.5, 'CO2 Concentration (ppm)')\n</code></pre> <p></p> <p>Looking at the data, we can see that there is clearly a periodic trend, with a period of roughly 1 year. We can also see that the data is increasing over time, which is also expected. This looks roughly linear, although it may have a non-linear component. This information will be useful when we come to choose our kernel.</p> <p>First, we'll construct our GPJax dataset, and will standardise the outputs, to match our assumption that the data has zero mean.</p> <pre><code>test_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1)\ny_scaler = StandardScaler().fit(train_y)\nstandardised_train_y = y_scaler.transform(train_y)\n\nD = gpx.Dataset(X=train_x, y=standardised_train_y)\n</code></pre> <p>Having constructed our dataset, we'll now define our kernel. We'll use a kernel which is composed of the sum of a linear kernel and a periodic kernel, as we saw in the previous section that this kernel is able to capture both the periodic and linear trends in the data. We'll also add an RBF kernel to the sum, which will allow us to capture any non-linear trends in the data:</p> \\[\\text{Kernel = Linear + Periodic + RBF}\\] <pre><code>mean = gpx.mean_functions.Zero()\nrbf_kernel = gpx.kernels.RBF(lengthscale=100.0)\nperiodic_kernel = gpx.kernels.Periodic()\nlinear_kernel = gpx.kernels.Linear(variance=0.001)\nsum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel])\nfinal_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])\n\nprior = gpx.gps.Prior(mean_function=mean, kernel=final_kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n\nposterior = prior * likelihood\n</code></pre> <p>With our model constructed, let's now fit it to the data, by minimising the negative log marginal likelihood of the data:</p> <pre><code>def loss(posterior, data):\n    return -gpx.objectives.conjugate_mll(posterior, data)\n\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=loss,\n    train_data=D,\n    optim=ox.adamw(learning_rate=1e-2),\n    num_iters=500,\n    key=key,\n)\n</code></pre> <pre><code>  0%|          | 0/500 [00:00&lt;?, ?it/s]\n</code></pre> <p>Now we can obtain the model's prediction over a period of time which includes the training data, as well as 8 years before and after the training data:</p> <pre><code>latent_dist = opt_posterior.predict(test_x, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean.reshape(-1, 1)\npredictive_std = jnp.sqrt(predictive_dist.variance).reshape(-1, 1)\n</code></pre> <p>Let's plot the model's predictions over this period of time:</p> <pre><code>fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\n    train_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5\n)\nax.fill_between(\n    test_x.squeeze(),\n    predictive_mean.squeeze() - 2 * predictive_std.squeeze(),\n    predictive_mean.squeeze() + 2 * predictive_std.squeeze(),\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    test_x,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f7564ca3e50&gt;\n</code></pre> <p></p> <p>We can see that the model seems to have captured the periodic trend in the data, as well as the (roughly) linear trend. This enables our model to make reasonable seeming predictions over the 8 years before and after the training data. Let's zoom in on the period from 2010 onwards:</p> <pre><code>fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\n    train_x[train_x &gt;= 2010],\n    standardised_train_y[train_x &gt;= 2010],\n    \"x\",\n    label=\"Observations\",\n    color=cols[0],\n    alpha=0.5,\n)\nax.fill_between(\n    test_x[test_x &gt;= 2010].squeeze(),\n    predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010],\n    label=\"Predictive mean\",\n    color=cols[1],\n)\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f7564ad1840&gt;\n</code></pre> <p></p> <p>This certainly looks like a reasonable fit to the data, with sensible extrapolation beyond the training data, which finishes in November 2022. Moreover, the learned parameters of the kernel are interpretable. Let's take a look at the learned period of the periodic kernel:</p> <pre><code>print(\n    \"Periodic Kernel Period:\"\n    f\" {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\"\n)\n</code></pre> <pre><code>Periodic Kernel Period: \u001b[38;2;79;201;177mPositiveReal\u001b[0m\u001b[38;2;255;213;3m(\u001b[0m\u001b[38;2;105;105;105m # 1 (8 B)\u001b[0m\n  \u001b[38;2;156;220;254mvalue\u001b[0m\u001b[38;2;212;212;212m=\u001b[0mArray(0.99992299, dtype=float64),\n  \u001b[38;2;156;220;254m_tag\u001b[0m\u001b[38;2;212;212;212m=\u001b[0m\u001b[38;2;207;144;120m\"'positive'\"\u001b[0m\n\u001b[38;2;255;213;3m)\u001b[0m\n</code></pre> <p>This tells us that the periodic trend learned has a period of \\(\\approx 1\\). This makes intuitive sense, as the unit of the input data is years, and we can see that the periodic trend tends to repeat itself roughly every year!</p>"},{"location":"_examples/intro_to_kernels/#defining-kernels-on-non-euclidean-spaces","title":"Defining Kernels on Non-Euclidean Spaces","text":"<p>In this notebook, we have focused solely on kernels whose domain resides in Euclidean space. However, what if one wished to work with data whose domain is non-Euclidean? For instance, one may wish to work with graph-structured data, or data which lies on a manifold, or even strings. Fortunately, kernels exist for a wide variety of domains. Whilst this is beyond the scope of this notebook, feel free to checkout out our notebook on graph kernels for an introduction on how to define the Mat\u00e9rn kernel on graph-structured data, and there are a wide variety of resources online for learning about defining kernels in other domains. In terms of open-source libraries, the Geometric Kernels library could be a good place to start if you're interested in looking at how these kernels may be implemented, with the additional benefit that it is compatible with GPJax.</p>"},{"location":"_examples/intro_to_kernels/#further-reading","title":"Further Reading","text":"<p>Congratulations on making it this far! We hope that this guide has given you a good introduction to kernels and how they can be used in GPJax. If you're interested in learning more about kernels, we recommend the following resources, which have also been used as inspiration for this guide:</p> <ul> <li>Gaussian Processes for Machine Learning - Chapter 4 provides a comprehensive overview of kernels, diving deep into some of the technical details and also providing some kernels defined on non-Euclidean spaces such as strings.</li> <li>David Duvenaud's Kernel Cookbook is a great resource for learning about kernels, and also provides some information about some of the pitfalls people commonly encounter when using the Mat\u00e9rn family of kernels. His PhD thesis, Automatic Model Construction with Gaussian Processes, also provides some in-depth recipes for how one may incorporate their prior knowledge when constructing kernels.</li> <li>Finally, please check out our more advanced kernel guide, which details some more kernels available in GPJax as well as how one may combine kernels together to form more complex kernels.</li> </ul>"},{"location":"_examples/intro_to_kernels/#system-configuration","title":"System Configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</code></pre> <pre><code>Author: Thomas Christie\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\ngpjax     : 0.11.1\nsklearn   : 1.6.1\nmatplotlib: 3.10.3\noptax     : 0.2.4\njaxtyping : 0.3.2\njax       : 0.6.0\npandas    : 2.2.3\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/likelihoods_guide/","title":"Likelihood guide","text":"<p>In this notebook, we will walk users through the process of creating a new likelihood in GPJax.</p>"},{"location":"_examples/likelihoods_guide/#background","title":"Background","text":"<p>In this section we'll provide a short introduction to likelihoods and why they are important. For users who are already familiar with likelihoods, feel free to skip to the next section, and for users who would like more information than is provided here, please see our introduction to Gaussian processes notebook.</p>"},{"location":"_examples/likelihoods_guide/#what-is-a-likelihood","title":"What is a likelihood?","text":"<p>We adopt the notation of our introduction to Gaussian processes notebook where we have a Gaussian process (GP) \\(f(\\cdot)\\sim\\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))\\) and a dataset \\(\\mathbf{y} = \\{y_n\\}_{n=1}^N\\) observed at corresponding inputs \\(\\mathbf{x} = \\{x_n\\}_{n=1}^N\\). The evaluation of \\(f\\) at \\(\\mathbf{x}\\) is denoted by \\(\\mathbf{f} = \\{f(x_n)\\}_{n=1}^N\\). The likelihood function of the GP is then given by $$ \\begin{align}     \\label{eq:likelihood_fn}     p(\\mathbf{y}\\mid \\mathbf{f}) = \\prod_{n=1}^N p(y_n\\mid f(x_n))\\,. \\end{align} $$ Conceptually, this conditional distribution describes the probability of the observed data, conditional on the latent function values.</p>"},{"location":"_examples/likelihoods_guide/#why-is-the-likelihood-important","title":"Why is the likelihood important?","text":"<p>Choosing the correct likelihood function when building a GP, or any Bayesian model for that matter, is crucial. The likelihood function encodes our assumptions about the data and the noise that we expect to observe. For example, if we are modelling air pollution, then we would not expect to observe negative values of pollution. In this case, we would choose a likelihood function that is only defined for positive values. Similarly, if our data is the proportion of people who voted for a particular political party, then we would expect to observe values between 0 and 1. In this case, we would choose a likelihood function that is only defined for values between 0 and 1.</p>"},{"location":"_examples/likelihoods_guide/#likelihoods-in-gpjax","title":"Likelihoods in GPJax","text":"<p>In GPJax, all likelihoods are a subclass of the <code>AbstractLikelihood</code> class. This base abstract class contains the three core methods that all likelihoods must implement: <code>predict</code>, <code>link_function</code>, and <code>expected_log_likelihood</code>. We will discuss each of these methods in the forthcoming sections, but first, we will show how to instantiate a likelihood object. To do this, we'll need a dataset.</p> <pre><code>import jax\n\n# Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\nfrom examples.utils import use_mpl_style\nimport gpjax as gpx\n\nconfig.update(\"jax_enable_x64\", True)\n\n\n# set the default style for plotting\nuse_mpl_style()\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nkey = jr.key(42)\n\nn = 50\nx = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0)\nxtest = jnp.linspace(-3, 3, 100)[:, None]\nf = lambda x: jnp.sin(x)\ny = f(x) + 0.1 * jr.normal(key, shape=x.shape)\nD = gpx.Dataset(x, y)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\")\nax.plot(x, f(x), label=\"Latent function\")\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7ff4c862eec0&gt;\n</code></pre> <p></p> <p>In this example, our observations have support \\([-3, 3]\\) and are generated from a sinusoidal function with Gaussian noise. As such, our response values \\(\\mathbf{y}\\) range between \\(-1\\) and \\(1\\), subject to Gaussian noise. Due to this, a Gaussian likelihood is appropriate for this dataset as it allows for negative values.</p> <p>As we see in \\eqref{eq:likelihood_fn}, the likelihood function factorises over the \\(n\\) observations. As such, we must provide this information to GPJax when instantiating a likelihood object. We do this by specifying the <code>num_datapoints</code> argument.</p> <pre><code>gpx.likelihoods.Gaussian(num_datapoints=D.n)\n</code></pre> <pre><code>Gaussian( # NonNegativeReal: 1 (8 B)\n  obs_stddev=NonNegativeReal( # 1 (8 B)\n    value=Array(1., dtype=float64, weak_type=True),\n    _tag=\"'non_negative'\"\n  ),\n  num_datapoints=50,\n  integrator=&lt;gpjax.integrators.AnalyticalGaussianIntegrator object at 0x7ff4c87247c0&gt;\n)\n</code></pre>"},{"location":"_examples/likelihoods_guide/#likelihood-parameters","title":"Likelihood parameters","text":"<p>Some likelihoods, such as the Gaussian likelihood, contain parameters that we seek to infer. In the case of the Gaussian likelihood, we have a single parameter \\(\\sigma^2\\) that determines the observation noise. In GPJax, we can specify the value of \\(\\sigma\\) when instantiating the likelihood object. If we do not specify a value, then the likelihood will be initialised with a default value. In the case of the Gaussian likelihood, the default value is \\(1.0\\). If we instead wanted to initialise the likelihood standard deviation with a value of \\(0.5\\), then we would do this as follows:</p> <pre><code>gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.5)\n</code></pre> <pre><code>Gaussian( # NonNegativeReal: 1 (8 B)\n  obs_stddev=NonNegativeReal( # 1 (8 B)\n    value=Array(0.5, dtype=float64, weak_type=True),\n    _tag=\"'non_negative'\"\n  ),\n  num_datapoints=50,\n  integrator=&lt;gpjax.integrators.AnalyticalGaussianIntegrator object at 0x7ff4c87247c0&gt;\n)\n</code></pre>"},{"location":"_examples/likelihoods_guide/#prediction","title":"Prediction","text":"<p>The <code>predict</code> method of a likelihood object transforms the latent distribution of the Gaussian process. In the case of a Gaussian likelihood, this simply applies the observational noise value to the diagonal values of the covariance matrix. For other likelihoods, this may be a more complex transformation. For example, the Bernoulli likelihood transforms the latent distribution of the Gaussian process into a distribution over binary values.</p> <p>We visualise this below for the Gaussian likelihood function. In blue we can see samples of \\(\\mathbf{f}^{\\star}\\), whilst in red we see samples of \\(\\mathbf{y}^{\\star}\\).</p> <pre><code>kernel = gpx.kernels.Matern32()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(kernel=kernel, mean_function=meanf)\n\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.1)\n\nposterior = prior * likelihood\n\nlatent_dist = posterior.predict(xtest, D)\n\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\n\nfor ax in axes.ravel():\n    subkey, _ = jr.split(subkey)\n    ax.plot(\n        latent_dist.sample(sample_shape=(1,), key=subkey).T,\n        lw=1,\n        color=cols[0],\n        label=\"Latent samples\",\n    )\n    ax.plot(\n        likelihood.predict(latent_dist).sample(sample_shape=(1,), key=subkey).T,\n        \"o\",\n        markersize=5,\n        alpha=0.3,\n        color=cols[1],\n        label=\"Predictive samples\",\n    )\n</code></pre> <p></p> <p>Similarly, for a Bernoulli likelihood function, the samples of \\(y\\) would be binary.</p> <pre><code>likelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\n\n\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\n\nfor ax in axes.ravel():\n    subkey, _ = jr.split(subkey)\n    ax.plot(\n        latent_dist.sample(sample_shape=(1,), key=subkey).T,\n        lw=1,\n        color=cols[0],\n        label=\"Latent samples\",\n    )\n    ax.plot(\n        likelihood.predict(latent_dist).sample(sample_shape=(1,), key=subkey).T,\n        \"o\",\n        markersize=3,\n        alpha=0.5,\n        color=cols[1],\n        label=\"Predictive samples\",\n    )\n</code></pre> <p></p>"},{"location":"_examples/likelihoods_guide/#link-functions","title":"Link functions","text":"<p>In the above figure, we can see the latent samples being constrained to be either 0 or 1 when a Bernoulli likelihood is specified. This is achieved by the <code>inverse link_function</code> \\(\\eta(\\cdot)\\) of the likelihood. The link function is a deterministic function that maps the latent distribution of the Gaussian process to the support of the likelihood function. For example, the link function of the Bernoulli likelihood that is used in GPJax is the inverse probit function $$ \\eta(x) = 0.5\\left(1 + \\Phi\\left(\\frac{x}{\\sqrt{2}}\\right) * (1-2)\\right)\\,, $$ where \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution.</p> <p>A table of commonly used link functions and their corresponding likelihood can be found here.</p>"},{"location":"_examples/likelihoods_guide/#expected-log-likelihood","title":"Expected log likelihood","text":"<p>The final method that is associated with a likelihood function in GPJax is the expected log-likelihood. This term is evaluated in the stochastic variational Gaussian process in the ELBO term. For a variational approximation \\(q(f)= \\mathcal{N}(f\\mid m, S)\\), the ELBO can be written as</p> \\[ \\begin{align}     \\mathcal{L}(q) = \\mathbb{E}_{f\\sim q(f)}\\left[ p(\\mathbf{y}\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,. \\end{align} \\] <p>As both \\(q(f)\\) and \\(p(f)\\) are Gaussian distributions, the Kullback-Leibler term can be analytically computed. However, the expectation term is not always so easy to compute. Fortunately, the bound in \\eqref{eq:elbo} can be decomposed as a sum of the datapoints</p> \\[ \\begin{align}     \\mathcal{L}(q) = \\sum_{n=1}^N \\mathbb{E}_{f\\sim q(f)}\\left[ p(y_n\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,. \\end{align} \\] <p>This simplifies computation of the expectation as it is now a series of \\(N\\) 1-dimensional integrals. As such, GPJax by default uses quadrature to compute these integrals. However, for some likelihoods, such as the Gaussian likelihood, the expectation can be computed analytically. In these cases, we can supply an object that inherits from <code>AbstractIntegrator</code> to the likelihood upon instantiation. To see this, let us consider a Gaussian likelihood where we'll first define a variational approximation to the posterior.</p> <pre><code>z = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1)\nq = gpx.variational_families.VariationalGaussian(posterior=posterior, inducing_inputs=z)\n\n\ndef q_moments(x):\n    qx = q(x)\n    return qx.mean, qx.variance\n\n\nmean, variance = jax.vmap(q_moments)(x[:, None])\n</code></pre> <p>Now that we have the variational mean and variational (co)variance, we can compute the expected log-likelihood using the <code>expected_log_likelihood</code> method of the likelihood object.</p> <pre><code>jnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance))\n</code></pre> <pre><code>Array(-47.69541299, dtype=float64)\n</code></pre> <p>However, had we wanted to do this using quadrature, then we would have done the following:</p> <pre><code>lquad = gpx.likelihoods.Gaussian(\n    num_datapoints=D.n,\n    obs_stddev=jnp.array([0.1]),\n    integrator=gpx.integrators.GHQuadratureIntegrator(num_points=20),\n)\n</code></pre> <p>However, this is not recommended for the Gaussian likelihood given that the expectation can be computed analytically.</p>"},{"location":"_examples/likelihoods_guide/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\ngpjax     : 0.11.1\njax       : 0.6.0\nmatplotlib: 3.10.3\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/oceanmodelling/","title":"Gaussian Processes for Vector Fields and Ocean Current Modelling","text":"<p>In this notebook, we use Gaussian processes to learn vector-valued functions. We will be recreating the results by Berlinghieri et al. (2023) by an application to real-world ocean surface velocity data, collected via surface drifters.</p> <p>Surface drifters are measurement devices that measure the dynamics and circulation patterns of the world's oceans. Studying and predicting ocean currents are important to climate research, for example, forecasting and predicting oil spills, oceanographic surveying of eddies and upwelling, or providing information on the distribution of biomass in ecosystems. We will be using the Gulf Drifters Open dataset, which contains all publicly available surface drifter trajectories from the Gulf of Mexico spanning 28 years.</p> <pre><code>from dataclasses import (\n    dataclass,\n    field,\n)\n\nfrom jax import (\n    config,\n    hessian,\n)\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nimport numpyro.distributions as npd\nimport pandas as pd\n\nfrom examples.utils import use_mpl_style\nfrom gpjax.kernels.computations import DenseKernelComputation\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\n# set the default style for plotting\nuse_mpl_style()\n\nkey = jr.key(42)\n\ncolors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/oceanmodelling/#data-loading-and-preprocessing","title":"Data loading and preprocessing","text":"<p>The real dataset has been binned into an \\(N=34\\times16\\) grid, equally spaced over the longitude-latitude interval \\([-90.8,-83.8] \\times [24.0,27.5]\\). Each bin has a size \\(\\approx 0.21\\times0.21\\), and contains the average velocity across all measurements that fall inside it.</p> <p>We will call this binned ocean data the ground truth, and label it with the vector field $$ \\mathbf{F} \\equiv \\mathbf{F}(\\mathbf{x}), $$ where \\(\\mathbf{x} = (x^{(0)}\\),\\(x^{(1)})^\\text{T}\\), with a vector basis in the standard Cartesian directions (dimensions will be indicated by superscripts).</p> <p>We shall label the ground truth \\(D_0=\\left\\{ \\left(\\mathbf{x}_{0,i} , \\mathbf{y}_{0,i} \\right)\\right\\}_{i=1}^N\\), where \\(\\mathbf{y}_{0,i}\\) is the 2-dimensional velocity vector at the \\(i\\)-th location, \\(\\mathbf{x}_{0,i}\\). The training dataset contains simulated measurements from ocean drifters \\(D_T=\\left\\{\\left(\\mathbf{x}_{T,i}, \\mathbf{y}_{T,i} \\right)\\right\\}_{i=1}^{N_T}\\), \\(N_T = 20\\) in this case (the subscripts indicate the ground truth and the simulated measurements respectively).</p> <pre><code># function to place data from csv into correct array shape\ndef prepare_data(df):\n    pos = jnp.array([df[\"lon\"], df[\"lat\"]])\n    vel = jnp.array([df[\"ubar\"], df[\"vbar\"]])\n    # extract shape stored as 'metadata' in the test data\n    try:\n        shape = (int(df[\"shape\"][1]), int(df[\"shape\"][0]))  # shape = (34,16)\n        return pos, vel, shape\n    except KeyError:\n        return pos, vel\n\n\n# loading in data\n\ngulf_data_train = pd.read_csv(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_train.csv\"\n)\ngulf_data_test = pd.read_csv(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_test.csv\"\n)\n\n\npos_test, vel_test, shape = prepare_data(gulf_data_test)\npos_train, vel_train = prepare_data(gulf_data_train)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\nax.quiver(\n    pos_test[0],\n    pos_test[1],\n    vel_test[0],\n    vel_test[1],\n    color=colors[0],\n    label=\"Ocean Current\",\n    angles=\"xy\",\n    scale=10,\n)\nax.quiver(\n    pos_train[0],\n    pos_train[1],\n    vel_train[0],\n    vel_train[1],\n    color=colors[1],\n    alpha=0.7,\n    label=\"Drifter\",\n    angles=\"xy\",\n    scale=10,\n)\n\nax.set(\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n)\nax.legend(\n    framealpha=0.0,\n    ncols=2,\n    fontsize=\"medium\",\n    bbox_to_anchor=(0.5, -0.3),\n    loc=\"lower center\",\n)\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f24bc0bea10&gt;\n</code></pre> <p></p>"},{"location":"_examples/oceanmodelling/#problem-setting","title":"Problem Setting","text":"<p>We aim to obtain estimates for \\(\\mathbf{F}\\) at the set of points \\(\\left\\{ \\mathbf{x}_{0,i} \\right\\}_{i=1}^N\\) using Gaussian processes, followed by a comparison of the latent model to the ground truth \\(D_0\\). Note that \\(D_0\\) is not passed into any functions used  by GPJax, and is only used to compare against the two GP models at the end of the notebook.</p> <p>Since \\(\\mathbf{F}\\) is a vector-valued function, we require GPs that can directly learn vector-valued functions<sup>1</sup>. To implement this in GPJax, the problem can be changed to learn a scalar-valued function by 'massaging' the data into a \\(2N\\times2N\\) problem, such that each dimension of our GP is associated with a component of \\(\\mathbf{y}_{T,i}\\).</p> <p>For a particular measurement \\(\\mathbf{y}\\) (training or testing) at location \\(\\mathbf{x}\\), the components \\((y^{(0)}, y^{(1)})\\) are described by the latent vector field \\(\\mathbf{F}\\), such that</p> \\[ \\mathbf{y} = \\mathbf{F}(\\mathbf{x}) = \\left(\\begin{array}{l} f^{(0)}\\left(\\mathbf{x}\\right) \\\\ f^{(1)}\\left(\\mathbf{x}\\right) \\end{array}\\right), \\] <p>where each \\(f^{(z)}\\left(\\mathbf{x}\\right), z \\in \\{0,1\\}\\) is a scalar-valued function.</p> <p>Now consider the scalar-valued function \\(g: \\mathbb{R}^2 \\times\\{0,1\\} \\rightarrow \\mathbb{R}\\), such that</p> \\[ g \\left(\\mathbf{x} , 0 \\right) = f^{(0)} ( \\mathbf{x} ), \\text{and } g \\left( \\mathbf{x}, 1 \\right)=f^{(1)}\\left(\\mathbf{x}\\right).  \\] <p>We have increased the input dimension by 1, from the 2D \\(\\mathbf{x}\\) to the 3D \\(\\mathbf{X} = \\left(\\mathbf{x}, 0\\right)\\) or \\(\\mathbf{X} = \\left(\\mathbf{x}, 1\\right)\\).</p> <p>By choosing the value of the third dimension, 0 or 1, we may now incorporate this information into the computation of the kernel.  We therefore make new 3D datasets \\(D_{T,3D} = \\left\\{\\left( \\mathbf{X}_{T,i},\\mathbf{Y}_{T,i} \\right) \\right\\} _{i=0}^{2N_T}\\) and \\(D_{0,3D} = \\left\\{\\left( \\mathbf{X}_{0,i},\\mathbf{Y}_{0,i} \\right) \\right\\} _{i=0}^{2N}\\) that incorporates this new labelling, such that for each dataset (indicated by the subscript \\(D = 0\\) or \\(D=T\\)),</p> \\[ X_{D,i} = \\left( \\mathbf{x}_{D,i}, z \\right), \\] <p>and</p> \\[ Y_{D,i} = y_{D,i}^{(z)}, \\] <p>where \\(z = 0\\) if \\(i\\) is odd and \\(z=1\\) if \\(i\\) is even.</p> <pre><code># Change vectors x -&gt; X = (x,z), and vectors y -&gt; Y = (y,z) via the artificial z label\ndef label_position(data):\n    # introduce alternating z label\n    n_points = len(data[0])\n    label = jnp.tile(jnp.array([0.0, 1.0]), n_points)\n    return jnp.vstack((jnp.repeat(data, repeats=2, axis=1), label)).T\n\n\n# change vectors y -&gt; Y by reshaping the velocity measurements\ndef stack_velocity(data):\n    return data.T.flatten().reshape(-1, 1)\n\n\ndef dataset_3d(pos, vel):\n    return gpx.Dataset(label_position(pos), stack_velocity(vel))\n\n\n# label and place the training data into a Dataset object to be used by GPJax\ndataset_train = dataset_3d(pos_train, vel_train)\n\n# we also require the testing data to be relabelled for later use, such that we can query the 2Nx2N GP at the test points\ndataset_ground_truth = dataset_3d(pos_test, vel_test)\n</code></pre>"},{"location":"_examples/oceanmodelling/#velocity-dimension-decomposition","title":"Velocity (dimension) decomposition","text":"<p>Having labelled the data, we are now in a position to use GPJax to learn the function \\(g\\), and hence \\(\\mathbf{F}\\). A naive approach to the problem is to apply a GP prior directly to the velocities of each dimension independently, which is called the velocity GP. For our prior, we choose an isotropic mean 0 over all dimensions of the GP, and a piecewise kernel that depends on the \\(z\\) labels of the inputs, such that for two inputs \\(\\mathbf{X} = \\left( \\mathbf{x}, z \\right )\\) and \\(\\mathbf{X}^\\prime = \\left( \\mathbf{x}^\\prime, z^\\prime \\right )\\),</p> \\[ k_{\\text{vel}} \\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right)= \\begin{cases}k^{(z)}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right) &amp; \\text { if } z=z^{\\prime} \\\\ 0 &amp; \\text { if } z \\neq z^{\\prime}, \\end{cases} \\] <p>where \\(k^{(z)}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)\\) are the user chosen kernels for each dimension. What this means is that there are no correlations between the \\(x^{(0)}\\) and \\(x^{(1)}\\) dimensions for all choices \\(\\mathbf{X}\\) and \\(\\mathbf{X}^{\\prime}\\), since there are no off-diagonal elements in the Gram matrix populated by this choice.</p> <p>To implement this approach in GPJax, we define <code>VelocityKernel</code> in the following cell, following the steps outlined in the custom kernels notebook. This modular implementation takes the choice of user kernels as its class attributes: <code>kernel0</code> and <code>kernel1</code>. We must additionally pass the argument <code>active_dims = [0,1]</code>, which is an attribute of the base class <code>AbstractKernel</code>, into the chosen kernels. This is necessary such that the subsequent likelihood optimisation does not optimise over the artificial label dimension.</p> <pre><code>class VelocityKernel(gpx.kernels.AbstractKernel):\n    def __init__(\n        self,\n        kernel0: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1]),\n        kernel1: gpx.kernels.AbstractKernel = gpx.kernels.RBF(active_dims=[0, 1]),\n    ):\n        self.kernel0 = kernel0\n        self.kernel1 = kernel1\n        super().__init__(compute_engine=DenseKernelComputation())\n\n    def __call__(\n        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        # standard RBF-SE kernel is x and x' are on the same output, otherwise returns 0\n\n        z = jnp.array(X[2], dtype=int)\n        zp = jnp.array(Xp[2], dtype=int)\n\n        # achieve the correct value via 'switches' that are either 1 or 0\n        k0_switch = ((z + 1) % 2) * ((zp + 1) % 2)\n        k1_switch = z * zp\n\n        return k0_switch * self.kernel0(X, Xp) + k1_switch * self.kernel1(X, Xp)\n</code></pre>"},{"location":"_examples/oceanmodelling/#gpjax-implementation","title":"GPJax implementation","text":"<p>Next, we define the model in GPJax. The prior is defined using \\(k_{\\text{vel}}\\left(\\mathbf{X}, \\mathbf{X}^\\prime \\right)\\) and 0 mean and 0 observation noise. We choose a Gaussian marginal log-likelihood (MLL).</p> <pre><code>def initialise_gp(kernel, mean, dataset):\n    prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n    likelihood = gpx.likelihoods.Gaussian(\n        num_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)\n    )\n    posterior = prior * likelihood\n    return posterior\n\n\n# Define the velocity GP\nmean = gpx.mean_functions.Zero()\nkernel = VelocityKernel()\nvelocity_posterior = initialise_gp(kernel, mean, dataset_train)\n</code></pre> <p>With a model now defined, we can proceed to optimise the hyperparameters of our likelihood over \\(D_0\\). This is done by minimising the MLL using <code>BFGS</code>. We also plot its value at each step to visually confirm that we have found the minimum. See the  introduction to Gaussian Processes notebook for more information on optimising the MLL.</p> <pre><code>def optimise_mll(posterior, dataset, NIters=1000, key=key):\n    # define the MLL using dataset_train\n    objective = lambda p, d: -gpx.objectives.conjugate_mll(p, d)\n    # Optimise to minimise the MLL\n    opt_posterior, history = gpx.fit_scipy(\n        model=posterior,\n        objective=objective,\n        train_data=dataset,\n    )\n    return opt_posterior\n\n\nopt_velocity_posterior = optimise_mll(velocity_posterior, dataset_train)\n</code></pre> <pre><code>Optimization terminated successfully.\n         Current function value: -26.620707\n         Iterations: 42\n         Function evaluations: 70\n         Gradient evaluations: 70\n</code></pre>"},{"location":"_examples/oceanmodelling/#comparison","title":"Comparison","text":"<p>We next obtain the latent distribution of the GP of \\(g\\) at \\(\\mathbf{x}_{0,i}\\), then extract its mean and standard at the test locations, \\(\\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})\\), as well as the standard deviation (we will use it at the very end).</p> <pre><code>def latent_distribution(opt_posterior, pos_3d, dataset_train):\n    latent = opt_posterior.predict(pos_3d, train_data=dataset_train)\n    latent_mean = latent.mean\n    latent_std = latent.stddev()\n    return latent_mean, latent_std\n\n\n# extract latent mean and std of g, redistribute into vectors to model F\nvelocity_mean, velocity_std = latent_distribution(\n    opt_velocity_posterior, dataset_ground_truth.X, dataset_train\n)\n\ndataset_latent_velocity = dataset_3d(pos_test, velocity_mean)\n</code></pre> <p>We now replot the ground truth (testing data) \\(D_0\\), the predicted latent vector field \\(\\mathbf{F}_{\\text{latent}}(\\mathbf{x_i})\\), and a heatmap of the residuals at each location \\(\\mathbf{R}(\\mathbf{x}_{0,i}) = \\mathbf{y}_{0,i} - \\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})\\), as well as \\(\\left|\\left|\\mathbf{R}(\\mathbf{x}_{0,i})\\right|\\right|\\).</p> <pre><code># Residuals between ground truth and estimate\n\n\ndef plot_vector_field(ax, dataset, **kwargs):\n    ax.quiver(\n        dataset.X[::2][:, 0],\n        dataset.X[::2][:, 1],\n        dataset.y[::2],\n        dataset.y[1::2],\n        **kwargs,\n    )\n\n\ndef prepare_ax(ax, X, Y, title, **kwargs):\n    ax.set(\n        xlim=[X.min() - 0.1, X.max() + 0.1],\n        ylim=[Y.min() + 0.1, Y.max() + 0.1],\n        aspect=\"equal\",\n        title=title,\n        ylabel=\"latitude\",\n        **kwargs,\n    )\n\n\ndef residuals(dataset_latent, dataset_ground_truth):\n    return jnp.sqrt(\n        (dataset_latent.y[::2] - dataset_ground_truth.y[::2]) ** 2\n        + (dataset_latent.y[1::2] - dataset_ground_truth.y[1::2]) ** 2\n    )\n\n\ndef plot_fields(\n    dataset_ground_truth, dataset_trajectory, dataset_latent, shape=shape, scale=10\n):\n    X = dataset_ground_truth.X[:, 0][::2]\n    Y = dataset_ground_truth.X[:, 1][::2]\n    # make figure\n    fig, ax = plt.subplots(1, 3, figsize=(12.0, 3.0), sharey=True)\n\n    # ground truth\n    plot_vector_field(\n        ax[0],\n        dataset_ground_truth,\n        color=colors[0],\n        label=\"Ocean Current\",\n        angles=\"xy\",\n        scale=scale,\n    )\n    plot_vector_field(\n        ax[0],\n        dataset_trajectory,\n        color=colors[1],\n        label=\"Drifter\",\n        angles=\"xy\",\n        scale=scale,\n    )\n    prepare_ax(ax[0], X, Y, \"Ground Truth\", xlabel=\"Longitude\")\n\n    # Latent estimate of vector field F\n    plot_vector_field(ax[1], dataset_latent, color=colors[0], angles=\"xy\", scale=scale)\n    plot_vector_field(\n        ax[1], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale\n    )\n    prepare_ax(ax[1], X, Y, \"GP Estimate\", xlabel=\"Longitude\")\n\n    # residuals\n    residuals_vel = jnp.flip(\n        residuals(dataset_latent, dataset_ground_truth).reshape(shape), axis=0\n    )\n    im = ax[2].imshow(\n        residuals_vel,\n        extent=[X.min(), X.max(), Y.min(), Y.max()],\n        cmap=\"jet\",\n        vmin=0,\n        vmax=1.0,\n        interpolation=\"spline36\",\n    )\n    plot_vector_field(\n        ax[2], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale\n    )\n    prepare_ax(ax[2], X, Y, \"Residuals\", xlabel=\"Longitude\")\n    fig.colorbar(im, fraction=0.027, pad=0.04, orientation=\"vertical\")\n\n    fig.legend(\n        framealpha=0.0,\n        ncols=2,\n        fontsize=\"medium\",\n        bbox_to_anchor=(0.5, -0.03),\n        loc=\"lower center\",\n    )\n\n\nplot_fields(dataset_ground_truth, dataset_train, dataset_latent_velocity)\n</code></pre> <p></p> <p>From the latent estimate we can see the velocity GP struggles to reconstruct features of the ground truth. This is because our construction of the kernel placed an independent prior on each physical dimension, which cannot be assumed. Therefore, we need a different approach that can implicitly incorporate this dependence at a fundamental level. To achieve this we will require a Helmholtz Decomposition.</p>"},{"location":"_examples/oceanmodelling/#helmholtz-decomposition","title":"Helmholtz decomposition","text":"<p>In 2 dimensions, a twice continuously differentiable and compactly supported vector field \\(\\mathbf{F}: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) can be expressed as the sum of the gradient of a scalar potential \\(\\Phi: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\), called the potential function, and the vorticity operator of another scalar potential \\(\\Psi: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\), called the stream function (Berlinghieri et al. (2023)) such that $$ \\mathbf{F}=\\operatorname{grad} \\Phi+\\operatorname{rot} \\Psi, $$ where $$ \\operatorname{grad} \\Phi:=\\left[\\begin{array}{l} \\partial \\Phi / \\partial x^{(0)} \\ \\partial \\Phi / \\partial x^{(1)} \\end{array}\\right] \\text { and } \\operatorname{rot} \\Psi:=\\left[\\begin{array}{c} \\partial \\Psi / \\partial x^{(1)} \\ -\\partial \\Psi / \\partial x^{(0)} \\end{array}\\right].  $$</p> <p>This is reminiscent of a 3 dimensional Helmholtz decomposition.</p> <p>The 2 dimensional decomposition motivates a different approach: placing priors on \\(\\Psi\\) and \\(\\Phi\\), allowing us to make assumptions directly about fundamental properties of \\(\\mathbf{F}\\). If we choose independent GP priors such that \\(\\Phi \\sim \\mathcal{G P}\\left(0, k_{\\Phi}\\right)\\) and \\(\\Psi \\sim \\mathcal{G P}\\left(0, k_{\\Psi}\\right)\\), then \\(\\mathbf{F} \\sim \\mathcal{G P} \\left(0, k_\\text{Helm}\\right)\\) (since acting linear operations on a GPs give GPs).</p> <p>For \\(\\mathbf{X}, \\mathbf{X}^{\\prime} \\in \\mathbb{R}^2 \\times \\left\\{0,1\\right\\}\\) and \\(z, z^\\prime \\in \\{0,1\\}\\),</p> \\[ \\boxed{ k_{\\mathrm{Helm}}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)_{z,z^\\prime} =  \\frac{\\partial^2 k_{\\Phi}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)}{\\partial x^{(z)} \\partial\\left(x^{\\prime}\\right)^{(z^\\prime)}}+(-1)^{z+z^\\prime} \\frac{\\partial^2 k_{\\Psi}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)}{\\partial x^{(1-z)} \\partial\\left(x^{\\prime}\\right)^{(1-z^\\prime)}}}.  \\] <p>where \\(x^{(z)}\\) and \\((x^\\prime)^{(z^\\prime)}\\) are the \\(z\\) and \\(z^\\prime\\) components of \\(\\mathbf{X}\\) and \\({\\mathbf{X}}^{\\prime}\\) respectively.</p> <p>We compute the second derivatives using <code>jax.hessian</code>. In the following implementation, for a kernel \\(k(\\mathbf{x}, \\mathbf{x}^{\\prime})\\), this computes the Hessian matrix with respect to the components of \\(\\mathbf{x}\\)</p> \\[ \\frac{\\partial^2 k\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)}{\\partial x^{(z)} \\partial x^{(z^\\prime)}}.  \\] <p>Note that we have operated \\(\\dfrac{\\partial}{\\partial x^{(z)}}\\), not \\(\\dfrac{\\partial}{\\partial \\left(x^\\prime \\right)^{(z)}}\\), as the boxed equation suggests. This is not an issue if we choose stationary kernels \\(k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = k(\\mathbf{x} - \\mathbf{x}^{\\prime})\\) , as the partial derivatives with respect to the components have the following exchange symmetry:</p> \\[ \\frac{\\partial}{\\partial x^{(z)}} = - \\frac{\\partial}{\\partial \\left( x^\\prime \\right)^{(z)}}, \\] <p>for either \\(z\\).</p> <pre><code>@dataclass\nclass HelmholtzKernel(gpx.kernels.stationary.StationaryKernel):\n    # initialise Phi and Psi kernels as any stationary kernel in gpJax\n    potential_kernel: gpx.kernels.stationary.StationaryKernel = field(\n        default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n    )\n    stream_kernel: gpx.kernels.stationary.StationaryKernel = field(\n        default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n    )\n    compute_engine = DenseKernelComputation()\n\n    def __call__(\n        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        # obtain indices for k_helm, implement in the correct sign between the derivatives\n        z = jnp.array(X[2], dtype=int)\n        zp = jnp.array(Xp[2], dtype=int)\n        sign = (-1) ** (z + zp)\n\n        # convert to array to correctly index, -ve sign due to exchange symmetry (only true for stationary kernels)\n        potential_dvtve = -jnp.array(\n            hessian(self.potential_kernel)(X, Xp), dtype=jnp.float64\n        )[z][zp]\n        stream_dvtve = -jnp.array(\n            hessian(self.stream_kernel)(X, Xp), dtype=jnp.float64\n        )[1 - z][1 - zp]\n\n        return potential_dvtve + sign * stream_dvtve\n</code></pre>"},{"location":"_examples/oceanmodelling/#gpjax-implementation_1","title":"GPJax implementation","text":"<p>We repeat the same steps as with the velocity GP model, replacing <code>VelocityKernel</code> with <code>HelmholtzKernel</code>.</p> <pre><code># Redefine Gaussian process with Helmholtz kernel\nkernel = HelmholtzKernel()\nhelmholtz_posterior = initialise_gp(kernel, mean, dataset_train)\n# Optimise hyperparameters using BFGS\nopt_helmholtz_posterior = optimise_mll(helmholtz_posterior, dataset_train)\n</code></pre> <pre><code>Optimization terminated successfully.\n         Current function value: -28.611975\n         Iterations: 35\n         Function evaluations: 58\n         Gradient evaluations: 58\n</code></pre>"},{"location":"_examples/oceanmodelling/#comparison_1","title":"Comparison","text":"<p>We again plot the ground truth (testing data) \\(D_0\\), the predicted latent vector field \\(\\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})\\), and a heatmap of the residuals at each location \\(R(\\mathbf{x}_{0,i}) = \\mathbf{y}_{0,i} - \\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})\\) and \\(\\left|\\left|R(\\mathbf{x}_{0,i}) \\right|\\right|\\).</p> <pre><code># obtain latent distribution, extract x and y values over g\nhelmholtz_mean, helmholtz_std = latent_distribution(\n    opt_helmholtz_posterior, dataset_ground_truth.X, dataset_train\n)\ndataset_latent_helmholtz = dataset_3d(pos_test, helmholtz_mean)\n\nplot_fields(dataset_ground_truth, dataset_train, dataset_latent_helmholtz)\n</code></pre> <p></p> <p>Visually, the Helmholtz model performs better than the velocity model, preserving the local structure of the \\(\\mathbf{F}\\). Since we placed priors on \\(\\Phi\\) and \\(\\Psi\\), the construction of \\(\\mathbf{F}\\) allows for correlations between the dimensions (non-zero off-diagonal elements in the Gram matrix populated by \\(k_\\text{Helm}\\left(\\mathbf{X},\\mathbf{X}^{\\prime}\\right)\\) ).</p>"},{"location":"_examples/oceanmodelling/#negative-log-predictive-densities","title":"Negative log predictive densities","text":"<p>Lastly, we directly compare the velocity and Helmholtz models by computing the negative log predictive densities for each model. This is a quantitative metric that measures the probability of the ground truth given the data.</p> \\[ \\mathrm{NLPD}=-\\sum_{i=1}^{2N} \\log \\left(  p\\left(\\mathcal{Y}_i = Y_{0,i} \\mid \\mathbf{X}_{i}\\right) \\right), \\] <p>where each \\(p\\left(\\mathcal{Y}_i \\mid \\mathbf{X}_i \\right)\\) is the marginal Gaussian distribution over \\(\\mathcal{Y}_i\\) at each test location, and \\(Y_{i,0}\\) is the \\(i\\)-th component of the (massaged) test data that we reserved at the beginning of the notebook in \\(D_0\\). A smaller value is better, since the deviation of the ground truth and the model are small in this case.</p> <pre><code># ensure testing data alternates between x0 and x1 components\ndef nlpd(mean, std, vel_test):\n    vel_query = jnp.column_stack((vel_test[0], vel_test[1])).flatten()\n    normal = npd.Normal(loc=mean, scale=std)\n    return -jnp.sum(normal.log_prob(vel_query))\n\n\n# compute nlpd for velocity and helmholtz\nnlpd_vel = nlpd(velocity_mean, velocity_std, vel_test)\nnlpd_helm = nlpd(helmholtz_mean, helmholtz_std, vel_test)\n\nprint(\"NLPD for Velocity: %.2f \\nNLPD for Helmholtz: %.2f\" % (nlpd_vel, nlpd_helm))\n</code></pre> <pre><code>NLPD for Velocity: 730.13 \nNLPD for Helmholtz: -280.59\n</code></pre> <p>The Helmholtz model outperforms the velocity model, as indicated by the lower NLPD score.</p> <p></p>"},{"location":"_examples/oceanmodelling/#footnote","title":"Footnote","text":"<p>Kernels for vector-valued functions have been studied in the literature, see Alvarez et al. (2012)</p>"},{"location":"_examples/oceanmodelling/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Ivan Shalashilin'\n</code></pre> <pre><code>Author: Ivan Shalashilin\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njax       : 0.6.0\nmatplotlib: 3.10.3\ngpjax     : 0.11.1\nnumpyro   : 0.18.0\npandas    : 2.2.3\njaxtyping : 0.3.2\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/poisson/","title":"Count data regression","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via Markov chain Monte Carlo (MCMC). We focus on a count data regression task here and use BlackJax for sampling.</p> <pre><code>import blackjax\nfrom flax import nnx\nimport jax\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom examples.utils import use_mpl_style\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\n\n# set the default style for plotting\nuse_mpl_style()\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nkey = jr.key(42)\n</code></pre>"},{"location":"_examples/poisson/#dataset","title":"Dataset","text":"<p>For count data regression, the Poisson distribution is a natural choice for the likelihood function. The probability mass function of the Poisson distribution is given by</p> \\[ p(y \\,|\\, \\lambda) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!},\\] <p>where \\(y\\) is the count and the parameter \\(\\lambda \\in \\mathbb{R}_{&gt;0}\\) is the rate of the Poisson distribution.</p> <p>We than set \\(\\lambda = \\exp(f)\\) where \\(f\\) is the latent Gaussian process. The exponential function is the link function for the Poisson distribution: it maps the output of a GP to the positive real line, which is suitable for modeling count data.</p> <p>We simulate a dataset \\(\\mathcal{D} = \\{(\\mathbf{X}, \\mathbf{y})\\}\\) with inputs \\(\\mathbf{X} \\in \\mathbb{R}^d\\) and corresponding count outputs \\(\\mathbf{y}\\). We store our data \\(\\mathcal{D}\\) as a GPJax <code>Dataset</code>.</p> <pre><code>key, subkey = jr.split(key)\nn = 50\nx = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\nf = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\ny = jr.poisson(key, jnp.exp(f(x)))\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[1])\nax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\nax.legend()\n</code></pre> <pre><code>/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/jaxtyping/_decorator.py:473: UserWarning: y is not of type float64.Got y.dtype=int64. This may lead to numerical instability.\n  out = fn(*args, **kwargs)\n\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x7f9dbafe1960&gt;\n</code></pre> <p></p>"},{"location":"_examples/poisson/#gaussian-process-definition","title":"Gaussian Process definition","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. We adopt the Poisson likelihood available in GPJax.</p> <pre><code>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Constant()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Poisson(num_datapoints=D.n)\n</code></pre> <p>We construct the posterior through the product of our prior and likelihood.</p> <pre><code>posterior = prior * likelihood\nprint(type(posterior))\n</code></pre> <pre><code>&lt;class 'gpjax.gps.NonConjugatePosterior'&gt;\n</code></pre> <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. Here, we show how to use MCMC methods.</p>"},{"location":"_examples/poisson/#mcmc-inference","title":"MCMC inference","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"_examples/poisson/#mcmc-through-blackjax","title":"MCMC through BlackJax","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 200 values from our Markov chain. In practice, drawing more samples will be necessary.</p> <pre><code># Adapted from BlackJax's introduction notebook.\nnum_adapt = 1000\nnum_samples = 500\n\n\ngraphdef, params, *static_state = nnx.split(posterior, gpx.parameters.Parameter, ...)\nparams_bijection = gpx.parameters.DEFAULT_BIJECTION\n\n# Transform the parameters to the unconstrained space\nparams = gpx.parameters.transform(params, params_bijection, inverse=True)\n\n\ndef logprob_fn(params):\n    params = gpx.parameters.transform(params, params_bijection)\n    model = nnx.merge(graphdef, params, *static_state)\n    return gpx.objectives.log_posterior_density(model, D)\n\n\nstep_size = 1e-3\ninverse_mass_matrix = jnp.ones(53)\nnuts = blackjax.nuts(logprob_fn, step_size, inverse_mass_matrix)\n\nstate = nuts.init(params)\n\nstep = jax.jit(nuts.step)\n\n\ndef one_step(state, rng_key):\n    state, info = step(rng_key, state)\n    return state, (state, info)\n\n\nkeys = jax.random.split(key, num_samples)\n_, (states, infos) = jax.lax.scan(one_step, state, keys, unroll=10)\n</code></pre>"},{"location":"_examples/poisson/#sampler-efficiency","title":"Sampler efficiency","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain).</p> <pre><code>acceptance_rate = jnp.mean(infos.acceptance_rate)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</code></pre> <pre><code>Acceptance rate: 1.00\n</code></pre> <pre><code>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.prior.kernel.lengthscale.value)\nax1.plot(states.position.prior.kernel.variance.value)\nax2.plot(states.position.latent.value[:, 1, :])\nax0.set_title(\"Kernel Lengthscale\")\nax1.set_title(\"Kernel Variance\")\nax2.set_title(\"Latent Function (index = 1)\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Latent Function (index = 1)')\n</code></pre> <p></p>"},{"location":"_examples/poisson/#prediction","title":"Prediction","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every \\(n\\)-th sample where \\(n\\) is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p> <pre><code>thin_factor = 20\nposterior_samples = []\n\nfor i in range(0, num_samples, thin_factor):\n    sample_params = jtu.tree_map(lambda samples, i=i: samples[i], states.position)\n    sample_params = gpx.parameters.transform(sample_params, params_bijection)\n    model = nnx.merge(graphdef, sample_params, *static_state)\n    latent_dist = model.predict(xtest, train_data=D)\n    predictive_dist = model.likelihood(latent_dist)\n    posterior_samples.append(predictive_dist.sample(key=key, sample_shape=(10,)))\n\nposterior_samples = jnp.vstack(posterior_samples)\nlower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(posterior_samples, axis=0)\n</code></pre> <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> <pre><code>fig, ax = plt.subplots()\nax.plot(\n    x, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7\n)\nax.plot(\n    xtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1\n)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=cols[0],\n    label=\"95% CI\",\n)\n</code></pre> <pre><code>&lt;matplotlib.collections.FillBetweenPolyCollection at 0x7f9dba8a7a90&gt;\n</code></pre> <p></p>"},{"location":"_examples/poisson/#system-configuration","title":"System configuration","text":"<pre><code>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Francesco Zanetta\"\n</code></pre> <pre><code>Author: Francesco Zanetta\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njaxtyping : 0.3.2\nblackjax  : 1.2.5\njax       : 0.6.0\ngpjax     : 0.11.1\nmatplotlib: 3.10.3\nflax      : 0.10.6\n\nWatermark: 2.5.0\n</code></pre> <pre><code>\n</code></pre>"},{"location":"_examples/regression/","title":"Regression","text":"<p>In this notebook we demonstrate how to fit a Gaussian process regression model.</p> <pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom examples.utils import (\n    clean_legend,\n    use_mpl_style,\n)\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nkey = jr.key(123)\n\n# set the default style for plotting\nuse_mpl_style()\n\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/regression/#dataset","title":"Dataset","text":"<p>With the necessary modules imported, we simulate a dataset \\(\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}\\) with inputs \\(\\boldsymbol{x}\\) sampled uniformly on \\((-3., 3)\\) and corresponding independent noisy outputs</p> \\[\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).\\] <p>We store our data \\(\\mathcal{D}\\) as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p> <pre><code>n = 100\nnoise = 0.3\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\n</code></pre> <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise.</p> <pre><code>fig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f1744b52350&gt;\n</code></pre> <p></p> <p>Our aim in this tutorial will be to reconstruct the latent function from our noisy observations \\(\\mathcal{D}\\) via Gaussian process regression. We begin by defining a Gaussian process prior in the next section.</p>"},{"location":"_examples/regression/#defining-the-prior","title":"Defining the prior","text":"<p>A zero-mean Gaussian process (GP) places a prior distribution over real-valued functions \\(f(\\cdot)\\) where \\(f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})\\) for any finite collection of inputs \\(\\boldsymbol{x}\\).</p> <p>Here \\(\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) is the Gram matrix generated by a user-specified symmetric, non-negative definite kernel function \\(k(\\cdot, \\cdot')\\) with \\([\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)\\). The choice of kernel function is critical as, among other things, it governs the smoothness of the outputs that our GP can generate.</p> <p>For simplicity, we consider a radial basis function (RBF) kernel:</p> \\[k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).\\] <p>On paper a GP is written as \\(f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))\\), we can reciprocate this process in GPJax via defining a <code>Prior</code> with our chosen <code>RBF</code> kernel.</p> <pre><code>kernel = gpx.kernels.RBF()  # 1-dimensional input\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n</code></pre> <p>The above construction forms the foundation for GPJax's models. Moreover, the GP prior we have just defined can be represented by a TensorFlow Probability multivariate Gaussian distribution. Such functionality enables trivial sampling, and the evaluation of the GP's mean and covariance .</p> <pre><code>prior_dist = prior.predict(xtest)\n\nprior_mean = prior_dist.mean\nprior_std = prior_dist.variance\nsamples = prior_dist.sample(key=key, sample_shape=(20,))\n\n\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\n    xtest.flatten(),\n    prior_mean - prior_std,\n    prior_mean + prior_std,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\n</code></pre> <p></p>"},{"location":"_examples/regression/#constructing-the-posterior","title":"Constructing the posterior","text":"<p>Having defined our GP, we proceed to define a description of our data \\(\\mathcal{D}\\) conditional on our knowledge of \\(f(\\cdot)\\) --- this is exactly the notion of a likelihood function \\(p(\\mathcal{D} | f(\\cdot))\\). While the choice of likelihood is a critical in Bayesian modelling, for simplicity we consider a Gaussian with noise parameter \\(\\alpha\\)</p> \\[p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).\\] <p>This is defined in GPJax through calling a <code>Gaussian</code> instance.</p> <pre><code>likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n</code></pre> <p>The posterior is proportional to the prior multiplied by the likelihood, written as</p> <p>$$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$</p> <p>Mimicking this construct, the posterior is established in GPJax through the <code>*</code> operator.</p> <pre><code>posterior = prior * likelihood\n</code></pre>"},{"location":"_examples/regression/#parameter-state","title":"Parameter state","text":"<p>As outlined in the PyTrees documentation, parameters are contained within the model and for the leaves of the PyTree. Consequently, in this particular model, we have three parameters: the kernel lengthscale, kernel variance and the observation noise variance. Whilst we have initialised each of these to 1, we can learn Type 2 MLEs for each of these parameters by optimising the marginal log-likelihood (MLL).</p> <pre><code>print(-gpx.objectives.conjugate_mll(posterior, D))\n</code></pre> <pre><code>132.6462594200374\n</code></pre> <p>We can now define an optimiser. For this example we'll use the <code>bfgs</code> optimiser.</p> <pre><code>opt_posterior, history = gpx.fit_scipy(\n    model=posterior,\n    # we use the negative mll as we are minimising\n    objective=lambda p, d: -gpx.objectives.conjugate_mll(p, d),\n    train_data=D,\n)\n\nprint(-gpx.objectives.conjugate_mll(opt_posterior, D))\n</code></pre> <pre><code>Optimization terminated successfully.\n         Current function value: 55.469226\n         Iterations: 12\n         Function evaluations: 19\n         Gradient evaluations: 19\n55.469226471183696\n</code></pre>"},{"location":"_examples/regression/#prediction","title":"Prediction","text":"<p>Equipped with the posterior and a set of optimised hyperparameter values, we are now in a position to query our GP's predictive distribution at novel test inputs. To do this, we use our defined <code>posterior</code> and <code>likelihood</code> at our test inputs to obtain the predictive distribution as a <code>Distrax</code> multivariate Gaussian upon which <code>mean</code> and <code>stddev</code> can be used to extract the predictive mean and standard deviatation.</p> <pre><code>latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean\npredictive_std = jnp.sqrt(predictive_dist.variance)\n</code></pre> <p>With the predictions and their uncertainty acquired, we illustrate the GP's performance at explaining the data \\(\\mathcal{D}\\) and recovering the underlying latent function of interest.</p> <pre><code>fig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f171812d330&gt;\n</code></pre> <p></p>"},{"location":"_examples/regression/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</code></pre> <pre><code>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njaxtyping : 0.3.2\njax       : 0.6.0\nmatplotlib: 3.10.3\ngpjax     : 0.11.1\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/uncollapsed_vi/","title":"Sparse Stochastic Variational Inference","text":"<p>In this notebook we demonstrate how to implement sparse variational Gaussian processes (SVGPs) of Hensman et al. (2015). In particular, this approximation framework provides a tractable option for working with non-conjugate Gaussian processes with more than ~5000 data points. However, for conjugate models of less than 5000 data points, we recommend using the marginal log-likelihood approach presented in the regression notebook. Though we illustrate SVGPs here with a conjugate regression example, the same GPJax code works for general likelihoods, such as a Bernoulli for classification.</p> <pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\n\nfrom examples.utils import use_mpl_style\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    import gpjax.kernels as jk\n\nkey = jr.key(123)\n\n# set the default style for plotting\nuse_mpl_style()\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</code></pre>"},{"location":"_examples/uncollapsed_vi/#dataset","title":"Dataset","text":"<p>With the necessary modules imported, we simulate a dataset \\(\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{5000}\\) with inputs \\(\\boldsymbol{x}\\) sampled uniformly on \\((-5, 5)\\) and corresponding binary outputs</p> \\[\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4 * \\boldsymbol{x}) + \\sin(2 * \\boldsymbol{x}), \\textbf{I} * (0.2)^{2} \\right).\\] <p>We store our data \\(\\mathcal{D}\\) as a GPJax <code>Dataset</code> and create test inputs for later.</p> <pre><code>n = 50000\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)\n</code></pre>"},{"location":"_examples/uncollapsed_vi/#sparse-gps-via-inducing-inputs","title":"Sparse GPs via inducing inputs","text":"<p>Despite their endowment with elegant theoretical properties, GPs are burdened with prohibitive \\(\\mathcal{O}(n^3)\\) inference and \\(\\mathcal{O}(n^2)\\) memory costs in the number of data points \\(n\\) due to the necessity of computing inverses and determinants of the kernel Gram matrix \\(\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) during inference and hyperparameter learning. Sparse GPs seek to resolve tractability through low-rank approximations.</p> <p>Their name originates with the idea of using subsets of the data to approximate the kernel matrix, with sparseness occurring through the selection of the data points. Given inputs \\(\\boldsymbol{x}\\) and outputs \\(\\boldsymbol{y}\\) the task was to select an \\(m&lt;n\\) lower-dimensional dataset \\((\\boldsymbol{z},\\boldsymbol{\\tilde{y}}) \\subset (\\boldsymbol{x}, \\boldsymbol{y})\\) to train a Gaussian process on instead. By generalising the set of selected points \\(\\boldsymbol{z}\\), known as inducing inputs, to remove the restriction of being part of the dataset, we can arrive at a flexible low-rank approximation framework of the model using functions of \\(\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}\\) to replace the true covariance matrix \\(\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) at significantly lower costs. For example,  review many popular approximation schemes in this vein. However, because the model and the approximation are intertwined, assigning performance and faults to one or the other becomes tricky.</p> <p>On the other hand, sparse variational Gaussian processes (SVGPs) approximate the posterior, not the model. These provide a low-rank approximation scheme via variational inference. Here we posit a family of densities parameterised by \"variational parameters\". We then seek to find the closest family member to the posterior by minimising the Kullback-Leibler divergence over the variational parameters. The fitted variational density then serves as a proxy for the exact posterior. This procedure makes variational methods efficiently solvable via off-the-shelf optimisation techniques whilst retaining the true-underlying model. Furthermore, SVGPs offer further cost reductions with mini-batch stochastic gradient descent   and address non-conjugacy . We show a cost comparison between the approaches below, where \\(b\\) is the mini-batch size.</p> GPs sparse GPs SVGP Inference cost \\(\\mathcal{O}(n^3)\\) \\(\\mathcal{O}(n m^2)\\) \\(\\mathcal{O}(b m^2 + m^3)\\) Memory cost \\(\\mathcal{O}(n^2)\\) \\(\\mathcal{O}(n m)\\) \\(\\mathcal{O}(b m + m^2)\\) <p>To apply SVGP inference to our dataset, we begin by initialising \\(m = 50\\) equally spaced inducing inputs \\(\\boldsymbol{z}\\) across our observed data's support. These are depicted below via horizontal black lines.</p> <pre><code>z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.vlines(\n    z,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\nax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\n</code></pre> <pre><code>[Text(0.5, 0, '$x$'), Text(0, 0.5, '$f(x)$')]\n</code></pre> <p></p> <p>The inducing inputs will summarise our dataset, and since they are treated as variational parameters, their locations will be optimised. The next step to SVGP is to define a variational family.</p>"},{"location":"_examples/uncollapsed_vi/#defining-the-variational-process","title":"Defining the variational process","text":"<p>We begin by considering the form of the posterior distribution for all function values \\(f(\\cdot)\\)</p> \\[\\begin{align} p(f(\\cdot) | \\mathcal{D}) = \\int p(f(\\cdot)|f(\\boldsymbol{x})) p(f(\\boldsymbol{x})|\\mathcal{D}) \\text{d}f(\\boldsymbol{x}). \\qquad (\\dagger) \\end{align}\\] <p>To arrive at an approximation framework, we assume some redundancy in the data. Instead of predicting \\(f(\\cdot)\\) with function values at the datapoints \\(f(\\boldsymbol{x})\\), we assume this can be achieved with only function values at \\(m\\) inducing inputs \\(\\boldsymbol{z}\\)</p> \\[ p(f(\\cdot) | \\mathcal{D}) \\approx \\int p(f(\\cdot)|f(\\boldsymbol{z})) p(f(\\boldsymbol{z})|\\mathcal{D}) \\text{d}f(\\boldsymbol{z}). \\qquad (\\star) \\] <p>This lower dimensional integral results in computational savings in the model's predictive component from \\(p(f(\\cdot)|f(\\boldsymbol{x}))\\) to \\(p(f(\\cdot)|f(\\boldsymbol{z}))\\) where inverting \\(\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}\\) is replaced with inverting \\(\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}\\). However, since we did not observe our data \\(\\mathcal{D}\\) at \\(\\boldsymbol{z}\\) we ask, what exactly is the posterior \\(p(f(\\boldsymbol{z})|\\mathcal{D})\\)?</p> <p>Notice this is simply obtained by substituting \\(\\boldsymbol{z}\\) into \\((\\dagger)\\), but we arrive back at square one with computing the expensive integral. To side-step this, we consider replacing \\(p(f(\\boldsymbol{z})|\\mathcal{D})\\) in \\((\\star)\\) with a cheap-to-compute approximate distribution \\(q(f(\\boldsymbol{z}))\\)</p> <p>$$ q(f(\\cdot)) = \\int p(f(\\cdot)|f(\\boldsymbol{z})) q(f(\\boldsymbol{z})) \\text{d}f(\\boldsymbol{z}). \\qquad (\\times) $$</p> <p>To measure the quality of the approximation, we consider the Kullback-Leibler divergence \\(\\operatorname{KL}(\\cdot || \\cdot)\\) from our approximate process \\(q(f(\\cdot))\\) to the true process \\(p(f(\\cdot)|\\mathcal{D})\\). By parametrising \\(q(f(\\boldsymbol{z}))\\) over a variational family of distributions, we can optimise Kullback-Leibler divergence with respect to the variational parameters. Moreover, since inducing input locations \\(\\boldsymbol{z}\\) augment the model, they themselves can be treated as variational parameters without altering the true underlying model \\(p(f(\\boldsymbol{z})|\\mathcal{D})\\). This is exactly what gives SVGPs great flexibility whilst retaining robustness to overfitting.</p> <p>It is popular to elect a Gaussian variational distribution \\(q(f(\\boldsymbol{z})) = \\mathcal{N}(f(\\boldsymbol{z}); \\mathbf{m}, \\mathbf{S})\\) with parameters \\(\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S}\\}\\), since conjugacy is provided between \\(q(f(\\boldsymbol{z}))\\) and \\(p(f(\\cdot)|f(\\boldsymbol{z}))\\) so that the resulting variational process \\(q(f(\\cdot))\\) is a GP. We can implement this in GPJax by the following.</p> <pre><code>meanf = gpx.mean_functions.Zero()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\nkernel = jk.RBF()  # 1-dimensional inputs\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\np = prior * likelihood\nq = gpx.variational_families.VariationalGaussian(posterior=p, inducing_inputs=z)\n</code></pre> <p>Here, the variational process \\(q(\\cdot)\\) depends on the prior through \\(p(f(\\cdot)|f(\\boldsymbol{z}))\\) in \\((\\times)\\).</p>"},{"location":"_examples/uncollapsed_vi/#inference","title":"Inference","text":""},{"location":"_examples/uncollapsed_vi/#evidence-lower-bound","title":"Evidence lower bound","text":"<p>With our model defined, we seek to infer the optimal inducing inputs \\(\\boldsymbol{z}\\), variational mean \\(\\mathbf{m}\\) and covariance \\(\\mathbf{S}\\) that define our approximate posterior. To achieve this, we maximise the evidence lower bound (ELBO) with respect to \\(\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S} \\}\\), a proxy for minimising the Kullback-Leibler divergence. Moreover, as hinted by its name, the ELBO is a lower bound to the marginal log-likelihood, providing a tractable objective to optimise the model's hyperparameters akin to the conjugate setting. For further details on this, see Sections 3.1 and 4.1 of the excellent review paper .</p>"},{"location":"_examples/uncollapsed_vi/#mini-batching","title":"Mini-batching","text":"<p>Despite introducing inducing inputs into our model, inference can still be intractable with large datasets. To circumvent this, optimisation can be done using stochastic mini-batches.</p> <pre><code>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.02,\n    warmup_steps=75,\n    decay_steps=2000,\n    end_value=0.001,\n)\n\nopt_posterior, history = gpx.fit(\n    model=q,\n    # we are minimizing the elbo so we negate it\n    objective=lambda p, d: -gpx.objectives.elbo(p, d),\n    train_data=D,\n    optim=ox.adam(learning_rate=schedule),\n    num_iters=3000,\n    key=jr.key(42),\n    batch_size=128,\n)\n</code></pre> <pre><code>  0%|          | 0/3000 [00:00&lt;?, ?it/s]\n</code></pre>"},{"location":"_examples/uncollapsed_vi/#predictions","title":"Predictions","text":"<p>With optimisation complete, we can use our inferred parameter set to make predictions at novel inputs akin to all other models within GPJax on our variational process object \\(q(\\cdot)\\) (for example, see the regression notebook).</p> <pre><code>latent_dist = opt_posterior(xtest)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean\nsigma = jnp.sqrt(predictive_dist.variance)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.vlines(\n    opt_posterior.inducing_inputs.value,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7fc174f84e80&gt;\n</code></pre> <p></p>"},{"location":"_examples/uncollapsed_vi/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'\n</code></pre> <pre><code>Author: Thomas Pinder, Daniel Dodd &amp; Zeel B Patel\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njaxtyping : 0.3.2\noptax     : 0.2.4\ngpjax     : 0.11.1\njax       : 0.6.0\nmatplotlib: 3.10.3\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"_examples/yacht/","title":"UCI Data Benchmarking","text":"<p>In this notebook, we will show how to apply GPJax on a benchmark UCI regression problem. These kind of tasks are often used in the research community to benchmark and assess new techniques against those already in the literature. Much of the code contained in this notebook can be adapted to applied problems concerning datasets other than the one presented here.</p> <pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom examples.utils import use_mpl_style\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\n# set the default style for plotting\nuse_mpl_style()\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nkey = jr.key(42)\n</code></pre>"},{"location":"_examples/yacht/#data-loading","title":"Data Loading","text":"<p>We'll be using the Yacht dataset from the UCI machine learning data repository. Each observation describes the hydrodynamic performance of a yacht through its resistance. The dataset contains 6 covariates and a single positive, real valued response variable. There are 308 observations in the dataset, so we can comfortably use a conjugate regression Gaussian process here (for more more details, checkout the Regression notebook).</p> <pre><code>try:\n    yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\nexcept FileNotFoundError:\n    yacht = pd.read_fwf(\n        \"docs/_examples/data/yacht_hydrodynamics.data\", header=None\n    ).values[:-1, :]\n\nX = yacht[:, :-1]\ny = yacht[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"_examples/yacht/#preprocessing","title":"Preprocessing","text":"<p>With a dataset loaded, we'll now preprocess it such that it is more amenable to modelling with a Gaussian process.</p>"},{"location":"_examples/yacht/#data-partitioning","title":"Data Partitioning","text":"<p>We'll first partition our data into a training and testing split. We'll fit our Gaussian process to the training data and evaluate its performance on the test data. This allows us to investigate how effectively our Gaussian process generalises to out-of-sample datapoints and ensure that we are not overfitting. We'll hold 30% of our data back for testing purposes.</p> <pre><code>Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\n</code></pre>"},{"location":"_examples/yacht/#response-variable","title":"Response Variable","text":"<p>We'll now process our response variable \\(\\mathbf{y}\\). As the below plots show, the data has a very long tail and is certainly not Gaussian. However, we would like to model a Gaussian response variable so that we can adopt a Gaussian likelihood function and leverage the model's conjugacy. To achieve this, we'll first log-scale the data, to bring the long right tail in closer to the data's mean. We'll then standardise the data such that is distributed according to a unit normal distribution. Both of these transformations are invertible through the log-normal expectation and variance formulae and the the inverse standardisation identity, should we ever need our model's predictions to be back on the scale of the original dataset.</p> <p>For transforming both the input and response variable, all transformations will be done with respect to the training data where relevant.</p> <pre><code>log_ytr = np.log(ytr)\nlog_yte = np.log(yte)\n\ny_scaler = StandardScaler().fit(log_ytr)\nscaled_ytr = y_scaler.transform(log_ytr)\nscaled_yte = y_scaler.transform(log_yte)\n</code></pre> <p>We can see the effect of these transformations in the below three panels.</p> <pre><code>fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\nax[0].hist(ytr, bins=30, color=cols[1])\nax[0].set_title(\"y\")\nax[1].hist(log_ytr, bins=30, color=cols[1])\nax[1].set_title(\"log(y)\")\nax[2].hist(scaled_ytr, bins=30, color=cols[1])\nax[2].set_title(\"scaled log(y)\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'scaled log(y)')\n</code></pre> <p></p>"},{"location":"_examples/yacht/#input-variable","title":"Input Variable","text":"<p>We'll now transform our input variable \\(\\mathbf{X}\\) to be distributed according to a unit Gaussian.</p> <pre><code>x_scaler = StandardScaler().fit(Xtr)\nscaled_Xtr = x_scaler.transform(Xtr)\nscaled_Xte = x_scaler.transform(Xte)\n</code></pre>"},{"location":"_examples/yacht/#model-fitting","title":"Model fitting","text":"<p>With data now loaded and preprocessed, we'll proceed to defining a Gaussian process model and optimising its parameters. This notebook purposefully does not go into great detail on this process, so please see notebooks such as the Regression notebook and Classification notebook for further information.</p>"},{"location":"_examples/yacht/#model-specification","title":"Model specification","text":"<p>We'll use a radial basis function kernel to parameterise the Gaussian process in this notebook. As we have 5 covariates, we'll assign each covariate its own lengthscale parameter. This form of kernel is commonly known as an automatic relevance determination (ARD) kernel.</p> <p>In practice, the exact form of kernel used should be selected such that it represents your understanding of the data. For example, if you were to model temperature; a process that we know to be periodic, then you would likely wish to select a periodic kernel. Having Gaussian-ised our data somewhat, we'll also adopt a Gaussian likelihood function.</p> <pre><code>n_train, n_covariates = scaled_Xtr.shape\nkernel = gpx.kernels.RBF(\n    active_dims=list(range(n_covariates)),\n    variance=jnp.var(scaled_ytr),\n    lengthscale=0.1 * jnp.ones((n_covariates,)),\n)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train)\n\nposterior = prior * likelihood\n</code></pre>"},{"location":"_examples/yacht/#model-optimisation","title":"Model Optimisation","text":"<p>With a model now defined, we can proceed to optimise the hyperparameters of our model using Scipy.</p> <pre><code>training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\n\nopt_posterior, history = gpx.fit_scipy(\n    model=posterior,\n    # we use the negative mll as we are minimising\n    objective=lambda p, d: -gpx.objectives.conjugate_mll(p, d),\n    train_data=training_data,\n)\n\nprint(-gpx.objectives.conjugate_mll(opt_posterior, training_data))\n</code></pre> <pre><code>/home/runner/.local/share/hatch/env/virtual/gpjax/9bz-h8Il/docs/lib/python3.10/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n\n\n         Current function value: -6.805748\n         Iterations: 22\n         Function evaluations: 61\n         Gradient evaluations: 49\n\n\n-6.805747503890231\n</code></pre>"},{"location":"_examples/yacht/#prediction","title":"Prediction","text":"<p>With an optimal set of parameters learned, we can make predictions on the set of data that we held back right at the start. We'll do this in the usual way by first computing the latent function's distribution before computing the predictive posterior distribution.</p> <pre><code>latent_dist = opt_posterior(scaled_Xte, training_data)\npredictive_dist = likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean\npredictive_stddev = jnp.sqrt(predictive_dist.variance)\n</code></pre>"},{"location":"_examples/yacht/#evaluation","title":"Evaluation","text":"<p>We'll now show how the performance of our Gaussian process can be evaluated by numerically and visually.</p>"},{"location":"_examples/yacht/#metrics","title":"Metrics","text":"<p>To numerically assess the performance of our model, two commonly used metrics are root mean squared error (RMSE) and the R2 coefficient. RMSE is simply the square root of the squared difference between predictions and actuals. A value of 0 for this metric implies that our model has 0 generalisation error on the test set. R2 measures the amount of variation within the data that is explained by the model. This can be useful when designing variance reduction methods such as control variates as it allows you to understand what proportion of the data's variance will be soaked up. A perfect model here would score 1 for R2 score, whereas predicting the data's mean would score 0 and models doing worse than simple mean predictions can score less than 0.</p> <pre><code>rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nr2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nprint(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")\n</code></pre> <pre><code>Results:\n    RMSE:  0.0102\n    R2:  0.99\n</code></pre> <p>Both of these metrics seem very promising, so, based off these, we can be quite happy that our first attempt at modelling the Yacht data is promising.</p>"},{"location":"_examples/yacht/#diagnostic-plots","title":"Diagnostic plots","text":"<p>To accompany the above metrics, we can also produce residual plots to explore exactly where our model's shortcomings lie. If we define a residual as the true value minus the prediction, then we can produce three plots:</p> <ol> <li>Predictions vs. actuals.</li> <li>Predictions vs. residuals.</li> <li>Residual density.</li> </ol> <p>The first plot allows us to explore if our model struggles to predict well for larger or smaller values by observing where the model deviates more from the line \\(y=x\\). In the second plot we can inspect whether or not there were outliers or structure within the errors of our model. A well-performing model would have predictions close to and symmetrically distributed either side of \\(y=0\\). Such a plot can be useful for diagnosing heteroscedasticity. Finally, by plotting a histogram of our residuals we can observe whether or not there is any skew to our residuals.</p> <pre><code>residuals = scaled_yte.squeeze() - predictive_mean\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\n\nax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\nax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\nax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\n\nax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\nax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\nax[1].set_ylim([-1.0, 1.0])\nax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n\nax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\nax[2].set_title(\"Residuals\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Residuals')\n</code></pre> <p></p> <p>From this, we can see that our model is struggling to predict the smallest values of the Yacht's hydrodynamic and performs increasingly well as the Yacht's hydrodynamic performance increases. This is likely due to the original data's heavy right-skew, and successive modelling attempts may wish to introduce a heteroscedastic likelihood function that would enable more flexible modelling of the smaller response values.</p>"},{"location":"_examples/yacht/#system-configuration","title":"System configuration","text":"<pre><code>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre> <pre><code>Author: Thomas Pinder\n\nLast updated: Tue May 20 2025\n\nPython implementation: CPython\nPython version       : 3.10.16\nIPython version      : 8.36.0\n\njaxtyping : 0.3.2\njax       : 0.6.0\nmatplotlib: 3.10.3\npandas    : 2.2.3\nnumpy     : 2.2.6\nsklearn   : 1.6.1\ngpjax     : 0.11.1\n\nWatermark: 2.5.0\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>GPJax<ul> <li>Citation</li> <li>Dataset</li> <li>Distributions</li> <li>Fit</li> <li>GPs</li> <li>Integrators</li> <li>Kernels<ul> <li>Approximations<ul> <li>RFF</li> </ul> </li> <li>Base</li> <li>Computations<ul> <li>Base</li> <li>Basis Functions</li> <li>Constant Diagonal</li> <li>Dense</li> <li>Diagonal</li> <li>Eigen</li> </ul> </li> <li>Non Euclidean<ul> <li>Graph</li> <li>Utils</li> </ul> </li> <li>Nonstationary<ul> <li>Arccosine</li> <li>Linear</li> <li>Polynomial</li> </ul> </li> <li>Stationary<ul> <li>Base</li> <li>Mat\u00e9rn12</li> <li>Mat\u00e9rn32</li> <li>Mat\u00e9rn52</li> <li>Periodic</li> <li>Powered Exponential</li> <li>Rational Quadratic</li> <li>RBF</li> <li>Utils</li> <li>White</li> </ul> </li> </ul> </li> <li>Likelihoods</li> <li>Lower Cholesky</li> <li>Mean Functions</li> <li>Numpyro Extras</li> <li>Objectives</li> <li>Parameters</li> <li>Scan</li> <li>Typing</li> <li>Variational Families</li> </ul> </li> </ul>"},{"location":"api/citation/","title":"Citation","text":""},{"location":"api/dataset/","title":"Dataset","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset","title":"Dataset  <code>dataclass</code>","text":"<pre><code>Dataset(X=None, y=None)\n</code></pre> <p>Base class for datasets.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Optional[Num[Array, 'N D']]</code>, default:                   <code>None</code> )           \u2013            <p>input data.</p> </li> <li> <code>y</code>               (<code>Optional[Num[Array, 'N Q']]</code>, default:                   <code>None</code> )           \u2013            <p>output data.</p> </li> </ul>"},{"location":"api/dataset/#gpjax.dataset.Dataset.n","title":"n  <code>property</code>","text":"<pre><code>n\n</code></pre> <p>Number of observations.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.in_dim","title":"in_dim  <code>property</code>","text":"<pre><code>in_dim\n</code></pre> <p>Dimension of the inputs, \\(X\\).</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_supervised","title":"is_supervised","text":"<pre><code>is_supervised()\n</code></pre> <p>Returns <code>True</code> if the dataset is supervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_unsupervised","title":"is_unsupervised","text":"<pre><code>is_unsupervised()\n</code></pre> <p>Returns <code>True</code> if the dataset is unsupervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Combine two datasets. Right hand dataset is stacked beneath the left.</p>"},{"location":"api/distributions/","title":"Distributions","text":""},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution","title":"GaussianDistribution","text":"<pre><code>GaussianDistribution(loc, scale, validate_args=None)\n</code></pre> <p>               Bases: <code>Distribution</code></p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.mean","title":"mean  <code>property</code>","text":"<pre><code>mean\n</code></pre> <p>Calculates the mean.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.variance","title":"variance  <code>property</code>","text":"<pre><code>variance\n</code></pre> <p>Calculates the variance.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.covariance_matrix","title":"covariance_matrix  <code>property</code>","text":"<pre><code>covariance_matrix\n</code></pre> <p>Calculates the covariance matrix.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.entropy","title":"entropy","text":"<pre><code>entropy()\n</code></pre> <p>Calculates the entropy of the distribution.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.median","title":"median","text":"<pre><code>median()\n</code></pre> <p>Calculates the median.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.mode","title":"mode","text":"<pre><code>mode()\n</code></pre> <p>Calculates the mode.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.covariance","title":"covariance","text":"<pre><code>covariance()\n</code></pre> <p>Calculates the covariance matrix.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.stddev","title":"stddev","text":"<pre><code>stddev()\n</code></pre> <p>Calculates the standard deviation.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.log_prob","title":"log_prob","text":"<pre><code>log_prob(y)\n</code></pre> <p>Calculates the log pdf of the multivariate Gaussian.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>Float[Array, ' N']</code>)           \u2013            <p>the value of which to calculate the log probability.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>The log probability of the value as a scalar array.</p> </li> </ul>"},{"location":"api/fit/","title":"Fit","text":""},{"location":"api/fit/#gpjax.fit.fit","title":"fit","text":"<pre><code>fit(*, model, objective, train_data, optim, params_bijection=DEFAULT_BIJECTION, key=jr.PRNGKey(42), num_iters=100, batch_size=-1, log_rate=10, verbose=True, unroll=1, safe=True)\n</code></pre> <p>Train a Module model with respect to a supplied objective function. Optimisers used here should originate from Optax.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; import jax.random as jr\n    &gt;&gt;&gt; import optax as ox\n    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; from gpjax.parameters import PositiveReal, Static\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # (1) Create a dataset:\n    &gt;&gt;&gt; X = jnp.linspace(0.0, 10.0, 100)[:, None]\n    &gt;&gt;&gt; y = 2.0 * X + 1.0 + 10 * jr.normal(jr.PRNGKey(0), X.shape)\n    &gt;&gt;&gt; D = gpx.Dataset(X, y)\n    &gt;&gt;&gt; # (2) Define your model:\n    &gt;&gt;&gt; class LinearModel(nnx.Module):\n    &gt;&gt;&gt;     def __init__(self, weight: float, bias: float):\n    &gt;&gt;&gt;         self.weight = PositiveReal(weight)\n    &gt;&gt;&gt;         self.bias = Static(bias)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt;     def __call__(self, x):\n    &gt;&gt;&gt;         return self.weight.value * x + self.bias.value\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; model = LinearModel(weight=1.0, bias=1.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # (3) Define your loss function:\n    &gt;&gt;&gt; def mse(model, data):\n    &gt;&gt;&gt;     pred = model(data.X)\n    &gt;&gt;&gt;     return jnp.mean((pred - data.y) ** 2)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # (4) Train!\n    &gt;&gt;&gt; trained_model, history = gpx.fit(\n    &gt;&gt;&gt;     model=model, objective=mse, train_data=D, optim=ox.sgd(0.001), num_iters=1000\n    &gt;&gt;&gt; )\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Model</code>)           \u2013            <p>The model Module to be optimised.</p> </li> <li> <code>objective</code>               (<code>Objective</code>)           \u2013            <p>The objective function that we are optimising with respect to.</p> </li> <li> <code>train_data</code>               (<code>Dataset</code>)           \u2013            <p>The training data to be used for the optimisation.</p> </li> <li> <code>optim</code>               (<code>GradientTransformation</code>)           \u2013            <p>The Optax optimiser that is to be used for learning a parameter set.</p> </li> <li> <code>num_iters</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of optimisation steps to run. Defaults to 100.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>The size of the mini-batch to use. Defaults to -1 (i.e. full batch).</p> </li> <li> <code>key</code>               (<code>KeyArray</code>, default:                   <code>PRNGKey(42)</code> )           \u2013            <p>The random key to use for the optimisation batch selection. Defaults to jr.PRNGKey(42).</p> </li> <li> <code>log_rate</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>How frequently the objective function's value should be printed. Defaults to 10.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to print the training loading bar. Defaults to True.</p> </li> <li> <code>unroll</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of unrolled steps to use for the optimisation. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Model, Array]</code>           \u2013            <p>A tuple comprising the optimised model and training history.</p> </li> </ul>"},{"location":"api/fit/#gpjax.fit.fit_scipy","title":"fit_scipy","text":"<pre><code>fit_scipy(*, model, objective, train_data, max_iters=500, verbose=True, safe=True)\n</code></pre> <p>Train a Module model with respect to a supplied Objective function. Optimisers used here should originate from Optax. todo</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Model</code>)           \u2013            <p>the model Module to be optimised.</p> </li> <li> <code>objective</code>               (<code>Objective</code>)           \u2013            <p>The objective function that we are optimising with respect to.</p> </li> <li> <code>train_data</code>               (<code>Dataset</code>)           \u2013            <p>The training data to be used for the optimisation.</p> </li> <li> <code>max_iters</code>               (<code>int</code>, default:                   <code>500</code> )           \u2013            <p>The maximum number of optimisation steps to run. Defaults to 500.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to print the information about the optimisation. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Model, Array]</code>           \u2013            <p>A tuple comprising the optimised model and training history.</p> </li> </ul>"},{"location":"api/fit/#gpjax.fit.fit_lbfgs","title":"fit_lbfgs","text":"<pre><code>fit_lbfgs(*, model, objective, train_data, params_bijection=DEFAULT_BIJECTION, max_iters=100, safe=True, max_linesearch_steps=32, gtol=1e-05)\n</code></pre> <p>Train a Module model with respect to a supplied Objective function.</p> <p>Uses Optax's LBFGS implementation and a jax.lax.while loop.</p> <p>Args:      model: the model Module to be optimised.      objective: The objective function that we are optimising with          respect to.      train_data (Dataset): The training data to be used for the optimisation.      max_iters (int): The maximum number of optimisation steps to run. Defaults          to 500.      safe (bool): Whether to check the types of the inputs.      max_linesearch_steps (int): The maximum number of linesearch steps to use         for finding the stepsize.     gtol (float): Terminate the optimisation if the L2 norm of the gradient is         below this threshold.</p> <p>Returns:      A tuple comprising the optimised model and final loss.</p>"},{"location":"api/fit/#gpjax.fit.get_batch","title":"get_batch","text":"<pre><code>get_batch(train_data, batch_size, key)\n</code></pre> <p>Batch the data into mini-batches. Sampling is done with replacement.</p> <p>Parameters:</p> <ul> <li> <code>train_data</code>               (<code>Dataset</code>)           \u2013            <p>The training dataset.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size.</p> </li> <li> <code>key</code>               (<code>KeyArray</code>)           \u2013            <p>The random key to use for the batch selection.</p> </li> </ul>"},{"location":"api/fit/#gpjax.fit.get_batch--returns","title":"Returns","text":"<pre><code>Dataset: The batched dataset.\n</code></pre>"},{"location":"api/gps/","title":"GPs","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior","title":"AbstractPrior","text":"<pre><code>AbstractPrior(kernel, mean_function, jitter=1e-06)\n</code></pre> <p>               Bases: <code>Module</code>, <code>Generic[M, K]</code></p> <p>Abstract Gaussian process prior.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>kernel object inheriting from AbstractKernel.</p> </li> <li> <code>mean_function</code>               (<code>M</code>)           \u2013            <p>mean function object inheriting from AbstractMeanFunction.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the Gaussian process at the given points.</p> <p>The output of this function is a TensorFlow probability distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments to pass to the GP's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs)\n</code></pre> <p>Evaluate the predictive distribution.</p> <p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to the predict method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to the predict method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.Prior","title":"Prior","text":"<pre><code>Prior(kernel, mean_function, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractPrior[M, K]</code></p> <p>A Gaussian process prior object.</p> <p>The GP is parameterised by a mean and kernel function.</p> <p>A Gaussian process prior parameterised by a mean function \\(m(\\cdot)\\) and a kernel function \\(k(\\cdot, \\cdot)\\) is given by \\(p(f(\\cdot)) = \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))\\).</p> <p>To invoke a <code>Prior</code> distribution, a kernel and mean function must be specified.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; kernel = gpx.kernels.RBF()\n    &gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n    &gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>kernel object inheriting from AbstractKernel.</p> </li> <li> <code>mean_function</code>               (<code>M</code>)           \u2013            <p>mean function object inheriting from AbstractMeanFunction.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.Prior.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the Gaussian process at the given points.</p> <p>The output of this function is a TensorFlow probability distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments to pass to the GP's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.Prior.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Combine the prior with a likelihood to form a posterior distribution.</p> <p>The product of a prior and likelihood is proportional to the posterior distribution. By computing the product of a GP prior and a likelihood object, a posterior GP object will be returned. Mathematically, this can be described by: <p>p(f(\u22c5)\u2223y)\u221dp(y\u2223f(\u22c5))p(f(\u22c5)), p(f(\\cdot) \\mid y) \\propto p(y \\mid f(\\cdot))p(f(\\cdot)), p(f(\u22c5)\u2223y)\u221dp(y\u2223f(\u22c5))p(f(\u22c5)),</p> where \\(p(y | f(\\cdot))\\) is the likelihood and \\(p(f(\\cdot))\\) is the prior.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n    &gt;&gt;&gt; kernel = gpx.kernels.RBF()\n    &gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n    &gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n    &gt;&gt;&gt; prior * likelihood\n</code></pre> Args:     other (Likelihood): The likelihood distribution of the observed dataset.</p> <p>Returns     Posterior: The relevant GP posterior for the given prior and         likelihood. Special cases are accounted for where the model         is conjugate.</p>"},{"location":"api/gps/#gpjax.gps.Prior.predict","title":"predict","text":"<pre><code>predict(test_inputs)\n</code></pre> <p>Compute the predictive prior distribution for a given set of parameters. The output of this function is a function that computes a TFP distribution for a given set of inputs.</p> <p>In the following example, we compute the predictive prior distribution and then evaluate it on the interval :math:<code>[0, 1]</code>:</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; kernel = gpx.kernels.RBF()\n    &gt;&gt;&gt; mean_function = gpx.mean_functions.Zero()\n    &gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=mean_function, kernel=kernel)\n    &gt;&gt;&gt; prior.predict(jnp.linspace(0, 1, 100)[:, None])\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>test_inputs</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The inputs at which to evaluate the prior distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx","title":"sample_approx","text":"<pre><code>sample_approx(num_samples, key, num_features=100)\n</code></pre> <p>Approximate samples from the Gaussian process prior.</p> <p>Build an approximate sample from the Gaussian process prior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>In particular, we approximate the Gaussian processes' prior as the finite feature approximation \\(\\hat{f}(x) = \\sum_{i=1}^m\\phi_i(x)\\theta_i\\) where \\(\\phi_i\\) are \\(m\\) features sampled from the Fourier feature decomposition of the model's kernel and \\(\\theta_i\\) are samples from a unit Gaussian.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>In the following example, we build 10 such samples and then evaluate them over the interval \\([0, 1]\\):</p> <p>For a <code>prior</code> distribution, the following code snippet will build and evaluate an approximate sample.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; import jax.random as jr\n    &gt;&gt;&gt; key = jr.PRNGKey(123)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n    &gt;&gt;&gt; kernel = gpx.kernels.RBF(n_dims=1)\n    &gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; sample_fn = prior.sample_approx(10, key)\n    &gt;&gt;&gt; sample_fn(jnp.linspace(0, 1, 100).reshape(-1, 1))\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>num_samples</code>               (<code>int</code>)           \u2013            <p>The desired number of samples.</p> </li> <li> <code>key</code>               (<code>KeyArray</code>)           \u2013            <p>The random seed used for the sample(s).</p> </li> <li> <code>num_features</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of features used when approximating the kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalSample</code> (              <code>FunctionalSample</code> )          \u2013            <p>A function representing an approximate sample from the Gaussian process prior.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior","title":"AbstractPosterior","text":"<pre><code>AbstractPosterior(prior, likelihood, jitter=1e-06)\n</code></pre> <p>               Bases: <code>Module</code>, <code>Generic[P, L]</code></p> <p>Abstract Gaussian process posterior.</p> <p>The base GP posterior object conditioned on an observed dataset. All posterior objects should inherit from this class.</p> <p>Parameters:</p> <ul> <li> <code>prior</code>               (<code>AbstractPrior</code>)           \u2013            <p>The prior distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood distribution.</p> </li> <li> <code>jitter</code>               (<code>float</code>, default:                   <code>1e-06</code> )           \u2013            <p>A small constant added to the diagonal of the covariance matrix to ensure numerical stability.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments to pass to the GP's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs)\n</code></pre> <p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to the predict method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to the predict method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior","title":"ConjugatePosterior","text":"<pre><code>ConjugatePosterior(prior, likelihood, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractPosterior[P, GL]</code></p> <p>A Conjuate Gaussian process posterior object.</p> <p>A Gaussian process posterior distribution when the constituent likelihood function is a Gaussian distribution. In such cases, the latent function values \\(f\\) can be analytically integrated out of the posterior distribution. As such, many computational operations can be simplified; something we make use of in this object.</p> <p>For a Gaussian process prior \\(p(\\mathbf{f})\\) and a Gaussian likelihood \\(p(y | \\mathbf{f}) = \\mathcal{N}(y\\mid \\mathbf{f}, \\sigma^2))\\) where \\(\\mathbf{f} = f(\\mathbf{x})\\), the predictive posterior distribution at a set of inputs \\(\\mathbf{x}\\) is given by <p>p(f\u22c6\u2223y)=\u222bp(f\u22c6,f\u2223y)=N(f\u22c6\u03bc\u2223y,\u03a3\u2223y \\begin{align} p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star}, \\mathbf{f} \\mid \\mathbf{y})\\\\     &amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} \\end{align} p(f\u22c6\u2223y)\u200b=\u222bp(f\u22c6,f\u2223y)=N(f\u22c6\u03bc\u2223y\u200b,\u03a3\u2223y\u200b\u200b\u200b</p> where <p>\u03bc\u2223y=k(x\u22c6,x)(k(x,x\u2032)+\u03c32In)\u22121y\u03a3\u2223y=k(x\u22c6,x\u22c6\u2032)\u2212k(x\u22c6,x)(k(x,x\u2032)+\u03c32In)\u22121k(x,x\u22c6). \\begin{align} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\\\ \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}). \\end{align} \u03bc\u2223y\u200b\u03a3\u2223y\u200b\u200b=k(x\u22c6,x)(k(x,x\u2032)+\u03c32In\u200b)\u22121y=k(x\u22c6,x\u22c6\u2032)\u2212k(x\u22c6,x)(k(x,x\u2032)+\u03c32In\u200b)\u22121k(x,x\u22c6).\u200b\u200b</p></p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; prior = gpx.gps.Prior(\n            mean_function = gpx.mean_functions.Zero(),\n            kernel = gpx.kernels.RBF()\n        )\n    &gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; posterior = prior * likelihood\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>prior</code>               (<code>AbstractPrior</code>)           \u2013            <p>The prior distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood distribution.</p> </li> <li> <code>jitter</code>               (<code>float</code>, default:                   <code>1e-06</code> )           \u2013            <p>A small constant added to the diagonal of the covariance matrix to ensure numerical stability.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments to pass to the GP's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict","title":"predict","text":"<pre><code>predict(test_inputs, train_data)\n</code></pre> <p>Query the predictive posterior distribution.</p> <p>Conditional on a training data set, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density.</p> <p>The predictive distribution of a conjugate GP is given by $$     p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star} \\mathbf{f} \\mid \\mathbf{y})\\     &amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}{\\mid \\mathbf{y}} $$ where $$     \\boldsymbol{\\mu}{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\     \\boldsymbol{\\Sigma}{\\mid \\mathbf{y}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}). $$</p> <p>The conditioning set is a GPJax <code>Dataset</code> object, whilst predictions are made on a regular Jax array.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; xtrain = jnp.linspace(0, 1).reshape(-1, 1)\n    &gt;&gt;&gt; ytrain = jnp.sin(xtrain)\n    &gt;&gt;&gt; D = gpx.Dataset(X=xtrain, y=ytrain)\n    &gt;&gt;&gt; xtest = jnp.linspace(0, 1).reshape(-1, 1)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; prior = gpx.gps.Prior(mean_function = gpx.mean_functions.Zero(), kernel = gpx.kernels.RBF())\n    &gt;&gt;&gt; posterior = prior * gpx.likelihoods.Gaussian(num_datapoints = D.n)\n    &gt;&gt;&gt; predictive_dist = posterior(xtest, D)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>test_inputs</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>A Jax array of test inputs at which the predictive distribution is evaluated.</p> </li> <li> <code>train_data</code>               (<code>Dataset</code>)           \u2013            <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A function that accepts an input array and returns the predictive distribution as a <code>GaussianDistribution</code>.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx","title":"sample_approx","text":"<pre><code>sample_approx(num_samples, train_data, key, num_features=100, solver_algorithm=Cholesky())\n</code></pre> <p>Draw approximate samples from the Gaussian process posterior.</p> <p>Build an approximate sample from the Gaussian process posterior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>Unlike when building approximate samples from a Gaussian process prior, decompositions based on Fourier features alone rarely give accurate samples. Therefore, we must also include an additional set of features (known as canonical features) to better model the transition from Gaussian process prior to Gaussian process posterior. For more details see Wilson et. al. (2020).</p> <p>In particular, we approximate the Gaussian processes' posterior as the finite feature approximation \\(\\hat{f}(x) = \\sum_{i=1}^m \\phi_i(x)\\theta_i + \\sum{j=1}^N v_jk(.,x_j)\\) where \\(\\phi_i\\) are m features sampled from the Fourier feature decomposition of the model's kernel and \\(k(., x_j)\\) are N canonical features. The Fourier weights \\(\\theta_i\\) are samples from a unit Gaussian. See Wilson et. al. (2020) for expressions for the canonical weights \\(v_j\\).</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>Parameters:</p> <ul> <li> <code>num_samples</code>               (<code>int</code>)           \u2013            <p>The desired number of samples.</p> </li> <li> <code>key</code>               (<code>KeyArray</code>)           \u2013            <p>The random seed used for the sample(s).</p> </li> <li> <code>num_features</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of features used when approximating the kernel.</p> </li> <li> <code>solver_algorithm</code>               (<code>Optional[Algorithm]</code>, default:                   <code>Cholesky()</code> )           \u2013            <p>The algorithm to use for the solves of the inverse of the covariance matrix. See the CoLA documentation for which solver to pick. For PSD matrices, CoLA currently recommends Cholesky() for small matrices and CG() for larger matrices. Select Auto() to let CoLA decide. Defaults to Cholesky().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalSample</code> (              <code>FunctionalSample</code> )          \u2013            <p>A function representing an approximate sample from the Gaussian</p> </li> <li> <code>FunctionalSample</code>           \u2013            <p>process prior.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior","title":"NonConjugatePosterior","text":"<pre><code>NonConjugatePosterior(prior, likelihood, latent=None, jitter=1e-06, key=jr.PRNGKey(42))\n</code></pre> <p>               Bases: <code>AbstractPosterior[P, NGL]</code></p> <p>A non-conjugate Gaussian process posterior object.</p> <p>A Gaussian process posterior object for models where the likelihood is non-Gaussian. Unlike the <code>ConjugatePosterior</code> object, the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p> <p>Parameters:</p> <ul> <li> <code>prior</code>               (<code>AbstractPrior</code>)           \u2013            <p>The prior distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood distribution.</p> </li> <li> <code>jitter</code>               (<code>float</code>, default:                   <code>1e-06</code> )           \u2013            <p>A small constant added to the diagonal of the covariance matrix to ensure numerical stability.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>The arguments to pass to the GP's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A multivariate normal random variable representation of the Gaussian process.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict","title":"predict","text":"<pre><code>predict(test_inputs, train_data)\n</code></pre> <p>Query the predictive posterior distribution.</p> <p>Conditional on a set of training data, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density. Note, to gain predictions on the scale of the original data, the returned distribution will need to be transformed through the likelihood function's inverse link function.</p> <p>Parameters:</p> <ul> <li> <code>train_data</code>               (<code>Dataset</code>)           \u2013            <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A function that accepts an input array and returns the predictive distribution as a <code>dx.Distribution</code>.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.construct_posterior","title":"construct_posterior","text":"<pre><code>construct_posterior(prior, likelihood)\n</code></pre> <p>Utility function for constructing a posterior object from a prior and likelihood. The function will automatically select the correct posterior object based on the likelihood.</p> <p>Parameters:</p> <ul> <li> <code>prior</code>               (<code>Prior</code>)           \u2013            <p>The Prior distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood that represents our beliefs around the distribution of the data.</p> </li> </ul>"},{"location":"api/gps/#gpjax.gps.construct_posterior--returns","title":"Returns","text":"<pre><code>AbstractPosterior: A posterior distribution. If the likelihood is\n    Gaussian, then a `ConjugatePosterior` will be returned. Otherwise,\n    a `NonConjugatePosterior` will be returned.\n</code></pre>"},{"location":"api/integrators/","title":"Integrators","text":""},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator","title":"AbstractIntegrator","text":"<p>Base class for integrators.</p>"},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.integrate","title":"integrate  <code>abstractmethod</code>","text":"<pre><code>integrate(fun, y, mean, variance, likelihood)\n</code></pre> <p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> <ul> <li> <code>fun</code>               (<code>Callable</code>)           \u2013            <p>The function to be integrated.</p> </li> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The mean of the variational distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variance of the variational distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood function.</p> </li> </ul>"},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.__call__","title":"__call__","text":"<pre><code>__call__(fun, y, mean, variance, likelihood)\n</code></pre> <p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> <ul> <li> <code>fun</code>               (<code>Callable</code>)           \u2013            <p>The function to be integrated.</p> </li> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The mean of the variational distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variance of the variational distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood function.</p> </li> </ul>"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator","title":"GHQuadratureIntegrator","text":"<pre><code>GHQuadratureIntegrator(num_points=20)\n</code></pre> <p>               Bases: <code>AbstractIntegrator</code></p> <p>Compute an integral using Gauss-Hermite quadrature.</p> <p>Gauss-Hermite quadrature is a method for approximating integrals through a weighted sum of function evaluations at specific points $$ \\int F(t)\\exp(-t^2)\\mathrm{d}t \\approx \\sum_{j=1}^J w_j F(t_j) $$ where \\(t_j\\) and \\(w_j\\) are the roots and weights of the \\(J\\)-th order Hermite polynomial \\(H_J(t)\\) that we can look up in table link.</p> <p>Parameters:</p> <ul> <li> <code>num_points</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The number of points to use in the quadrature. Defaults to 20.</p> </li> </ul>"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.__call__","title":"__call__","text":"<pre><code>__call__(fun, y, mean, variance, likelihood)\n</code></pre> <p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> <ul> <li> <code>fun</code>               (<code>Callable</code>)           \u2013            <p>The function to be integrated.</p> </li> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The mean of the variational distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variance of the variational distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood function.</p> </li> </ul>"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.integrate","title":"integrate","text":"<pre><code>integrate(fun, y, mean, variance, likelihood)\n</code></pre> <p>Compute a quadrature integral.</p> <p>Parameters:</p> <ul> <li> <code>fun</code>               (<code>Callable</code>)           \u2013            <p>the likelihood to be integrated.</p> </li> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>the observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>the mean of the variational distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>the variance of the variational distribution.</p> </li> <li> <code>likelihood</code>               (<code>L | None</code>)           \u2013            <p>the likelihood function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The expected log likelihood as an array of shape (N,).</p> </li> </ul>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator","title":"AnalyticalGaussianIntegrator","text":"<p>               Bases: <code>AbstractIntegrator</code></p> <p>Compute the analytical integral of a Gaussian likelihood.</p> <p>When the likelihood function is Gaussian, the integral can be computed in closed form. For a Gaussian likelihood \\(p(y|f) = \\mathcal{N}(y|f, \\sigma^2)\\) and a variational distribution \\(q(f) = \\mathcal{N}(f|m, s)\\), the expected log-likelihood is given by $$ \\mathbb{E}_{q(f)}[\\log p(y|f)] = -\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2) + \\frac{1}{\\sigma^2}((y-m)^2 + s)\\right) $$</p>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator.__call__","title":"__call__","text":"<pre><code>__call__(fun, y, mean, variance, likelihood)\n</code></pre> <p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> <ul> <li> <code>fun</code>               (<code>Callable</code>)           \u2013            <p>The function to be integrated.</p> </li> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The mean of the variational distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variance of the variational distribution.</p> </li> <li> <code>likelihood</code>               (<code>AbstractLikelihood</code>)           \u2013            <p>The likelihood function.</p> </li> </ul>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator.integrate","title":"integrate","text":"<pre><code>integrate(fun, y, mean, variance, likelihood)\n</code></pre> <p>Compute a Gaussian integral.</p> <p>Parameters:</p> <ul> <li> <code>fun</code>               (<code>Callable</code>)           \u2013            <p>The Gaussian likelihood to be integrated.</p> </li> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The mean of the variational distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variance of the variational distribution.</p> </li> <li> <code>likelihood</code>               (<code>Gaussian</code>)           \u2013            <p>The Gaussian likelihood function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>Float[Array, 'N']: The expected log likelihood.</p> </li> </ul>"},{"location":"api/likelihoods/","title":"Likelihoods","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood","title":"AbstractLikelihood","text":"<pre><code>AbstractLikelihood(num_datapoints, integrator=GHQuadratureIntegrator())\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Abstract base class for likelihoods.</p> <p>All likelihoods must inherit from this class and implement the <code>predict</code> and <code>link_function</code> methods.</p> <p>Parameters:</p> <ul> <li> <code>num_datapoints</code>               (<code>int</code>)           \u2013            <p>the number of data points.</p> </li> <li> <code>integrator</code>               (<code>AbstractIntegrator</code>, default:                   <code>GHQuadratureIntegrator()</code> )           \u2013            <p>The integrator to be used for computing expected log likelihoods. Must be an instance of <code>AbstractIntegrator</code>.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distribution</code>           \u2013            <p>The predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs)\n</code></pre> <p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distribution</code>           \u2013            <p>npd.Distribution: The predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function","title":"link_function  <code>abstractmethod</code>","text":"<pre><code>link_function(f)\n</code></pre> <p>Return the link function of the likelihood function.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Float[Array, ...]</code>)           \u2013            <p>the latent Gaussian process values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distribution</code>           \u2013            <p>npd.Distribution: The distribution of observations, y, given values of the Gaussian process, f.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.expected_log_likelihood","title":"expected_log_likelihood","text":"<pre><code>expected_log_likelihood(y, mean, variance)\n</code></pre> <p>Compute the expected log likelihood.</p> <p>For a variational distribution \\(q(f)\\sim\\mathcal{N}(m, s)\\) and a likelihood \\(p(y|f)\\), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational mean.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational variance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>Float[Array, ' N']</code> )          \u2013            <p>The expected log likelihood.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian","title":"Gaussian","text":"<pre><code>Gaussian(num_datapoints, obs_stddev=1.0, integrator=AnalyticalGaussianIntegrator())\n</code></pre> <p>               Bases: <code>AbstractLikelihood</code></p> <p>Gaussian likelihood object.</p> <p>Parameters:</p> <ul> <li> <code>num_datapoints</code>               (<code>int</code>)           \u2013            <p>the number of data points.</p> </li> <li> <code>obs_stddev</code>               (<code>Union[ScalarFloat, Float[Array, '#N']]</code>, default:                   <code>1.0</code> )           \u2013            <p>the standard deviation of the Gaussian observation noise.</p> </li> <li> <code>integrator</code>               (<code>AbstractIntegrator</code>, default:                   <code>AnalyticalGaussianIntegrator()</code> )           \u2013            <p>The integrator to be used for computing expected log likelihoods. Must be an instance of <code>AbstractIntegrator</code>. For the Gaussian likelihood, this defaults to the <code>AnalyticalGaussianIntegrator</code>, as the expected log likelihood can be computed analytically.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distribution</code>           \u2013            <p>The predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.expected_log_likelihood","title":"expected_log_likelihood","text":"<pre><code>expected_log_likelihood(y, mean, variance)\n</code></pre> <p>Compute the expected log likelihood.</p> <p>For a variational distribution \\(q(f)\\sim\\mathcal{N}(m, s)\\) and a likelihood \\(p(y|f)\\), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational mean.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational variance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>Float[Array, ' N']</code> )          \u2013            <p>The expected log likelihood.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function","title":"link_function","text":"<pre><code>link_function(f)\n</code></pre> <p>The link function of the Gaussian likelihood.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Float[Array, ...]</code>)           \u2013            <p>Function values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Normal</code>           \u2013            <p>npd.Normal: The likelihood function.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict","title":"predict","text":"<pre><code>predict(dist)\n</code></pre> <p>Evaluate the Gaussian likelihood.</p> <p>Evaluate the Gaussian likelihood function at a given predictive distribution. Computationally, this is equivalent to summing the observation noise term to the diagonal elements of the predictive distribution's covariance matrix.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>Distribution</code>)           \u2013            <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MultivariateNormal</code>           \u2013            <p>npd.Distribution: The predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli","title":"Bernoulli","text":"<pre><code>Bernoulli(num_datapoints, integrator=GHQuadratureIntegrator())\n</code></pre> <p>               Bases: <code>AbstractLikelihood</code></p> <p>Parameters:</p> <ul> <li> <code>num_datapoints</code>               (<code>int</code>)           \u2013            <p>the number of data points.</p> </li> <li> <code>integrator</code>               (<code>AbstractIntegrator</code>, default:                   <code>GHQuadratureIntegrator()</code> )           \u2013            <p>The integrator to be used for computing expected log likelihoods. Must be an instance of <code>AbstractIntegrator</code>.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distribution</code>           \u2013            <p>The predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.expected_log_likelihood","title":"expected_log_likelihood","text":"<pre><code>expected_log_likelihood(y, mean, variance)\n</code></pre> <p>Compute the expected log likelihood.</p> <p>For a variational distribution \\(q(f)\\sim\\mathcal{N}(m, s)\\) and a likelihood \\(p(y|f)\\), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational mean.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational variance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>Float[Array, ' N']</code> )          \u2013            <p>The expected log likelihood.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function","title":"link_function","text":"<pre><code>link_function(f)\n</code></pre> <p>The probit link function of the Bernoulli likelihood.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Float[Array, ...]</code>)           \u2013            <p>Function values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BernoulliProbs</code>           \u2013            <p>npd.Bernoulli: The likelihood function.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict","title":"predict","text":"<pre><code>predict(dist)\n</code></pre> <p>Evaluate the pointwise predictive distribution.</p> <p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>[npd.MultivariateNormal, GaussianDistribution].</code>)           \u2013            <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BernoulliProbs</code>           \u2013            <p>npd.Bernoulli: The pointwise predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson","title":"Poisson","text":"<pre><code>Poisson(num_datapoints, integrator=GHQuadratureIntegrator())\n</code></pre> <p>               Bases: <code>AbstractLikelihood</code></p> <p>Parameters:</p> <ul> <li> <code>num_datapoints</code>               (<code>int</code>)           \u2013            <p>the number of data points.</p> </li> <li> <code>integrator</code>               (<code>AbstractIntegrator</code>, default:                   <code>GHQuadratureIntegrator()</code> )           \u2013            <p>The integrator to be used for computing expected log likelihoods. Must be an instance of <code>AbstractIntegrator</code>.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distribution</code>           \u2013            <p>The predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.expected_log_likelihood","title":"expected_log_likelihood","text":"<pre><code>expected_log_likelihood(y, mean, variance)\n</code></pre> <p>Compute the expected log likelihood.</p> <p>For a variational distribution \\(q(f)\\sim\\mathcal{N}(m, s)\\) and a likelihood \\(p(y|f)\\), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The observed response variable.</p> </li> <li> <code>mean</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational mean.</p> </li> <li> <code>variance</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The variational variance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>Float[Array, ' N']</code> )          \u2013            <p>The expected log likelihood.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.link_function","title":"link_function","text":"<pre><code>link_function(f)\n</code></pre> <p>The link function of the Poisson likelihood.</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Float[Array, ...]</code>)           \u2013            <p>Function values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Poisson</code>           \u2013            <p>npd.Poisson: The likelihood function.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.predict","title":"predict","text":"<pre><code>predict(dist)\n</code></pre> <p>Evaluate the pointwise predictive distribution.</p> <p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>Union[MultivariateNormal, GaussianDistribution]</code>)           \u2013            <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Poisson</code>           \u2013            <p>npd.Poisson: The pointwise predictive distribution.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit","title":"inv_probit","text":"<pre><code>inv_probit(x)\n</code></pre> <p>Compute the inverse probit function.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '*N']</code>)           \u2013            <p>A vector of values.</p> </li> </ul>"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit--returns","title":"Returns","text":"<pre><code>Float[Array, \"*N\"]: The inverse probit of the input vector.\n</code></pre>"},{"location":"api/lower_cholesky/","title":"Lower Cholesky","text":""},{"location":"api/lower_cholesky/#gpjax.lower_cholesky.lower_cholesky","title":"lower_cholesky","text":"<pre><code>lower_cholesky(A)\n</code></pre> <p>Returns the lower Cholesky factor of a linear operator.</p> <p>Parameters:</p> <ul> <li> <code>A</code>               (<code>LinearOperator</code>)           \u2013            <p>The input linear operator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Triangular</code> (              <code>Triangular</code> )          \u2013            <p>The lower Cholesky factor of A.</p> </li> </ul>"},{"location":"api/mean_functions/","title":"Mean Functions","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction","title":"AbstractMeanFunction","text":"<pre><code>AbstractMeanFunction()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Mean function that is used to parameterise the Gaussian process.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(x)\n</code></pre> <p>Evaluate the mean function at the given points. This method is required for all subclasses.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The point at which to evaluate the mean function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N O']</code>           \u2013            <p>Float[Array, \"1]: The evaluated mean function.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The sum of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to multiply.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The product of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant","title":"Constant","text":"<pre><code>Constant(constant=0.0)\n</code></pre> <p>               Bases: <code>AbstractMeanFunction</code></p> <p>Constant mean function.</p> <p>A constant mean function. This function returns a repeated scalar value for all inputs.  The scalar value itself can be treated as a model hyperparameter and learned during training but defaults to 1.0.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The sum of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to multiply.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The product of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Evaluate the mean function at the given points.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The point at which to evaluate the mean function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N O']</code>           \u2013            <p>Float[Array, \"1\"]: The evaluated mean function.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero","title":"Zero","text":"<pre><code>Zero()\n</code></pre> <p>               Bases: <code>Constant</code></p> <p>Zero mean function.</p> <p>The zero mean function. This function returns a zero scalar value for all inputs. Unlike the Constant mean function, the constant scalar zero is fixed, and cannot be treated as a model hyperparameter and learned during training.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Evaluate the mean function at the given points.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The point at which to evaluate the mean function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N O']</code>           \u2013            <p>Float[Array, \"1\"]: The evaluated mean function.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The sum of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to multiply.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The product of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction","title":"CombinationMeanFunction","text":"<pre><code>CombinationMeanFunction(means, operator, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractMeanFunction</code></p> <p>A base class for products or sums of AbstractMeanFunctions.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The sum of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two mean functions.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractMeanFunction</code>)           \u2013            <p>The other mean function to multiply.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractMeanFunction</code> (              <code>AbstractMeanFunction</code> )          \u2013            <p>The product of the two mean functions.</p> </li> </ul>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Evaluate combination kernel on a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The point at which to evaluate the mean function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N O']</code>           \u2013            <p>Float[Array, \" Q\"]: The evaluated mean function.</p> </li> </ul>"},{"location":"api/numpyro_extras/","title":"Numpyro Extras","text":""},{"location":"api/numpyro_extras/#gpjax.numpyro_extras.FillTriangularTransform","title":"FillTriangularTransform","text":"<p>               Bases: <code>Transform</code></p> <p>Transform that maps a vector of length n(n+1)/2 to an n x n lower triangular matrix. The ordering is assumed to be:    (0,0), (1,0), (1,1), (2,0), (2,1), (2,2), ..., (n-1, n-1)</p>"},{"location":"api/numpyro_extras/#gpjax.numpyro_extras.FillTriangularTransform.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward transformation.</p>"},{"location":"api/numpyro_extras/#gpjax.numpyro_extras.FillTriangularTransform.__call__--parameters","title":"Parameters","text":"<p>x : array_like, shape (..., L)     Input vector with L = n(n+1)/2 for some integer n.</p>"},{"location":"api/numpyro_extras/#gpjax.numpyro_extras.FillTriangularTransform.__call__--returns","title":"Returns","text":"<p>y : array_like, shape (..., n, n)     Lower-triangular matrix (with zeros in the upper triangle) filled in     row-major order (i.e. [ (0,0), (1,0), (1,1), ... ]).</p>"},{"location":"api/objectives/","title":"Objectives","text":""},{"location":"api/objectives/#gpjax.objectives.conjugate_mll","title":"conjugate_mll","text":"<pre><code>conjugate_mll(posterior, data)\n</code></pre> <p>Evaluate the marginal log-likelihood of the Gaussian process.</p> <p>Compute the marginal log-likelihood function of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here enables exact estimation of the Gaussian process' latent function values.</p> <p>For a training dataset \\(\\{x_n, y_n\\}_{n=1}^N\\), set of test inputs \\(\\mathbf{x}^{\\star}\\) the corresponding latent function evaluations are given by \\(\\mathbf{f}=f(\\mathbf{x})\\) and \\(\\mathbf{f}^{\\star}f(\\mathbf{x}^{\\star})\\), the marginal log-likelihood is given by:</p> <p>log\u2061p(y)=\u222bp(y\u2223f)p(f,f\u22c6)df\u22c6=0.5(\u2212y\u22a4(k(x,x\u2032)+\u03c32IN)\u22121y\u2212log\u2061\u2223k(x,x\u2032)+\u03c32IN\u2223\u2212nlog\u20612\u03c0). \\begin{align}     \\log p(\\mathbf{y}) &amp; = \\int p(\\mathbf{y}\\mid\\mathbf{f})     p(\\mathbf{f}, \\mathbf{f}^{\\star})\\mathrm{d}\\mathbf{f}^{\\star}\\\\     &amp; = 0.5\\left(-\\mathbf{y}^{\\top}\\left(k(\\mathbf{x}, \\mathbf{x}')     + \\sigma^2\\mathbf{I}_N\\right)^{-1}\\mathbf{y} \\right.\\\\     &amp; \\quad\\left. -\\log\\lvert k(\\mathbf{x}, \\mathbf{x}')     + \\sigma^2\\mathbf{I}_N\\rvert - n\\log 2\\pi \\right). \\end{align} logp(y)\u200b=\u222bp(y\u2223f)p(f,f\u22c6)df\u22c6=0.5(\u2212y\u22a4(k(x,x\u2032)+\u03c32IN\u200b)\u22121y\u2212log\u2223k(x,x\u2032)+\u03c32IN\u200b\u2223\u2212nlog2\u03c0).\u200b\u200b</p> Example <p>import gpjax as gpx</p> <p>xtrain = jnp.linspace(0, 1).reshape(-1, 1) ytrain = jnp.sin(xtrain) D = gpx.Dataset(X=xtrain, y=ytrain)</p> <p>meanf = gpx.mean_functions.Constant() kernel = gpx.kernels.RBF() likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n) prior = gpx.gps.Prior(mean_function = meanf, kernel=kernel) posterior = prior * likelihood</p> <p>gpx.objectives.conjugate_mll(posterior, D)</p> <p>Our goal is to maximise the marginal log-likelihood. Therefore, when optimising the model's parameters with respect to the parameters, we use the negative marginal log-likelihood. This can be realised through</p> <p>nmll = lambda p, d: -gpx.objectives.conjugate_mll(p, d)</p> <p>Parameters:</p> <ul> <li> <code>posterior</code>               (<code>ConjugatePosterior</code>)           \u2013            <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> </li> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>: The training dataset used to compute the marginal log-likelihood.</p> </li> </ul>"},{"location":"api/objectives/#gpjax.objectives.conjugate_mll--returns","title":"Returns","text":"<pre><code>ScalarFloat: The marginal log-likelihood of the Gaussian process.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.conjugate_loocv","title":"conjugate_loocv","text":"<pre><code>conjugate_loocv(posterior, data)\n</code></pre> <p>Evaluate the leave-one-out log predictive probability of the Gaussian process following section 5.4.2 of Rasmussen et al. 2006 - Gaussian Processes for Machine Learning. This metric calculates the average performance of all models that can be obtained by training on all but one data point, and then predicting the left out data point.</p> <p>The returned metric can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here enables exact estimation of the Gaussian process' latent function values.</p> <p>For a given <code>ConjugatePosterior</code> object, the following code snippet shows how the leave-one-out log predicitive probability can be evaluated.</p> Example <p>import gpjax as gpx ... xtrain = jnp.linspace(0, 1).reshape(-1, 1) ytrain = jnp.sin(xtrain) D = gpx.Dataset(X=xtrain, y=ytrain) ... meanf = gpx.mean_functions.Constant() kernel = gpx.kernels.RBF() likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n) prior = gpx.gps.Prior(mean_function = meanf, kernel=kernel) posterior = prior * likelihood ... gpx.objectives.conjugate_loocv(posterior, D)</p> <p>Our goal is to maximise the leave-one-out log predictive probability. Therefore, when optimising the model's parameters with respect to the parameters, we use the negative leave-one-out log predictive probability. This can be realised through</p> <p>nloocv = lambda p, d: -gpx.objectives.conjugate_loocv(p, d)</p> <p>Parameters:</p> <ul> <li> <code>posterior</code>               (<code>ConjugatePosterior</code>)           \u2013            <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> </li> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>: The training dataset used to compute the marginal log-likelihood.</p> </li> </ul>"},{"location":"api/objectives/#gpjax.objectives.conjugate_loocv--returns","title":"Returns","text":"<pre><code>ScalarFloat: The marginal log-likelihood of the Gaussian process.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.log_posterior_density","title":"log_posterior_density","text":"<pre><code>log_posterior_density(posterior, data)\n</code></pre> <p>The log-posterior density of a non-conjugate Gaussian process. This is sometimes referred to as the marginal log-likelihood.</p> <p>Evaluate the log-posterior density of a Gaussian process.</p> <p>Compute the marginal log-likelihood, or log-posterior density of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here is general and will work for any likelihood support by GPJax.</p> <p>Unlike the marginal_log_likelihood function of the <code>ConjugatePosterior</code> object, the marginal_log_likelihood function of the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p> <p>Parameters:</p> <ul> <li> <code>posterior</code>               (<code>NonConjugatePosterior</code>)           \u2013            <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> </li> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>The training dataset used to compute the marginal log-likelihood.</p> </li> </ul>"},{"location":"api/objectives/#gpjax.objectives.log_posterior_density--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log-posterior density of the Gaussian process.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.elbo","title":"elbo","text":"<pre><code>elbo(variational_family, data)\n</code></pre> <p>Compute the evidence lower bound of a variational approximation.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> <ul> <li> <code>variational_family</code>               (<code>VF</code>)           \u2013            <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> </li> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>The training data for which we should maximise the ELBO with respect to.</p> </li> </ul>"},{"location":"api/objectives/#gpjax.objectives.elbo--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.variational_expectation","title":"variational_expectation","text":"<pre><code>variational_expectation(variational_family, data)\n</code></pre> <p>Compute the variational expectation.</p> <p>Compute the expectation of our model's log-likelihood under our variational distribution. Batching can be done here to speed up computation.</p> <p>Parameters:</p> <ul> <li> <code>variational_family</code>               (<code>VF</code>)           \u2013            <p>The variational family that we are using to approximate the posterior.</p> </li> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>The batch for which the expectation should be computed for.</p> </li> </ul>"},{"location":"api/objectives/#gpjax.objectives.variational_expectation--returns","title":"Returns","text":"<pre><code>Array: The expectation of the model's log-likelihood under our variational\n    distribution.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.collapsed_elbo","title":"collapsed_elbo","text":"<pre><code>collapsed_elbo(variational_family, data)\n</code></pre> <p>Compute a single step of the collapsed evidence lower bound.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> <ul> <li> <code>variational_family</code>               (<code>VF</code>)           \u2013            <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> </li> <li> <code>data</code>               (<code>Dataset</code>)           \u2013            <p>The training data for which we should maximise the ELBO with respect to.</p> </li> </ul>"},{"location":"api/objectives/#gpjax.objectives.collapsed_elbo--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation.\n</code></pre>"},{"location":"api/parameters/","title":"Parameters","text":""},{"location":"api/parameters/#gpjax.parameters.Parameter","title":"Parameter","text":"<pre><code>Parameter(value, tag, **kwargs)\n</code></pre> <p>               Bases: <code>Variable[T]</code></p> <p>Parameter base class.</p> <p>All trainable parameters in GPJax should inherit from this class.</p>"},{"location":"api/parameters/#gpjax.parameters.NonNegativeReal","title":"NonNegativeReal","text":"<pre><code>NonNegativeReal(value, tag='non_negative', **kwargs)\n</code></pre> <p>               Bases: <code>Parameter[T]</code></p> <p>Parameter that is non-negative.</p>"},{"location":"api/parameters/#gpjax.parameters.PositiveReal","title":"PositiveReal","text":"<pre><code>PositiveReal(value, tag='positive', **kwargs)\n</code></pre> <p>               Bases: <code>Parameter[T]</code></p> <p>Parameter that is strictly positive.</p>"},{"location":"api/parameters/#gpjax.parameters.Real","title":"Real","text":"<pre><code>Real(value, tag='real', **kwargs)\n</code></pre> <p>               Bases: <code>Parameter[T]</code></p> <p>Parameter that can take any real value.</p>"},{"location":"api/parameters/#gpjax.parameters.SigmoidBounded","title":"SigmoidBounded","text":"<pre><code>SigmoidBounded(value, tag='sigmoid', **kwargs)\n</code></pre> <p>               Bases: <code>Parameter[T]</code></p> <p>Parameter that is bounded between 0 and 1.</p>"},{"location":"api/parameters/#gpjax.parameters.Static","title":"Static","text":"<pre><code>Static(value, tag='static', **kwargs)\n</code></pre> <p>               Bases: <code>Variable[T]</code></p> <p>Static parameter that is not trainable.</p>"},{"location":"api/parameters/#gpjax.parameters.LowerTriangular","title":"LowerTriangular","text":"<pre><code>LowerTriangular(value, tag='lower_triangular', **kwargs)\n</code></pre> <p>               Bases: <code>Parameter[T]</code></p> <p>Parameter that is a lower triangular matrix.</p>"},{"location":"api/parameters/#gpjax.parameters.transform","title":"transform","text":"<pre><code>transform(params, params_bijection, inverse=False)\n</code></pre> <p>Transforms parameters using a bijector.</p> <p>Example: <pre><code>    &gt;&gt;&gt; from gpjax.parameters import PositiveReal, transform\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; import numpyro.distributions.transforms as npt\n    &gt;&gt;&gt; from flax import nnx\n    &gt;&gt;&gt; params = nnx.State(\n    &gt;&gt;&gt;     {\n    &gt;&gt;&gt;         \"a\": PositiveReal(jnp.array([1.0])),\n    &gt;&gt;&gt;         \"b\": PositiveReal(jnp.array([2.0])),\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; params_bijection = {'positive': npt.SoftplusTransform()}\n    &gt;&gt;&gt; transformed_params = transform(params, params_bijection)\n    &gt;&gt;&gt; print(transformed_params[\"a\"].value)\n     [1.3132617]\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>params</code>               (<code>State</code>)           \u2013            <p>A nnx.State object containing parameters to be transformed.</p> </li> <li> <code>params_bijection</code>               (<code>Dict[str, Transform]</code>)           \u2013            <p>A dictionary mapping parameter types to bijectors.</p> </li> <li> <code>inverse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to apply the inverse transformation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>State</code> (              <code>State</code> )          \u2013            <p>A new nnx.State object containing the transformed parameters.</p> </li> </ul>"},{"location":"api/scan/","title":"Scan","text":""},{"location":"api/scan/#gpjax.scan.vscan","title":"vscan","text":"<pre><code>vscan(f, init, xs, length=None, reverse=False, unroll=1, log_rate=10, log_value=True)\n</code></pre> <p>Scan with verbose output.</p> <p>This is based on code from this excellent blog post.</p> Example <p>import jax.numpy as jnp ... def f(carry, x): ...     return carry + x, carry + x init = 0 xs = jnp.arange(10) vscan(f, init, xs) (Array(45, dtype=int32), Array([ 0,  1,  3,  6, 10, 15, 21, 28, 36, 45], dtype=int32))</p> <p>Parameters:</p> <ul> <li> <code>f</code>               (<code>Callable[[Carry, X], Tuple[Carry, Y]]</code>)           \u2013            <p>A function that takes in a carry and an input and returns a tuple of a new carry and an output.</p> </li> <li> <code>init</code>               (<code>Carry</code>)           \u2013            <p>The initial carry.</p> </li> <li> <code>xs</code>               (<code>X</code>)           \u2013            <p>The inputs.</p> </li> <li> <code>length</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The length of the inputs. If None, then the length of the inputs is inferred.</p> </li> <li> <code>reverse</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to scan in reverse.</p> </li> <li> <code>unroll</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of iterations to unroll.</p> </li> <li> <code>log_rate</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The rate at which to log the progress bar.</p> </li> <li> <code>log_value</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the value of the objective function.</p> </li> </ul>"},{"location":"api/scan/#gpjax.scan.vscan--returns","title":"Returns","text":"<pre><code>Tuple[Carry, list[Y]]: A tuple of the final carry and the outputs.\n</code></pre>"},{"location":"api/typing/","title":"Typing","text":""},{"location":"api/typing/#gpjax.typing.FunctionalSample","title":"FunctionalSample  <code>module-attribute</code>","text":"<pre><code>FunctionalSample = Callable[[Float[Array, 'N D']], Float[Array, 'N B']]\n</code></pre> <p>Type alias for functions representing \\(B\\) samples from a model, to be evaluated on any set of \\(N\\) inputs (of dimension \\(D\\)) and returning the evaluations of each (potentially approximate) sample draw across these inputs.</p>"},{"location":"api/variational_families/","title":"Variational Families","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily","title":"AbstractVariationalFamily","text":"<pre><code>AbstractVariationalFamily(posterior)\n</code></pre> <p>               Bases: <code>Module</code>, <code>Generic[L]</code></p> <p>Abstract base class used to represent families of distributions that can be used within variational inference.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs)\n</code></pre> <p>Predict the GP's output given the input.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian","title":"AbstractVariationalGaussian","text":"<pre><code>AbstractVariationalGaussian(posterior, inducing_inputs, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractVariationalFamily[L]</code></p> <p>The variational Gaussian family of probability distributions.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.num_inducing","title":"num_inducing  <code>property</code>","text":"<pre><code>num_inducing\n</code></pre> <p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(*args, **kwargs)\n</code></pre> <p>Predict the GP's output given the input.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian","title":"VariationalGaussian","text":"<pre><code>VariationalGaussian(posterior, inducing_inputs, variational_mean=None, variational_root_covariance=None, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractVariationalGaussian[L]</code></p> <p>The variational Gaussian family of probability distributions.</p> <p>The variational family is \\(q(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}u\\), where \\(u = f(z)\\) are the function values at the inducing inputs \\(z\\) and the distribution over the inducing inputs is \\(q(u) = \\mathcal{N}(\\mu, S)\\).  We parameterise this over \\(\\mu\\) and \\(sqrt\\) with \\(S = sqrt sqrt^{\\top}\\).</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.num_inducing","title":"num_inducing  <code>property</code>","text":"<pre><code>num_inducing\n</code></pre> <p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl()\n</code></pre> <p>Compute the prior KL divergence.</p> <p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(\u03bcz,Kzz)], \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)]\\\\ &amp; = \\operatorname{KL}[ \\mathcal{N}(\\mu, S) \\mid\\mid N(\\mu z, \\mathbf{K}_{zz}) ], \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(\u03bcz,Kzz\u200b)],\u200b\u200b</p> where \\(u = f(z)\\) and \\(z\\) are the inducing inputs.</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>ScalarFloat</code> )          \u2013            <p>The KL-divergence between our variational approximation and the GP prior.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict","title":"predict","text":"<pre><code>predict(test_inputs)\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral \\(q(f(t)) = \\int p(f(t)\\mid u) q(u) \\mathrm{d}u\\), which can be computed in closed form as: <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt).     \\mathcal{N}\\left(f(t); \\mu t + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} (\\mu - \\mu z),  \\mathbf{K}_{tt} - \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} S \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt}\\right). N(f(t);\u03bct+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b).</p></p> <p>Parameters:</p> <ul> <li> <code>test_inputs</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The test inputs at which we wish to make a prediction.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the low-rank GP at the test inputs.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian","title":"WhitenedVariationalGaussian","text":"<pre><code>WhitenedVariationalGaussian(posterior, inducing_inputs, variational_mean=None, variational_root_covariance=None, jitter=1e-06)\n</code></pre> <p>               Bases: <code>VariationalGaussian[L]</code></p> <p>The whitened variational Gaussian family of probability distributions.</p> <p>The variational family is \\(q(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}u\\), where \\(u = f(z)\\) are the function values at the inducing inputs \\(z\\) and the distribution over the inducing inputs is \\(q(u) = \\mathcal{N}(Lz \\mu + mz, Lz S Lz^{\\top})\\). We parameterise this over \\(\\mu\\) and \\(sqrt\\) with \\(S = sqrt sqrt^{\\top}\\).</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.num_inducing","title":"num_inducing  <code>property</code>","text":"<pre><code>num_inducing\n</code></pre> <p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl()\n</code></pre> <p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(0,I)]. \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)]\\\\     &amp; = \\operatorname{KL}[N(\\mu  , S)\\mid\\mid N(0, I)]. \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(0,I)].\u200b\u200b</p></p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>ScalarFloat</code> )          \u2013            <p>The KL-divergence between our variational approximation and the GP prior.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict","title":"predict","text":"<pre><code>predict(test_inputs)\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t)) = \\int p(f(t)\\midu) q(u) du, which can be computed in closed form as <p>N(f(t);\u03bct+KtzLz\u22a4\u03bc,Ktt\u2212KtzKzz\u22121Kzt+KtzLz\u22a4SLz\u22121Kzt).     \\mathcal{N}\\left(f(t); \\mu t  +  \\mathbf{K}_{tz} \\mathbf{L}z^{\\top} \\mu  ,  \\mathbf{K}_{tt}  -  \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt}  +  \\mathbf{K}_{tz} \\mathbf{L}z^{\\top} S \\mathbf{L}z^{-1} \\mathbf{K}_{zt} \\right). N(f(t);\u03bct+Ktz\u200bLz\u22a4\u03bc,Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bLz\u22a4SLz\u22121Kzt\u200b).</p></p> <p>Parameters:</p> <ul> <li> <code>test_inputs</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The test inputs at which we wish to make a prediction.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the low-rank GP at the test inputs.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian","title":"NaturalVariationalGaussian","text":"<pre><code>NaturalVariationalGaussian(posterior, inducing_inputs, natural_vector=None, natural_matrix=None, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractVariationalGaussian[L]</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is \\(q(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}u\\), where \\(u = f(z)\\) are the function values at the inducing inputs \\(z\\) and the distribution over the inducing inputs is \\(q(u) = N(\\mu, S)\\). Expressing the variational distribution, in the form of the exponential family, \\(q(u) = exp(\\theta^{\\top} T(u) - a(\\theta))\\), gives rise to the natural parameterisation \\(\\theta  = (\\theta_{1}, \\theta_{2}) = (S^{-1}\\mu, -S^{-1}/2)\\), to perform model inference, where \\(T(u) = [u, uu^{\\top}]\\) are the sufficient statistics.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.num_inducing","title":"num_inducing  <code>property</code>","text":"<pre><code>num_inducing\n</code></pre> <p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl()\n</code></pre> <p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(mz,Kzz)], \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)] \\\\     &amp; = \\operatorname{KL}[N(\\mu, S)\\mid\\mid N(mz, \\mathbf{K}_{zz})], \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(mz,Kzz\u200b)],\u200b\u200b</p> with \\(\\mu\\) and \\(S\\) computed from the natural parameterisation \\(\\theta  = (S^{-1}\\mu  , -S^{-1}/2)\\).</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>ScalarFloat</code> )          \u2013            <p>The KL-divergence between our variational approximation and the GP prior.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict","title":"predict","text":"<pre><code>predict(test_inputs)\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs \\(t\\).</p> <p>This is the integral \\(q(f(t)) = \\int p(f(t)\\mid u) q(u) \\mathrm{d}u\\), which can be computed in closed form as <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt),      \\mathcal{N}\\left(f(t); \\mu  t + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} (\\mu   - \\mu  z),  \\mathbf{K}_{tt} - \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} S \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} \\right), N(f(t);\u03bct+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b),</p> with \\(\\mu\\) and \\(S\\) computed from the natural parameterisation \\(\\theta = (S^{-1}\\mu  , -S^{-1}/2)\\).</p> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>A function that accepts a set of test points and will return the predictive distribution at those points.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian","title":"ExpectationVariationalGaussian","text":"<pre><code>ExpectationVariationalGaussian(posterior, inducing_inputs, expectation_vector=None, expectation_matrix=None, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractVariationalGaussian[L]</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is \\(q(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}u\\), where \\(u = f(z)\\) are the function values at the inducing inputs \\(z\\) and the distribution over the inducing inputs is \\(q(u) = \\mathcal{N}(\\mu, S)\\). Expressing the variational distribution, in the form of the exponential family, \\(q(u) = exp(\\theta^{\\top} T(u) - a(\\theta))\\), gives rise to the natural parameterisation \\(\\theta  = (\\theta_{1}, \\theta_{2}) = (S^{-1}\\mu  , -S^{-1}/2)\\) and sufficient statistics \\(T(u) = [u, uu^{\\top}]\\). The expectation parameters are given by \\(\\nu = \\int T(u) q(u) \\mathrm{d}u\\). This gives a parameterisation, \\(\\nu = (\\nu_{1}, \\nu_{2}) = (\\mu  , S + uu^{\\top})\\) to perform model inference over.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.num_inducing","title":"num_inducing  <code>property</code>","text":"<pre><code>num_inducing\n</code></pre> <p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl()\n</code></pre> <p>Evaluate the prior KL-divergence.</p> <p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061(q(f(\u22c5))\u2223\u2223p(\u22c5))=KL\u2061(q(u)\u2223\u2223p(u))=KL\u2061(N(\u03bc,S)\u2223\u2223N(mz,Kzz)), \\begin{align} \\operatorname{KL}(q(f(\\cdot))\\mid\\mid p(\\cdot)) &amp; = \\operatorname{KL}(q(u)\\mid\\mid p(u)) \\\\     &amp; =\\operatorname{KL}(\\mathcal{N}(\\mu, S)\\mid\\mid \\mathcal{N}(m_z, K_{zz})), \\end{align} KL(q(f(\u22c5))\u2223\u2223p(\u22c5))\u200b=KL(q(u)\u2223\u2223p(u))=KL(N(\u03bc,S)\u2223\u2223N(mz\u200b,Kzz\u200b)),\u200b\u200b</p> where \\(\\mu\\) and \\(S\\) are the expectation parameters of the variational distribution and \\(m_z\\) and \\(K_{zz}\\) are the mean and covariance of the prior distribution.</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>ScalarFloat</code> )          \u2013            <p>The KL-divergence between our variational approximation and the GP prior.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict","title":"predict","text":"<pre><code>predict(test_inputs)\n</code></pre> <p>Evaluate the predictive distribution.</p> <p>Compute the predictive distribution of the GP at the test inputs \\(t\\).</p> <p>This is the integral \\(q(f(t)) = \\int p(f(t)\\mid u)q(u)\\mathrm{d}u\\), which can be computed in closed form as  which can be computed in closed form as <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt) \\mathcal{N}(f(t); \\mu_t + \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}(\\mu - \\mu_z), \\mathbf{K}_{tt} - \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}\\mathbf{K}_{zt} + \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}\\mathbf{S} \\mathbf{K}_{zz}^{-1}\\mathbf{K}_{zt}) N(f(t);\u03bct\u200b+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz\u200b),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b)</p></p> <p>with \\(\\mu\\) and \\(S\\) computed from the expectation parameterisation \\(\\eta = (\\mu, S + uu^\\top)\\).</p> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the GP at the test inputs \\(t\\).</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian","title":"CollapsedVariationalGaussian","text":"<pre><code>CollapsedVariationalGaussian(posterior, inducing_inputs, jitter=1e-06)\n</code></pre> <p>               Bases: <code>AbstractVariationalGaussian[GL]</code></p> <p>Collapsed variational Gaussian.</p> <p>Collapsed variational Gaussian family of probability distributions. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.num_inducing","title":"num_inducing  <code>property</code>","text":"<pre><code>num_inducing\n</code></pre> <p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Arguments of the variational family's <code>predict</code> method.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments of the variational family's <code>predict</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The output of the variational family's <code>predict</code> method.</p> </li> </ul>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict","title":"predict","text":"<pre><code>predict(test_inputs, train_data)\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> <ul> <li> <code>test_inputs</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The test inputs \\(t\\) at which to make predictions.</p> </li> <li> <code>train_data</code>               (<code>Dataset</code>)           \u2013            <p>The training data that was used to fit the GP.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the collapsed variational Gaussian process at the test inputs \\(t\\).</p> </li> </ul>"},{"location":"api/kernels/base/","title":"Base","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel","title":"AbstractKernel","text":"<pre><code>AbstractKernel(active_dims=None, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base kernel class.</p> <p>This class is the base class for all kernels in GPJax. It provides the basic functionality for evaluating a kernel function on a pair of inputs, as well as the ability to combine kernels using addition and multiplication.</p> <p>The class also provides a method for slicing the input matrix to select the relevant columns for the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>the indices of the input dimensions that are active in the kernel's evaluation, represented by a list of integers or a slice object. Defaults to a full slice.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>the number of input dimensions of the kernel.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>the computation engine that is used to compute the kernel's cross-covariance and gram matrices. Defaults to DenseKernelComputation.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(x, y)\n</code></pre> <p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, ' D']</code>)           \u2013            <p>the left hand input of the kernel function.</p> </li> <li> <code>y</code>               (<code>Num[Array, ' D']</code>)           \u2013            <p>The right hand input of the kernel function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>The evaluated kernel function at the supplied inputs.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant","title":"Constant","text":"<pre><code>Constant(active_dims=None, constant=jnp.array(0.0), compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>A constant kernel. This kernel evaluates to a constant for all inputs. The scalar value itself can be treated as a model hyperparameter and learned during training.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__","title":"__call__","text":"<pre><code>__call__(x, y)\n</code></pre> <p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The left hand input of the kernel function.</p> </li> <li> <code>y</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The right hand input of the kernel function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>ScalarFloat</code> )          \u2013            <p>The evaluated kernel function at the supplied inputs.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel","title":"CombinationKernel","text":"<pre><code>CombinationKernel(kernels, operator, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>A base class for products or sums of MeanFunctions.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__","title":"__call__","text":"<pre><code>__call__(x, y)\n</code></pre> <p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The left hand input of the kernel function.</p> </li> <li> <code>y</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>The right hand input of the kernel function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code> (              <code>ScalarFloat</code> )          \u2013            <p>The evaluated kernel function at the supplied inputs.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/","title":"RFF","text":"<p>Compute Random Fourier Feature (RFF) kernel approximations.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF","title":"RFF","text":"<pre><code>RFF(base_kernel, num_basis_fns=50, frequencies=None, compute_engine=BasisFunctionComputation(), key=jr.PRNGKey(0))\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>Computes an approximation of the kernel using Random Fourier Features.</p> <p>All stationary kernels are equivalent to the Fourier transform of a probability distribution. We call the corresponding distribution the spectral density. Using a finite number of basis functions, we can compute the spectral density using a Monte-Carlo approximation. This is done by sampling from the spectral density and computing the Fourier transform of the samples. The kernel is then approximated by the inner product of the Fourier transform of the samples with the Fourier transform of the data.</p> <p>The key reference for this implementation is the following papers: - 'Random Features for Large-Scale Kernel Machines' by Rahimi and Recht (2008). - 'On the Error of Random Fourier Features' by Sutherland and Schneider (2015).</p> <p>Parameters:</p> <ul> <li> <code>base_kernel</code>               (<code>StationaryKernel</code>)           \u2013            <p>The base kernel to be approximated.</p> </li> <li> <code>num_basis_fns</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>The number of basis functions to use in the approximation.</p> </li> <li> <code>frequencies</code>               (<code>Float[Array, 'M D'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The frequencies to use in the approximation. If None, the frequencies are sampled from the spectral density of the base kernel.</p> </li> <li> <code>compute_engine</code>               (<code>BasisFunctionComputation</code>, default:                   <code>BasisFunctionComputation()</code> )           \u2013            <p>The computation engine to use for the basis function computation.</p> </li> <li> <code>key</code>               (<code>KeyArray</code>, default:                   <code>PRNGKey(0)</code> )           \u2013            <p>The random key to use for sampling the frequencies.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__call__","title":"__call__","text":"<pre><code>__call__(x, y)\n</code></pre> <p>Superfluous for RFFs.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features","title":"compute_features","text":"<pre><code>compute_features(x)\n</code></pre> <p>Compute the features for the inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>A \\(N \\times D\\) array of inputs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N L']</code>           \u2013            <p>Float[Array, \"N L\"]: A \\(N \\times L\\) array of features where \\(L = 2M\\).</p> </li> </ul>"},{"location":"api/kernels/computations/base/","title":"Base","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation","title":"AbstractKernelComputation","text":"<p>Abstract class for kernel computations.</p> <p>This class defines the interface for computing the covariance matrix of a kernel function. It is used to compute the Gram matrix, cross-covariance, and diagonal variance of a kernel function. Each computation engine implements the computation of these quantities in a different way. Subclasses implement computations as private methods. If a non-standard interface is required, the subclass should override the public methods of this class.</p>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram","title":"gram","text":"<pre><code>gram(kernel, x)\n</code></pre> <p>For a given kernel, compute Gram covariance operator of the kernel function on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the inputs to the kernel function of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dense</code>           \u2013            <p>The Gram covariance of the kernel function as a linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(kernel, x, y)\n</code></pre> <p>For a given kernel, compute the cross-covariance matrix on an a pair of input matrices with shape <code>(N, D)</code> and <code>(M, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The computed cross-covariance of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal","title":"diagonal","text":"<pre><code>diagonal(kernel, inputs)\n</code></pre> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>inputs</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Diagonal</code>           \u2013            <p>The computed diagonal variance as a <code>Diagonal</code> linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/basis_functions/","title":"Basis Functions","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation","title":"BasisFunctionComputation","text":"<p>               Bases: <code>AbstractKernelComputation</code></p> <p>Compute engine class for finite basis function approximations to a kernel.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.gram","title":"gram","text":"<pre><code>gram(kernel, x)\n</code></pre> <p>For a given kernel, compute Gram covariance operator of the kernel function on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the inputs to the kernel function of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dense</code>           \u2013            <p>The Gram covariance of the kernel function as a linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(kernel, x, y)\n</code></pre> <p>For a given kernel, compute the cross-covariance matrix on an a pair of input matrices with shape <code>(N, D)</code> and <code>(M, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The computed cross-covariance of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.diagonal","title":"diagonal","text":"<pre><code>diagonal(kernel, inputs)\n</code></pre> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>AbstractKernel</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>inputs</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>The input matrix.</p> </li> </ul>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features","title":"compute_features","text":"<pre><code>compute_features(kernel, x)\n</code></pre> <p>Compute the features for the inputs.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Float[Array, 'N D']</code>)           \u2013            <p>the inputs to the kernel function of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N L']</code>           \u2013            <p>A matrix of shape \\(N \\times L\\) representing the random fourier features where \\(L = 2M\\).</p> </li> </ul>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.scaling","title":"scaling","text":"<pre><code>scaling(kernel)\n</code></pre> <p>Compute the scaling factor for the covariance matrix.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '']</code>           \u2013            <p>A scalar array representing the scaling factor.</p> </li> </ul>"},{"location":"api/kernels/computations/constant_diagonal/","title":"Constant Diagonal","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation","title":"ConstantDiagonalKernelComputation","text":"<p>               Bases: <code>AbstractKernelComputation</code></p> <p>Computation engine for constant diagonal kernels.</p>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(kernel, x, y)\n</code></pre> <p>For a given kernel, compute the cross-covariance matrix on an a pair of input matrices with shape <code>(N, D)</code> and <code>(M, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The computed cross-covariance of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal","title":"diagonal","text":"<pre><code>diagonal(kernel, inputs)\n</code></pre> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>inputs</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Diagonal</code>           \u2013            <p>The computed diagonal variance as a <code>Diagonal</code> linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/dense/","title":"Dense","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation","title":"DenseKernelComputation","text":"<p>               Bases: <code>AbstractKernelComputation</code></p> <p>Dense kernel computation class. Operations with the kernel assume a dense gram matrix structure.</p>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.gram","title":"gram","text":"<pre><code>gram(kernel, x)\n</code></pre> <p>For a given kernel, compute Gram covariance operator of the kernel function on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the inputs to the kernel function of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dense</code>           \u2013            <p>The Gram covariance of the kernel function as a linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(kernel, x, y)\n</code></pre> <p>For a given kernel, compute the cross-covariance matrix on an a pair of input matrices with shape <code>(N, D)</code> and <code>(M, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The computed cross-covariance of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.diagonal","title":"diagonal","text":"<pre><code>diagonal(kernel, inputs)\n</code></pre> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>inputs</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Diagonal</code>           \u2013            <p>The computed diagonal variance as a <code>Diagonal</code> linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/diagonal/","title":"Diagonal","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation","title":"DiagonalKernelComputation","text":"<p>               Bases: <code>AbstractKernelComputation</code></p> <p>Diagonal kernel computation class. Operations with the kernel assume a diagonal Gram matrix.</p>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(kernel, x, y)\n</code></pre> <p>For a given kernel, compute the cross-covariance matrix on an a pair of input matrices with shape <code>(N, D)</code> and <code>(M, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The computed cross-covariance of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.diagonal","title":"diagonal","text":"<pre><code>diagonal(kernel, inputs)\n</code></pre> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>inputs</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Diagonal</code>           \u2013            <p>The computed diagonal variance as a <code>Diagonal</code> linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/eigen/","title":"Eigen","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation","title":"EigenKernelComputation","text":"<p>               Bases: <code>AbstractKernelComputation</code></p> <p>Eigen kernel computation class. Kernels who operate on an eigen-decomposed structure should use this computation object.</p>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.gram","title":"gram","text":"<pre><code>gram(kernel, x)\n</code></pre> <p>For a given kernel, compute Gram covariance operator of the kernel function on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the inputs to the kernel function of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dense</code>           \u2013            <p>The Gram covariance of the kernel function as a linear operator.</p> </li> </ul>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(kernel, x, y)\n</code></pre> <p>For a given kernel, compute the cross-covariance matrix on an a pair of input matrices with shape <code>(N, D)</code> and <code>(M, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The computed cross-covariance of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.diagonal","title":"diagonal","text":"<pre><code>diagonal(kernel, inputs)\n</code></pre> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape <code>(N, D)</code>.</p> <p>Parameters:</p> <ul> <li> <code>kernel</code>               (<code>K</code>)           \u2013            <p>the kernel function.</p> </li> <li> <code>inputs</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Diagonal</code>           \u2013            <p>The computed diagonal variance as a <code>Diagonal</code> linear operator.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/","title":"Graph","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel","title":"GraphKernel","text":"<pre><code>GraphKernel(laplacian, active_dims=None, lengthscale=1.0, variance=1.0, smoothness=1.0, n_dims=None, compute_engine=EigenKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The Mat\u00e9rn graph kernel defined on the vertex set of a graph.</p> <p>A Mat\u00e9rn graph kernel defined on the vertices of a graph.</p> <p>Computes the covariance for pairs of vertices \\((v_i, v_j)\\) with variance \\(\\sigma^2\\): $$ k(v_i, v_j) = \\sigma^2 \\exp\\Bigg(-\\frac{\\lVert v_i - v_j \\rVert^2_2}{2\\ell^2}\\Bigg) $$ where \\(\\ell\\) is the lengthscale parameter and \\(\\sigma^2\\) is the variance.</p> <p>The key reference for this object is Borovitskiy et. al., (2020).</p> <p>Parameters:</p> <ul> <li> <code>laplacian</code>               (<code>Num[Array, 'N N']</code>)           \u2013            <p>the Laplacian matrix of the graph.</p> </li> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[ScalarFloat, Float[Array, ' D'], Parameter]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Parameter]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>smoothness</code>               (<code>ScalarFloat</code>, default:                   <code>1.0</code> )           \u2013            <p>the smoothness parameter of the Mat\u00e9rn kernel.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>EigenKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.spectral_density","title":"spectral_density  <code>property</code>","text":"<pre><code>spectral_density\n</code></pre> <p>The spectral density of the kernel.</p> <p>Returns:</p> <ul> <li> <code>Normal | StudentT</code>           \u2013            <p>Callable[[Float[Array, \"D\"]], Float[Array, \"D\"]]: The spectral density function.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/non_euclidean/utils/","title":"Utils","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd","title":"jax_gather_nd","text":"<pre><code>jax_gather_nd(params, indices)\n</code></pre> <p>Slice a <code>params</code> array at a set of <code>indices</code>.</p> <p>This is a reimplementation of TensorFlow's <code>gather_nd</code> function: link</p> <p>Parameters:</p> <ul> <li> <code>params</code>               (<code>Float[Array, ' N *rest']</code>)           \u2013            <p>an arbitrary array with leading axes of length \\(N\\) upon which we shall slice.</p> </li> <li> <code>indices</code>               (<code>Int[Array, ' M 1']</code>)           \u2013            <p>an integer array of length \\(M\\) with values in the range \\([0, N)\\) whose value at index \\(i\\) will be used to slice <code>params</code> at index \\(i\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' M *rest']</code>           \u2013            <p>An arbitrary array with leading axes of length \\(M\\).</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/","title":"Arccosine","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine","title":"ArcCosine","text":"<pre><code>ArcCosine(active_dims=None, order=0, variance=1.0, weight_variance=1.0, bias_variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>The ArCosine kernel.</p> <p>This kernel is non-stationary and resembles the behavior of neural networks. See Section 3.1 of Cho and Saul (2011) for additional details.</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>order</code>               (<code>Literal[0, 1, 2]</code>, default:                   <code>0</code> )           \u2013            <p>The order of the kernel. Must be 0, 1 or 2.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>The variance of the kernel \u03c3.</p> </li> <li> <code>weight_variance</code>               (<code>Union[WeightVarianceCompatible, Variable[WeightVariance]]</code>, default:                   <code>1.0</code> )           \u2013            <p>The weight variance of the kernel.</p> </li> <li> <code>bias_variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>The bias variance of the kernel.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/","title":"Linear","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear","title":"Linear","text":"<pre><code>Linear(active_dims=None, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>The linear kernel.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with variance \\(\\sigma^2\\): $$ k(x, y) = \\sigma^2 x^{\\top}y $$</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/","title":"Polynomial","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial","title":"Polynomial","text":"<pre><code>Polynomial(active_dims=None, degree=2, shift=1.0, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>The Polynomial kernel with variable degree.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with variance \\(\\sigma^2\\): $$ k(x, y) = (\\alpha + \\sigma^2 x y)^d $$ where \\(\\sigma^\\in \\mathbb{R}_{&gt;0}\\) is the kernel's variance parameter, shift parameter \\(\\alpha\\) and integer degree \\(d\\).</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>degree</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>The degree of the polynomial.</p> </li> <li> <code>shift</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>The shift parameter of the kernel.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>The variance of the kernel.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/","title":"Base","text":""},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel","title":"StationaryKernel","text":"<pre><code>StationaryKernel(active_dims=None, lengthscale=1.0, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>AbstractKernel</code></p> <p>Base class for stationary kernels.</p> <p>Stationary kernels are a class of kernels that are invariant to translations in the input space. They can be isotropic or anisotropic, meaning that they can have a single lengthscale for all input dimensions or a different lengthscale for each input dimension.</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.spectral_density","title":"spectral_density  <code>property</code>","text":"<pre><code>spectral_density\n</code></pre> <p>The spectral density of the kernel.</p> <p>Returns:</p> <ul> <li> <code>Normal | StudentT</code>           \u2013            <p>Callable[[Float[Array, \"D\"]], Float[Array, \"D\"]]: The spectral density function.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(x, y)\n</code></pre> <p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, ' D']</code>)           \u2013            <p>the left hand input of the kernel function.</p> </li> <li> <code>y</code>               (<code>Num[Array, ' D']</code>)           \u2013            <p>The right hand input of the kernel function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>The evaluated kernel function at the supplied inputs.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/base/#gpjax.kernels.stationary.base.StationaryKernel.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/","title":"Mat\u00e9rn12","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12","title":"Matern12","text":"<pre><code>Matern12(active_dims=None, lengthscale=1.0, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 0.5.</p> <p>Computes the covariance on a pair of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\).</p> \\[ k(x, y) = \\sigma^2\\exp\\Bigg(-\\frac{\\lvert x-y \\rvert}{2\\ell^2}\\Bigg) \\] <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/","title":"Mat\u00e9rn32","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32","title":"Matern32","text":"<pre><code>Matern32(active_dims=None, lengthscale=1.0, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 1.5.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\).</p> \\[ k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{3}\\lvert x-y \\rvert}{\\ell^2} \\ \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{3}\\lvert x-y\\rvert}{\\ell^2} \\Bigg) \\] <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/","title":"Mat\u00e9rn52","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52","title":"Matern52","text":"<pre><code>Matern52(active_dims=None, lengthscale=1.0, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 2.5.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\).</p> \\[ k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{5}\\lvert x-y \\rvert}{\\ell^2} + \\frac{5\\lvert x - y \\rvert^2}{3\\ell^2} \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{5}\\lvert x-y\\rvert}{\\ell^2} \\Bigg) \\] <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/","title":"Periodic","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic","title":"Periodic","text":"<pre><code>Periodic(active_dims=None, lengthscale=1.0, variance=1.0, period=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The periodic kernel.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with length-scale parameter \\(\\ell\\), variance \\(\\sigma^2\\) and period \\(p\\). $$ k(x, y) = \\sigma^2 \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\left(\\frac{\\sin (\\pi (x_i - y_i)/p)}{\\ell}\\right)^2 \\right) $$ Key reference is MacKay 1998 - \"Introduction to Gaussian processes\".</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>the indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>period</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the period of the kernel p.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>the number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>the computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.spectral_density","title":"spectral_density  <code>property</code>","text":"<pre><code>spectral_density\n</code></pre> <p>The spectral density of the kernel.</p> <p>Returns:</p> <ul> <li> <code>Normal | StudentT</code>           \u2013            <p>Callable[[Float[Array, \"D\"]], Float[Array, \"D\"]]: The spectral density function.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/","title":"Powered Exponential","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential","title":"PoweredExponential","text":"<pre><code>PoweredExponential(active_dims=None, lengthscale=1.0, variance=1.0, power=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The powered exponential family of kernels.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with length-scale parameter \\(\\ell\\), \\(\\sigma\\) and power \\(\\kappa\\). $$ k(x, y)=\\sigma^2\\exp\\Bigg(-\\Big(\\frac{\\lVert x-y\\rVert^2}{\\ell^2}\\Big)^\\kappa\\Bigg) $$</p> <p>This also equivalent to the symmetric generalized normal distribution. See Diggle and Ribeiro (2007) - \"Model-based Geostatistics\". and https://en.wikipedia.org/wiki/Generalized_normal_distribution#Symmetric_version</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>the indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>power</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the power of the kernel \u03ba.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>the number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>the computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.spectral_density","title":"spectral_density  <code>property</code>","text":"<pre><code>spectral_density\n</code></pre> <p>The spectral density of the kernel.</p> <p>Returns:</p> <ul> <li> <code>Normal | StudentT</code>           \u2013            <p>Callable[[Float[Array, \"D\"]], Float[Array, \"D\"]]: The spectral density function.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/","title":"Rational Quadratic","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic","title":"RationalQuadratic","text":"<pre><code>RationalQuadratic(active_dims=None, lengthscale=1.0, variance=1.0, alpha=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The Rational Quadratic kernel.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\). $$ k(x,y)=\\sigma^2\\exp\\Bigg(1+\\frac{\\lVert x-y\\rVert^2_2}{2\\alpha\\ell^2}\\Bigg) $$</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>alpha</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the alpha parameter of the kernel \u03b1.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.spectral_density","title":"spectral_density  <code>property</code>","text":"<pre><code>spectral_density\n</code></pre> <p>The spectral density of the kernel.</p> <p>Returns:</p> <ul> <li> <code>Normal | StudentT</code>           \u2013            <p>Callable[[Float[Array, \"D\"]], Float[Array, \"D\"]]: The spectral density function.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/","title":"RBF","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF","title":"RBF","text":"<pre><code>RBF(active_dims=None, lengthscale=1.0, variance=1.0, n_dims=None, compute_engine=DenseKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The Radial Basis Function (RBF) kernel.</p> <p>Computes the covariance for pair of inputs \\((x, y)\\) with lengthscale parameter \\(\\ell\\) and variance \\(\\sigma^2\\): $$ k(x,y)=\\sigma^2\\exp\\Bigg(- \\frac{\\lVert x - y \\rVert^2_2}{2 \\ell^2} \\Bigg) $$</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>lengthscale</code>               (<code>Union[LengthscaleCompatible, Variable[Lengthscale]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the lengthscale(s) of the kernel \u2113. If a scalar or an array of length 1, the kernel is isotropic, meaning that the same lengthscale is used for all input dimensions. If an array with length &gt; 1, the kernel is anisotropic, meaning that a different lengthscale is used for each input.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions. If <code>lengthscale</code> is an array, this argument is ignored.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>DenseKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/utils/","title":"Utils","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution","title":"build_student_t_distribution","text":"<pre><code>build_student_t_distribution(nu)\n</code></pre> <p>Build a Student's t distribution with a fixed smoothness parameter.</p> <p>For a fixed half-integer smoothness parameter, compute the spectral density of a Mat\u00e9rn kernel; a Student's t distribution.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>               (<code>int</code>)           \u2013            <p>The smoothness parameter of the Mat\u00e9rn kernel.</p> </li> </ul>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution--returns","title":"Returns","text":"<pre><code>tfp.Distribution: A Student's t distribution with the same smoothness parameter.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance","title":"squared_distance","text":"<pre><code>squared_distance(x, y)\n</code></pre> <p>Compute the squared distance between a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>First input.</p> </li> <li> <code>y</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>Second input.</p> </li> </ul>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The squared distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance","title":"euclidean_distance","text":"<pre><code>euclidean_distance(x, y)\n</code></pre> <p>Compute the euclidean distance between a pair of inputs.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>First input.</p> </li> <li> <code>y</code>               (<code>Float[Array, ' D']</code>)           \u2013            <p>Second input.</p> </li> </ul>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The euclidean distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/white/","title":"White","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White","title":"White","text":"<pre><code>White(active_dims=None, variance=1.0, n_dims=None, compute_engine=ConstantDiagonalKernelComputation())\n</code></pre> <p>               Bases: <code>StationaryKernel</code></p> <p>The White noise kernel.</p> <p>Computes the covariance for pairs of inputs \\((x, y)\\) with variance \\(\\sigma^2\\): $$ k(x, y) = \\sigma^2 \\delta(x-y) $$</p> <p>Parameters:</p> <ul> <li> <code>active_dims</code>               (<code>Union[list[int], slice, None]</code>, default:                   <code>None</code> )           \u2013            <p>The indices of the input dimensions that the kernel operates on.</p> </li> <li> <code>variance</code>               (<code>Union[ScalarFloat, Variable[ScalarArray]]</code>, default:                   <code>1.0</code> )           \u2013            <p>the variance of the kernel \u03c3.</p> </li> <li> <code>n_dims</code>               (<code>Union[int, None]</code>, default:                   <code>None</code> )           \u2013            <p>The number of input dimensions.</p> </li> <li> <code>compute_engine</code>               (<code>AbstractKernelComputation</code>, default:                   <code>ConstantDiagonalKernelComputation()</code> )           \u2013            <p>The computation engine that the kernel uses to compute the covariance matrix</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.spectral_density","title":"spectral_density  <code>property</code>","text":"<pre><code>spectral_density\n</code></pre> <p>The spectral density of the kernel.</p> <p>Returns:</p> <ul> <li> <code>Normal | StudentT</code>           \u2013            <p>Callable[[Float[Array, \"D\"]], Float[Array, \"D\"]]: The spectral density function.</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.cross_covariance","title":"cross_covariance","text":"<pre><code>cross_covariance(x, y)\n</code></pre> <p>Compute the cross-covariance matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the first input matrix of shape <code>(N, D)</code>.</p> </li> <li> <code>y</code>               (<code>Num[Array, 'M D']</code>)           \u2013            <p>the second input matrix of shape <code>(M, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, 'N M']</code>           \u2013            <p>The cross-covariance matrix of the kernel of shape <code>(N, M)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.gram","title":"gram","text":"<pre><code>gram(x)\n</code></pre> <p>Compute the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>The gram matrix of the kernel of shape <code>(N, N)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.diagonal","title":"diagonal","text":"<pre><code>diagonal(x)\n</code></pre> <p>Compute the diagonal of the gram matrix of the kernel.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Num[Array, 'N D']</code>)           \u2013            <p>the input matrix of shape <code>(N, D)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, ' N']</code>           \u2013            <p>The diagonal of the gram matrix of the kernel of shape <code>(N,)</code>.</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.slice_input","title":"slice_input","text":"<pre><code>slice_input(x)\n</code></pre> <p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[Array, '... D']</code>)           \u2013            <p>the matrix or vector that is to be sliced.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[Array, '... Q']</code>           \u2013            <p>The sliced form of the input matrix.</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> <p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the sum of the two kernels.</p> </li> </ul>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre> <p>Multiply two kernels together.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>AbstractKernel</code>)           \u2013            <p>The kernel to be multiplied with the current kernel.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AbstractKernel</code> (              <code>AbstractKernel</code> )          \u2013            <p>A new kernel that is the product of the two kernels.</p> </li> </ul>"}]}