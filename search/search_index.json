{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe1 Home","text":""},{"location":"#welcome-to-gpjax","title":"Welcome to GPJax!","text":"<p>GPJax is a didactic Gaussian process (GP) library in JAX, supporting GPU acceleration and just-in-time compilation. We seek to provide a flexible API to enable researchers to rapidly prototype and develop new ideas.</p> <p></p>"},{"location":"#hello-gp","title":"\"Hello, GP!\"","text":"<p>Typing GP models is as simple as the maths we would write on paper, as shown below.</p> PythonMath <pre><code>import gpjax as gpx\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.RBF()\nprior = gpx.gps.Prior(mean_function = mean, kernel = kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints = 123)\nposterior = prior * likelihood\n</code></pre> <p>k(\u22c5,\u22c5\u2032)=\u03c32exp\u2061(\u2212\u2225\u22c5\u2212\u22c5\u2032\u2225222\u21132)p(f(\u22c5))=GP(0,k(\u22c5,\u22c5\u2032))p(y \u2223 f(\u22c5))=N(y \u2223 f(\u22c5),\u03c3n2)p(f(\u22c5) \u2223 y)\u221dp(f(\u22c5))p(y \u2223 f(\u22c5)) . \\begin{align} k(\\cdot, \\cdot') &amp; = \\sigma^2\\exp\\left(-\\frac{\\lVert \\cdot- \\cdot'\\rVert_2^2}{2\\ell^2}\\right)\\\\ p(f(\\cdot)) &amp; = \\mathcal{GP}(\\mathbf{0}, k(\\cdot, \\cdot')) \\\\ p(y\\,|\\, f(\\cdot)) &amp; = \\mathcal{N}(y\\,|\\, f(\\cdot), \\sigma_n^2) \\\\ \\\\ p(f(\\cdot) \\,|\\, y) &amp; \\propto p(f(\\cdot))p(y\\,|\\, f(\\cdot))\\,. \\end{align} k(\u22c5,\u22c5\u2032)p(f(\u22c5))p(y\u2223f(\u22c5))p(f(\u22c5)\u2223y)\u200b=\u03c32exp(\u22122\u21132\u2225\u22c5\u2212\u22c5\u2032\u222522\u200b\u200b)=GP(0,k(\u22c5,\u22c5\u2032))=N(y\u2223f(\u22c5),\u03c3n2\u200b)\u221dp(f(\u22c5))p(y\u2223f(\u22c5)).\u200b\u200b</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Install</p> <p>GPJax can be installed via pip. See our installation guide for further details.</p> <pre><code>pip install gpjax\n</code></pre> <p>New</p> <p>New to GPs? Then why not check out our introductory notebook that starts from Bayes' theorem and univariate Gaussian distributions.</p> <p>Begin</p> <p>Looking for a good place to start? Then why not begin with our regression notebook.</p>"},{"location":"#citing-gpjax","title":"Citing GPJax","text":"<p>If you use GPJax in your research, please cite our JOSS paper.</p> <pre><code>@article{Pinder2022,\n  doi = {10.21105/joss.04455},\n  url = {https://doi.org/10.21105/joss.04455},\n  year = {2022},\n  publisher = {The Open Journal},\n  volume = {7},\n  number = {75},\n  pages = {4455},\n  author = {Thomas Pinder and Daniel Dodd},\n  title = {GPJax: A Gaussian Process Framework in JAX},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"\ud83d\udc65 Code of conduct","text":"<p>Like the technical community as a whole, the GPJax team and community is made up of a mixture of professionals and volunteers from all over the world, working on every aspect of the mission - including mentorship, teaching and connecting people.</p> <p>Diversity is one of our huge strengths, but it can also lead to communication issues and unhappiness. To that end, we have a few ground rules that we ask people to adhere to when they're participating within this community and project. These rules apply equally to founders, mentors and those seeking help and guidance.</p> <p>This isn't an exhaustive list of things that you can't do. Rather, take it in the spirit in which it's intended - a guide to make it easier to enrich all of us, the technical community and the conferences and usergroups we hope to guide new speakers to.</p> <p>This code of conduct applies to all communication: this includes IRC, the mailing list, and other forums such as Skype, Google+ Hangouts, etc.</p> <ul> <li>Be welcoming, friendly, and patient.</li> <li>Be considerate. Your work will be used by other people, and you in turn will depend on the work of others. Any decision you make will affect users and colleagues, and you should take those consequences into account when making decisions.</li> <li>Be respectful. Not all of us will agree all the time, but disagreement is no excuse for poor behaviour and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It's important to remember that a community where people feel uncomfortable or threatened is not a productive one. Members of the GPJax community should be respectful when dealing with other members as well as with people outside the GPJax community and with user groups/conferences, usergroup/conference organizers.</li> <li>Be careful in the words that you choose. Remember that sexist, racist, and other exclusionary jokes can be offensive to those around you. Be kind and welcoming to others. Do not insult or put down other participants. Behave professionally. The harassment and exclusionary behaviour towards others is unacceptable. This includes, but is not limited to:</li> <li>Violent threats or language directed against another person.</li> <li>Discriminatory jokes and language.</li> <li>Posting sexually explicit or violent material.</li> <li>Posting (or threatening to post) other people's personally identifying information (\"doxing\").</li> <li>Personal insults, especially those using racist or sexist terms.</li> <li>Unwelcome sexual attention.</li> <li>Advocating for, or encouraging, any of the above behaviour.</li> <li>Repeated harassment of others. In general, if someone asks you to stop, then stop.</li> <li>When we disagree, we try to understand why. Disagreements, both social and technical, happen all the time and GPJax is no exception. It is important that we resolve disagreements and differing views constructively. Remember that we're different. The strength of GPJax comes from its varied community, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn't mean that they're wrong. Don't forget that it is human to err and blaming each other doesn't get us anywhere, rather offer to help resolving issues and to help learn from mistakes.</li> </ul> <p>Text adapted from the Speak Up! project and Django</p>"},{"location":"GOVERNANCE/","title":"GPJax Governance Document","text":""},{"location":"GOVERNANCE/#the-project","title":"The Project","text":"<p>GPJax is an open-source library that supports Gaussian process modelling in the JAX scientific computation ecosystem. The abstractions provided in GPJax are designed to mimic the underlying maths through, making the library easy to use for both researchers and practitioners alike.</p> <p>GPJax was created by Thomas Pinder as a Single-Maintainer Houseplant project following the BDFL model of governance. We have since moved to the governance model of Specialty Library and benefited from a community of contributors. This document outlines the governance structure for the current status.</p>"},{"location":"GOVERNANCE/#roles","title":"Roles","text":"<ul> <li>Contributors: Anyone who contributes to GPJAx is considered a contributor. This   includes submitting code, filing issues, reviewing pull requests, and participating in   discussions. They are listed under:</li> <li>https://github.com/JaxGaussianProcesses/GPJax/graphs/contributors</li> <li>Core contributors: Core contributors are contributors who have made significant   contributions to the GPJax project, for example large modules or functionality.</li> <li>GPJax gardeners: Gardeners are core contributors who are responsible for maintaining   the project and making decisions about its future direction. GPJax gardeners have the   ability to merge pull requests into the GPJax repository. GPJax gardeners also take on   administrative tasks such as website maintenance.</li> <li>Currently daniel-dodd@,      henrymoss@, st--@, and      thomaspinder@ are the gardeners of GPJax.</li> </ul>"},{"location":"GOVERNANCE/#responsibility","title":"Responsibility","text":"<p>We cannot hold anyone responsible really since we are all doing free work here, but some general expectations are: * Contributors are responsible for following the project's code of conduct and   contributing to the project in a positive and constructive manner. Contributors are   also responsible for testing their code and ensuring that it meets the project's   standards. * Core contributors are expected to review pull requests and provide feedback to   contributors. They also make decisions about the architecture and implementation of   the module/functionality they contributed to. Also the \u201cif you broke something please   fix it\u201d applies. * Maintainers are responsible for monitoring the benchmark, the documentation and the   website are up to date and built passed, update dependency and apply best practice</p> <p>In addition to these specific responsibilities, all contributors are encouraged to participate in discussions about the project and to help out in any way they can.</p>"},{"location":"GOVERNANCE/#decision-making","title":"Decision-making","text":"<p>Decisions about the GPJax project are made by consensus among the GPJax gardeners. This means that all GPJax gardeners must agree on a decision before it can be implemented. If a consensus cannot be reached, we will flip a (virtual) coin.</p>"},{"location":"GOVERNANCE/#communication","title":"Communication","text":"<p>Communication about the GPJax project takes place in the following channels:</p> <p>GitHub issues: Issues are used to track bugs, feature requests, and other tasks. GitHub discussion: Discussions are used to answer user questions, scope for features, and discuss solutions to bugs. GitHub pull requests: Pull requests propose changes to the GPJax codebase. Slack: The GPJax Slack Channel is used for internal communication about the project for core contributors.</p>"},{"location":"GOVERNANCE/#contributing","title":"Contributing","text":"<p>Anyone is welcome to contribute to the GPJax project. Contributions can be made in the form of code, documentation, or other forms of support. To learn more about how to contribute, please see the contributing guide.</p>"},{"location":"GOVERNANCE/#code-of-conduct","title":"Code of conduct","text":"<p>All contributors to the GPJax project are expected to follow the project's code of conduct. The code of conduct outlines the expected behavior of contributors and helps to ensure a welcoming and productive environment for all.</p> <p>Any breaches of the code of conduct should be reported using our contact form.</p>"},{"location":"GOVERNANCE/#governance-changes","title":"Governance changes","text":"<p>This governance document is subject to change. Changes to the governance document must be approved by consensus among the core contributors.</p>"},{"location":"GOVERNANCE/#contact","title":"Contact","text":"<p>If you have any questions about the GPJax project, please feel free to contact the maintainers or reach out over Slack.</p> <p>This file was adapted from BlackJAX.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#how-can-i-contribute","title":"How can I contribute?","text":"<p>GPJax welcomes contributions from interested individuals or groups. There are many ways to contribute, including:</p> <ul> <li>Answering questions on our discussions   page.</li> <li>Raising issues related to bugs   or desired enhancements.</li> <li>Contributing or improving the   docs or   examples.</li> <li>Fixing outstanding issues   (bugs).</li> <li>Extending or improving our codebase.</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of conduct","text":"<p>As a contributor to GPJax, you can help us keep the community open and inclusive. Please read and follow our Code of Conduct.</p>"},{"location":"contributing/#opening-issues-and-getting-support","title":"Opening issues and getting support","text":"<p>Please open issues on Github Issue Tracker. Here you can mention</p> <p>You can ask a question or start a discussion in the Discussion section on Github.</p>"},{"location":"contributing/#contributing-to-the-source-code","title":"Contributing to the source code","text":"<p>Submitting code contributions to GPJax is done via a GitHub pull request. Our preferred workflow is to first fork the GitHub repository, clone it to your local machine, and develop on a feature branch. Once you're happy with your changes, install our <code>pre-commit hooks</code>, <code>commit</code> and <code>push</code> your code.</p> <p>New to this? Don't panic, our guide below will walk you through every detail!</p> <p>Note</p> <p>Before opening a pull request we recommend you check our pull request checklist.</p>"},{"location":"contributing/#step-by-step-guide","title":"Step-by-step guide:","text":"<ol> <li> <p>Click here to Fork GPJax's   codebase (alternatively, click the 'Fork' button towards the top right of   the main repository page). This   adds a copy of the codebase to your GitHub user account.</p> </li> <li> <p>Clone your GPJax fork from your GitHub account to your local disk, and add   the base repository as a remote:   <pre><code>$ git clone git@github.com:&lt;your GitHub handle&gt;/GPJax.git\n$ cd GPJax\n$ git remote add upstream git@github.com:GPJax.git\n</code></pre></p> </li> <li> <p>Create a <code>feature</code> branch to hold your development changes:</p> </li> </ol> <p><pre><code>$ git checkout -b my-feature\n</code></pre>   Always use a <code>feature</code> branch. It's good practice to avoid   work on the <code>main</code> branch of any repository.</p> <ol> <li>We use Poetry for packaging and dependency management, and project requirements are in       <code>pyproject.toml</code>. We suggest using a virtual environment for development. For those using Apple Silicon chips, we advise using Conda miniforge. Once the virtual environment is activated, run:</li> </ol> <pre><code>$ poetry install\n</code></pre> <p>At this point we recommend you check your installation passes the supplied unit tests:</p> <pre><code>$ poetry run pytest\n</code></pre> <ol> <li>Install the pre-commit hooks.</li> </ol> <pre><code>$ pre-commit install\n</code></pre> <p>Please ensure you have done this before committing any files. If   successful, this will print the following output <code>pre-commit installed at   .git/hooks/pre-commit</code>.</p> <ol> <li>At this point you can manually run the pre-commit hooks with the following command:</li> </ol> <pre><code>poetry run pre-commit run --all-files\n</code></pre> <ol> <li>Add changed files using <code>git add</code> and then <code>git commit</code> files to record your   changes locally:</li> </ol> <p><pre><code>$ git add modified_files\n$ git commit\n</code></pre>   After committing, it is a good idea to sync with the base repository in case   there have been any changes:</p> <pre><code>$ git fetch upstream\n$ git rebase upstream/main\n</code></pre> <p>Then push the changes to your GitHub account with:</p> <pre><code>$ git push -u origin my-feature\n</code></pre> <ol> <li>Go to the GitHub web page of your fork of the GPJax repo. Click the 'Pull   request' button to send your changes to the project's maintainers for   review.</li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<p>We welcome both complete or \"work in progress\" pull requests. Before opening one, we recommended you check the following guidelines to ensure a smooth review process.</p> <p>My contribution is a \"work in progress\":</p> <p>Please prefix the title of incomplete contributions with <code>[WIP]</code> (to indicate a work in progress). WIPs are useful to:</p> <ol> <li>Indicate you are working on something to avoid duplicated work.</li> <li>Request broad review of functionality or API.</li> <li>Seek collaborators.</li> </ol> <p>In the description of the pull request, we recommend you outline where work needs doing. For example, do some tests need writing?</p> <p>My contribution is complete:</p> <p>If addressing an issue, please use the pull request title to describe the issue and mention the issue number in the pull request description. This will make sure a link back to the original issue is created. Then before making your pull request, we recommend you check the following:</p> <ul> <li>Do all public methods have informative docstrings that describe their   function, input(s) and output(s)?</li> <li>Do the pre-commit hooks pass?</li> <li>Do the tests pass when everything is rebuilt from scratch?</li> <li> <p>Documentation and high-coverage tests are necessary for enhancements to be   accepted. Test coverage can be checked with:</p> <pre><code>$ poetry run pytest tests --cov=./ --cov-report=html\n</code></pre> </li> </ul> <p>Navigate to the newly created folder <code>htmlcov</code> and open <code>index.html</code> to view   the coverage report.</p> <p>This guide was derived from PyMC's guide to contributing.</p>"},{"location":"design/","title":"\ud83c\udfa8 Design principles","text":""},{"location":"design/#design-principles","title":"Design Principles","text":"<p><code>GPJax</code> is designed to be a Gaussian process package that provides an accurate representation of the underlying maths. Variable names are chosen to closely match the notation in (Rasmussen and Williams, 2006)<sup>1</sup>. We here list the notation used in <code>GPJax</code> with its corresponding mathematical quantity.</p>"},{"location":"design/#gaussian-process-notation","title":"Gaussian process notation","text":"On paper GPJax code Description nnn n Number of train inputs x=(x1,\u2026,xn)\\boldsymbol{x} = (x_1,\\dotsc,x_{n})x=(x1\u200b,\u2026,xn\u200b) x Train inputs y=(y1,\u2026,yn)\\boldsymbol{y} = (y_1,\\dotsc,y_{n})y=(y1\u200b,\u2026,yn\u200b) y Train labels t\\boldsymbol{t}t t Test inputs f(\u22c5)f(\\cdot)f(\u22c5) f Latent function modelled as a GP f(x)f({\\boldsymbol{x}})f(x) fx Latent function at inputs x\\boldsymbol{x}x \u03bcx\\boldsymbol{\\mu}_{\\boldsymbol{x}}\u03bcx\u200b mux Prior mean at inputs x\\boldsymbol{x}x Kxx\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}Kxx\u200b Kxx Kernel Gram matrix at inputs x\\boldsymbol{x}x Lx\\mathbf{L}_{\\boldsymbol{x}}Lx\u200b Lx Lower Cholesky decomposition of Kxx\\boldsymbol{K}_{\\boldsymbol{x}\\boldsymbol{x}}Kxx\u200b Ktx\\mathbf{K}_{\\boldsymbol{t}\\boldsymbol{x}}Ktx\u200b Ktx Cross-covariance between inputs t\\boldsymbol{t}t and x\\boldsymbol{x}x"},{"location":"design/#sparse-gaussian-process-notation","title":"Sparse Gaussian process notation","text":"On paper GPJax code Description mmm m Number of inducing inputs z=(z1,\u2026,zm)\\boldsymbol{z} = (z_1,\\dotsc,z_{m})z=(z1\u200b,\u2026,zm\u200b) z Inducing inputs u=(u1,\u2026,um)\\boldsymbol{u} = (u_1,\\dotsc,u_{m})u=(u1\u200b,\u2026,um\u200b) u Inducing outputs"},{"location":"design/#package-style","title":"Package style","text":"<p>Prior to building GPJax, the developers of GPJax have benefited greatly from the GPFlow and GPyTorch packages. As such, many of the design principles in GPJax are inspired by the excellent precursory packages. Documentation designs have been greatly inspired by the exceptional Flax docs.</p> <ol> <li> <p>Rasmussen, C. E. and Williams, C. K. (2006) Gaussian Processes for Machine Learning. 3. MIT press Cambridge, MA.\u00a0\u21a9</p> </li> </ol>"},{"location":"give_me_the_code/","title":"Give me the code","text":""},{"location":"give_me_the_code/#kernel-guide","title":"Kernel Guide","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import dataclass\nfrom typing import Dict\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom simple_pytree import static_field\nimport tensorflow_probability.substrates.jax as tfp\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.base.param import param_field\nkey = jr.key(123)\ntfb = tfp.bijectors\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkernels = [\ngpx.kernels.Matern12(),\ngpx.kernels.Matern32(),\ngpx.kernels.Matern52(),\ngpx.kernels.RBF(),\ngpx.kernels.Polynomial(),\ngpx.kernels.Polynomial(degree=2),\n]\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nmeanf = gpx.mean_functions.Zero()\nfor k, ax in zip(kernels, axes.ravel()):\nprior = gpx.gps.Prior(mean_function=meanf, kernel=k)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nax.plot(x, y.T, alpha=0.7)\nax.set_title(k.name)\nslice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3], lengthscale=jnp.ones((3,)))\nprint(f\"Lengthscales: {slice_kernel.lengthscale}\")\n# Inputs\nx_matrix = jr.normal(key, shape=(50, 5))\n# Compute the Gram matrix\nK = slice_kernel.gram(x_matrix)\nprint(K.shape)\nk1 = gpx.kernels.RBF()\nk2 = gpx.kernels.Polynomial()\nsum_k = gpx.kernels.SumKernel(kernels=[k1, k2])\nfig, ax = plt.subplots(ncols=3, figsize=(9, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(sum_k.gram(x).to_dense())\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nk3 = gpx.kernels.Matern32()\nprod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])\nfig, ax = plt.subplots(ncols=4, figsize=(12, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(k3.gram(x).to_dense())\nim3 = ax[3].matshow(prod_k.gram(x).to_dense())\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nfig.colorbar(im3, ax=ax[3], fraction=0.05)\ndef angular_distance(x, y, c):\nreturn jnp.abs((x - y + c) % (c * 2) - c)\nbij = tfb.SoftClip(low=jnp.array(4.0, dtype=jnp.float64))\n@dataclass\nclass Polar(gpx.kernels.AbstractKernel):\nperiod: float = static_field(2 * jnp.pi)\ntau: float = param_field(jnp.array([5.0]), bijector=bij)\ndef __call__(\nself, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]\n) -&gt; Float[Array, \"1\"]:\nc = self.period / 2.0\nt = angular_distance(x, y, c)\nK = (1 + self.tau * t / c) * jnp.clip(1 - t / c, 0, jnp.inf) ** self.tau\nreturn K.squeeze()\n# Simulate data\nangles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1)\nn = 20\nnoise = 0.2\nX = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0)\ny = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise\nD = gpx.Dataset(X=X, y=y)\n# Define polar Gaussian process\nPKern = Polar()\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\ncircular_posterior = gpx.gps.Prior(mean_function=meanf, kernel=PKern) * likelihood\n# Optimise GP's marginal log-likelihood using BFGS\nopt_posterior, history = gpx.fit_scipy(\nmodel=circular_posterior,\nobjective=jit(gpx.objectives.ConjugateMLL(negative=True)),\ntrain_data=D,\n)\nposterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D))\nmu = posterior_rv.mean()\none_sigma = posterior_rv.stddev()\nfig = plt.figure(figsize=(7, 3.5))\ngridspec = fig.add_gridspec(1, 1)\nax = plt.subplot(gridspec[0], polar=True)\nax.fill_between(\nangles.squeeze(),\nmu - one_sigma,\nmu + one_sigma,\nalpha=0.3,\nlabel=r\"1 Posterior s.d.\",\ncolor=cols[1],\nlw=0,\n)\nax.fill_between(\nangles.squeeze(),\nmu - 3 * one_sigma,\nmu + 3 * one_sigma,\nalpha=0.15,\nlabel=r\"3 Posterior s.d.\",\ncolor=cols[1],\nlw=0,\n)\nax.plot(angles, mu, label=\"Posterior mean\")\nax.scatter(D.X, D.y, alpha=1, label=\"Observations\")\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#sparse-stochastic-variational-inference","title":"Sparse Stochastic Variational Inference","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nimport gpjax.kernels as jk\nkey = jr.key(123)\ntfb = tfp.bijectors\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 50000\nnoise = 0.2\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)\nz = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.vlines(\nz,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=1,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\nax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\nprior = gpx.gps.Prior(mean_function=meanf, kernel=jk.RBF())\np = prior * likelihood\nq = gpx.variational_families.VariationalGaussian(posterior=p, inducing_inputs=z)\nnegative_elbo = gpx.objectives.ELBO(negative=True)\nprint(gpx.cite(negative_elbo))\nnegative_elbo = jit(negative_elbo)\nschedule = ox.warmup_cosine_decay_schedule(\ninit_value=0.0,\npeak_value=0.01,\nwarmup_steps=75,\ndecay_steps=1500,\nend_value=0.001,\n)\nopt_posterior, history = gpx.fit(\nmodel=q,\nobjective=negative_elbo,\ntrain_data=D,\noptim=ox.adam(learning_rate=schedule),\nnum_iters=3000,\nkey=jr.key(42),\nbatch_size=128,\n)\nlatent_dist = opt_posterior(xtest)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\nxtest.flatten(),\nmeanf - 2 * sigma,\nmeanf + 2 * sigma,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.vlines(\nopt_posterior.inducing_inputs,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=1,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend()\ntriangular_transform = tfb.FillScaleTriL(\ndiag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter)\n)\nreparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform)\nopt_rep, history = gpx.fit(\nmodel=reparameterised_q,\nobjective=negative_elbo,\ntrain_data=D,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=3000,\nkey=jr.key(42),\nbatch_size=128,\n)\nlatent_dist = opt_rep(xtest)\npredictive_dist = opt_rep.posterior.likelihood(latent_dist)\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\nxtest.flatten(),\nmeanf - 2 * sigma,\nmeanf + 2 * sigma,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.vlines(\nopt_rep.inducing_inputs,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=1,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'\n</code></pre>"},{"location":"give_me_the_code/#regression","title":"Regression","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 100\nnoise = 0.3\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\nkernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nprior_dist = prior.predict(xtest)\nprior_mean = prior_dist.mean()\nprior_std = prior_dist.variance()\nsamples = prior_dist.sample(seed=key, sample_shape=(20,))\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\nxtest.flatten(),\nprior_mean - prior_std,\nprior_mean + prior_std,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\n# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n# optim = ox.chain(\n#     ox.adam(learning_rate=0.01),\n#     ox.masked(ox.set_to_zero(), static_tree)\n#     )\nprint(gpx.cite(negative_mll))\nnegative_mll = jit(negative_mll)\nopt_posterior, history = gpx.fit_scipy(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=D,\n)\nlatent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\nxtest,\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nxtest,\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nxtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</code></pre>"},{"location":"give_me_the_code/#deep-kernel-learning","title":"Deep Kernel Learning","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import (\ndataclass,\nfield,\n)\nfrom typing import Any\nimport flax\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom scipy.signal import sawtooth\nfrom gpjax.base import static_field\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.base import param_field\nimport gpjax.kernels as jk\nfrom gpjax.kernels import DenseKernelComputation\nfrom gpjax.kernels.base import AbstractKernel\nfrom gpjax.kernels.computations import AbstractKernelComputation\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 500\nnoise = 0.2\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nytest = f(xtest)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\nax.plot(xtest, ytest, label=\"True function\")\nax.legend(loc=\"best\")\n@dataclass\nclass DeepKernelFunction(AbstractKernel):\nbase_kernel: AbstractKernel = None\nnetwork: nn.Module = static_field(None)\ndummy_x: jax.Array = static_field(None)\nkey: jax.Array = static_field(jr.key(123))\nnn_params: Any = field(init=False, repr=False)\ndef __post_init__(self):\nif self.base_kernel is None:\nraise ValueError(\"base_kernel must be specified\")\nif self.network is None:\nraise ValueError(\"network must be specified\")\nself.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))\ndef __call__(\nself, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n) -&gt; Float[Array, \"1\"]:\nstate = self.network.init(self.key, x)\nxt = self.network.apply(state, x)\nyt = self.network.apply(state, y)\nreturn self.base_kernel(xt, yt)\nfeature_space_dim = 3\nclass Network(nn.Module):\n\"\"\"A simple MLP.\"\"\"\n@nn.compact\ndef __call__(self, x):\nx = nn.Dense(features=32)(x)\nx = nn.relu(x)\nx = nn.Dense(features=64)(x)\nx = nn.relu(x)\nx = nn.Dense(features=feature_space_dim)(x)\nreturn x\nforward_linear = Network()\nbase_kernel = gpx.kernels.Matern52(\nactive_dims=list(range(feature_space_dim)),\nlengthscale=jnp.ones((feature_space_dim,)),\n)\nkernel = DeepKernelFunction(\nnetwork=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x\n)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\nschedule = ox.warmup_cosine_decay_schedule(\ninit_value=0.0,\npeak_value=0.01,\nwarmup_steps=75,\ndecay_steps=700,\nend_value=0.0,\n)\noptimiser = ox.chain(\nox.clip(1.0),\nox.adamw(learning_rate=schedule),\n)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=jax.jit(gpx.objectives.ConjugateMLL(negative=True)),\ntrain_data=D,\noptim=optimiser,\nnum_iters=800,\nkey=key,\n)\nlatent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\npredictive_mean + 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#count-data-regression","title":"Count data regression","text":"<pre><code>import blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\nfrom jax import config\nfrom jaxtyping import install_import_hook\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\ntfd = tfp.distributions\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey, subkey = jr.split(key)\nn = 50\nx = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\nf = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\ny = jr.poisson(key, jnp.exp(f(x)))\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[1])\nax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\nax.legend()\nkernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Constant()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Poisson(num_datapoints=D.n)\nposterior = prior * likelihood\nprint(type(posterior))\n# Adapted from BlackJax's introduction notebook.\nnum_adapt = 100\nnum_samples = 200\nlpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\nadapt = blackjax.window_adaptation(\nblackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n# Initialise the chain\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\ndef one_step(state, rng_key):\nstate, info = kernel(rng_key, state)\nreturn state, (state, info)\nkeys = jax.random.split(rng_key, num_samples)\n_, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\nreturn states, infos\n# Sample from the posterior distribution\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nacceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\nfig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.constrain().prior.kernel.variance)\nax1.plot(states.position.constrain().prior.kernel.lengthscale)\nax2.plot(states.position.constrain().prior.mean_function.constant)\nax0.set_title(\"Kernel variance\")\nax1.set_title(\"Kernel lengthscale\")\nax2.set_title(\"Mean function constant\")\nthin_factor = 10\nsamples = []\nfor i in range(num_adapt, num_samples + num_adapt, thin_factor):\nsample = jtu.tree_map(lambda samples: samples[i], states.position)\nsample = sample.constrain()\nlatent_dist = sample.predict(xtest, train_data=D)\npredictive_dist = sample.likelihood(latent_dist)\nsamples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\nsamples = jnp.vstack(samples)\nlower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(samples, axis=0)\nfig, ax = plt.subplots()\nax.plot(\nx, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7\n)\nax.plot(\nxtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1\n)\nax.fill_between(\nxtest.flatten(),\nlower_ci.flatten(),\nupper_ci.flatten(),\nalpha=0.2,\ncolor=cols[0],\nlabel=\"95% CI\",\n)\n%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Francesco Zanetta\"\n</code></pre>"},{"location":"give_me_the_code/#introduction-to-bayesian-optimisation","title":"Introduction to Bayesian Optimisation","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float, Int\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom typing import List, Tuple\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.typing import Array, FunctionalSample, ScalarFloat\nfrom jaxopt import ScipyBoundedMinimize\nkey = jr.key(42)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ndef standardised_forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:\nmean = 0.45321\nstd = 4.4258\nreturn ((6 * x - 2) ** 2 * jnp.sin(12 * x - 4) - mean) / std\nlower_bound = jnp.array([0.0])\nupper_bound = jnp.array([1.0])\ninitial_sample_num = 5\ninitial_x = tfp.mcmc.sample_halton_sequence(\ndim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = standardised_forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\ndef return_optimised_posterior(\ndata: gpx.Dataset, prior: gpx.base.Module, key: Array\n) -&gt; gpx.base.Module:\nlikelihood = gpx.likelihoods.Gaussian(\nnum_datapoints=data.n, obs_stddev=jnp.array(1e-6)\n)  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value\nlikelihood = likelihood.replace_trainable(obs_stddev=False)\nposterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=data)\nnegative_mll = jit(negative_mll)\nopt_posterior, _ = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=data,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=1000,\nsafe=True,\nkey=key,\nverbose=False,\n)\nreturn opt_posterior\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, key)\napprox_sample = opt_posterior.sample_approx(\nnum_samples=1, train_data=D, key=key, num_features=500\n)\nutility_fn = lambda x: approx_sample(x)[0][0]\ndef optimise_sample(\nsample: FunctionalSample,\nkey: Int[Array, \"\"],\nlower_bound: Float[Array, \"D\"],\nupper_bound: Float[Array, \"D\"],\nnum_initial_sample_points: int,\n) -&gt; ScalarFloat:\ninitial_sample_points = jr.uniform(\nkey,\nshape=(num_initial_sample_points, lower_bound.shape[0]),\ndtype=jnp.float64,\nminval=lower_bound,\nmaxval=upper_bound,\n)\ninitial_sample_y = sample(initial_sample_points)\nbest_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])\n# We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.\nnegative_utility_fn = lambda x: sample(x)[0][0]\nlbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")\nbounds = (lower_bound, upper_bound)\nx_star = lbfgsb.run(best_x, bounds=bounds).params\nreturn x_star\nx_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100)\ny_star = standardised_forrester(x_star)\ndef plot_bayes_opt(\nposterior: gpx.base.Module,\nsample: FunctionalSample,\ndataset: gpx.Dataset,\nqueried_x: ScalarFloat,\n) -&gt; None:\nplt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\nforrester_y = standardised_forrester(plt_x)\nsample_y = sample(plt_x)\nlatent_dist = posterior.predict(plt_x, train_data=dataset)\npredictive_dist = posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])\nax.fill_between(\nplt_x.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\nplt_x,\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nplt_x,\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(plt_x, sample_y, label=\"Posterior Sample\")\nax.plot(\nplt_x,\nforrester_y,\nlabel=\"Forrester Function\",\ncolor=cols[0],\nlinestyle=\"--\",\nlinewidth=2,\n)\nax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\nax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\nax.scatter(\nqueried_x,\nsample(queried_x),\nlabel=\"Posterior Sample Optimum\",\nmarker=\"*\",\ncolor=cols[3],\nzorder=3,\n)\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nplt.show()\nplot_bayes_opt(opt_posterior, approx_sample, D, x_star)\nbo_iters = 5\n# Set up initial dataset\ninitial_x = tfp.mcmc.sample_halton_sequence(\ndim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = standardised_forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\nfor i in range(bo_iters):\nkey, subkey = jr.split(key)\n# Generate optimised posterior using previously observed data\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, subkey)\n# Draw a sample from the posterior, and find the minimiser of it\napprox_sample = opt_posterior.sample_approx(\nnum_samples=1, train_data=D, key=subkey, num_features=500\n)\nx_star = optimise_sample(\napprox_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100\n)\nplot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n# Evaluate the black-box function at the best point observed so far, and add it to the dataset\ny_star = standardised_forrester(x_star)\nprint(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")\nD = D + gpx.Dataset(X=x_star, y=y_star)\nfig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\ncumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y)\nax.plot(fn_evaluations, cumulative_best_y)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.axhline(y=-1.463, linestyle=\"--\", label=\"True Minimum\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Best Observed Value\")\nax.legend()\nplt.show()\ndef standardised_six_hump_camel(x: Float[Array, \"N 2\"]) -&gt; Float[Array, \"N 1\"]:\nmean = 1.12767\nstd = 1.17500\nx1 = x[..., :1]\nx2 = x[..., 1:]\nterm1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2\nterm2 = x1 * x2\nterm3 = (-4 + 4 * x2**2) * x2**2\nreturn (term1 + term2 + term3 - mean) / std\nx1 = jnp.linspace(-2, 2, 100)\nx2 = jnp.linspace(-1, 1, 100)\nx1, x2 = jnp.meshgrid(x1, x2)\nx = jnp.stack([x1.flatten(), x2.flatten()], axis=1)\ny = standardised_six_hump_camel(x)\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurf = ax.plot_surface(\nx1,\nx2,\ny.reshape(x1.shape[0], x2.shape[0]),\nlinewidth=0,\ncmap=cm.coolwarm,\nantialiased=False,\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nplt.show()\nx_star_one = jnp.array([[0.0898, -0.7126]])\nx_star_two = jnp.array([[-0.0898, 0.7126]])\nfig, ax = plt.subplots()\ncontour_plot = ax.contourf(\nx1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\nx_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\"\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2])\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\nlower_bound = jnp.array([-2.0, -1.0])\nupper_bound = jnp.array([2.0, 1.0])\ninitial_sample_num = 5\nbo_iters = 12\nnum_experiments = 5\nbo_experiment_results = []\nfor experiment in range(num_experiments):\nprint(f\"Starting Experiment: {experiment + 1}\")\n# Set up initial dataset\ninitial_x = tfp.mcmc.sample_halton_sequence(\ndim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n)\ninitial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)\ninitial_y = standardised_six_hump_camel(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\nfor i in range(bo_iters):\nkey, subkey = jr.split(key)\n# Generate optimised posterior\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\nactive_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0\n)\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, subkey)\n# Draw a sample from the posterior, and find the minimiser of it\napprox_sample = opt_posterior.sample_approx(\nnum_samples=1, train_data=D, key=subkey, num_features=500\n)\nx_star = optimise_sample(\napprox_sample,\nsubkey,\nlower_bound,\nupper_bound,\nnum_initial_sample_points=1000,\n)\n# Evaluate the black-box function at the best point observed so far, and add it to the dataset\ny_star = standardised_six_hump_camel(x_star)\nprint(\nf\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value:\"\nf\" {y_star}\"\n)\nD = D + gpx.Dataset(X=x_star, y=y_star)\nbo_experiment_results.append(D)\nrandom_experiment_results = []\nfor i in range(num_experiments):\nkey, subkey = jr.split(key)\ninitial_x = bo_experiment_results[i].X[:5]\ninitial_y = bo_experiment_results[i].y[:5]\nfinal_x = jr.uniform(\nkey,\nshape=(bo_iters, 2),\ndtype=jnp.float64,\nminval=lower_bound,\nmaxval=upper_bound,\n)\nfinal_y = standardised_six_hump_camel(final_x)\nrandom_x = jnp.concatenate([initial_x, final_x], axis=0)\nrandom_y = jnp.concatenate([initial_y, final_y], axis=0)\nrandom_experiment_results.append(gpx.Dataset(X=random_x, y=random_y))\ndef obtain_log_regret_statistics(\nexperiment_results: List[gpx.Dataset],\nglobal_minimum: ScalarFloat,\n) -&gt; Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:\nlog_regret_results = []\nfor exp_result in experiment_results:\nobservations = exp_result.y\ncumulative_best_observations = jax.lax.associative_scan(\njax.numpy.minimum, observations\n)\nregret = cumulative_best_observations - global_minimum\nlog_regret = jnp.log(regret)\nlog_regret_results.append(log_regret)\nlog_regret_results = jnp.array(log_regret_results)\nlog_regret_mean = jnp.mean(log_regret_results, axis=0)\nlog_regret_std = jnp.std(log_regret_results, axis=0)\nreturn log_regret_mean, log_regret_std\nbo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(\nbo_experiment_results, -1.8377\n)\n(\nrandom_log_regret_mean,\nrandom_log_regret_std,\n) = obtain_log_regret_statistics(random_experiment_results, -1.8377)\nfig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\nax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\")\nax.fill_between(\nfn_evaluations,\nbo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],\nbo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],\nalpha=0.2,\n)\nax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\")\nax.fill_between(\nfn_evaluations,\nrandom_log_regret_mean[:, 0] - random_log_regret_std[:, 0],\nrandom_log_regret_mean[:, 0] + random_log_regret_std[:, 0],\nalpha=0.2,\n)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Log Regret\")\nax.legend()\nplt.show()\nfig, ax = plt.subplots()\ncontour_plot = ax.contourf(\nx1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\nx_star_one[0][0],\nx_star_one[0][1],\nmarker=\"*\",\ncolor=cols[2],\nlabel=\"Global Minimum\",\nzorder=2,\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2)\nax.scatter(\nbo_experiment_results[1].X[:, 0],\nbo_experiment_results[1].X[:, 1],\nmarker=\"x\",\ncolor=cols[1],\nlabel=\"Bayesian Optimisation Queries\",\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</code></pre>"},{"location":"give_me_the_code/#new-to-gaussian-processes","title":"New to Gaussian Processes?","text":"<pre><code>import warnings\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow_probability.substrates.jax as tfp\nfrom docs.examples.utils import confidence_ellipse\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ntfd = tfp.distributions\nud1 = tfd.Normal(0.0, 1.0)\nud2 = tfd.Normal(-1.0, 0.5)\nud3 = tfd.Normal(0.25, 1.5)\nxs = jnp.linspace(-5.0, 5.0, 500)\nfig, ax = plt.subplots()\nfor d in [ud1, ud2, ud3]:\nax.plot(\nxs,\njnp.exp(d.log_prob(xs)),\nlabel=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",\n)\nax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2)\nax.legend(loc=\"best\")\nkey = jr.key(123)\nd1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2))\nd2 = tfd.MultivariateNormalTriL(\njnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]]))\n)\nd3 = tfd.MultivariateNormalTriL(\njnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]]))\n)\ndists = [d1, d2, d3]\nxvals = jnp.linspace(-5.0, 5.0, 500)\nyvals = jnp.linspace(-5.0, 5.0, 500)\nxx, yy = jnp.meshgrid(xvals, yvals)\npos = jnp.empty(xx.shape + (2,))\npos.at[:, :, 0].set(xx)\npos.at[:, :, 1].set(yy)\nfig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\ntitles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)\nfor a, t, d in zip([ax0, ax1, ax2], titles, dists):\nd_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(\nxx.shape\n)\ncntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)\nfor c in cntf.collections:\nc.set_edgecolor(\"face\")\na.set_xlim(-2.75, 2.75)\na.set_ylim(-2.75, 2.75)\nsamples = d.sample(seed=key, sample_shape=(5000,))\nxsample, ysample = samples[:, 0], samples[:, 1]\nconfidence_ellipse(\nxsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n)\nconfidence_ellipse(\nxsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"\n)\na.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)\na.set(xlabel=\"x\", ylabel=\"y\", title=t)\nn = 1000\nx = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,))\nkey, subkey = jr.split(key)\ny = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,))\nkey, subkey = jr.split(subkey)\nxfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\nyfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\ndf = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\ng = sns.jointplot(\ndata=df,\nx=\"x\",\ny=\"y\",\nhue=\"idx\",\nmarker=\".\",\nspace=0.0,\nxlim=(-4.0, 4.0),\nylim=(-4.0, 4.0),\nheight=4,\nmarginal_ticks=False,\nlegend=False,\npalette=\"inferno\",\nmarginal_kws={\n\"fill\": True,\n\"linewidth\": 1,\n\"color\": cols[1],\n\"alpha\": 0.3,\n\"bw_adjust\": 2,\n\"cmap\": cmap,\n},\njoint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},\n)\ng.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\ng.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\ng.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\nconfidence_ellipse(\nxfull,\nyfull,\ng.ax_joint,\nedgecolor=\"#3f3f3f\",\nn_std=1.0,\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nconfidence_ellipse(\nxfull,\nyfull,\ng.ax_joint,\nedgecolor=\"#3f3f3f\",\nn_std=2.0,\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nconfidence_ellipse(\nxfull,\nyfull,\ng.ax_joint,\nedgecolor=\"#3f3f3f\",\nn_std=3.0,\nlinestyle=\"--\",\nlinewidth=0.5,\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#gaussian-processes-for-vector-fields-and-ocean-current-modelling","title":"Gaussian Processes for Vector Fields and Ocean Current Modelling","text":"<pre><code>from jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import dataclass, field\nfrom jax import hessian\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow_probability as tfp\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\n# Enable Float64 for more stable matrix inversions.\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncolors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n# function to place data from csv into correct array shape\ndef prepare_data(df):\npos = jnp.array([df[\"lon\"], df[\"lat\"]])\nvel = jnp.array([df[\"ubar\"], df[\"vbar\"]])\n# extract shape stored as 'metadata' in the test data\ntry:\nshape = (int(df[\"shape\"][1]), int(df[\"shape\"][0]))  # shape = (34,16)\nreturn pos, vel, shape\nexcept KeyError:\nreturn pos, vel\n# loading in data\ngulf_data_train = pd.read_csv(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_train.csv\"\n)\ngulf_data_test = pd.read_csv(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_test.csv\"\n)\npos_test, vel_test, shape = prepare_data(gulf_data_test)\npos_train, vel_train = prepare_data(gulf_data_train)\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\nax.quiver(\npos_test[0],\npos_test[1],\nvel_test[0],\nvel_test[1],\ncolor=colors[0],\nlabel=\"Ocean Current\",\nangles=\"xy\",\nscale=10,\n)\nax.quiver(\npos_train[0],\npos_train[1],\nvel_train[0],\nvel_train[1],\ncolor=colors[1],\nalpha=0.7,\nlabel=\"Drifter\",\nangles=\"xy\",\nscale=10,\n)\nax.set(\nxlabel=\"Longitude\",\nylabel=\"Latitude\",\n)\nax.legend(\nframealpha=0.0,\nncols=2,\nfontsize=\"medium\",\nbbox_to_anchor=(0.5, -0.3),\nloc=\"lower center\",\n)\nplt.show()\n# Change vectors x -&gt; X = (x,z), and vectors y -&gt; Y = (y,z) via the artificial z label\ndef label_position(data):\n# introduce alternating z label\nn_points = len(data[0])\nlabel = jnp.tile(jnp.array([0.0, 1.0]), n_points)\nreturn jnp.vstack((jnp.repeat(data, repeats=2, axis=1), label)).T\n# change vectors y -&gt; Y by reshaping the velocity measurements\ndef stack_velocity(data):\nreturn data.T.flatten().reshape(-1, 1)\ndef dataset_3d(pos, vel):\nreturn gpx.Dataset(label_position(pos), stack_velocity(vel))\n# label and place the training data into a Dataset object to be used by GPJax\ndataset_train = dataset_3d(pos_train, vel_train)\n# we also require the testing data to be relabelled for later use, such that we can query the 2Nx2N GP at the test points\ndataset_ground_truth = dataset_3d(pos_test, vel_test)\n@dataclass\nclass VelocityKernel(gpx.kernels.AbstractKernel):\nkernel0: gpx.kernels.AbstractKernel = field(\ndefault_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n)\nkernel1: gpx.kernels.AbstractKernel = field(\ndefault_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n)\ndef __call__(\nself, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n) -&gt; Float[Array, \"1\"]:\n# standard RBF-SE kernel is x and x' are on the same output, otherwise returns 0\nz = jnp.array(X[2], dtype=int)\nzp = jnp.array(Xp[2], dtype=int)\n# achieve the correct value via 'switches' that are either 1 or 0\nk0_switch = ((z + 1) % 2) * ((zp + 1) % 2)\nk1_switch = z * zp\nreturn k0_switch * self.kernel0(X, Xp) + k1_switch * self.kernel1(X, Xp)\ndef initialise_gp(kernel, mean, dataset):\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nlikelihood = gpx.likelihoods.Gaussian(\nnum_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)\n)\nposterior = prior * likelihood\nreturn posterior\n# Define the velocity GP\nmean = gpx.mean_functions.Zero()\nkernel = VelocityKernel()\nvelocity_posterior = initialise_gp(kernel, mean, dataset_train)\ndef optimise_mll(posterior, dataset, NIters=1000, key=key):\n# define the MLL using dataset_train\nobjective = gpx.objectives.ConjugateMLL(negative=True)\n# Optimise to minimise the MLL\nopt_posterior, history = gpx.fit_scipy(\nmodel=posterior,\nobjective=objective,\ntrain_data=dataset,\n)\nreturn opt_posterior\nopt_velocity_posterior = optimise_mll(velocity_posterior, dataset_train)\ndef latent_distribution(opt_posterior, pos_3d, dataset_train):\nlatent = opt_posterior.predict(pos_3d, train_data=dataset_train)\nlatent_mean = latent.mean()\nlatent_std = latent.stddev()\nreturn latent_mean, latent_std\n# extract latent mean and std of g, redistribute into vectors to model F\nvelocity_mean, velocity_std = latent_distribution(\nopt_velocity_posterior, dataset_ground_truth.X, dataset_train\n)\ndataset_latent_velocity = dataset_3d(pos_test, velocity_mean)\n# Residuals between ground truth and estimate\ndef plot_vector_field(ax, dataset, **kwargs):\nax.quiver(\ndataset.X[::2][:, 0],\ndataset.X[::2][:, 1],\ndataset.y[::2],\ndataset.y[1::2],\n**kwargs,\n)\ndef prepare_ax(ax, X, Y, title, **kwargs):\nax.set(\nxlim=[X.min() - 0.1, X.max() + 0.1],\nylim=[Y.min() + 0.1, Y.max() + 0.1],\naspect=\"equal\",\ntitle=title,\nylabel=\"latitude\",\n**kwargs,\n)\ndef residuals(dataset_latent, dataset_ground_truth):\nreturn jnp.sqrt(\n(dataset_latent.y[::2] - dataset_ground_truth.y[::2]) ** 2\n+ (dataset_latent.y[1::2] - dataset_ground_truth.y[1::2]) ** 2\n)\ndef plot_fields(\ndataset_ground_truth, dataset_trajectory, dataset_latent, shape=shape, scale=10\n):\nX = dataset_ground_truth.X[:, 0][::2]\nY = dataset_ground_truth.X[:, 1][::2]\n# make figure\nfig, ax = plt.subplots(1, 3, figsize=(12.0, 3.0), sharey=True)\n# ground truth\nplot_vector_field(\nax[0],\ndataset_ground_truth,\ncolor=colors[0],\nlabel=\"Ocean Current\",\nangles=\"xy\",\nscale=scale,\n)\nplot_vector_field(\nax[0],\ndataset_trajectory,\ncolor=colors[1],\nlabel=\"Drifter\",\nangles=\"xy\",\nscale=scale,\n)\nprepare_ax(ax[0], X, Y, \"Ground Truth\", xlabel=\"Longitude\")\n# Latent estimate of vector field F\nplot_vector_field(ax[1], dataset_latent, color=colors[0], angles=\"xy\", scale=scale)\nplot_vector_field(\nax[1], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale\n)\nprepare_ax(ax[1], X, Y, \"GP Estimate\", xlabel=\"Longitude\")\n# residuals\nresiduals_vel = jnp.flip(\nresiduals(dataset_latent, dataset_ground_truth).reshape(shape), axis=0\n)\nim = ax[2].imshow(\nresiduals_vel,\nextent=[X.min(), X.max(), Y.min(), Y.max()],\ncmap=\"jet\",\nvmin=0,\nvmax=1.0,\ninterpolation=\"spline36\",\n)\nplot_vector_field(\nax[2], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale\n)\nprepare_ax(ax[2], X, Y, \"Residuals\", xlabel=\"Longitude\")\nfig.colorbar(im, fraction=0.027, pad=0.04, orientation=\"vertical\")\nfig.legend(\nframealpha=0.0,\nncols=2,\nfontsize=\"medium\",\nbbox_to_anchor=(0.5, -0.03),\nloc=\"lower center\",\n)\nplt.show()\nplot_fields(dataset_ground_truth, dataset_train, dataset_latent_velocity)\n@dataclass\nclass HelmholtzKernel(gpx.kernels.AbstractKernel):\n# initialise Phi and Psi kernels as any stationary kernel in gpJax\npotential_kernel: gpx.kernels.AbstractKernel = field(\ndefault_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n)\nstream_kernel: gpx.kernels.AbstractKernel = field(\ndefault_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n)\ndef __call__(\nself, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n) -&gt; Float[Array, \"1\"]:\n# obtain indices for k_helm, implement in the correct sign between the derivatives\nz = jnp.array(X[2], dtype=int)\nzp = jnp.array(Xp[2], dtype=int)\nsign = (-1) ** (z + zp)\n# convert to array to correctly index, -ve sign due to exchange symmetry (only true for stationary kernels)\npotential_dvtve = -jnp.array(\nhessian(self.potential_kernel)(X, Xp), dtype=jnp.float64\n)[z][zp]\nstream_dvtve = -jnp.array(\nhessian(self.stream_kernel)(X, Xp), dtype=jnp.float64\n)[1 - z][1 - zp]\nreturn potential_dvtve + sign * stream_dvtve\n# Redefine Gaussian process with Helmholtz kernel\nkernel = HelmholtzKernel()\nhelmholtz_posterior = initialise_gp(kernel, mean, dataset_train)\n# Optimise hyperparameters using BFGS\nopt_helmholtz_posterior = optimise_mll(helmholtz_posterior, dataset_train)\n# obtain latent distribution, extract x and y values over g\nhelmholtz_mean, helmholtz_std = latent_distribution(\nopt_helmholtz_posterior, dataset_ground_truth.X, dataset_train\n)\ndataset_latent_helmholtz = dataset_3d(pos_test, helmholtz_mean)\nplot_fields(dataset_ground_truth, dataset_train, dataset_latent_helmholtz)\n# ensure testing data alternates between x0 and x1 components\ndef nlpd(mean, std, vel_test):\nvel_query = jnp.column_stack((vel_test[0], vel_test[1])).flatten()\nnormal = tfp.substrates.jax.distributions.Normal(loc=mean, scale=std)\nreturn -jnp.sum(normal.log_prob(vel_query))\n# compute nlpd for velocity and helmholtz\nnlpd_vel = nlpd(velocity_mean, velocity_std, vel_test)\nnlpd_helm = nlpd(helmholtz_mean, helmholtz_std, vel_test)\nprint(\"NLPD for Velocity: %.2f \\nNLPD for Helmholtz: %.2f\" % (nlpd_vel, nlpd_helm))\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Ivan Shalashilin'\n</code></pre>"},{"location":"give_me_the_code/#gaussian-processes-barycentres","title":"Gaussian Processes Barycentres","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport typing as tp\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy.linalg as jsl\nfrom jaxtyping import install_import_hook\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax.distributions as tfd\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 100\nn_test = 200\nn_datasets = 5\nx = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1)\nxtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1)\nf = lambda x, a, b: a + jnp.sin(b * x)\nys = []\nfor _i in range(n_datasets):\nkey, subkey = jr.split(key)\nvertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)\nperiod = jr.uniform(subkey, minval=0.75, maxval=1.25)\nnoise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)\nnoise = jr.normal(subkey, shape=x.shape) * noise_amount\nys.append(f(x, vertical_shift, period) + noise)\ny = jnp.hstack(ys)\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\")\nplt.show()\ndef fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:\nif y.ndim == 1:\ny = y.reshape(-1, 1)\nD = gpx.Dataset(X=x, y=y)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\nposterior = (\ngpx.gps.Prior(\nmean_function=gpx.mean_functions.Constant(), kernel=gpx.kernels.RBF()\n)\n* likelihood\n)\nopt_posterior, _ = gpx.fit_scipy(\nmodel=posterior,\nobjective=gpx.objectives.ConjugateMLL(negative=True),\ntrain_data=D,\n)\nlatent_dist = opt_posterior.predict(xtest, train_data=D)\nreturn opt_posterior.likelihood(latent_dist)\nposterior_preds = [fit_gp(x, i) for i in ys]\ndef sqrtm(A: jax.Array):\nreturn jnp.real(jsl.sqrtm(A))\ndef wasserstein_barycentres(\ndistributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array\n):\ncovariances = [d.covariance() for d in distributions]\ncov_stack = jnp.stack(covariances)\nstack_sqrt = jax.vmap(sqrtm)(cov_stack)\ndef step(covariance_candidate: jax.Array, idx: None):\ninner_term = jax.vmap(sqrtm)(\njnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)\n)\nfixed_point = jnp.tensordot(weights, inner_term, axes=1)\nreturn fixed_point, fixed_point\nreturn step\nweights = jnp.ones((n_datasets,)) / n_datasets\nmeans = jnp.stack([d.mean() for d in posterior_preds])\nbarycentre_mean = jnp.tensordot(weights, means, axes=1)\nstep_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights))\ninitial_covariance = jnp.eye(n_test)\nbarycentre_covariance, sequence = jax.lax.scan(\nstep_fn, initial_covariance, jnp.arange(100)\n)\nL = jnp.linalg.cholesky(barycentre_covariance)\nbarycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L)\ndef plot(\ndist: tfd.MultivariateNormalTriL,\nax,\ncolor: str,\nlabel: str = None,\nci_alpha: float = 0.2,\nlinewidth: float = 1.0,\nzorder: int = 0,\n):\nmu = dist.mean()\nsigma = dist.stddev()\nax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)\nax.fill_between(\nxtest.squeeze(),\nmu - sigma,\nmu + sigma,\nalpha=ci_alpha,\ncolor=color,\nzorder=zorder,\n)\nfig, ax = plt.subplots()\n[plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds]\nplot(\nbarycentre_process,\nax,\ncolor=cols[0],\nlabel=\"Barycentre\",\nci_alpha=0.5,\nlinewidth=2,\nzorder=1,\n)\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#sparse-gaussian-process-regression","title":"Sparse Gaussian Process Regression","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 2500\nnoise = 0.5\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\nytest = f(xtest)\nn_inducing = 50\nz = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\nax.vlines(\nx=z,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=0.5,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend(loc=\"best\")\nplt.show()\nmeanf = gpx.mean_functions.Constant()\nkernel = gpx.kernels.RBF()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior * likelihood\nq = gpx.variational_families.CollapsedVariationalGaussian(\nposterior=posterior, inducing_inputs=z\n)\nelbo = gpx.objectives.CollapsedELBO(negative=True)\nprint(gpx.cite(elbo))\nelbo = jit(elbo)\nopt_posterior, history = gpx.fit(\nmodel=q,\nobjective=elbo,\ntrain_data=D,\noptim=ox.adamw(learning_rate=1e-2),\nnum_iters=500,\nkey=key,\n)\nfig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")\nlatent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\ninducing_points = opt_posterior.inducing_inputs\nsamples = latent_dist.sample(seed=key, sample_shape=(20,))\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\nax.plot(\nxtest,\nytest,\nlabel=\"Latent function\",\ncolor=cols[1],\nlinestyle=\"-\",\nlinewidth=1,\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nax.plot(\nxtest,\npredictive_mean + 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nax.vlines(\nx=inducing_points,\nymin=ytest.min(),\nymax=ytest.max(),\nalpha=0.3,\nlinewidth=0.5,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nplt.show()\nfull_rank_model = gpx.gps.Prior(\nmean_function=gpx.mean_functions.Zero(), kernel=gpx.kernels.RBF()\n) * gpx.likelihoods.Gaussian(num_datapoints=D.n)\nnegative_mll = jit(gpx.objectives.ConjugateMLL(negative=True).step)\n%timeit negative_mll(full_rank_model, D).block_until_ready()\nnegative_elbo = jit(gpx.objectives.CollapsedELBO(negative=True).step)\n%timeit negative_elbo(q, D).block_until_ready()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Daniel Dodd'\n</code></pre>"},{"location":"give_me_the_code/#uci-data-benchmarking","title":"UCI Data Benchmarking","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\nmean_squared_error,\nr2_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\n# Enable Float64 for more stable matrix inversions.\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ntry:\nyacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\nexcept FileNotFoundError:\nyacht = pd.read_fwf(\n\"docs/examples/data/yacht_hydrodynamics.data\", header=None\n).values[:-1, :]\nX = yacht[:, :-1]\ny = yacht[:, -1].reshape(-1, 1)\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\nlog_ytr = np.log(ytr)\nlog_yte = np.log(yte)\ny_scaler = StandardScaler().fit(log_ytr)\nscaled_ytr = y_scaler.transform(log_ytr)\nscaled_yte = y_scaler.transform(log_yte)\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\nax[0].hist(ytr, bins=30, color=cols[1])\nax[0].set_title(\"y\")\nax[1].hist(log_ytr, bins=30, color=cols[1])\nax[1].set_title(\"log(y)\")\nax[2].hist(scaled_ytr, bins=30, color=cols[1])\nax[2].set_title(\"scaled log(y)\")\nx_scaler = StandardScaler().fit(Xtr)\nscaled_Xtr = x_scaler.transform(Xtr)\nscaled_Xte = x_scaler.transform(Xte)\nn_train, n_covariates = scaled_Xtr.shape\nkernel = gpx.kernels.RBF(\nactive_dims=list(range(n_covariates)),\nvariance=np.var(scaled_ytr),\nlengthscale=0.1 * np.ones((n_covariates,)),\n)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train)\nposterior = prior * likelihood\ntraining_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\nnegative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))\nopt_posterior, history = gpx.fit_scipy(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=training_data,\n)\nlatent_dist = opt_posterior(scaled_Xte, training_data)\npredictive_dist = likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_stddev = predictive_dist.stddev()\nrmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nr2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nprint(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")\nresiduals = scaled_yte.squeeze() - predictive_mean\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\nax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\nax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\nax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\nax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\nax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\nax[1].set_ylim([-1.0, 1.0])\nax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\nax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\nax[2].set_title(\"Residuals\")\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#introduction-to-kernels","title":"Introduction to Kernels","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nfrom docs.examples.utils import clean_legend\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.typing import Array\nfrom sklearn.preprocessing import StandardScaler\nkey = jr.key(42)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkernels = [\ngpx.kernels.Matern12(),\ngpx.kernels.Matern32(),\ngpx.kernels.Matern52(),\ngpx.kernels.RBF(),\n]\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nmeanf = gpx.mean_functions.Zero()\nfor k, ax in zip(kernels, axes.ravel()):\nprior = gpx.gps.Prior(mean_function=meanf, kernel=k)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nax.plot(x, y.T, alpha=0.7)\nax.set_title(k.name)\n# Forrester function\ndef forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:\nreturn (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\nn = 13\ntraining_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1)\ntraining_y = forrester(training_x)\nD = gpx.Dataset(X=training_x, y=training_y)\ntest_x = jnp.linspace(0, 1, 100).reshape(-1, 1)\ntest_y = forrester(test_x)\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\nlengthscale=jnp.array(0.1)\n)  # Initialise our kernel lengthscale to 0.1\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nlikelihood = gpx.likelihoods.Gaussian(\nnum_datapoints=D.n, obs_stddev=jnp.array(1e-3)\n)  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value\nlikelihood = likelihood.replace_trainable(obs_stddev=False)\nno_opt_posterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(no_opt_posterior, train_data=D)\nopt_posterior, history = gpx.fit_scipy(\nmodel=no_opt_posterior,\nobjective=negative_mll,\ntrain_data=D,\n)\ndef plot_ribbon(ax, x, dist, color):\nmean = dist.mean()\nstd = dist.stddev()\nax.plot(x, mean, label=\"Predictive mean\", color=color)\nax.fill_between(\nx.squeeze(),\nmean - 2 * std,\nmean + 2 * std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=color,\n)\nax.plot(x, mean - 2 * std, linestyle=\"--\", linewidth=1, color=color)\nax.plot(x, mean + 2 * std, linestyle=\"--\", linewidth=1, color=color)\nopt_latent_dist = opt_posterior.predict(test_x, train_data=D)\nopt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)\nopt_predictive_mean = opt_predictive_dist.mean()\nopt_predictive_std = opt_predictive_dist.stddev()\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6))\nax1.plot(\ntest_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5)\nplot_ribbon(ax1, test_x, opt_predictive_dist, color=cols[1])\nax1.set_title(\"Posterior with Hyperparameter Optimisation\")\nax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nno_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D)\nno_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)\nax2.plot(\ntest_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5)\nplot_ribbon(ax2, test_x, no_opt_predictive_dist, color=cols[1])\nax2.set_title(\"Posterior without Hyperparameter Optimisation\")\nax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nno_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale\nno_opt_variance = no_opt_posterior.prior.kernel.variance\nopt_lengthscale = opt_posterior.prior.kernel.lengthscale\nopt_variance = opt_posterior.prior.kernel.variance\nprint(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\")\nprint(\nf\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\"\n)\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Periodic()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Periodic Kernel\")\nplt.show()\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Linear()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Linear Kernel\")\nplt.show()\nkernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\")\nplt.show()\nkernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\")\nplt.show()\nco2_data = pd.read_csv(\n\"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\"\n)\nco2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12]\ntrain_x = co2_data[\"decimal date\"].values[:, None]\ntrain_y = co2_data[\"average\"].values[:, None]\nfig, ax = plt.subplots()\nax.plot(train_x, train_y)\nax.set_title(\"CO2 Concentration in the Atmosphere\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO2 Concentration (ppm)\")\nplt.show()\ntest_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1)\ny_scaler = StandardScaler().fit(train_y)\nstandardised_train_y = y_scaler.transform(train_y)\nD = gpx.Dataset(X=train_x, y=standardised_train_y)\nmean = gpx.mean_functions.Zero()\nrbf_kernel = gpx.kernels.RBF(lengthscale=100.0)\nperiodic_kernel = gpx.kernels.Periodic()\nlinear_kernel = gpx.kernels.Linear(variance=0.001)\nsum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel])\nfinal_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])\nprior = gpx.gps.Prior(mean_function=mean, kernel=final_kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=D,\noptim=ox.adamw(learning_rate=1e-2),\nnum_iters=500,\nkey=key,\n)\nlatent_dist = opt_posterior.predict(test_x, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean().reshape(-1, 1)\npredictive_std = predictive_dist.stddev().reshape(-1, 1)\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\ntrain_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5\n)\nax.fill_between(\ntest_x.squeeze(),\npredictive_mean.squeeze() - 2 * predictive_std.squeeze(),\npredictive_mean.squeeze() + 2 * predictive_std.squeeze(),\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\ntest_x,\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\ntest_x,\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\ntrain_x[train_x &gt;= 2010],\nstandardised_train_y[train_x &gt;= 2010],\n\"x\",\nlabel=\"Observations\",\ncolor=cols[0],\nalpha=0.5,\n)\nax.fill_between(\ntest_x[test_x &gt;= 2010].squeeze(),\npredictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\ntest_x[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\ntest_x[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\ntest_x[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010],\nlabel=\"Predictive mean\",\ncolor=cols[1],\n)\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nprint(\n\"Periodic Kernel Period:\"\nf\" {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\"\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</code></pre>"},{"location":"give_me_the_code/#introduction-to-decision-making-with-gpjax","title":"Introduction to Decision Making with GPJax","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport gpjax as gpx\nfrom gpjax.decision_making.utility_functions import (\nThompsonSampling,\n)\nfrom gpjax.decision_making.utility_maximizer import (\nContinuousSinglePointUtilityMaximizer,\n)\nfrom gpjax.decision_making.decision_maker import UtilityDrivenDecisionMaker\nfrom gpjax.decision_making.utils import (\nOBJECTIVE,\nbuild_function_evaluator,\n)\nfrom gpjax.decision_making.posterior_handler import PosteriorHandler\nfrom gpjax.decision_making.search_space import ContinuousSearchSpace\nfrom gpjax.typing import (\nArray,\nFloat,\n)\nkey = jr.key(42)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ndef forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:\nreturn (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\nfunction_evaluator = build_function_evaluator({OBJECTIVE: forrester})\nlower_bounds = jnp.array([0.0])\nupper_bounds = jnp.array([1.0])\nsearch_space = ContinuousSearchSpace(\nlower_bounds=lower_bounds, upper_bounds=upper_bounds\n)\ninitial_x = search_space.sample(5, key)\ninitial_datasets = function_evaluator(initial_x)\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nlikelihood_builder = lambda n: gpx.likelihoods.Gaussian(\nnum_datapoints=n, obs_stddev=jnp.array(1e-3)\n)\nposterior_handler = PosteriorHandler(\nprior,\nlikelihood_builder=likelihood_builder,\noptimization_objective=gpx.objectives.ConjugateMLL(negative=True),\noptimizer=ox.adam(learning_rate=0.01),\nnum_optimization_iters=1000,\n)\nposterior_handlers = {OBJECTIVE: posterior_handler}\nutility_function_builder = ThompsonSampling(num_features=500)\nacquisition_maximizer = ContinuousSinglePointUtilityMaximizer(\nnum_initial_samples=100, num_restarts=1\n)\ndef plot_bo_iteration(\ndm: UtilityDrivenDecisionMaker, last_queried_points: Float[Array, \"B D\"]\n):\nposterior = dm.posteriors[OBJECTIVE]\ndataset = dm.datasets[OBJECTIVE]\nplt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\nforrester_y = forrester(plt_x.squeeze(axis=-1))\nutility_fn = dm.current_utility_functions[0]\nsample_y = -utility_fn(plt_x)\nlatent_dist = posterior.predict(plt_x, train_data=dataset)\npredictive_dist = posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(plt_x.squeeze(), predictive_mean, label=\"Predictive Mean\", color=cols[1])\nax.fill_between(\nplt_x.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\nplt_x.squeeze(),\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nplt_x.squeeze(),\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(plt_x.squeeze(), sample_y, label=\"Posterior Sample\")\nax.plot(\nplt_x.squeeze(),\nforrester_y,\nlabel=\"Forrester Function\",\ncolor=cols[0],\nlinestyle=\"--\",\nlinewidth=2,\n)\nax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\nax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\nax.scatter(\nlast_queried_points[0],\n-utility_fn(last_queried_points[0][None, ...]),\nlabel=\"Posterior Sample Optimum\",\nmarker=\"*\",\ncolor=cols[3],\nzorder=3,\n)\nax.legend(loc=\"center left\", bbox_to_anchor=(0.950, 0.5))\nplt.show()\ndm = UtilityDrivenDecisionMaker(\nsearch_space=search_space,\nposterior_handlers=posterior_handlers,\ndatasets=initial_datasets,\nutility_function_builder=utility_function_builder,\nutility_maximizer=acquisition_maximizer,\nbatch_size=1,\nkey=key,\npost_ask=[plot_bo_iteration],\npost_tell=[],\n)\nresults = dm.run(\n6,\nblack_box_function_evaluator=function_evaluator,\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</code></pre>"},{"location":"give_me_the_code/#classification","title":"Classification","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nfrom time import time\nimport blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nimport jax.tree_util as jtu\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom tqdm import trange\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\ntfd = tfp.distributions\nidentity_matrix = jnp.eye\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey, subkey = jr.split(key)\nx = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0)\ny = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.scatter(x, y)\nkernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Constant()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\nposterior = prior * likelihood\nprint(type(posterior))\nnegative_lpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=True))\noptimiser = ox.adam(learning_rate=0.01)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_lpd,\ntrain_data=D,\noptim=ox.adamw(learning_rate=0.01),\nnum_iters=1000,\nkey=key,\n)\nmap_latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(map_latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - predictive_std,\npredictive_mean + predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"One sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\npredictive_mean + predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\nimport cola\nfrom gpjax.lower_cholesky import lower_cholesky\ngram, cross_covariance = (kernel.gram, kernel.cross_covariance)\njitter = 1e-6\n# Compute (latent) function value map estimates at training points:\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nKxx = cola.PSD(Kxx)\nLx = lower_cholesky(Kxx)\nf_hat = Lx @ opt_posterior.latent\n# Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f):\nH = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]\nL = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)\n# H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I\nL_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True)\nH_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\nLH = jnp.linalg.cholesky(H_inv)\nlaplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH)\ndef construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:\nmap_latent_dist = opt_posterior.predict(xtest, train_data=D)\nKxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nKxx = cola.PSD(Kxx)\n# Kxx\u207b\u00b9 Kxt\nKxx_inv_Kxt = cola.solve(Kxx, Kxt)\n# Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt\nlaplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Kxt.T, H_inv), Kxx_inv_Kxt)\nmean = map_latent_dist.mean()\ncovariance = map_latent_dist.covariance() + laplace_cov_term\nL = jnp.linalg.cholesky(covariance)\nreturn tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L)\nlaplace_latent_dist = construct_laplace(xtest)\npredictive_dist = opt_posterior.likelihood(laplace_latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - predictive_std,\npredictive_mean + predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"One sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\npredictive_mean + predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\nnum_adapt = 500\nnum_samples = 500\nlpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\nadapt = blackjax.window_adaptation(\nblackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n# Initialise the chain\nstart = time()\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\nprint(f\"Adaption time taken: {time() - start: .1f} seconds\")\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\ndef one_step(state, rng_key):\nstate, info = kernel(rng_key, state)\nreturn state, (state, info)\nkeys = jax.random.split(rng_key, num_samples)\n_, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\nreturn states, infos\n# Sample from the posterior distribution\nstart = time()\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nprint(f\"Sampling time taken: {time() - start: .1f} seconds\")\nacceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\nfig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.prior.kernel.lengthscale)\nax1.plot(states.position.prior.kernel.variance)\nax2.plot(states.position.latent[:, 1, :])\nax0.set_title(\"Kernel Lengthscale\")\nax1.set_title(\"Kernel Variance\")\nax2.set_title(\"Latent Function (index = 1)\")\nthin_factor = 20\nposterior_samples = []\nfor i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):\nsample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)\nsample = sample.constrain()\nlatent_dist = sample.predict(xtest, train_data=D)\npredictive_dist = sample.likelihood(latent_dist)\nposterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\nposterior_samples = jnp.vstack(posterior_samples)\nlower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(posterior_samples, axis=0)\nfig, ax = plt.subplots()\nax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7)\nax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1)\nax.fill_between(\nxtest.flatten(),\nlower_ci.flatten(),\nupper_ci.flatten(),\nalpha=0.2,\ncolor=cols[1],\nlabel=\"95\\\\% CI\",\n)\nax.plot(\nxtest,\nlower_ci.flatten(),\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\nupper_ci.flatten(),\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\n%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\"\n</code></pre>"},{"location":"give_me_the_code/#graph-kernels","title":"Graph Kernels","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport random\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport optax as ox\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.key(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nvertex_per_side = 20\nn_edges_to_remove = 30\np = 0.8\nG = nx.barbell_graph(vertex_per_side, 0)\nrandom.seed(123)\n[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\npos = nx.spring_layout(G, seed=123)  # positions for all nodes\nnx.draw(\nG, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n)\nL = nx.laplacian_matrix(G).toarray()\nx = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\ntrue_kernel = gpx.kernels.GraphKernel(\nlaplacian=L,\nlengthscale=2.3,\nvariance=3.2,\nsmoothness=6.1,\n)\nprior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=true_kernel)\nfx = prior(x)\ny = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)\nD = gpx.Dataset(X=x, y=y)\nnx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\nvmin, vmax = y.min(), y.max()\nsm = plt.cm.ScalarMappable(\ncmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\nax = plt.gca()\ncbar = plt.colorbar(sm, ax=ax)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nkernel = gpx.kernels.GraphKernel(laplacian=L)\nprior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=kernel)\nposterior = prior * likelihood\nprint(gpx.cite(kernel))\nopt_posterior, training_history = gpx.fit_scipy(\nmodel=posterior,\nobjective=gpx.objectives.ConjugateMLL(negative=True),\ntrain_data=D,\n)\ninitial_dist = likelihood(posterior(x, D))\npredictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\ninitial_mean = initial_dist.mean()\nlearned_mean = predictive_dist.mean()\nrmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\ninitial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\nlearned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\nprint(\nf\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\nf\" {learned_rmse: .2f}\"\n)\nerror = jnp.abs(learned_mean - y.squeeze())\nnx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\nvmin, vmax = error.min(), error.max()\nsm = plt.cm.ScalarMappable(\ncmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nax = plt.gca()\ncbar = plt.colorbar(sm, ax=ax)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#likelihood-guide","title":"Likelihood guide","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport gpjax as gpx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey = jr.key(123)\nn = 50\nx = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0)\nxtest = jnp.linspace(-3, 3, 100)[:, None]\nf = lambda x: jnp.sin(x)\ny = f(x) + 0.1 * jr.normal(key, shape=x.shape)\nD = gpx.Dataset(x, y)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\")\nax.plot(x, f(x), label=\"Latent function\")\nax.legend()\ngpx.likelihoods.Gaussian(num_datapoints=D.n)\ngpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.5)\nkernel = gpx.kernels.Matern32()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(kernel=kernel, mean_function=meanf)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.1)\nposterior = prior * likelihood\nlatent_dist = posterior.predict(xtest, D)\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\nfor ax in axes.ravel():\nsubkey, _ = jr.split(subkey)\nax.plot(\nlatent_dist.sample(sample_shape=(1,), seed=subkey).T,\nlw=1,\ncolor=cols[0],\nlabel=\"Latent samples\",\n)\nax.plot(\nlikelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n\"o\",\nmarkersize=5,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Predictive samples\",\n)\nlikelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\nfor ax in axes.ravel():\nsubkey, _ = jr.split(subkey)\nax.plot(\nlatent_dist.sample(sample_shape=(1,), seed=subkey).T,\nlw=1,\ncolor=cols[0],\nlabel=\"Latent samples\",\n)\nax.plot(\nlikelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n\"o\",\nmarkersize=3,\nalpha=0.5,\ncolor=cols[1],\nlabel=\"Predictive samples\",\n)\nz = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1)\nq = gpx.variational_families.VariationalGaussian(posterior=posterior, inducing_inputs=z)\ndef q_moments(x):\nqx = q(x)\nreturn qx.mean(), qx.variance()\nmean, variance = jax.vmap(q_moments)(x[:, None])\njnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance))\nlquad = gpx.likelihoods.Gaussian(\nnum_datapoints=D.n,\nobs_stddev=jnp.array([0.1]),\nintegrator=gpx.integrators.GHQuadratureIntegrator(num_points=20),\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-version","title":"Stable version","text":"<p>The latest stable release of <code>GPJax</code> can be installed via <code>pip</code>:</p> <pre><code>pip install gpjax\n</code></pre> <p>Check your installation</p> <p>We recommend you check your installation version: <pre><code>python -c 'import gpjax; print(gpjax.__version__)'\n</code></pre></p>"},{"location":"installation/#gputpu-support","title":"GPU/TPU support","text":"<p>Fancy using GPJax on GPU/TPU? Then you'll need to install JAX with the relevant hardware acceleration support as detailed in the JAX installation guide.</p>"},{"location":"installation/#development-version","title":"Development version","text":"<p>Warning</p> <p>This version is possibly unstable and may contain bugs.</p> <p>The latest development version of <code>GPJax</code> can be installed via running following:</p> <pre><code>git clone https://github.com/thomaspinder/GPJax.git\ncd GPJax\npoetry install\n</code></pre> <p>Tip</p> <p>We advise you create virtual environment before installing:</p> <pre><code>conda create -n gpjax_experimental python=3.10.0\nconda activate gpjax_experimental\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>poetry run pytest tests/\n</code></pre>"},{"location":"sharp_bits/","title":"\ud83d\udd2a Sharp bits","text":""},{"location":"sharp_bits/#the-sharp-bits","title":"\ud83d\udd2a The sharp bits","text":""},{"location":"sharp_bits/#pseudo-randomness","title":"Pseudo-randomness","text":"<p>Libraries like NumPy and Scipy use stateful pseudorandom number generators (PRNGs). However, the PRNG in JAX is stateless. This means that for a given function, the return always returns the same result unless the seed is changed. This is a good thing, but it means that we need to be careful when using JAX's PRNGs.</p> <p>To examine what it means for a PRNG to be stateful, consider the following example:</p> <p><pre><code>import numpy as np\nimport jax.random as jr\nkey = jr.key(123)\n# NumPy\nprint('NumPy:')\nprint(np.random.random())\nprint(np.random.random())\nprint('\\nJAX:')\nprint(jr.uniform(key))\nprint(jr.uniform(key))\nprint('\\nSplitting key')\nkey, subkey = jr.split(key)\nprint(jr.uniform(subkey))\n</code></pre> <pre><code>NumPy:\n0.5194454541172852\n0.9815886617924413\nJAX:\n0.95821166\n0.95821166\nSplitting key\n0.23886406\n</code></pre> We can see that, in libraries like NumPy, the PRNG key's state is incremented whenever a pseudorandom call is made. This can make debugging difficult to manage as it is not always clear when a PRNG is being used. In JAX, the PRNG key is not incremented, so the same key will always return the same result. This has further positive benefits for reproducibility.</p> <p>GPJax relies on JAX's PRNGs for all random number generation. Whilst we try wherever possible to handle the PRNG key's state for you, care must be taken when defining your own models and inference schemes to ensure that the PRNG key is handled correctly. The JAX documentation has an excellent section on this.</p>"},{"location":"sharp_bits/#bijectors","title":"Bijectors","text":"<p>Parameters such as the kernel's lengthscale or variance have their support defined on a constrained subset of the real-line. During gradient-based optimisation, as we approach the set's boundary, it becomes possible that we could step outside of the set's support and introduce a numerical and mathematical error into our model. For example, consider the lengthscale parameter \u2113\\ell\u2113, which we know must be strictly positive. If at ttht^{\\text{th}}tth iterate, our current estimate of \u2113\\ell\u2113 was 0.02 and our derivative informed us that \u2113\\ell\u2113 should decrease, then if our learning rate is greater is than 0.03, we would end up with a negative variance term. We visualise this issue below where the red cross denotes the invalid lengthscale value that would be obtained, were we to optimise in the unconstrained parameter space.</p> <p></p> <p>A simple but impractical solution would be to use a tiny learning rate which would reduce the possibility of stepping outside of the parameter's support. However, this would be incredibly costly and does not eradicate the problem. An alternative solution is to apply a functional mapping to the parameter that projects it from a constrained subspace of the real-line onto the entire real-line. Here, gradient updates are applied in the unconstrained parameter space before transforming the value back to the original support of the parameters. Such a transformation is known as a bijection.</p> <p></p> <p>To help understand this, we show the effect of using a log-exp bijector in the above figure. We have six points on the positive real line that range from 0.1 to 3 depicted by a blue cross. We then apply the bijector by log-transforming the constrained value. This gives us the points' unconstrained value which we depict by a red circle. It is this value that we apply gradient updates to. When we wish to recover the constrained value, we apply the inverse of the bijector, which is the exponential function in this case. This gives us back the blue cross.</p> <p>In GPJax, we supply bijective functions using Tensorflow Probability. In our PyTrees doc document, we detail how the user can define their own bijectors and attach them to the parameter(s) of their model.</p>"},{"location":"sharp_bits/#positive-definiteness","title":"Positive-definiteness","text":"<p>\"Symmetric positive definiteness is one of the highest accolades to which a matrix can aspire\" - Nicholas Highman, Accuracy and stability of numerical algorithms (Higham, 2002)<sup>1</sup></p>"},{"location":"sharp_bits/#why-is-positive-definiteness-important","title":"Why is positive-definiteness important?","text":"<p>The Gram matrix of a kernel, a concept that we explore more in our kernels notebook and our PyTree notebook, is a symmetric positive definite matrix. As such, we have a range of tools at our disposal to make subsequent operations on the covariance matrix faster. One of these tools is the Cholesky factorisation that uniquely decomposes any symmetric positive-definite matrix \u03a3\\mathbf{\\Sigma}\u03a3 by</p> <p><p>\u03a3=LL\u22a4 , \\begin{align}     \\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\top}\\,, \\end{align} \u03a3=LL\u22a4,\u200b\u200b</p> where L\\mathbf{L}L is a lower triangular matrix.</p> <p>We make use of this result in GPJax when solving linear systems of equations of the form Ax=b\\mathbf{A}\\boldsymbol{x} = \\boldsymbol{b}Ax=b. Whilst seemingly abstract at first, such problems are frequently encountered when constructing Gaussian process models. One such example is frequently encountered in the regression setting for learning Gaussian process kernel hyperparameters. Here we have labels y\u223cN(f(x),\u03c32I)\\boldsymbol{y} \\sim \\mathcal{N}(f(\\boldsymbol{x}), \\sigma^2\\mathbf{I})y\u223cN(f(x),\u03c32I) with f(x)\u223cN(0,Kxx)f(\\boldsymbol{x}) \\sim \\mathcal{N}(\\boldsymbol{0}, \\mathbf{K}_{\\boldsymbol{xx}})f(x)\u223cN(0,Kxx\u200b) arising from zero-mean Gaussian process prior and Gram matrix Kxx\\mathbf{K}_{\\boldsymbol{xx}}Kxx\u200b at the inputs x\\boldsymbol{x}x. Here the marginal log-likelihood comprises the following form</p> <p>log\u2061p(y)=0.5(\u2212y\u22a4(Kxx+\u03c32I)\u22121y\u2212log\u2061\u2223Kxx+\u03c32I\u2223\u2212nlog\u2061(2\u03c0)), \\begin{align}     \\log p(\\boldsymbol{y}) = 0.5\\left(-\\boldsymbol{y}^{\\top}\\left(\\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I} \\right)^{-1}\\boldsymbol{y} -\\log\\lvert \\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I}\\rvert -n\\log(2\\pi)\\right) , \\end{align} logp(y)=0.5(\u2212y\u22a4(Kxx\u200b+\u03c32I)\u22121y\u2212log\u2223Kxx\u200b+\u03c32I\u2223\u2212nlog(2\u03c0)),\u200b\u200b</p> <p>and the goal of inference is to maximise kernel hyperparameters (contained in the Gram matrix Kxx\\mathbf{K}_{\\boldsymbol{xx}}Kxx\u200b) and likelihood hyperparameters (contained in the noise covariance \u03c32I\\sigma^2\\mathbf{I}\u03c32I). Computing the marginal log-likelihood (and its gradients), draws our attention to the term</p> <p>(Kxx+\u03c32I)\u22121\u23dfAy, \\begin{align}     \\underbrace{\\left(\\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I} \\right)^{-1}}_{\\mathbf{A}}\\boldsymbol{y}, \\end{align} A(Kxx\u200b+\u03c32I)\u22121\u200b\u200by,\u200b\u200b</p> <p>then we can see a solution can be obtained by solving the corresponding system of equations. By working with L=chol\u2061A\\mathbf{L} = \\operatorname{chol}{\\mathbf{A}}L=cholA instead of A\\mathbf{A}A, we save a significant amount of floating-point operations (flops) by solving two triangular systems of equations (one for L\\mathbf{L}L and another for L\u22a4\\mathbf{L}^{\\top}L\u22a4) instead of one dense system of equations. Solving two triangular systems of equations has complexity O(n3/6)\\mathcal{O}(n^3/6)O(n3/6); a vast improvement compared to regular solvers that have O(n3)\\mathcal{O}(n^3)O(n3) complexity in the number of datapoints nnn.</p>"},{"location":"sharp_bits/#the-cholesky-drawback","title":"The Cholesky drawback","text":"<p>While the computational acceleration provided by using Cholesky factors instead of dense matrices is hopefully now apparent, an awkward numerical instability gotcha can arise due to floating-point rounding errors. When we evaluate a covariance function on a set of points that are very close to one another, eigenvalues of the corresponding Gram matrix can get very small. While not mathematically less than zero, the smallest eigenvalues can become negative-valued due to finite-precision numerical errors. This becomes a problem when we want to compute a Cholesky factor since this requires that the input matrix is numerically positive-definite. If there are negative eigenvalues, this violates the requirements and results in a \"Cholesky failure\".</p> <p>To resolve this, we apply some numerical jitter to the diagonals of any Gram matrix. Typically this is very small, with 10\u2212610^{-6}10\u22126 being the system default. However, for some problems, this amount may need to be increased.</p>"},{"location":"sharp_bits/#slow-to-evaluate","title":"Slow-to-evaluate","text":"<p>Famously, a regular Gaussian process model (as detailed in our regression notebook) will scale cubically in the number of data points. Consequently, if you try to fit your Gaussian process model to a data set containing more than several thousand data points, then you will likely incur a significant computational overhead. In such cases, we recommend using Sparse Gaussian processes to alleviate this issue.</p> <p>When the data contains less than around 50000 data points, we recommend using the collapsed evidence lower bound objective (Titsias, 2009)<sup>2</sup> to optimise the parameters of your sparse Gaussian process model. Such a model will scale linearly in the number of data points and quadratically in the number of inducing points. We demonstrate its use in our sparse regression notebook.</p> <p>For data sets exceeding 50000 data points, even the sparse Gaussian process outlined above will become computationally infeasible. In such cases, we recommend using the uncollapsed evidence lower bound objective (Hensman et al., 2013)<sup>3</sup> that allows stochastic mini-batch optimisation of the parameters of your sparse Gaussian process model. Such a model will scale linearly in the batch size and quadratically in the number of inducing points. We demonstrate its use in our sparse stochastic variational inference notebook.</p> <ol> <li> <p>Higham, N. J. (2002) Accuracy and Stability of Numerical Algorithms. Second. Society for Industrial and Applied Mathematics. DOI: 10.1137/1.9780898718027.\u00a0\u21a9</p> </li> <li> <p>Titsias, M. (2009) Variational learning of inducing variables in sparse Gaussian processes. In: Proceedings of the twelth international conference on artificial intelligence and statistics, 2009, pp. 567\u2013574. Proceedings of machine learning research. PMLR.\u00a0\u21a9</p> </li> <li> <p>Hensman, J., Fusi, N. and Lawrence, N. D. (2013) Gaussian processes for big data. Artificial intelligence and statistics.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>GPJax<ul> <li>Base<ul> <li>Module</li> <li>Param</li> </ul> </li> <li>Citation</li> <li>Dataset</li> <li>Decision Making<ul> <li>Decision Maker</li> <li>Posterior Handler</li> <li>Search Space</li> <li>Test Functions<ul> <li>Continuous Functions</li> <li>Non Conjugate Functions</li> </ul> </li> <li>Utility Functions<ul> <li>Base</li> <li>Thompson Sampling</li> </ul> </li> <li>Utility Maximizer</li> <li>Utils</li> </ul> </li> <li>Distributions</li> <li>Fit</li> <li>GPs</li> <li>Integrators</li> <li>Kernels<ul> <li>Approximations<ul> <li>Rff</li> </ul> </li> <li>Base</li> <li>Computations<ul> <li>Base</li> <li>Basis Functions</li> <li>Constant Diagonal</li> <li>Dense</li> <li>Diagonal</li> <li>Eigen</li> </ul> </li> <li>Non Euclidean<ul> <li>Graph</li> <li>Utils</li> </ul> </li> <li>Nonstationary<ul> <li>Arccosine</li> <li>Linear</li> <li>Polynomial</li> </ul> </li> <li>Stationary<ul> <li>Mat\u00e9rn12</li> <li>Mat\u00e9rn32</li> <li>Mat\u00e9rn52</li> <li>Periodic</li> <li>Powered Exponential</li> <li>Rational Quadratic</li> <li>RBF</li> <li>Utils</li> <li>White</li> </ul> </li> </ul> </li> <li>Likelihoods</li> <li>Lower Cholesky</li> <li>Mean Functions</li> <li>Objectives</li> <li>Progress Bar</li> <li>Scan</li> <li>Typing</li> <li>Variational Families</li> </ul> </li> </ul>"},{"location":"api/citation/","title":"Citation","text":""},{"location":"api/citation/#gpjax.citation","title":"<code>gpjax.citation</code>","text":""},{"location":"api/citation/#gpjax.citation.CitationType","title":"<code>CitationType = Union[None, str, Dict[str, str]]</code>  <code>module-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation","title":"<code>AbstractCitation</code>  <code>dataclass</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.citation_key","title":"<code>citation_key: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.authors","title":"<code>authors: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.title","title":"<code>title: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.year","title":"<code>year: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.__init__","title":"<code>__init__(citation_key: Union[str, None] = None, authors: Union[str, None] = None, title: Union[str, None] = None, year: Union[str, None] = None) -&gt; None</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.as_str","title":"<code>as_str() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation","title":"<code>NullCitation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.NullCitation.citation_key","title":"<code>citation_key: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.authors","title":"<code>authors: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.title","title":"<code>title: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.year","title":"<code>year: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.as_str","title":"<code>as_str() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.__init__","title":"<code>__init__(citation_key: Union[str, None] = None, authors: Union[str, None] = None, title: Union[str, None] = None, year: Union[str, None] = None) -&gt; None</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation","title":"<code>PhDThesisCitation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.citation_key","title":"<code>citation_key: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.authors","title":"<code>authors: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.title","title":"<code>title: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.year","title":"<code>year: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.school","title":"<code>school: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.institution","title":"<code>institution: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.citation_type","title":"<code>citation_type: CitationType = 'phdthesis'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.as_str","title":"<code>as_str() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.__init__","title":"<code>__init__(citation_key: Union[str, None] = None, authors: Union[str, None] = None, title: Union[str, None] = None, year: Union[str, None] = None, school: Union[str, None] = None, institution: Union[str, None] = None, citation_type: CitationType = 'phdthesis') -&gt; None</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation","title":"<code>PaperCitation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.PaperCitation.citation_key","title":"<code>citation_key: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.authors","title":"<code>authors: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.title","title":"<code>title: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.year","title":"<code>year: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.booktitle","title":"<code>booktitle: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.citation_type","title":"<code>citation_type: CitationType = 'inproceedings'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.as_str","title":"<code>as_str() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.__init__","title":"<code>__init__(citation_key: Union[str, None] = None, authors: Union[str, None] = None, title: Union[str, None] = None, year: Union[str, None] = None, booktitle: Union[str, None] = None, citation_type: CitationType = 'inproceedings') -&gt; None</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation","title":"<code>BookCitation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.BookCitation.citation_key","title":"<code>citation_key: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.authors","title":"<code>authors: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.title","title":"<code>title: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.year","title":"<code>year: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.publisher","title":"<code>publisher: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.volume","title":"<code>volume: Union[str, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.citation_type","title":"<code>citation_type: CitationType = 'book'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.as_str","title":"<code>as_str() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.__init__","title":"<code>__init__(citation_key: Union[str, None] = None, authors: Union[str, None] = None, title: Union[str, None] = None, year: Union[str, None] = None, publisher: Union[str, None] = None, volume: Union[str, None] = None, citation_type: CitationType = 'book') -&gt; None</code>","text":""},{"location":"api/citation/#gpjax.citation.cite","title":"<code>cite(tree) -&gt; AbstractCitation</code>","text":""},{"location":"api/citation/#gpjax.citation._","title":"<code>_(tree) -&gt; PaperCitation</code>","text":""},{"location":"api/dataset/","title":"Dataset","text":""},{"location":"api/dataset/#gpjax.dataset","title":"<code>gpjax.dataset</code>","text":""},{"location":"api/dataset/#gpjax.dataset.__all__","title":"<code>__all__ = ['Dataset']</code>  <code>module-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset","title":"<code>Dataset</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Pytree</code></p> <p>Base class for datasets.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset--attributes","title":"Attributes","text":"<pre><code>X (Optional[Num[Array, \"N D\"]]): input data.\ny (Optional[Num[Array, \"N Q\"]]): output data.\n</code></pre>"},{"location":"api/dataset/#gpjax.dataset.Dataset.X","title":"<code>X: Optional[Num[Array, 'N D']] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.y","title":"<code>y: Optional[Num[Array, 'N Q']] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>Number of observations.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.in_dim","title":"<code>in_dim: int</code>  <code>property</code>","text":"<p>Dimension of the inputs, XXX.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__init__","title":"<code>__init__(X: Optional[Num[Array, 'N D']] = None, y: Optional[Num[Array, 'N Q']] = None) -&gt; None</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Checks that the shapes of XXX and yyy are compatible, and provides warnings regarding the precision of XXX and yyy.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Returns a string representation of the dataset.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_supervised","title":"<code>is_supervised() -&gt; bool</code>","text":"<p>Returns <code>True</code> if the dataset is supervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_unsupervised","title":"<code>is_unsupervised() -&gt; bool</code>","text":"<p>Returns <code>True</code> if the dataset is unsupervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__add__","title":"<code>__add__(other: Dataset) -&gt; Dataset</code>","text":"<p>Combine two datasets. Right hand dataset is stacked beneath the left.</p>"},{"location":"api/distributions/","title":"Distributions","text":""},{"location":"api/distributions/#gpjax.distributions","title":"<code>gpjax.distributions</code>","text":""},{"location":"api/distributions/#gpjax.distributions.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/distributions/#gpjax.distributions.DistrT","title":"<code>DistrT = TypeVar('DistrT', bound=tfd.Distribution)</code>  <code>module-attribute</code>","text":""},{"location":"api/distributions/#gpjax.distributions.__all__","title":"<code>__all__ = ['GaussianDistribution']</code>  <code>module-attribute</code>","text":""},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>             Bases: <code>Distribution</code></p> <p>Multivariate Gaussian distribution with a linear operator scale matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>Optional[Float[Array, ' N']]</code> <p>The mean of the distribution. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Optional[LinearOperator]</code> <p>The scale matrix of the distribution. Defaults to None.</p> <code>None</code>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate Gaussian distribution with a linear operator scale matrix.\n</code></pre>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.loc","title":"<code>loc = loc</code>  <code>instance-attribute</code>","text":""},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.scale","title":"<code>scale = cola.PSD(scale)</code>  <code>instance-attribute</code>","text":""},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.event_shape","title":"<code>event_shape: Tuple</code>  <code>property</code>","text":"<p>Returns the event shape.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.__init__","title":"<code>__init__(loc: Optional[Float[Array, ' N']] = None, scale: Optional[LinearOperator] = None) -&gt; None</code>","text":"<p>Initialises the distribution.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.mean","title":"<code>mean() -&gt; Float[Array, ' N']</code>","text":"<p>Calculates the mean.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.median","title":"<code>median() -&gt; Float[Array, ' N']</code>","text":"<p>Calculates the median.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.mode","title":"<code>mode() -&gt; Float[Array, ' N']</code>","text":"<p>Calculates the mode.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.covariance","title":"<code>covariance() -&gt; Float[Array, 'N N']</code>","text":"<p>Calculates the covariance matrix.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.variance","title":"<code>variance() -&gt; Float[Array, ' N']</code>","text":"<p>Calculates the variance.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.stddev","title":"<code>stddev() -&gt; Float[Array, ' N']</code>","text":"<p>Calculates the standard deviation.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.entropy","title":"<code>entropy() -&gt; ScalarFloat</code>","text":"<p>Calculates the entropy of the distribution.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.log_prob","title":"<code>log_prob(y: Float[Array, ' N']) -&gt; ScalarFloat</code>","text":"<p>Calculates the log pdf of the multivariate Gaussian.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Optional[Float[Array, ' N']]</code> <p>the value of which to calculate the log probability.</p> required"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.log_prob--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log probability of the value.\n</code></pre>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.sample","title":"<code>sample(seed: KeyArray, sample_shape: Tuple[int, ...])</code>","text":"<p>See <code>Distribution.sample</code>.</p>"},{"location":"api/distributions/#gpjax.distributions.GaussianDistribution.kl_divergence","title":"<code>kl_divergence(other: GaussianDistribution) -&gt; ScalarFloat</code>","text":""},{"location":"api/fit/","title":"Fit","text":""},{"location":"api/fit/#gpjax.fit","title":"<code>gpjax.fit</code>","text":""},{"location":"api/fit/#gpjax.fit.ModuleModel","title":"<code>ModuleModel = TypeVar('ModuleModel', bound=Module)</code>  <code>module-attribute</code>","text":""},{"location":"api/fit/#gpjax.fit.__all__","title":"<code>__all__ = ['fit', 'get_batch']</code>  <code>module-attribute</code>","text":""},{"location":"api/fit/#gpjax.fit.fit","title":"<code>fit(*, model: ModuleModel, objective: Union[AbstractObjective, Callable[[ModuleModel, Dataset], ScalarFloat]], train_data: Dataset, optim: ox.GradientTransformation, key: KeyArray, num_iters: Optional[int] = 100, batch_size: Optional[int] = -1, log_rate: Optional[int] = 10, verbose: Optional[bool] = True, unroll: Optional[int] = 1, safe: Optional[bool] = True) -&gt; Tuple[ModuleModel, Array]</code>","text":"<p>Train a Module model with respect to a supplied Objective function. Optimisers used here should originate from Optax.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; import optax as ox\n&gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (1) Create a dataset:\n&gt;&gt;&gt; X = jnp.linspace(0.0, 10.0, 100)[:, None]\n&gt;&gt;&gt; y = 2.0 * X + 1.0 + 10 * jr.normal(jr.key(0), X.shape)\n&gt;&gt;&gt; D = gpx.Dataset(X, y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (2) Define your model:\n&gt;&gt;&gt; class LinearModel(gpx.base.Module):\nweight: float = gpx.base.param_field()\nbias: float = gpx.base.param_field()\ndef __call__(self, x):\nreturn self.weight * x + self.bias\n&gt;&gt;&gt; model = LinearModel(weight=1.0, bias=1.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (3) Define your loss function:\n&gt;&gt;&gt; class MeanSquareError(gpx.objectives.AbstractObjective):\ndef evaluate(self, model: LinearModel, train_data: gpx.Dataset) -&gt; float:\nreturn jnp.mean((train_data.y - model(train_data.X)) ** 2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; loss = MeanSqaureError()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (4) Train!\n&gt;&gt;&gt; trained_model, history = gpx.fit(\nmodel=model, objective=loss, train_data=D, optim=ox.sgd(0.001), num_iters=1000\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model Module to be optimised.</p> required <code>objective</code> <code>Objective</code> <p>The objective function that we are optimising with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data to be used for the optimisation.</p> required <code>optim</code> <code>GradientTransformation</code> <p>The Optax optimiser that is to be used for learning a parameter set.</p> required <code>num_iters</code> <code>Optional[int]</code> <p>The number of optimisation steps to run. Defaults to 100.</p> <code>100</code> <code>batch_size</code> <code>Optional[int]</code> <p>The size of the mini-batch to use. Defaults to -1 (i.e. full batch).</p> <code>-1</code> <code>key</code> <code>Optional[KeyArray]</code> <p>The random key to use for the optimisation batch selection. Defaults to jr.key(42).</p> required <code>log_rate</code> <code>Optional[int]</code> <p>How frequently the objective function's value should be printed. Defaults to 10.</p> <code>10</code> <code>verbose</code> <code>Optional[bool]</code> <p>Whether to print the training loading bar. Defaults to True.</p> <code>True</code> <code>unroll</code> <code>int</code> <p>The number of unrolled steps to use for the optimisation. Defaults to 1.</p> <code>1</code>"},{"location":"api/fit/#gpjax.fit.fit--returns","title":"Returns","text":"<pre><code>Tuple[Module, Array]: A Tuple comprising the optimised model and training\n    history respectively.\n</code></pre>"},{"location":"api/fit/#gpjax.fit.fit_scipy","title":"<code>fit_scipy(*, model: ModuleModel, objective: Union[AbstractObjective, Callable[[ModuleModel, Dataset], ScalarFloat]], train_data: Dataset, max_iters: Optional[int] = 500, verbose: Optional[bool] = True, safe: Optional[bool] = True) -&gt; Tuple[ModuleModel, Array]</code>","text":"<p>Train a Module model with respect to a supplied Objective function. Optimisers used here should originate from Optax. todo</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model Module to be optimised.</p> required <code>objective</code> <code>Objective</code> <p>The objective function that we are optimising with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data to be used for the optimisation.</p> required <code>max_iters</code> <code>Optional[int]</code> <p>The maximum number of optimisation steps to run. Defaults to 500.</p> <code>500</code> <code>verbose</code> <code>Optional[bool]</code> <p>Whether to print the information about the optimisation. Defaults to True.</p> <code>True</code>"},{"location":"api/fit/#gpjax.fit.fit_scipy--returns","title":"Returns","text":"<pre><code>Tuple[Module, Array]: A Tuple comprising the optimised model and training\n    history respectively.\n</code></pre>"},{"location":"api/fit/#gpjax.fit.get_batch","title":"<code>get_batch(train_data: Dataset, batch_size: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Batch the data into mini-batches. Sampling is done with replacement.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>The training dataset.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>key</code> <code>KeyArray</code> <p>The random key to use for the batch selection.</p> required"},{"location":"api/fit/#gpjax.fit.get_batch--returns","title":"Returns","text":"<pre><code>Dataset: The batched dataset.\n</code></pre>"},{"location":"api/gps/","title":"GPs","text":""},{"location":"api/gps/#gpjax.gps","title":"<code>gpjax.gps</code>","text":""},{"location":"api/gps/#gpjax.gps.Kernel","title":"<code>Kernel = TypeVar('Kernel', bound=AbstractKernel)</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.MeanFunction","title":"<code>MeanFunction = TypeVar('MeanFunction', bound=AbstractMeanFunction)</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.Likelihood","title":"<code>Likelihood = TypeVar('Likelihood', bound=AbstractLikelihood)</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonGaussianLikelihood","title":"<code>NonGaussianLikelihood = TypeVar('NonGaussianLikelihood', bound=NonGaussian)</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.GaussianLikelihood","title":"<code>GaussianLikelihood = TypeVar('GaussianLikelihood', bound=Gaussian)</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.PriorType","title":"<code>PriorType = TypeVar('PriorType', bound=AbstractPrior)</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.__all__","title":"<code>__all__ = ['AbstractPrior', 'Prior', 'AbstractPosterior', 'ConjugatePosterior', 'NonConjugatePosterior', 'construct_posterior']</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior","title":"<code>AbstractPrior</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code>, <code>Generic[MeanFunction, Kernel]</code></p> <p>Abstract Gaussian process prior.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.kernel","title":"<code>kernel: Kernel</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.mean_function","title":"<code>mean_function: MeanFunction</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.__init__","title":"<code>__init__(kernel: Kernel, mean_function: MeanFunction, jitter: float = static_field(1e-06)) -&gt; None</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process at the given points.</p> <p>The output of this function is a TensorFlow probability distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Evaluate the predictive distribution.</p> <p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to the predict method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to the predict method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior","title":"<code>Prior</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractPrior[MeanFunction, Kernel]</code></p> <p>A Gaussian process prior object.</p> <p>The GP is parameterised by a mean and kernel function.</p> <p>A Gaussian process prior parameterised by a mean function m(\u22c5)m(\\cdot)m(\u22c5) and a kernel function k(\u22c5,\u22c5)k(\\cdot, \\cdot)k(\u22c5,\u22c5) is given by p(f(\u22c5))=GP(m(\u22c5),k(\u22c5,\u22c5))p(f(\\cdot)) = \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))p(f(\u22c5))=GP(m(\u22c5),k(\u22c5,\u22c5)).</p> <p>To invoke a <code>Prior</code> distribution, a kernel and mean function must be specified.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n</code></pre></p>"},{"location":"api/gps/#gpjax.gps.Prior.kernel","title":"<code>kernel: Kernel</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.Prior.mean_function","title":"<code>mean_function: MeanFunction</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.Prior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.Prior.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/gps/#gpjax.gps.Prior.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.Prior.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.Prior.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.Prior.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.Prior.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.Prior.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.Prior.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.Prior.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/gps/#gpjax.gps.Prior.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/gps/#gpjax.gps.Prior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process at the given points.</p> <p>The output of this function is a TensorFlow probability distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.Prior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.__init__","title":"<code>__init__(kernel: Kernel, mean_function: MeanFunction, jitter: float = static_field(1e-06)) -&gt; None</code>","text":""},{"location":"api/gps/#gpjax.gps.Prior.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Combine the prior with a likelihood to form a posterior distribution.</p> <p>The product of a prior and likelihood is proportional to the posterior distribution. By computing the product of a GP prior and a likelihood object, a posterior GP object will be returned. Mathematically, this can be described by: <p>p(f(\u22c5)\u2223y)\u221dp(y\u2223f(\u22c5))p(f(\u22c5)), p(f(\\cdot) \\mid y) \\propto p(y \\mid f(\\cdot))p(f(\\cdot)), p(f(\u22c5)\u2223y)\u221dp(y\u2223f(\u22c5))p(f(\u22c5)),</p> where p(y\u2223f(\u22c5))p(y | f(\\cdot))p(y\u2223f(\u22c5)) is the likelihood and p(f(\u22c5))p(f(\\cdot))p(f(\u22c5)) is the prior.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; prior * likelihood\n</code></pre> Args:     other (Likelihood): The likelihood distribution of the observed dataset.</p>"},{"location":"api/gps/#gpjax.gps.Prior.__mul__--returns","title":"Returns","text":"<pre><code>Posterior: The relevant GP posterior for the given prior and\n    likelihood. Special cases are accounted for where the model\n    is conjugate.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.__rmul__","title":"<code>__rmul__(other)</code>","text":"<p>Combine the prior with a likelihood to form a posterior distribution.</p> <p>Reimplement the multiplication operator to allow for order-invariant product of a likelihood and a prior i.e., likelihood * prior.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Likelihood</code> <p>The likelihood distribution of the observed dataset.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.__rmul__--returns","title":"Returns","text":"<pre><code>Posterior: The relevant GP posterior for the given prior and\n    likelihood. Special cases are accounted for where the model\n    is conjugate.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.predict","title":"<code>predict(test_inputs: Num[Array, 'N D']) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive prior distribution for a given set of parameters. The output of this function is a function that computes a TFP distribution for a given set of inputs.</p> <p>In the following example, we compute the predictive prior distribution and then evaluate it on the interval :math:<code>[0, 1]</code>:</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n&gt;&gt;&gt;\n&gt;&gt;&gt; prior.predict(jnp.linspace(0, 1, 100))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D']</code> <p>The inputs at which to evaluate the prior distribution.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx","title":"<code>sample_approx(num_samples: int, key: KeyArray, num_features: Optional[int] = 100) -&gt; FunctionalSample</code>","text":"<p>Approximate samples from the Gaussian process prior.</p> <p>Build an approximate sample from the Gaussian process prior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>In particular, we approximate the Gaussian processes' prior as the finite feature approximation f^(x)=\u2211i=1m\u03d5i(x)\u03b8i\\hat{f}(x) = \\sum_{i=1}^m\\phi_i(x)\\theta_if^\u200b(x)=\u2211i=1m\u200b\u03d5i\u200b(x)\u03b8i\u200b where \u03d5i\\phi_i\u03d5i\u200b are mmm features sampled from the Fourier feature decomposition of the model's kernel and \u03b8i\\theta_i\u03b8i\u200b are samples from a unit Gaussian.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>In the following example, we build 10 such samples and then evaluate them over the interval [0,1][0, 1][0,1]:</p> <p>For a <code>prior</code> distribution, the following code snippet will build and evaluate an approximate sample.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; key = jr.key(123)\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n&gt;&gt;&gt;\n&gt;&gt;&gt; sample_fn = prior.sample_approx(10, key)\n&gt;&gt;&gt; sample_fn(jnp.linspace(0, 1, 100).reshape(-1, 1))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The desired number of samples.</p> required <code>key</code> <code>KeyArray</code> <p>The random seed used for the sample(s).</p> required <code>num_features</code> <code>int</code> <p>The number of features used when approximating the kernel.</p> <code>100</code>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx--returns","title":"Returns","text":"<pre><code>FunctionalSample: A function representing an approximate sample from the\n    Gaussian process prior.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior","title":"<code>AbstractPosterior</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code>, <code>Generic[PriorType, Likelihood]</code></p> <p>Abstract Gaussian process posterior.</p> <p>The base GP posterior object conditioned on an observed dataset. All posterior objects should inherit from this class.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.prior","title":"<code>prior: AbstractPrior[MeanFunction, Kernel]</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.likelihood","title":"<code>likelihood: Likelihood</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__init__","title":"<code>__init__(prior: AbstractPrior[MeanFunction, Kernel], likelihood: Likelihood, jitter: float = static_field(1e-06)) -&gt; None</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to the predict method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to the predict method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior","title":"<code>ConjugatePosterior</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractPosterior[PriorType, GaussianLikelihood]</code></p> <p>A Conjuate Gaussian process posterior object.</p> <p>A Gaussian process posterior distribution when the constituent likelihood function is a Gaussian distribution. In such cases, the latent function values fff can be analytically integrated out of the posterior distribution. As such, many computational operations can be simplified; something we make use of in this object.</p> <p>For a Gaussian process prior p(f)p(\\mathbf{f})p(f) and a Gaussian likelihood p(y\u2223f)=N(y\u2223f,\u03c32))p(y | \\mathbf{f}) = \\mathcal{N}(y\\mid \\mathbf{f}, \\sigma^2))p(y\u2223f)=N(y\u2223f,\u03c32)) where f=f(x)\\mathbf{f} = f(\\mathbf{x})f=f(x), the predictive posterior distribution at a set of inputs x\\mathbf{x}x is given by <p>p(f\u22c6\u2223y)=\u222bp(f\u22c6,f\u2223y)=N(f\u22c6\u03bc\u2223y,\u03a3\u2223y \\begin{align} p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star}, \\mathbf{f} \\mid \\mathbf{y})\\\\     &amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} \\end{align} p(f\u22c6\u2223y)\u200b=\u222bp(f\u22c6,f\u2223y)=N(f\u22c6\u03bc\u2223y\u200b,\u03a3\u2223y\u200b\u200b\u200b</p> where <p>\u03bc\u2223y=k(x\u22c6,x)(k(x,x\u2032)+\u03c32In)\u22121y\u03a3\u2223y=k(x\u22c6,x\u22c6\u2032)\u2212k(x\u22c6,x)(k(x,x\u2032)+\u03c32In)\u22121k(x,x\u22c6). \\begin{align} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\\\ \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}). \\end{align} \u03bc\u2223y\u200b\u03a3\u2223y\u200b\u200b=k(x\u22c6,x)(k(x,x\u2032)+\u03c32In\u200b)\u22121y=k(x\u22c6,x\u22c6\u2032)\u2212k(x\u22c6,x)(k(x,x\u2032)+\u03c32In\u200b)\u22121k(x,x\u22c6).\u200b\u200b</p></p> Example <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; prior = gpx.gps.Prior(\nmean_function = gpx.mean_functions.Zero(),\nkernel = gpx.kernels.RBF()\n)\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; posterior = prior * likelihood\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.prior","title":"<code>prior: AbstractPrior[MeanFunction, Kernel]</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.likelihood","title":"<code>likelihood: Likelihood</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.__init__","title":"<code>__init__(prior: AbstractPrior[MeanFunction, Kernel], likelihood: Likelihood, jitter: float = static_field(1e-06)) -&gt; None</code>","text":""},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict","title":"<code>predict(test_inputs: Num[Array, 'N D'], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Query the predictive posterior distribution.</p> <p>Conditional on a training data set, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density.</p> <p>The predictive distribution of a conjugate GP is given by $$     p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star} \\mathbf{f} \\mid \\mathbf{y})\\     &amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}} $$ where $$     \\boldsymbol{\\mu}{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\     \\boldsymbol{\\Sigma}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}). $$</p> <p>The conditioning set is a GPJax <code>Dataset</code> object, whilst predictions are made on a regular Jax array.</p> Example <p>For a <code>posterior</code> distribution, the following code snippet will evaluate the predictive distribution. <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; xtrain = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt; ytrain = jnp.sin(xtrain)\n&gt;&gt;&gt; D = gpx.Dataset(X=xtrain, y=ytrain)\n&gt;&gt;&gt; xtest = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function = gpx.mean_functions.Zero(), kernel = gpx.kernels.RBF())\n&gt;&gt;&gt; posterior = prior * gpx.likelihoods.Gaussian(num_datapoints = D.n)\n&gt;&gt;&gt; predictive_dist = posterior(xtest, D)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Num[Array, 'N D']</code> <p>A Jax array of test inputs at which the predictive distribution is evaluated.</p> required <code>train_data</code> <code>Dataset</code> <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> required"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts an input array and\n    returns the predictive distribution as a `GaussianDistribution`.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx","title":"<code>sample_approx(num_samples: int, train_data: Dataset, key: KeyArray, num_features: Optional[int] = 100) -&gt; FunctionalSample</code>","text":"<p>Draw approximate samples from the Gaussian process posterior.</p> <p>Build an approximate sample from the Gaussian process posterior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>Unlike when building approximate samples from a Gaussian process prior, decompositions based on Fourier features alone rarely give accurate samples. Therefore, we must also include an additional set of features (known as canonical features) to better model the transition from Gaussian process prior to Gaussian process posterior. For more details see Wilson et. al. (2020).</p> <p>In particular, we approximate the Gaussian processes' posterior as the finite feature approximation f^(x)=\u2211i=1m\u03d5i(x)\u03b8i+\u2211j=1Nvjk(.,xj)\\hat{f}(x) = \\sum_{i=1}^m \\phi_i(x)\\theta_i + \\sum{j=1}^N v_jk(.,x_j)f^\u200b(x)=\u2211i=1m\u200b\u03d5i\u200b(x)\u03b8i\u200b+\u2211j=1Nvj\u200bk(.,xj\u200b) where \u03d5i\\phi_i\u03d5i\u200b are m features sampled from the Fourier feature decomposition of the model's kernel and k(.,xj)k(., x_j)k(.,xj\u200b) are N canonical features. The Fourier weights \u03b8i\\theta_i\u03b8i\u200b are samples from a unit Gaussian. See Wilson et. al. (2020) for expressions for the canonical weights vjv_jvj\u200b.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The desired number of samples.</p> required <code>key</code> <code>KeyArray</code> <p>The random seed used for the sample(s).</p> required <code>num_features</code> <code>int</code> <p>The number of features used when approximating the kernel.</p> <code>100</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx--returns","title":"Returns","text":"<pre><code>FunctionalSample: A function representing an approximate sample from the Gaussian\nprocess prior.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior","title":"<code>NonConjugatePosterior</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractPosterior[PriorType, NonGaussianLikelihood]</code></p> <p>A non-conjugate Gaussian process posterior object.</p> <p>A Gaussian process posterior object for models where the likelihood is non-Gaussian. Unlike the <code>ConjugatePosterior</code> object, the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.prior","title":"<code>prior: AbstractPrior[MeanFunction, Kernel]</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.likelihood","title":"<code>likelihood: Likelihood</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.latent","title":"<code>latent: Union[Float[Array, 'N 1'], None] = param_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.key","title":"<code>key: KeyArray = static_field(PRNGKey(42))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__init__","title":"<code>__init__(prior: AbstractPrior[MeanFunction, Kernel], likelihood: Likelihood, jitter: float = static_field(1e-06), latent: Union[Float[Array, 'N 1'], None] = param_field(None), key: KeyArray = static_field(PRNGKey(42))) -&gt; None</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict","title":"<code>predict(test_inputs: Num[Array, 'N D'], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Query the predictive posterior distribution.</p> <p>Conditional on a set of training data, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density. Note, to gain predictions on the scale of the original data, the returned distribution will need to be transformed through the likelihood function's inverse link function.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> required"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts an\n    input array and returns the predictive distribution as\n    a `dx.Distribution`.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.construct_posterior","title":"<code>construct_posterior(prior, likelihood)</code>","text":"<p>Utility function for constructing a posterior object from a prior and likelihood. The function will automatically select the correct posterior object based on the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Prior</code> <p>The Prior distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood that represents our beliefs around the distribution of the data.</p> required"},{"location":"api/gps/#gpjax.gps.construct_posterior--returns","title":"Returns","text":"<pre><code>AbstractPosterior: A posterior distribution. If the likelihood is\n    Gaussian, then a `ConjugatePosterior` will be returned. Otherwise,\n    a `NonConjugatePosterior` will be returned.\n</code></pre>"},{"location":"api/integrators/","title":"Integrators","text":""},{"location":"api/integrators/#gpjax.integrators","title":"<code>gpjax.integrators</code>","text":""},{"location":"api/integrators/#gpjax.integrators.Likelihood","title":"<code>Likelihood = TypeVar('Likelihood', bound=Union['gpjax.likelihoods.AbstractLikelihood', None])</code>  <code>module-attribute</code>","text":""},{"location":"api/integrators/#gpjax.integrators.Gaussian","title":"<code>Gaussian = TypeVar('Gaussian', bound='gpjax.likelihoods.Gaussian')</code>  <code>module-attribute</code>","text":""},{"location":"api/integrators/#gpjax.integrators.__all__","title":"<code>__all__ = ['AbstractIntegrator', 'GHQuadratureIntegrator', 'AnalyticalGaussianIntegrator']</code>  <code>module-attribute</code>","text":""},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator","title":"<code>AbstractIntegrator</code>  <code>dataclass</code>","text":"<p>Base class for integrators.</p>"},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.integrate","title":"<code>integrate(fun: Callable, y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D'], likelihood: Likelihood) -&gt; Float[Array, ' N']</code>  <code>abstractmethod</code>","text":"<p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function to be integrated.</p> required <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> required"},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.__call__","title":"<code>__call__(fun: Callable, y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D'], likelihood: Likelihood) -&gt; Float[Array, ' N']</code>","text":"<p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function to be integrated.</p> required <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> required"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator","title":"<code>GHQuadratureIntegrator</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractIntegrator</code></p> <p>Compute an integral using Gauss-Hermite quadrature.</p> <p>Gauss-Hermite quadrature is a method for approximating integrals through a weighted sum of function evaluations at specific points <p>\u222bF(t)exp\u2061(\u2212t2)dt\u2248\u2211j=1JwjF(tj) \\int F(t)\\exp(-t^2)\\mathrm{d}t \\approx \\sum_{j=1}^J w_j F(t_j) \u222bF(t)exp(\u2212t2)dt\u2248j=1\u2211J\u200bwj\u200bF(tj\u200b)</p> where tjt_jtj\u200b and wjw_jwj\u200b are the roots and weights of the JJJ-th order Hermite polynomial HJ(t)H_J(t)HJ\u200b(t) that we can look up in table link.</p>"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.num_points","title":"<code>num_points: int = 20</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.__call__","title":"<code>__call__(fun: Callable, y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D'], likelihood: Likelihood) -&gt; Float[Array, ' N']</code>","text":"<p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function to be integrated.</p> required <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> required"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.__init__","title":"<code>__init__(num_points: int = 20) -&gt; None</code>","text":""},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.integrate","title":"<code>integrate(fun: Callable, y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D'], likelihood: Likelihood) -&gt; Float[Array, ' N']</code>","text":"<p>Compute a quadrature integral.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The likelihood to be integrated.</p> required <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> required <p>Returns:</p> Type Description <code>Float[Array, ' N']</code> <p>Float[Array, 'N']: The expected log likelihood.</p>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator","title":"<code>AnalyticalGaussianIntegrator</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractIntegrator</code></p> <p>Compute the analytical integral of a Gaussian likelihood.</p> <p>When the likelihood function is Gaussian, the integral can be computed in closed form. For a Gaussian likelihood p(y\u2223f)=N(y\u2223f,\u03c32)p(y|f) = \\mathcal{N}(y|f, \\sigma^2)p(y\u2223f)=N(y\u2223f,\u03c32) and a variational distribution q(f)=N(f\u2223m,s)q(f) = \\mathcal{N}(f|m, s)q(f)=N(f\u2223m,s), the expected log-likelihood is given by <p>Eq(f)[log\u2061p(y\u2223f)]=\u221212(log\u2061(2\u03c0\u03c32)+1\u03c32((y\u2212m)2+s)) \\mathbb{E}_{q(f)}[\\log p(y|f)] = -\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2) + \\frac{1}{\\sigma^2}((y-m)^2 + s)\\right) Eq(f)\u200b[logp(y\u2223f)]=\u221221\u200b(log(2\u03c0\u03c32)+\u03c321\u200b((y\u2212m)2+s))</p></p>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator.__call__","title":"<code>__call__(fun: Callable, y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D'], likelihood: Likelihood) -&gt; Float[Array, ' N']</code>","text":"<p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function to be integrated.</p> required <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> required"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator.integrate","title":"<code>integrate(fun: Callable, y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D'], likelihood: Gaussian) -&gt; Float[Array, ' N']</code>","text":"<p>Compute a Gaussian integral.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The Gaussian likelihood to be integrated.</p> required <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>Gaussian</code> <p>The Gaussian likelihood function.</p> required <p>Returns:</p> Type Description <code>Float[Array, ' N']</code> <p>Float[Array, 'N']: The expected log likelihood.</p>"},{"location":"api/likelihoods/","title":"Likelihoods","text":""},{"location":"api/likelihoods/#gpjax.likelihoods","title":"<code>gpjax.likelihoods</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.tfb","title":"<code>tfb = tfp.bijectors</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.NonGaussian","title":"<code>NonGaussian = Union[Poisson, Bernoulli]</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.__all__","title":"<code>__all__ = ['AbstractLikelihood', 'NonGaussian', 'Gaussian', 'Bernoulli', 'Poisson', 'inv_probit']</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood","title":"<code>AbstractLikelihood</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code></p> <p>Abstract base class for likelihoods.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.num_datapoints","title":"<code>num_datapoints: int = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.integrator","title":"<code>integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__init__","title":"<code>__init__(num_datapoints: int = static_field(), integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())) -&gt; None</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>  <code>abstractmethod</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>  <code>abstractmethod</code>","text":"<p>Return the link function of the likelihood function.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The distribution of observations, y, given values of the\n    Gaussian process, f.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.expected_log_likelihood","title":"<code>expected_log_likelihood(y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D']) -&gt; Float[Array, ' N']</code>","text":"<p>Compute the expected log likelihood.</p> <p>For a variational distribution q(f)\u223cN(m,s)q(f)\\sim\\mathcal{N}(m, s)q(f)\u223cN(m,s) and a likelihood p(y\u2223f)p(y|f)p(y\u2223f), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The variational mean.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variational variance.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>Float[Array, ' N']</code> <p>The expected log likelihood.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian","title":"<code>Gaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractLikelihood</code></p> <p>Gaussian likelihood object.</p> <p>Parameters:</p> Name Type Description Default <code>obs_stddev</code> <code>Union[ScalarFloat, Float[Array, '#N']]</code> <p>the standard deviation of the Gaussian observation noise.</p> <code>param_field(array(1.0), bijector=Softplus())</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.num_datapoints","title":"<code>num_datapoints: int = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.obs_stddev","title":"<code>obs_stddev: Union[ScalarFloat, Float[Array, '#N']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.integrator","title":"<code>integrator: AbstractIntegrator = static_field(AnalyticalGaussianIntegrator())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.__call__--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.expected_log_likelihood","title":"<code>expected_log_likelihood(y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D']) -&gt; Float[Array, ' N']</code>","text":"<p>Compute the expected log likelihood.</p> <p>For a variational distribution q(f)\u223cN(m,s)q(f)\\sim\\mathcal{N}(m, s)q(f)\u223cN(m,s) and a likelihood p(y\u2223f)p(y|f)p(y\u2223f), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The variational mean.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variational variance.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>Float[Array, ' N']</code> <p>The expected log likelihood.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.__init__","title":"<code>__init__(num_datapoints: int = static_field(), integrator: AbstractIntegrator = static_field(AnalyticalGaussianIntegrator()), obs_stddev: Union[ScalarFloat, Float[Array, '#N']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Normal</code>","text":"<p>The link function of the Gaussian likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ...]</code> <p>Function values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function--returns","title":"Returns","text":"<pre><code>tfd.Normal: The likelihood function.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict","title":"<code>predict(dist: Union[tfd.MultivariateNormalTriL, GaussianDistribution]) -&gt; tfd.MultivariateNormalFullCovariance</code>","text":"<p>Evaluate the Gaussian likelihood.</p> <p>Evaluate the Gaussian likelihood function at a given predictive distribution. Computationally, this is equivalent to summing the observation noise term to the diagonal elements of the predictive distribution's covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli","title":"<code>Bernoulli</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractLikelihood</code></p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.num_datapoints","title":"<code>num_datapoints: int = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.integrator","title":"<code>integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.__call__--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.expected_log_likelihood","title":"<code>expected_log_likelihood(y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D']) -&gt; Float[Array, ' N']</code>","text":"<p>Compute the expected log likelihood.</p> <p>For a variational distribution q(f)\u223cN(m,s)q(f)\\sim\\mathcal{N}(m, s)q(f)\u223cN(m,s) and a likelihood p(y\u2223f)p(y|f)p(y\u2223f), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The variational mean.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variational variance.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>Float[Array, ' N']</code> <p>The expected log likelihood.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.__init__","title":"<code>__init__(num_datapoints: int = static_field(), integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())) -&gt; None</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>","text":"<p>The probit link function of the Bernoulli likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ...]</code> <p>Function values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The likelihood function.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict","title":"<code>predict(dist: tfd.Distribution) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the pointwise predictive distribution.</p> <p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The pointwise predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson","title":"<code>Poisson</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractLikelihood</code></p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.num_datapoints","title":"<code>num_datapoints: int = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.integrator","title":"<code>integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.__call__--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.expected_log_likelihood","title":"<code>expected_log_likelihood(y: Float[Array, 'N D'], mean: Float[Array, 'N D'], variance: Float[Array, 'N D']) -&gt; Float[Array, ' N']</code>","text":"<p>Compute the expected log likelihood.</p> <p>For a variational distribution q(f)\u223cN(m,s)q(f)\\sim\\mathcal{N}(m, s)q(f)\u223cN(m,s) and a likelihood p(y\u2223f)p(y|f)p(y\u2223f), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array, 'N D']</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, 'N D']</code> <p>The variational mean.</p> required <code>variance</code> <code>Float[Array, 'N D']</code> <p>The variational variance.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>Float[Array, ' N']</code> <p>The expected log likelihood.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.__init__","title":"<code>__init__(num_datapoints: int = static_field(), integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())) -&gt; None</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>","text":"<p>The link function of the Poisson likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ...]</code> <p>Function values.</p> required <p>Returns:</p> Type Description <code>Distribution</code> <p>tfd.Distribution: The likelihood function.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.predict","title":"<code>predict(dist: tfd.Distribution) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the pointwise predictive distribution.</p> <p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required <p>Returns:</p> Type Description <code>Distribution</code> <p>tfd.Distribution: The pointwise predictive distribution.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit","title":"<code>inv_probit(x: Float[Array, ' *N']) -&gt; Float[Array, ' *N']</code>","text":"<p>Compute the inverse probit function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '*N']</code> <p>A vector of values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit--returns","title":"Returns","text":"<pre><code>Float[Array, \"*N\"]: The inverse probit of the input vector.\n</code></pre>"},{"location":"api/lower_cholesky/","title":"Lower Cholesky","text":""},{"location":"api/lower_cholesky/#gpjax.lower_cholesky","title":"<code>gpjax.lower_cholesky</code>","text":""},{"location":"api/lower_cholesky/#gpjax.lower_cholesky.lower_cholesky","title":"<code>lower_cholesky(A: cola.ops.LinearOperator)</code>","text":"<p>Returns the lower Cholesky factor of a linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>A linear operator.</p> required <p>Returns:</p> Type Description <p>cola.ops.LinearOperator: The lower Cholesky factor of A.</p>"},{"location":"api/lower_cholesky/#gpjax.lower_cholesky._","title":"<code>_(A: cola.ops.BlockDiag)</code>","text":""},{"location":"api/mean_functions/","title":"Mean Functions","text":""},{"location":"api/mean_functions/#gpjax.mean_functions","title":"<code>gpjax.mean_functions</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.SumMeanFunction","title":"<code>SumMeanFunction = partial(CombinationMeanFunction, operator=partial(jnp.sum, axis=0))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.ProductMeanFunction","title":"<code>ProductMeanFunction = partial(CombinationMeanFunction, operator=partial(jnp.sum, axis=0))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction","title":"<code>AbstractMeanFunction</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code></p> <p>Mean function that is used to parameterise the Gaussian process.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__","title":"<code>__call__(x: Num[Array, 'N D']) -&gt; Float[Array, 'N O']</code>  <code>abstractmethod</code>","text":"<p>Evaluate the mean function at the given points. This method is required for all subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__","title":"<code>__add__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__radd__","title":"<code>__radd__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__radd__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__","title":"<code>__mul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__rmul__","title":"<code>__rmul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__rmul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant","title":"<code>Constant</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractMeanFunction</code></p> <p>Constant mean function.</p> <p>A constant mean function. This function returns a repeated scalar value for all inputs.  The scalar value itself can be treated as a model hyperparameter and learned during training but defaults to 1.0.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.constant","title":"<code>constant: Float[Array, ' O'] = param_field(jnp.array([0.0]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__add__","title":"<code>__add__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__add__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__radd__","title":"<code>__radd__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__radd__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__mul__","title":"<code>__mul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__mul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__rmul__","title":"<code>__rmul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__rmul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__init__","title":"<code>__init__(constant: Float[Array, ' O'] = param_field(jnp.array([0.0]))) -&gt; None</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__","title":"<code>__call__(x: Num[Array, 'N D']) -&gt; Float[Array, 'N O']</code>","text":"<p>Evaluate the mean function at the given points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero","title":"<code>Zero</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Constant</code></p> <p>Zero mean function.</p> <p>The zero mean function. This function returns a zero scalar value for all inputs. Unlike the Constant mean function, the constant scalar zero is fixed, and cannot be treated as a model hyperparameter and learned during training.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.constant","title":"<code>constant: Float[Array, ' O'] = static_field(jnp.array([0.0]), init=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__call__","title":"<code>__call__(x: Num[Array, 'N D']) -&gt; Float[Array, 'N O']</code>","text":"<p>Evaluate the mean function at the given points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__add__","title":"<code>__add__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__add__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__radd__","title":"<code>__radd__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__radd__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__mul__","title":"<code>__mul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__mul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__rmul__","title":"<code>__rmul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__rmul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Zero.__init__","title":"<code>__init__(constant: Float[Array, ' O'] = static_field(jnp.array([0.0]), init=False)) -&gt; None</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction","title":"<code>CombinationMeanFunction</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractMeanFunction</code></p> <p>A base class for products or sums of AbstractMeanFunctions.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.means","title":"<code>means: List[AbstractMeanFunction] = items_list</code>  <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.operator","title":"<code>operator: Callable = operator</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__add__","title":"<code>__add__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__add__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__radd__","title":"<code>__radd__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__radd__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__mul__","title":"<code>__mul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__mul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__rmul__","title":"<code>__rmul__(other: Union[AbstractMeanFunction, Float[Array, ' O']]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__rmul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__init__","title":"<code>__init__(means: List[AbstractMeanFunction], operator: Callable, **kwargs) -&gt; None</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__","title":"<code>__call__(x: Num[Array, 'N D']) -&gt; Float[Array, 'N O']</code>","text":"<p>Evaluate combination kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \" Q\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/objectives/","title":"Objectives","text":""},{"location":"api/objectives/#gpjax.objectives","title":"<code>gpjax.objectives</code>","text":""},{"location":"api/objectives/#gpjax.objectives.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugatePosterior","title":"<code>ConjugatePosterior = TypeVar('ConjugatePosterior', bound='gpjax.gps.ConjugatePosterior')</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.NonConjugatePosterior","title":"<code>NonConjugatePosterior = TypeVar('NonConjugatePosterior', bound='gpjax.gps.NonConjugatePosterior')</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.VariationalFamily","title":"<code>VariationalFamily = TypeVar('VariationalFamily', bound='gpjax.variational_families.AbstractVariationalFamily')</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.NonConjugateMLL","title":"<code>NonConjugateMLL = LogPosteriorDensity</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective","title":"<code>AbstractObjective</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code></p> <p>Abstract base class for objectives.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.constant","title":"<code>constant: ScalarFloat = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__init__","title":"<code>__init__(negative: bool = static_field(False), constant: ScalarFloat = static_field(init=False, repr=False)) -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.step","title":"<code>step(*args, **kwargs) -&gt; ScalarFloat</code>  <code>abstractmethod</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL","title":"<code>ConjugateMLL</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.constant","title":"<code>constant: ScalarFloat = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.__init__","title":"<code>__init__(negative: bool = static_field(False), constant: ScalarFloat = static_field(init=False, repr=False)) -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.step","title":"<code>step(posterior: ConjugatePosterior, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the marginal log-likelihood of the Gaussian process.</p> <p>Compute the marginal log-likelihood function of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here enables exact estimation of the Gaussian process' latent function values.</p> <p>For a training dataset {xn,yn}n=1N\\{x_n, y_n\\}_{n=1}^N{xn\u200b,yn\u200b}n=1N\u200b, set of test inputs x\u22c6\\mathbf{x}^{\\star}x\u22c6 the corresponding latent function evaluations are given by f=f(x)\\mathbf{f}=f(\\mathbf{x})f=f(x) and f\u22c6f(x\u22c6)\\mathbf{f}^{\\star}f(\\mathbf{x}^{\\star})f\u22c6f(x\u22c6), the marginal log-likelihood is given by: <p>log\u2061p(y)=\u222bp(y\u2223f)p(f,f\u22c6df\u22c6=0.5(\u2212y\u22a4(k(x,x\u2032)+\u03c32IN)\u22121y\u2212log\u2061\u2223k(x,x\u2032)+\u03c32IN\u2223\u2212nlog\u20612\u03c0). \\begin{align}     \\log p(\\mathbf{y}) &amp; = \\int p(\\mathbf{y}\\mid\\mathbf{f})p(\\mathbf{f}, \\mathbf{f}^{\\star}\\mathrm{d}\\mathbf{f}^{\\star}\\\\     &amp;=0.5\\left(-\\mathbf{y}^{\\top}\\left(k(\\mathbf{x}, \\mathbf{x}') +\\sigma^2\\mathbf{I}_N  \\right)^{-1}\\mathbf{y}-\\log\\lvert k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_N\\rvert - n\\log 2\\pi \\right). \\end{align} logp(y)\u200b=\u222bp(y\u2223f)p(f,f\u22c6df\u22c6=0.5(\u2212y\u22a4(k(x,x\u2032)+\u03c32IN\u200b)\u22121y\u2212log\u2223k(x,x\u2032)+\u03c32IN\u200b\u2223\u2212nlog2\u03c0).\u200b\u200b</p></p> <p>For a given <code>ConjugatePosterior</code> object, the following code snippet shows how the marginal log-likelihood can be evaluated.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; xtrain = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt; ytrain = jnp.sin(xtrain)\n&gt;&gt;&gt; D = gpx.Dataset(X=xtrain, y=ytrain)\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Constant()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function = meanf, kernel=kernel)\n&gt;&gt;&gt; posterior = prior * likelihood\n&gt;&gt;&gt;\n&gt;&gt;&gt; mll = gpx.objectives.ConjugateMLL(negative=True)\n&gt;&gt;&gt; mll(posterior, train_data = D)\n</code></pre></p> <p>Our goal is to maximise the marginal log-likelihood. Therefore, when optimising the model's parameters with respect to the parameters, we use the negative marginal log-likelihood. This can be realised through</p> <pre><code>    mll = gpx.objectives.ConjugateMLL(negative=True)\n</code></pre> <p>For optimal performance, the marginal log-likelihood should be <code>jax.jit</code> compiled. <pre><code>    mll = jit(gpx.objectives.ConjugateMLL(negative=True))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> required <code>train_data</code> <code>Dataset</code> <p>The training dataset used to compute the marginal log-likelihood.</p> required"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The marginal log-likelihood of the Gaussian process for the\n    current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV","title":"<code>ConjugateLOOCV</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.constant","title":"<code>constant: ScalarFloat = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.__init__","title":"<code>__init__(negative: bool = static_field(False), constant: ScalarFloat = static_field(init=False, repr=False)) -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.step","title":"<code>step(posterior: ConjugatePosterior, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the leave-one-out log predictive probability of the Gaussian process following section 5.4.2 of Rasmussen et al. 2006 - Gaussian Processes for Machine Learning. This metric calculates the average performance of all models that can be obtained by training on all but one data point, and then predicting the left out data point.</p> <p>The returned metric can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here enables exact estimation of the Gaussian process' latent function values.</p> <p>For a given <code>ConjugatePosterior</code> object, the following code snippet shows how the leave-one-out log predicitive probability can be evaluated.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; xtrain = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt; ytrain = jnp.sin(xtrain)\n&gt;&gt;&gt; D = gpx.Dataset(X=xtrain, y=ytrain)\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Constant()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n&gt;&gt;&gt; prior = gpx.gps.Prior(mean_function = meanf, kernel=kernel)\n&gt;&gt;&gt; posterior = prior * likelihood\n&gt;&gt;&gt;\n&gt;&gt;&gt; loocv = gpx.objectives.ConjugateLOOCV(negative=True)\n&gt;&gt;&gt; loocv(posterior, train_data = D)\n</code></pre></p> <p>Our goal is to maximise the leave-one-out log predictive probability. Therefore, when optimising the model's parameters with respect to the parameters, we use the negative leave-one-out log predictive probability. This can be realised through</p> <pre><code>    mll = gpx.objectives.ConjugateLOOCV(negative=True)\n</code></pre> <p>For optimal performance, the objective should be <code>jax.jit</code> compiled. <pre><code>    mll = jit(gpx.objectives.ConjugateLOOCV(negative=True))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>The posterior distribution for which we want to compute the leave-one-out log predictive probability.</p> required <code>train_data</code> <code>Dataset</code> <p>The training dataset used to compute the leave-one-out log predictive probability..</p> required"},{"location":"api/objectives/#gpjax.objectives.ConjugateLOOCV.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The leave-one-out log predictive probability of the Gaussian\n    process for the current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity","title":"<code>LogPosteriorDensity</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractObjective</code></p> <p>The log-posterior density of a non-conjugate Gaussian process. This is sometimes referred to as the marginal log-likelihood.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.constant","title":"<code>constant: ScalarFloat = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.__init__","title":"<code>__init__(negative: bool = static_field(False), constant: ScalarFloat = static_field(init=False, repr=False)) -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.step","title":"<code>step(posterior: NonConjugatePosterior, data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the log-posterior density of a Gaussian process.</p> <p>Compute the marginal log-likelihood, or log-posterior density of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here is general and will work for any likelihood support by GPJax.</p> <p>Unlike the marginal_log_likelihood function of the <code>ConjugatePosterior</code> object, the marginal_log_likelihood function of the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NonConjugatePosterior</code> <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> required <code>data</code> <code>Dataset</code> <p>The training dataset used to compute the marginal log-likelihood.</p> required"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log-posterior density of the Gaussian process for the\n    current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO","title":"<code>ELBO</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.constant","title":"<code>constant: ScalarFloat = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ELBO.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ELBO.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.ELBO.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.__init__","title":"<code>__init__(negative: bool = static_field(False), constant: ScalarFloat = static_field(init=False, repr=False)) -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ELBO.step","title":"<code>step(variational_family: VariationalFamily, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Compute the evidence lower bound of a variational approximation.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data for which we should maximise the ELBO with respect to.</p> required"},{"location":"api/objectives/#gpjax.objectives.ELBO.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation for\n    the current model parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO","title":"<code>CollapsedELBO</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractObjective</code></p> <p>The collapsed evidence lower bound.</p> <p>Collapsed variational inference for a sparse Gaussian process regression model. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.constant","title":"<code>constant: ScalarFloat = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.__init__","title":"<code>__init__(negative: bool = static_field(False), constant: ScalarFloat = static_field(init=False, repr=False)) -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.step","title":"<code>step(variational_family: VariationalFamily, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Compute a single step of the collapsed evidence lower bound.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data for which we should maximise the ELBO with respect to.</p> required"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation for\n    the current model parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.variational_expectation","title":"<code>variational_expectation(variational_family: VariationalFamily, train_data: Dataset) -&gt; Float[Array, ' N']</code>","text":"<p>Compute the variational expectation.</p> <p>Compute the expectation of our model's log-likelihood under our variational distribution. Batching can be done here to speed up computation.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational family that we are using to approximate the posterior.</p> required <code>train_data</code> <code>Dataset</code> <p>The batch for which the expectation should be computed for.</p> required"},{"location":"api/objectives/#gpjax.objectives.variational_expectation--returns","title":"Returns","text":"<pre><code>Array: The expectation of the model's log-likelihood under our variational\n    distribution.\n</code></pre>"},{"location":"api/progress_bar/","title":"Progress Bar","text":""},{"location":"api/progress_bar/#gpjax.progress_bar","title":"<code>gpjax.progress_bar</code>","text":""},{"location":"api/progress_bar/#gpjax.progress_bar.__all__","title":"<code>__all__ = ['progress_bar']</code>  <code>module-attribute</code>","text":""},{"location":"api/progress_bar/#gpjax.progress_bar.progress_bar","title":"<code>progress_bar(num_iters: int, log_rate: int) -&gt; Callable</code>","text":"<p>Progress bar decorator for the body function of a <code>jax.lax.scan</code>.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; carry = jnp.array(0.0)\n&gt;&gt;&gt; iteration_numbers = jnp.arange(100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; @progress_bar(num_iters=iteration_numbers.shape[0], log_rate=10)\n&gt;&gt;&gt; def body_func(carry, x):\n&gt;&gt;&gt;    return carry, x\n&gt;&gt;&gt;\n&gt;&gt;&gt; carry, _ = jax.lax.scan(body_func, carry, iteration_numbers)\n</code></pre></p> <p>Adapted from this excellent blog post.</p> <p>Might be nice in future to directly create a general purpose <code>verbose scan</code> inplace of a for a jax.lax.scan, that takes the same arguments as a jax.lax.scan, but prints a progress bar.</p>"},{"location":"api/scan/","title":"Scan","text":""},{"location":"api/scan/#gpjax.scan","title":"<code>gpjax.scan</code>","text":""},{"location":"api/scan/#gpjax.scan.Carry","title":"<code>Carry = TypeVar('Carry')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.X","title":"<code>X = TypeVar('X')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.Y","title":"<code>Y = TypeVar('Y')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.__all__","title":"<code>__all__ = ['vscan']</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.vscan","title":"<code>vscan(f: Callable[[Carry, X], Tuple[Carry, Y]], init: Carry, xs: X, length: Optional[int] = None, reverse: Optional[bool] = False, unroll: Optional[int] = 1, log_rate: Optional[int] = 10, log_value: Optional[bool] = True) -&gt; Tuple[Carry, Shaped[Array, ...]]</code>","text":"<p>Scan with verbose output.</p> <p>This is based on code from this excellent blog post.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; def f(carry, x):\nreturn carry + x, carry + x\n&gt;&gt;&gt; init = 0\n&gt;&gt;&gt; xs = jnp.arange(10)\n&gt;&gt;&gt; vscan(f, init, xs)\n</code></pre> <pre><code>    (45, DeviceArray([ 0,  1,  3,  6, 10, 15, 21, 28, 36, 45], dtype=int32))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Carry, X], Tuple[Carry, Y]]</code> <p>A function that takes in a carry and an input and returns a tuple of a new carry and an output.</p> required <code>init</code> <code>Carry</code> <p>The initial carry.</p> required <code>xs</code> <code>X</code> <p>The inputs.</p> required <code>length</code> <code>Optional[int]</code> <p>The length of the inputs. If None, then the length of the inputs is inferred.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>Whether to scan in reverse.</p> <code>False</code> <code>unroll</code> <code>int</code> <p>The number of iterations to unroll.</p> <code>1</code> <code>log_rate</code> <code>int</code> <p>The rate at which to log the progress bar.</p> <code>10</code> <code>log_value</code> <code>bool</code> <p>Whether to log the value of the objective function.</p> <code>True</code>"},{"location":"api/scan/#gpjax.scan.vscan--returns","title":"Returns","text":"<pre><code>Tuple[Carry, list[Y]]: A tuple of the final carry and the outputs.\n</code></pre>"},{"location":"api/typing/","title":"Typing","text":""},{"location":"api/typing/#gpjax.typing","title":"<code>gpjax.typing</code>","text":""},{"location":"api/typing/#gpjax.typing.OldKeyArray","title":"<code>OldKeyArray = UInt32[JAXArray, '2']</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.JAXKeyArray","title":"<code>JAXKeyArray = Key[JAXArray, '']</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.KeyArray","title":"<code>KeyArray = Union[OldKeyArray, JAXKeyArray]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.Array","title":"<code>Array = Union[JAXArray, NumpyArray]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarBool","title":"<code>ScalarBool = Union[bool, Bool[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarInt","title":"<code>ScalarInt = Union[int, Int[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarFloat","title":"<code>ScalarFloat = Union[float, Float[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.VecNOrMatNM","title":"<code>VecNOrMatNM = Union[Float[Array, ' N'], Float[Array, 'N M']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.FunctionalSample","title":"<code>FunctionalSample = Callable[[Float[Array, 'N D']], Float[Array, 'N B']]</code>  <code>module-attribute</code>","text":"<p>Type alias for functions representing BBB samples from a model, to be evaluated on any set of NNN inputs (of dimension DDD) and returning the evaluations of each (potentially approximate) sample draw across these inputs.</p>"},{"location":"api/typing/#gpjax.typing.__all__","title":"<code>__all__ = ['KeyArray', 'ScalarBool', 'ScalarInt', 'ScalarFloat', 'FunctionalSample']</code>  <code>module-attribute</code>","text":""},{"location":"api/variational_families/","title":"Variational Families","text":""},{"location":"api/variational_families/#gpjax.variational_families","title":"<code>gpjax.variational_families</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.__all__","title":"<code>__all__ = ['AbstractVariationalFamily', 'AbstractVariationalGaussian', 'VariationalGaussian', 'WhitenedVariationalGaussian', 'NaturalVariationalGaussian', 'ExpectationVariationalGaussian', 'CollapsedVariationalGaussian']</code>  <code>module-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily","title":"<code>AbstractVariationalFamily</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code></p> <p>Abstract base class used to represent families of distributions that can be used within variational inference.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__init__","title":"<code>__init__(posterior: AbstractPosterior) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Predict the GP's output given the input.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's ``predict`` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian","title":"<code>AbstractVariationalGaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractVariationalFamily</code></p> <p>The variational Gaussian family of probability distributions.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, 'N D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Predict the GP's output given the input.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's ``predict`` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.__init__","title":"<code>__init__(posterior: AbstractPosterior, inducing_inputs: Float[Array, 'N D'], jitter: ScalarFloat = static_field(1e-06)) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian","title":"<code>VariationalGaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractVariationalGaussian</code></p> <p>The variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(\u03bc,S)q(u) = \\mathcal{N}(\\mu, S)q(u)=N(\u03bc,S).  We parameterise this over \u03bc\\mu\u03bc and sqrtsqrtsqrt with S=sqrtsqrt\u22a4S = sqrt sqrt^{\\top}S=sqrtsqrt\u22a4.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, 'N D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.variational_mean","title":"<code>variational_mean: Union[Float[Array, 'N 1'], None] = param_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.variational_root_covariance","title":"<code>variational_root_covariance: Float[Array, 'N N'] = param_field(None, bijector=tfb.FillTriangular())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__init__","title":"<code>__init__(posterior: AbstractPosterior, inducing_inputs: Float[Array, 'N D'], jitter: ScalarFloat = static_field(1e-06), variational_mean: Union[Float[Array, 'N 1'], None] = param_field(None), variational_root_covariance: Float[Array, 'N N'] = param_field(None, bijector=tfb.FillTriangular())) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the prior KL divergence.</p> <p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(\u03bcz,Kzz)], \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)]\\\\ &amp; = \\operatorname{KL}[ \\mathcal{N}(\\mu, S) \\mid\\mid N(\\mu z, \\mathbf{K}_{zz}) ], \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(\u03bcz,Kzz\u200b)],\u200b\u200b</p> where u=f(z)u = f(z)u=f(z) and zzz are the inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code> ScalarFloat: The KL-divergence between our variational\n    approximation and the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, 'N D']) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t))=\u222bp(f(t)\u2223u)q(u)duq(f(t)) = \\int p(f(t)\\mid u) q(u) \\mathrm{d}uq(f(t))=\u222bp(f(t)\u2223u)q(u)du, which can be computed in closed form as: <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt).     \\mathcal{N}\\left(f(t); \\mu t + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} (\\mu - \\mu z),  \\mathbf{K}_{tt} - \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} S \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt}\\right). N(f(t);\u03bct+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b).</p></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D']</code> <p>The test inputs at which we wish to make a prediction.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the low-rank GP at\n    the test inputs.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian","title":"<code>WhitenedVariationalGaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>VariationalGaussian</code></p> <p>The whitened variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(Lz\u03bc+mz,LzSLz\u22a4)q(u) = \\mathcal{N}(Lz \\mu + mz, Lz S Lz^{\\top})q(u)=N(Lz\u03bc+mz,LzSLz\u22a4). We parameterise this over \u03bc\\mu\u03bc and sqrtsqrtsqrt with S=sqrtsqrt\u22a4S = sqrt sqrt^{\\top}S=sqrtsqrt\u22a4.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, 'N D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.variational_mean","title":"<code>variational_mean: Union[Float[Array, 'N 1'], None] = param_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.variational_root_covariance","title":"<code>variational_root_covariance: Float[Array, 'N N'] = param_field(None, bijector=tfb.FillTriangular())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.__init__","title":"<code>__init__(posterior: AbstractPosterior, inducing_inputs: Float[Array, 'N D'], jitter: ScalarFloat = static_field(1e-06), variational_mean: Union[Float[Array, 'N 1'], None] = param_field(None), variational_root_covariance: Float[Array, 'N N'] = param_field(None, bijector=tfb.FillTriangular())) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(0,I)]. \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)]\\\\     &amp; = \\operatorname{KL}[N(\\mu  , S)\\mid\\mid N(0, I)]. \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(0,I)].\u200b\u200b</p></p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational\n    approximation and the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, 'N D']) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t)) = \\int p(f(t)\\midu) q(u) du, which can be computed in closed form as <p>N(f(t);\u03bct+KtzLz\u22a4\u03bc,Ktt\u2212KtzKzz\u22121Kzt+KtzLz\u22a4SLz\u22121Kzt).     \\mathcal{N}\\left(f(t); \\mu t  +  \\mathbf{K}_{tz} \\mathbf{L}z^{\\top} \\mu  ,  \\mathbf{K}_{tt}  -  \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt}  +  \\mathbf{K}_{tz} \\mathbf{L}z^{\\top} S \\mathbf{L}z^{-1} \\mathbf{K}_{zt} \\right). N(f(t);\u03bct+Ktz\u200bLz\u22a4\u03bc,Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bLz\u22a4SLz\u22121Kzt\u200b).</p></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D']</code> <p>The test inputs at which we wish to make a prediction.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the low-rank GP at\n    the test inputs.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian","title":"<code>NaturalVariationalGaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractVariationalGaussian</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(\u03bc,S)q(u) = N(\\mu, S)q(u)=N(\u03bc,S). Expressing the variational distribution, in the form of the exponential family, q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8))q(u) = exp(\\theta^{\\top} T(u) - a(\\theta))q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8)), gives rise to the natural parameterisation \u03b8=(\u03b81,\u03b82)=(S\u22121\u03bc,\u2212S\u22121/2)\\theta  = (\\theta_{1}, \\theta_{2}) = (S^{-1}\\mu, -S^{-1}/2)\u03b8=(\u03b81\u200b,\u03b82\u200b)=(S\u22121\u03bc,\u2212S\u22121/2), to perform model inference, where T(u)=[u,uu\u22a4]T(u) = [u, uu^{\\top}]T(u)=[u,uu\u22a4] are the sufficient statistics.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, 'N D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.natural_vector","title":"<code>natural_vector: Float[Array, 'M 1'] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.natural_matrix","title":"<code>natural_matrix: Float[Array, 'M M'] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__init__","title":"<code>__init__(posterior: AbstractPosterior, inducing_inputs: Float[Array, 'N D'], jitter: ScalarFloat = static_field(1e-06), natural_vector: Float[Array, 'M 1'] = None, natural_matrix: Float[Array, 'M M'] = None) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(mz,Kzz)], \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)] \\\\     &amp; = \\operatorname{KL}[N(\\mu, S)\\mid\\mid N(mz, \\mathbf{K}_{zz})], \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(mz,Kzz\u200b)],\u200b\u200b</p> with $\\mu$ and $S$ computed from the natural parameterisation $\\theta  = (S^{-1}\\mu  , -S^{-1}/2)$.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational approximation and\n    the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, 'N D']) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs $t$.</p> <p>This is the integral q(f(t))=\u222bp(f(t)\u2223u)q(u)duq(f(t)) = \\int p(f(t)\\mid u) q(u) \\mathrm{d}uq(f(t))=\u222bp(f(t)\u2223u)q(u)du, which can be computed in closed form as <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt),      \\mathcal{N}\\left(f(t); \\mu  t + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} (\\mu   - \\mu  z),  \\mathbf{K}_{tt} - \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} S \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} \\right), N(f(t);\u03bct+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b),</p> with \u03bc\\mu\u03bc and SSS computed from the natural parameterisation \u03b8=(S\u22121\u03bc,\u2212S\u22121/2)\\theta = (S^{-1}\\mu  , -S^{-1}/2)\u03b8=(S\u22121\u03bc,\u2212S\u22121/2).</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts a set of test points and will\n    return the predictive distribution at those points.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian","title":"<code>ExpectationVariationalGaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractVariationalGaussian</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(\u03bc,S)q(u) = \\mathcal{N}(\\mu, S)q(u)=N(\u03bc,S). Expressing the variational distribution, in the form of the exponential family, q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8))q(u) = exp(\\theta^{\\top} T(u) - a(\\theta))q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8)), gives rise to the natural parameterisation \u03b8=(\u03b81,\u03b82)=(S\u22121\u03bc,\u2212S\u22121/2)\\theta  = (\\theta_{1}, \\theta_{2}) = (S^{-1}\\mu  , -S^{-1}/2)\u03b8=(\u03b81\u200b,\u03b82\u200b)=(S\u22121\u03bc,\u2212S\u22121/2) and sufficient statistics T(u)=[u,uu\u22a4]T(u) = [u, uu^{\\top}]T(u)=[u,uu\u22a4]. The expectation parameters are given by \u03bd=\u222bT(u)q(u)du\\nu = \\int T(u) q(u) \\mathrm{d}u\u03bd=\u222bT(u)q(u)du. This gives a parameterisation, \u03bd=(\u03bd1,\u03bd2)=(\u03bc,S+uu\u22a4)\\nu = (\\nu_{1}, \\nu_{2}) = (\\mu  , S + uu^{\\top})\u03bd=(\u03bd1\u200b,\u03bd2\u200b)=(\u03bc,S+uu\u22a4) to perform model inference over.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, 'N D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.expectation_vector","title":"<code>expectation_vector: Float[Array, 'M 1'] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.expectation_matrix","title":"<code>expectation_matrix: Float[Array, 'M M'] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__init__","title":"<code>__init__(posterior: AbstractPosterior, inducing_inputs: Float[Array, 'N D'], jitter: ScalarFloat = static_field(1e-06), expectation_vector: Float[Array, 'M 1'] = None, expectation_matrix: Float[Array, 'M M'] = None) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Evaluate the prior KL-divergence.</p> <p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061(q(f(\u22c5))\u2223\u2223p(\u22c5))=KL\u2061(q(u)\u2223\u2223p(u))=KL\u2061(N(\u03bc,S)\u2223\u2223N(mz,Kzz)), \\begin{align} \\operatorname{KL}(q(f(\\cdot))\\mid\\mid p(\\cdot)) &amp; = \\operatorname{KL}(q(u)\\mid\\mid p(u)) \\\\     &amp; =\\operatorname{KL}(\\mathcal{N}(\\mu, S)\\mid\\mid \\mathcal{N}(m_z, K_{zz})), \\end{align} KL(q(f(\u22c5))\u2223\u2223p(\u22c5))\u200b=KL(q(u)\u2223\u2223p(u))=KL(N(\u03bc,S)\u2223\u2223N(mz\u200b,Kzz\u200b)),\u200b\u200b</p> where $\\mu$ and $S$ are the expectation parameters of the variational distribution and $m_z$ and $K_{zz}$ are the mean and covariance of the prior distribution.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational approximation and\n    the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, 'N D']) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the predictive distribution.</p> <p>Compute the predictive distribution of the GP at the test inputs $t$.</p> <p>This is the integral $q(f(t)) = \\int p(f(t)\\mid u)q(u)\\mathrm{d}u$, which can be computed in closed form as  which can be computed in closed form as <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt) \\mathcal{N}(f(t); \\mu_t + \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}(\\mu - \\mu_z), \\mathbf{K}_{tt} - \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}\\mathbf{K}_{zt} + \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}\\mathbf{S} \\mathbf{K}_{zz}^{-1}\\mathbf{K}_{zt}) N(f(t);\u03bct\u200b+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz\u200b),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b)</p></p> <p>with $\\mu$ and $S$ computed from the expectation parameterisation $\\eta = (\\mu, S + uu^\\top)$.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the GP at the\n    test inputs $t$.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian","title":"<code>CollapsedVariationalGaussian</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractVariationalGaussian</code></p> <p>Collapsed variational Gaussian.</p> <p>Collapsed variational Gaussian family of probability distributions. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, 'N D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__init__","title":"<code>__init__(posterior: AbstractPosterior, inducing_inputs: Float[Array, 'N D'], jitter: ScalarFloat = static_field(1e-06)) -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, 'N D'], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D']</code> <p>The test inputs $t$ at which to make predictions.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data that was used to fit the GP.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the collapsed\n    variational Gaussian process at the test inputs $t$.\n</code></pre>"},{"location":"api/base/module/","title":"Module","text":""},{"location":"api/base/module/#gpjax.base.module","title":"<code>gpjax.base.module</code>","text":""},{"location":"api/base/module/#gpjax.base.module.__all__","title":"<code>__all__ = ['Module', 'meta_leaves', 'meta_flatten', 'meta_map', 'meta', 'static_field']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Self","title":"<code>Self = TypeVar('Self')</code>  <code>module-attribute</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Module","title":"<code>Module</code>","text":"<p>             Bases: <code>Pytree</code></p>"},{"location":"api/base/module/#gpjax.base.module.Module.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Module.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/base/module/#gpjax.base.module.static_field","title":"<code>static_field(default: Any = dataclasses.MISSING, *, default_factory: Any = dataclasses.MISSING, init: bool = True, repr: bool = True, hash: Optional[bool] = None, compare: bool = True, metadata: Optional[Mapping[str, Any]] = None)</code>","text":""},{"location":"api/base/module/#gpjax.base.module.meta_leaves","title":"<code>meta_leaves(pytree: Module, *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; List[Tuple[Optional[Dict[str, Any]], Any]]</code>","text":"<p>Returns the meta of the leaves of the pytree.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>pytree to get the meta of.</p> required <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_leaves--returns","title":"Returns","text":"<pre><code>List[Tuple[Dict[str, Any], Any]]: meta of the leaves of the pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta_flatten","title":"<code>meta_flatten(pytree: Union[Module, Any], *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Union[Module, Any]</code>","text":"<p>Returns the meta of the Module.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>Module to get the meta of.</p> required <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_flatten--returns","title":"Returns","text":"<pre><code>Module: meta of the Module.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta_map","title":"<code>meta_map(f: Callable[[Any, Dict[str, Any]], Any], pytree: Union[Module, Any], *rest: Any, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Union[Module, Any]</code>","text":"<p>Apply a function to a Module where the first argument are the pytree leaves, and the second argument are the Module metadata leaves. Args:     f (Callable[[Any, Dict[str, Any]], Any]): The function to apply to the pytree.     pytree (Module): The pytree to apply the function to.     rest (Any, optional): Additional pytrees to apply the function to. Defaults to None.     is_leaf (Callable[[Any], bool], optional): predicate to determine if a node is a leaf. Defaults to None.</p>"},{"location":"api/base/module/#gpjax.base.module.meta_map--returns","title":"Returns","text":"<pre><code>Module: The transformed pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta","title":"<code>meta(pytree: Module, *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Module</code>","text":"<p>Returns the metadata of the Module as a pytree.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>pytree to get the metadata of.</p> required"},{"location":"api/base/module/#gpjax.base.module.meta--returns","title":"Returns","text":"<pre><code>Module: metadata of the pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.save_tree","title":"<code>save_tree(path: str, model: Module, overwrite: bool = False, iterate: int = None) -&gt; None</code>","text":""},{"location":"api/base/module/#gpjax.base.module.load_tree","title":"<code>load_tree(path: str, model: Module) -&gt; Module</code>","text":""},{"location":"api/base/param/","title":"Param","text":""},{"location":"api/base/param/#gpjax.base.param","title":"<code>gpjax.base.param</code>","text":""},{"location":"api/base/param/#gpjax.base.param.__all__","title":"<code>__all__ = ['param_field']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/param/#gpjax.base.param.param_field","title":"<code>param_field(default: Any = dataclasses.MISSING, *, bijector: Optional[tfb.Bijector] = None, trainable: bool = True, default_factory: Any = dataclasses.MISSING, init: bool = True, repr: bool = True, hash: Optional[bool] = None, compare: bool = True, metadata: Optional[Mapping[str, Any]] = None)</code>","text":""},{"location":"api/decision_making/decision_maker/","title":"Decision Maker","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker","title":"<code>gpjax.decision_making.decision_maker</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker","title":"<code>AbstractDecisionMaker</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>AbstractDecisionMaker abstract base class which handles the core decision making loop, where we sequentially decide on points to query our function of interest at. The decision making loop is split into two key steps, <code>ask</code> and <code>tell</code>. The <code>ask</code> step is typically used to decide which point to query next. The <code>tell</code> step is typically used to update models and datasets with newly queried points. These steps can be combined in a 'run' loop which alternates between asking which point to query next and telling the decision maker about the newly queried point having evaluated the black-box function of interest at this point.</p> <p>Attributes:</p> Name Type Description <code>search_space</code> <code>AbstractSearchSpace</code> <p>Search space over which we can evaluate the</p> <code>posterior_handlers</code> <code>Dict[str, PosteriorHandler]</code> <p>Dictionary of posterior handlers, which are used to update posteriors throughout the decision making loop. Note that the word <code>posteriors</code> is used for consistency with GPJax, but these objects are typically referred to as <code>models</code> in the model-based decision making literature. Tags are used to distinguish between posteriors. In a typical Bayesian optimisation setup one of the tags will be <code>OBJECTIVE</code>, defined in decision_making.utils.</p> <code>datasets</code> <code>Dict[str, Dataset]</code> <p>Dictionary of datasets, which are augmented with observations throughout the decision making loop. In a typical setup they are also used to update the posteriors, using the <code>posterior_handlers</code>. Tags are used to distinguish datasets, and correspond to tags in <code>posterior_handlers</code>.</p> <code>key</code> <code>KeyArray</code> <p>JAX random key, used to generate random numbers.</p> <code>batch_size</code> <code>int</code> <p>Number of points to query at each step of the decision making loop. Note that <code>SinglePointUtilityFunction</code>s are only capable of generating one point to be queried at each iteration of the decision making loop.</p> <code>post_ask</code> <code>List[Callable]</code> <p>List of functions to be executed after each ask step.</p> <code>post_tell</code> <code>List[Callable]</code> <p>List of functions to be executed after each tell step.</p>"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.search_space","title":"<code>search_space: AbstractSearchSpace</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.posterior_handlers","title":"<code>posterior_handlers: Dict[str, PosteriorHandler]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.datasets","title":"<code>datasets: Dict[str, Dataset]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.key","title":"<code>key: KeyArray</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.batch_size","title":"<code>batch_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.post_ask","title":"<code>post_ask: List[Callable]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.post_tell","title":"<code>post_tell: List[Callable]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.__init__","title":"<code>__init__(search_space: AbstractSearchSpace, posterior_handlers: Dict[str, PosteriorHandler], datasets: Dict[str, Dataset], key: KeyArray, batch_size: int, post_ask: List[Callable], post_tell: List[Callable]) -&gt; None</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.__post_init__","title":"<code>__post_init__()</code>","text":"<p>At initialisation we check that the posterior handlers and datasets are consistent (i.e. have the same tags), and then initialise the posteriors, optimizing them using the corresponding datasets.</p>"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.ask","title":"<code>ask(key: KeyArray) -&gt; Float[Array, 'B D']</code>  <code>abstractmethod</code>","text":"<p>Get the point(s) to be queried next.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyArray</code> <p>JAX PRNG key for controlling random state.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'B D']</code> <p>Float[Array, \"1 D\"]: Point to be queried next</p>"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.tell","title":"<code>tell(observation_datasets: Mapping[str, Dataset], key: KeyArray)</code>","text":"<p>Add newly observed data to datasets and update the corresponding posteriors.</p> <p>Parameters:</p> Name Type Description Default <code>observation_datasets</code> <code>Mapping[str, Dataset]</code> <p>Dictionary of datasets</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key for controlling random state.</p> required"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.AbstractDecisionMaker.run","title":"<code>run(n_steps: int, black_box_function_evaluator: FunctionEvaluator) -&gt; Mapping[str, Dataset]</code>","text":"<p>Run the decision making loop continuously for for <code>n_steps</code>. This is broken down into three main steps: 1. Call the <code>ask</code> method to get the point to be queried next. 2. Call the <code>black_box_function_evaluator</code> to evaluate the black box functions of interest at the point chosen to be queried. 3. Call the <code>tell</code> method to update the datasets and posteriors with the newly observed data.</p> <p>In addition to this, after the <code>ask</code> step, the functions in the <code>post_ask</code> list are executed, taking as arguments the decision maker and the point chosen to be queried next. Similarly, after the <code>tell</code> step, the functions in the <code>post_tell</code> list are executed, taking the decision maker as the sole argument.</p> <p>Parameters:</p> Name Type Description Default <code>n_steps</code> <code>int</code> <p>Number of steps to run the decision making loop for.</p> required <code>black_box_function_evaluator</code> <code>FunctionEvaluator</code> <p>Function evaluator which evaluates the black box functions of interest at supplied points.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Dataset]</code> <p>Mapping[str, Dataset]: Dictionary of datasets containing the observations</p> <code>Mapping[str, Dataset]</code> <p>made throughout the decision making loop, as well as the initial data</p> <code>Mapping[str, Dataset]</code> <p>supplied when initialising the <code>DecisionMaker</code>.</p>"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker","title":"<code>UtilityDrivenDecisionMaker</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractDecisionMaker</code></p> <p>UtilityDrivenDecisionMaker class which handles the core decision making loop in a typical model-based decision making setup. In this setup we use surrogate model(s) for the function(s) of interest, and define a utility function (often called the 'acquisition function' in the context of Bayesian optimisation) which characterises how useful it would be to query a given point within the search space given the data we have observed so far. This can then be used to decide which point(s) to query next.</p> <p>The decision making loop is split into two key steps, <code>ask</code> and <code>tell</code>. The <code>ask</code> step forms a <code>UtilityFunction</code> from the current <code>posteriors</code> and <code>datasets</code> and returns the point which maximises it. It also stores the formed utility function under the attribute <code>self.current_utility_function</code> so that it can be called, for instance for plotting, after the <code>ask</code> function has been called. The <code>tell</code> step adds a newly queried point to the <code>datasets</code> and updates the <code>posteriors</code>.</p> <p>This can be run as a typical ask-tell loop, or the <code>run</code> method can be used to run the decision making loop for a fixed number of steps. Moreover, the <code>run</code> method executes the functions in <code>post_ask</code> and <code>post_tell</code> after each ask and tell step respectively. This enables the user to add custom functionality, such as the ability to plot values of interest during the optimization process.</p> <p>Attributes:</p> Name Type Description <code>utility_function_builder</code> <code>AbstractUtilityFunctionBuilder</code> <p>Object which     builds utility functions from posteriors and datasets, to decide where     to query next. In a typical Bayesian optimisation setup the point chosen to     be queried next is the point which maximizes the utility function.</p> <code>utility_maximizer</code> <code>AbstractUtilityMaximizer</code> <p>Object which maximizes utility functions over the search space.</p>"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.search_space","title":"<code>search_space: AbstractSearchSpace</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.posterior_handlers","title":"<code>posterior_handlers: Dict[str, PosteriorHandler]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.datasets","title":"<code>datasets: Dict[str, Dataset]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.key","title":"<code>key: KeyArray</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.batch_size","title":"<code>batch_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.post_ask","title":"<code>post_ask: List[Callable]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.post_tell","title":"<code>post_tell: List[Callable]</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.utility_function_builder","title":"<code>utility_function_builder: AbstractUtilityFunctionBuilder</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.utility_maximizer","title":"<code>utility_maximizer: AbstractUtilityMaximizer</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.tell","title":"<code>tell(observation_datasets: Mapping[str, Dataset], key: KeyArray)</code>","text":"<p>Add newly observed data to datasets and update the corresponding posteriors.</p> <p>Parameters:</p> Name Type Description Default <code>observation_datasets</code> <code>Mapping[str, Dataset]</code> <p>Dictionary of datasets</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key for controlling random state.</p> required"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.run","title":"<code>run(n_steps: int, black_box_function_evaluator: FunctionEvaluator) -&gt; Mapping[str, Dataset]</code>","text":"<p>Run the decision making loop continuously for for <code>n_steps</code>. This is broken down into three main steps: 1. Call the <code>ask</code> method to get the point to be queried next. 2. Call the <code>black_box_function_evaluator</code> to evaluate the black box functions of interest at the point chosen to be queried. 3. Call the <code>tell</code> method to update the datasets and posteriors with the newly observed data.</p> <p>In addition to this, after the <code>ask</code> step, the functions in the <code>post_ask</code> list are executed, taking as arguments the decision maker and the point chosen to be queried next. Similarly, after the <code>tell</code> step, the functions in the <code>post_tell</code> list are executed, taking the decision maker as the sole argument.</p> <p>Parameters:</p> Name Type Description Default <code>n_steps</code> <code>int</code> <p>Number of steps to run the decision making loop for.</p> required <code>black_box_function_evaluator</code> <code>FunctionEvaluator</code> <p>Function evaluator which evaluates the black box functions of interest at supplied points.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Dataset]</code> <p>Mapping[str, Dataset]: Dictionary of datasets containing the observations</p> <code>Mapping[str, Dataset]</code> <p>made throughout the decision making loop, as well as the initial data</p> <code>Mapping[str, Dataset]</code> <p>supplied when initialising the <code>DecisionMaker</code>.</p>"},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.__init__","title":"<code>__init__(search_space: AbstractSearchSpace, posterior_handlers: Dict[str, PosteriorHandler], datasets: Dict[str, Dataset], key: KeyArray, batch_size: int, post_ask: List[Callable], post_tell: List[Callable], utility_function_builder: AbstractUtilityFunctionBuilder, utility_maximizer: AbstractUtilityMaximizer) -&gt; None</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/decision_making/decision_maker/#gpjax.decision_making.decision_maker.UtilityDrivenDecisionMaker.ask","title":"<code>ask(key: KeyArray) -&gt; Float[Array, 'B D']</code>","text":"<p>Get updated utility function(s) and return the point(s) which maximises it/them. This method also stores the utility function(s) in <code>self.current_utility_functions</code> so that they can be accessed after the ask function has been called. This is useful for non-deterministic utility functions, which may differ between calls to <code>ask</code> due to the splitting of <code>self.key</code>.</p> <p>Note that in general <code>SinglePointUtilityFunction</code>s are only capable of generating one point to be queried at each iteration of the decision making loop (i.e. <code>self.batch_size</code> must be 1). However, Thompson sampling can be used in a batched setting by drawing a batch of different samples from the GP posterior. This is done by calling <code>build_utility_function</code> with different keys sequentilly, and optimising each of these individual samples in sequence in order to obtain <code>self.batch_size</code> points to query next.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyArray</code> <p>JAX PRNG key for controlling random state.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'B D']</code> <p>Float[Array, \"B D\"]: Point(s) to be queried next.</p>"},{"location":"api/decision_making/posterior_handler/","title":"Posterior Handler","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler","title":"<code>gpjax.decision_making.posterior_handler</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.LikelihoodBuilder","title":"<code>LikelihoodBuilder = Callable[[int], AbstractLikelihood]</code>  <code>module-attribute</code>","text":"<p>Type alias for likelihood builders, which take the number of datapoints as input and return a likelihood object initialised with the given number of datapoints.</p>"},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler","title":"<code>PosteriorHandler</code>  <code>dataclass</code>","text":"<p>Class for handling the creation and updating of a GP posterior as new data is observed.</p> <p>Attributes:</p> Name Type Description <code>prior</code> <code>AbstractPrior</code> <p>Prior to use when forming the posterior.</p> <code>likelihood_builder</code> <code>LikelihoodBuilder</code> <p>Function which takes the number of</p> <code>optimization_objective</code> <code>AbstractObjective</code> <p>Objective to use for optimizing the</p> <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer to use for optimizing the</p> <code>num_optimization_iterations</code> <code>int</code> <p>Number of iterations to optimize</p>"},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.prior","title":"<code>prior: AbstractPrior</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.likelihood_builder","title":"<code>likelihood_builder: LikelihoodBuilder</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.optimization_objective","title":"<code>optimization_objective: AbstractObjective</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.optimizer","title":"<code>optimizer: ox.GradientTransformation</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.num_optimization_iters","title":"<code>num_optimization_iters: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.__init__","title":"<code>__init__(prior: AbstractPrior, likelihood_builder: LikelihoodBuilder, optimization_objective: AbstractObjective, optimizer: ox.GradientTransformation, num_optimization_iters: int) -&gt; None</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.get_posterior","title":"<code>get_posterior(dataset: Dataset, optimize: bool, key: Optional[KeyArray] = None) -&gt; AbstractPosterior</code>","text":"<p>Initialise (and optionally optimize) a posterior using the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to get posterior for.</p> required <code>optimize</code> <code>bool</code> <p>Whether to optimize the posterior hyperparameters.</p> required <code>key</code> <code>Optional[KeyArray]</code> <p>A JAX PRNG key which is used for optimizing the posterior</p> <code>None</code> <p>Returns:</p> Type Description <code>AbstractPosterior</code> <p>Posterior for the given dataset.</p>"},{"location":"api/decision_making/posterior_handler/#gpjax.decision_making.posterior_handler.PosteriorHandler.update_posterior","title":"<code>update_posterior(dataset: Dataset, previous_posterior: AbstractPosterior, optimize: bool, key: Optional[KeyArray] = None) -&gt; AbstractPosterior</code>","text":"<p>Update the given posterior with the given dataset. This needs to be done when the number of datapoints in the (training) dataset of the posterior changes, as the <code>AbstractLikelihood</code> class requires the number of datapoints to be specified. Hyperparameters may or may not be optimized, depending on the value of the <code>optimize</code> parameter. Note that the updated poterior will be initialised with the same prior hyperparameters as the previous posterior, but the likelihood will be re-initialised with the new number of datapoints, and hyperparameters set as in the <code>likelihood_builder</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to get posterior for.</p> required <code>previous_posterior</code> <code>AbstractPosterior</code> <p>Posterior being updated. This is supplied as one may</p> required <code>optimize</code> <code>bool</code> <p>Whether to optimize the posterior hyperparameters.</p> required <code>key</code> <code>Optional[KeyArray]</code> <p>A JAX PRNG key which is used for optimizing the posterior</p> <code>None</code>"},{"location":"api/decision_making/search_space/","title":"Search Space","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space","title":"<code>gpjax.decision_making.search_space</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.AbstractSearchSpace","title":"<code>AbstractSearchSpace</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>The <code>AbstractSearchSpace</code> class is an abstract base class for search spaces, which are used to define domains for sampling and optimisation functionality in GPJax.</p>"},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.AbstractSearchSpace.dimensionality","title":"<code>dimensionality: int</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Dimensionality of the search space. Returns:     int: Dimensionality of the search space.</p>"},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.AbstractSearchSpace.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.AbstractSearchSpace.sample","title":"<code>sample(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>  <code>abstractmethod</code>","text":"<p>Sample points from the search space. Args:     num_points (int): Number of points to be sampled from the search space.     key (KeyArray): JAX PRNG key. Returns:     Float[Array, \"N D\"]: <code>num_points</code> points sampled from the search space.</p>"},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace","title":"<code>ContinuousSearchSpace</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractSearchSpace</code></p> <p>The <code>ContinuousSearchSpace</code> class is used to bound the domain of continuous real functions of dimension DDD.</p>"},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace.lower_bounds","title":"<code>lower_bounds: Float[Array, ' D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace.upper_bounds","title":"<code>upper_bounds: Float[Array, ' D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace.dimensionality","title":"<code>dimensionality: int</code>  <code>property</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace.__init__","title":"<code>__init__(lower_bounds: Float[Array, ' D'], upper_bounds: Float[Array, ' D']) -&gt; None</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/decision_making/search_space/#gpjax.decision_making.search_space.ContinuousSearchSpace.sample","title":"<code>sample(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>","text":"<p>Sample points from the search space using a Halton sequence.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to be sampled from the search space.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:     Float[Array, \"N D\"]: <code>num_points</code> points sampled using the Halton sequence     from the search space.</p>"},{"location":"api/decision_making/utility_maximizer/","title":"Utility Maximizer","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer","title":"<code>gpjax.decision_making.utility_maximizer</code>","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.AbstractUtilityMaximizer","title":"<code>AbstractUtilityMaximizer = AbstractSinglePointUtilityMaximizer</code>  <code>module-attribute</code>","text":"<p>Type alias for a utility maximizer. Currently we only support single point utility functions, but in future may support batched utility functions.</p>"},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.AbstractSinglePointUtilityMaximizer","title":"<code>AbstractSinglePointUtilityMaximizer</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for single point utility function maximizers.</p>"},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.AbstractSinglePointUtilityMaximizer.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.AbstractSinglePointUtilityMaximizer.maximize","title":"<code>maximize(utility_function: SinglePointUtilityFunction, search_space: AbstractSearchSpace, key: KeyArray) -&gt; Float[Array, '1 D']</code>  <code>abstractmethod</code>","text":"<p>Maximize the given utility function over the search space provided.</p> <p>Parameters:</p> Name Type Description Default <code>utility_function</code> <code>UtilityFunction</code> <p>Utility function to be</p> required <code>search_space</code> <code>AbstractSearchSpace</code> <p>Search space over which to maximize</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Type Description <code>Float[Array, '1 D']</code> <p>Float[Array, \"1 D\"]: Point at which the utility function is maximized.</p>"},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.ContinuousSinglePointUtilityMaximizer","title":"<code>ContinuousSinglePointUtilityMaximizer</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractSinglePointUtilityMaximizer</code></p> <p>The <code>ContinuousUtilityMaximizer</code> class is used to maximize utility functions over the continuous domain with L-BFGS-B. First we sample the utility function at <code>num_initial_samples</code> points from the search space, and then we run L-BFGS-B from the best of these initial points. We run this process <code>num_restarts</code> number of times, each time sampling a different random set of <code>num_initial_samples</code>initial points.</p>"},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.ContinuousSinglePointUtilityMaximizer.num_initial_samples","title":"<code>num_initial_samples: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.ContinuousSinglePointUtilityMaximizer.num_restarts","title":"<code>num_restarts: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.ContinuousSinglePointUtilityMaximizer.__init__","title":"<code>__init__(num_initial_samples: int, num_restarts: int) -&gt; None</code>","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.ContinuousSinglePointUtilityMaximizer.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/decision_making/utility_maximizer/#gpjax.decision_making.utility_maximizer.ContinuousSinglePointUtilityMaximizer.maximize","title":"<code>maximize(utility_function: SinglePointUtilityFunction, search_space: ContinuousSearchSpace, key: KeyArray) -&gt; Float[Array, '1 D']</code>","text":""},{"location":"api/decision_making/utils/","title":"Utils","text":""},{"location":"api/decision_making/utils/#gpjax.decision_making.utils","title":"<code>gpjax.decision_making.utils</code>","text":""},{"location":"api/decision_making/utils/#gpjax.decision_making.utils.OBJECTIVE","title":"<code>OBJECTIVE: Final[str] = 'OBJECTIVE'</code>  <code>module-attribute</code>","text":"<p>Tag for the objective dataset/function in standard utility functions.</p>"},{"location":"api/decision_making/utils/#gpjax.decision_making.utils.FunctionEvaluator","title":"<code>FunctionEvaluator = Callable[[Float[Array, 'N D']], Dict[str, Dataset]]</code>  <code>module-attribute</code>","text":"<p>Type alias for function evaluators, which take an array of points of shape [N,D][N, D][N,D] and evaluate a set of functions at each point, returning a mapping from function tags to datasets of the evaluated points. This is the same as the <code>Observer</code> in Trieste: https://github.com/secondmind-labs/trieste/blob/develop/trieste/observer.py</p>"},{"location":"api/decision_making/utils/#gpjax.decision_making.utils.build_function_evaluator","title":"<code>build_function_evaluator(functions: Dict[str, Callable[[Float[Array, 'N D']], Float[Array, 'N 1']]]) -&gt; FunctionEvaluator</code>","text":"<p>Takes a dictionary of functions and returns a <code>FunctionEvaluator</code> which can be used to evaluate each of the functions at a supplied set of points and return a dictionary of datasets storing the evaluated points.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/","title":"Continuous Functions","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions","title":"<code>gpjax.decision_making.test_functions.continuous_functions</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction","title":"<code>AbstractContinuousTestFunction</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for continuous test functions.</p> <p>Attributes:</p> Name Type Description <code>search_space</code> <code>ContinuousSearchSpace</code> <p>Search space for the function.</p> <code>minimizer</code> <code>Float[Array, '1 D']</code> <p>Minimizer of the function (to 5 decimal places)</p> <code>minimum</code> <code>Float[Array, '1 1']</code> <p>Minimum of the function (to 5 decimal places).</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction.search_space","title":"<code>search_space: ContinuousSearchSpace</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction.minimizer","title":"<code>minimizer: Float[Array, '1 D']</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction.minimum","title":"<code>minimum: Float[Array, '1 1']</code>  <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction.generate_dataset","title":"<code>generate_dataset(num_points: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Generate a toy dataset from the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset of points sampled from the test function.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction.generate_test_points","title":"<code>generate_test_points(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>","text":"<p>Generate test points from the search space of the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N D']</code> <p>Float[Array, 'N D']: Test points sampled from the search space.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.AbstractContinuousTestFunction.evaluate","title":"<code>evaluate(x: Float[Array, 'N D']) -&gt; Float[Array, 'N 1']</code>  <code>abstractmethod</code>","text":"<p>Evaluate the test function at a set of points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'N D']</code> <p>Points to evaluate the test function at.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N 1']</code> <p>Float[Array, 'N 1']: Values of the test function at the points.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester","title":"<code>Forrester</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractContinuousTestFunction</code></p> <p>Forrester function introduced in 'Engineering design via surrogate modelling: a practical guide' (Forrester et al. 2008), rescaled to have zero mean and unit variance over [0,1][0, 1][0,1].</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.search_space","title":"<code>search_space = ContinuousSearchSpace(lower_bounds=jnp.array([0.0]), upper_bounds=jnp.array([1.0]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.minimizer","title":"<code>minimizer = jnp.array([[0.75725]])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.minimum","title":"<code>minimum = jnp.array([[-1.4528]])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.generate_dataset","title":"<code>generate_dataset(num_points: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Generate a toy dataset from the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset of points sampled from the test function.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.generate_test_points","title":"<code>generate_test_points(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>","text":"<p>Generate test points from the search space of the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N D']</code> <p>Float[Array, 'N D']: Test points sampled from the search space.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.__init__","title":"<code>__init__(search_space=ContinuousSearchSpace(lower_bounds=jnp.array([0.0]), upper_bounds=jnp.array([1.0])), minimizer=jnp.array([[0.75725]]), minimum=jnp.array([[-1.4528]])) -&gt; None</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Forrester.evaluate","title":"<code>evaluate(x: Float[Array, 'N D']) -&gt; Float[Array, 'N 1']</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice","title":"<code>LogarithmicGoldsteinPrice</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractContinuousTestFunction</code></p> <p>Logarithmic Goldstein-Price function introduced in 'A benchmark of kriging-based infill criteria for noisy optimization' (Picheny et al. 2013), which has zero mean and unit variance over [0,1]2[0, 1]^2[0,1]2.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.search_space","title":"<code>search_space = ContinuousSearchSpace(lower_bounds=jnp.array([0.0, 0.0]), upper_bounds=jnp.array([1.0, 1.0]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.minimizer","title":"<code>minimizer = jnp.array([[0.5, 0.25]])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.minimum","title":"<code>minimum = jnp.array([[-3.12913]])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.generate_dataset","title":"<code>generate_dataset(num_points: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Generate a toy dataset from the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset of points sampled from the test function.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.generate_test_points","title":"<code>generate_test_points(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>","text":"<p>Generate test points from the search space of the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N D']</code> <p>Float[Array, 'N D']: Test points sampled from the search space.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.__init__","title":"<code>__init__(search_space=ContinuousSearchSpace(lower_bounds=jnp.array([0.0, 0.0]), upper_bounds=jnp.array([1.0, 1.0])), minimizer=jnp.array([[0.5, 0.25]]), minimum=jnp.array([[-3.12913]])) -&gt; None</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.LogarithmicGoldsteinPrice.evaluate","title":"<code>evaluate(x: Float[Array, 'N D']) -&gt; Float[Array, 'N 1']</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic","title":"<code>Quadratic</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractContinuousTestFunction</code></p> <p>Toy quadratic function defined over [0,1][0, 1][0,1].</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.search_space","title":"<code>search_space = ContinuousSearchSpace(lower_bounds=jnp.array([0.0]), upper_bounds=jnp.array([1.0]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.minimizer","title":"<code>minimizer = jnp.array([[0.5]])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.minimum","title":"<code>minimum = jnp.array([[0.0]])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.generate_dataset","title":"<code>generate_dataset(num_points: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Generate a toy dataset from the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset of points sampled from the test function.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.generate_test_points","title":"<code>generate_test_points(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>","text":"<p>Generate test points from the search space of the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N D']</code> <p>Float[Array, 'N D']: Test points sampled from the search space.</p>"},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.__init__","title":"<code>__init__(search_space=ContinuousSearchSpace(lower_bounds=jnp.array([0.0]), upper_bounds=jnp.array([1.0])), minimizer=jnp.array([[0.5]]), minimum=jnp.array([[0.0]])) -&gt; None</code>","text":""},{"location":"api/decision_making/test_functions/continuous_functions/#gpjax.decision_making.test_functions.continuous_functions.Quadratic.evaluate","title":"<code>evaluate(x: Float[Array, 'N D']) -&gt; Float[Array, 'N 1']</code>","text":""},{"location":"api/decision_making/test_functions/non_conjugate_functions/","title":"Non Conjugate Functions","text":""},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions","title":"<code>gpjax.decision_making.test_functions.non_conjugate_functions</code>","text":""},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions.PoissonTestFunction","title":"<code>PoissonTestFunction</code>  <code>dataclass</code>","text":"<p>Test function for GPs utilising the Poisson likelihood. Function taken from https://docs.jaxgaussianprocesses.com/examples/poisson/#dataset.</p> <p>Attributes:</p> Name Type Description <code>search_space</code> <code>ContinuousSearchSpace</code> <p>Search space for the function.</p>"},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions.PoissonTestFunction.search_space","title":"<code>search_space = ContinuousSearchSpace(lower_bounds=jnp.array([-2.0]), upper_bounds=jnp.array([2.0]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions.PoissonTestFunction.__init__","title":"<code>__init__(search_space=ContinuousSearchSpace(lower_bounds=jnp.array([-2.0]), upper_bounds=jnp.array([2.0]))) -&gt; None</code>","text":""},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions.PoissonTestFunction.generate_dataset","title":"<code>generate_dataset(num_points: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Generate a toy dataset from the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset of points sampled from the test function.</p>"},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions.PoissonTestFunction.generate_test_points","title":"<code>generate_test_points(num_points: int, key: KeyArray) -&gt; Float[Array, 'N D']</code>","text":"<p>Generate test points from the search space of the test function.</p> <p>Parameters:</p> Name Type Description Default <code>num_points</code> <code>int</code> <p>Number of points to sample.</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N D']</code> <p>Float[Array, 'N D']: Test points sampled from the search space.</p>"},{"location":"api/decision_making/test_functions/non_conjugate_functions/#gpjax.decision_making.test_functions.non_conjugate_functions.PoissonTestFunction.evaluate","title":"<code>evaluate(x: Float[Array, 'N 1']) -&gt; Integer[Array, 'N 1']</code>  <code>abstractmethod</code>","text":"<p>Evaluate the test function at a set of points. Function taken from https://docs.jaxgaussianprocesses.com/examples/poisson/#dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'N D']</code> <p>Points to evaluate the test function at.</p> required <p>Returns:</p> Type Description <code>Integer[Array, 'N 1']</code> <p>Integer[Array, 'N 1']: Values of the test function at the points.</p>"},{"location":"api/decision_making/utility_functions/base/","title":"Base","text":""},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base","title":"<code>gpjax.decision_making.utility_functions.base</code>","text":""},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.SinglePointUtilityFunction","title":"<code>SinglePointUtilityFunction = Callable[[Float[Array, 'N D']], Float[Array, 'N 1']]</code>  <code>module-attribute</code>","text":"<p>Type alias for utility functions which don't support batching, and instead characterise the utility of querying a single point, rather than a batch of points. They take an array of points of shape [N,D][N, D][N,D] and return the value of the utility function at each point in an array of shape [N,1][N, 1][N,1].</p>"},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.UtilityFunction","title":"<code>UtilityFunction = SinglePointUtilityFunction</code>  <code>module-attribute</code>","text":"<p>Type alias for all utility functions. Currently we only support <code>SinglePointUtilityFunction</code>s, but in future may support batched utility functions too. Note that <code>UtilityFunction</code>s are maximised in order to decide which point, or batch of points, to query next.</p>"},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.AbstractUtilityFunctionBuilder","title":"<code>AbstractUtilityFunctionBuilder = AbstractSinglePointUtilityFunctionBuilder</code>  <code>module-attribute</code>","text":"<p>Type alias for utility function builders. For now this only include single point utility function builders, but in the future we may support batched utility function builders.</p>"},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.AbstractSinglePointUtilityFunctionBuilder","title":"<code>AbstractSinglePointUtilityFunctionBuilder</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class for building utility functions which don't support batches. As such, they characterise the utility of querying a single point next.</p>"},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.AbstractSinglePointUtilityFunctionBuilder.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.AbstractSinglePointUtilityFunctionBuilder.check_objective_present","title":"<code>check_objective_present(posteriors: Mapping[str, AbstractPosterior], datasets: Mapping[str, Dataset]) -&gt; None</code>","text":"<p>Check that the objective posterior and dataset are present in the posteriors and datasets.</p> <p>Parameters:</p> Name Type Description Default <code>posteriors</code> <code>Mapping[str, AbstractPosterior]</code> <p>Dictionary of posteriors to be</p> required <code>datasets</code> <code>Mapping[str, Dataset]</code> <p>Dictionary of datasets which may be used</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the objective posterior or dataset are not present in the</p>"},{"location":"api/decision_making/utility_functions/base/#gpjax.decision_making.utility_functions.base.AbstractSinglePointUtilityFunctionBuilder.build_utility_function","title":"<code>build_utility_function(posteriors: Mapping[str, AbstractPosterior], datasets: Mapping[str, Dataset], key: KeyArray) -&gt; SinglePointUtilityFunction</code>  <code>abstractmethod</code>","text":"<p>Build a <code>UtilityFunction</code> from a set of posteriors and datasets.</p> <p>Parameters:</p> Name Type Description Default <code>posteriors</code> <code>Mapping[str, AbstractPosterior]</code> <p>Dictionary of posteriors to be</p> required <code>datasets</code> <code>Mapping[str, Dataset]</code> <p>Dictionary of datasets which may be used</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used for random number generation.</p> required <p>Returns:</p> Name Type Description <code>SinglePointUtilityFunction</code> <code>SinglePointUtilityFunction</code> <p>Utility function to be maximised in order to</p> <code>SinglePointUtilityFunction</code> <p>decide which point to query next.</p>"},{"location":"api/decision_making/utility_functions/thompson_sampling/","title":"Thompson Sampling","text":""},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling","title":"<code>gpjax.decision_making.utility_functions.thompson_sampling</code>","text":""},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling.ThompsonSampling","title":"<code>ThompsonSampling</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractSinglePointUtilityFunctionBuilder</code></p> <p>Form a utility function by drawing an approximate sample from the posterior, using decoupled sampling as introduced in Wilson et. al. (2020). Note that we return the negative of the sample as the utility function, as utility functions are maximised.</p> <p>Note that this is a single batch utility function, as it doesn't support classical batching. However, Thompson sampling can be used in a batched setting by drawing a batch of different samples from the GP posterior. This can be done by calling <code>build_utility_function</code> with different keys, an example of which can be seen in the <code>ask</code> method of the <code>UtilityDrivenDecisionMaker</code> class. The samples can then be optimised sequentially.</p> <p>Attributes:</p> Name Type Description <code>num_features</code> <code>int</code> <p>The number of random Fourier features to use when drawing the approximate sample from the posterior. Defaults to 100.</p>"},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling.ThompsonSampling.num_features","title":"<code>num_features: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling.ThompsonSampling.check_objective_present","title":"<code>check_objective_present(posteriors: Mapping[str, AbstractPosterior], datasets: Mapping[str, Dataset]) -&gt; None</code>","text":"<p>Check that the objective posterior and dataset are present in the posteriors and datasets.</p> <p>Parameters:</p> Name Type Description Default <code>posteriors</code> <code>Mapping[str, AbstractPosterior]</code> <p>Dictionary of posteriors to be</p> required <code>datasets</code> <code>Mapping[str, Dataset]</code> <p>Dictionary of datasets which may be used</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the objective posterior or dataset are not present in the</p>"},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling.ThompsonSampling.__init__","title":"<code>__init__(num_features: int = 100) -&gt; None</code>","text":""},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling.ThompsonSampling.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/decision_making/utility_functions/thompson_sampling/#gpjax.decision_making.utility_functions.thompson_sampling.ThompsonSampling.build_utility_function","title":"<code>build_utility_function(posteriors: Mapping[str, ConjugatePosterior], datasets: Mapping[str, Dataset], key: KeyArray) -&gt; SinglePointUtilityFunction</code>","text":"<p>Draw an approximate sample from the posterior of the objective model and return the negative of this sample as a utility function, as utility functions are maximised.</p> <p>Parameters:</p> Name Type Description Default <code>posteriors</code> <code>Mapping[str, AbstractPosterior]</code> <p>Dictionary of posteriors to be</p> required <code>datasets</code> <code>Mapping[str, Dataset]</code> <p>Dictionary of datasets which may be used</p> required <code>key</code> <code>KeyArray</code> <p>JAX PRNG key used for random number generation. This can be</p> required <p>Returns:</p> Name Type Description <code>SinglePointUtilityFunction</code> <code>SinglePointUtilityFunction</code> <p>An appproximate sample from the objective model</p> <code>SinglePointUtilityFunction</code> <p>posterior to to be maximised in order to decide which point to query</p> <code>SinglePointUtilityFunction</code> <p>next.</p>"},{"location":"api/kernels/base/","title":"Base","text":""},{"location":"api/kernels/base/#gpjax.kernels.base","title":"<code>gpjax.kernels.base</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.SumKernel","title":"<code>SumKernel = partial(CombinationKernel, operator=jnp.sum)</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.ProductKernel","title":"<code>ProductKernel = partial(CombinationKernel, operator=jnp.prod)</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel","title":"<code>AbstractKernel</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Module</code></p> <p>Base kernel class.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = static_field('AbstractKernel')) -&gt; None</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__","title":"<code>__call__(x: Num[Array, ' D'], y: Num[Array, ' D']) -&gt; ScalarFloat</code>  <code>abstractmethod</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Num[Array, ' D']</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Num[Array, ' D']</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant","title":"<code>Constant</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>A constant kernel. This kernel evaluates to a constant for all inputs. The scalar value itself can be treated as a model hyperparameter and learned during training.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.constant","title":"<code>constant: ScalarFloat = param_field(jnp.array(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = static_field('AbstractKernel'), constant: ScalarFloat = param_field(jnp.array(0.0))) -&gt; None</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel","title":"<code>CombinationKernel</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>A base class for products or sums of MeanFunctions.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.kernels","title":"<code>kernels: List[AbstractKernel] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.operator","title":"<code>operator: Callable = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = static_field('AbstractKernel'), kernels: List[AbstractKernel] = None, operator: Callable = static_field(None)) -&gt; None</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/approximations/rff/","title":"Rff","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff","title":"<code>gpjax.kernels.approximations.rff</code>","text":"<p>Compute Random Fourier Feature (RFF) kernel approximations.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF","title":"<code>RFF</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>Computes an approximation of the kernel using Random Fourier Features.</p> <p>All stationary kernels are equivalent to the Fourier transform of a probability distribution. We call the corresponding distribution the spectral density. Using a finite number of basis functions, we can compute the spectral density using a Monte-Carlo approximation. This is done by sampling from the spectral density and computing the Fourier transform of the samples. The kernel is then approximated by the inner product of the Fourier transform of the samples with the Fourier transform of the data.</p> <p>The key reference for this implementation is the following papers: - 'Random Features for Large-Scale Kernel Machines' by Rahimi and Recht (2008). - 'On the Error of Random Fourier Features' by Sutherland and Schneider (2015).</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.base_kernel","title":"<code>base_kernel: Union[AbstractKernel, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.num_basis_fns","title":"<code>num_basis_fns: int = static_field(50)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.frequencies","title":"<code>frequencies: Union[Float[Array, 'M D'], None] = param_field(None, bijector=tfb.Identity())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_engine","title":"<code>compute_engine: BasisFunctionComputation = static_field(BasisFunctionComputation(), repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.key","title":"<code>key: KeyArray = static_field(PRNGKey(123))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__init__","title":"<code>__init__(compute_engine: BasisFunctionComputation = static_field(BasisFunctionComputation(), repr=False), active_dims: Optional[List[int]] = static_field(None), name: str = static_field('AbstractKernel'), base_kernel: Union[AbstractKernel, None] = None, num_basis_fns: int = static_field(50), frequencies: Union[Float[Array, 'M D'], None] = param_field(None, bijector=tfb.Identity()), key: KeyArray = static_field(PRNGKey(123))) -&gt; None</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Post-initialisation function.</p> <p>This function is called after the initialisation of the kernel. It is used to set the computation engine to be the basis function computation engine.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__call__","title":"<code>__call__(x: Float[Array, 'D 1'], y: Float[Array, 'D 1']) -&gt; None</code>","text":"<p>Superfluous for RFFs.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features","title":"<code>compute_features(x: Float[Array, 'N D']) -&gt; Float[Array, 'N L']</code>","text":"<p>Compute the features for the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'N D']</code> <p>A N\u00d7DN \\times DN\u00d7D array of inputs.</p> required"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features--returns","title":"Returns","text":"<pre><code>Float[Array, \"N L\"]: A N\u00d7LN \\times LN\u00d7L array of features where L=2ML = 2ML=2M.\n</code></pre>"},{"location":"api/kernels/computations/base/","title":"Base","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base","title":"<code>gpjax.kernels.computations.base</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation","title":"<code>AbstractKernelComputation</code>  <code>dataclass</code>","text":"<p>Abstract class for kernel computations.</p>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Num[Array, 'N D']) -&gt; LinearOperator</code>","text":"<p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>x</code> <code>Num[Array, 'N N']</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: Gram covariance operator of the kernel function.\n</code></pre>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Num[Array, 'N D'], y: Num[Array, 'M D']) -&gt; Float[Array, 'N M']</code>  <code>abstractmethod</code>","text":"<p>For a given kernel, compute the NxM gram matrix on an a pair of input matrices with shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>x</code> <code>Num[Array, 'N D']</code> <p>The first input matrix.</p> required <code>y</code> <code>Num[Array, 'M D']</code> <p>The second input matrix.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Num[Array, 'N D']) -&gt; Diagonal</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/","title":"Basis Functions","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions","title":"<code>gpjax.kernels.computations.basis_functions</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation","title":"<code>BasisFunctionComputation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernelComputation</code></p> <p>Compute engine class for finite basis function approximations to a kernel.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Num[Array, 'N D']) -&gt; Diagonal</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, 'N D'], y: Float[Array, 'M D']) -&gt; Float[Array, 'N M']</code>","text":"<p>Compute an approximate cross-covariance matrix.</p> <p>For a pair of inputs, compute the cross covariance matrix between the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>(Float[Array, \"N D\"]): A N\u00d7DN \\times DN\u00d7D array of inputs.</p> required <code>y</code> <code>Float[Array, 'M D']</code> <p>(Float[Array, \"M D\"]): A M\u00d7DM \\times DM\u00d7D array of inputs.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'N M']</code> <p>Float[Array, \"N M\"]: A $N \\times M$ array of cross-covariances.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.gram","title":"<code>gram(kernel: Kernel, inputs: Float[Array, 'N D']) -&gt; LinearOperator</code>","text":"<p>Compute an approximate Gram matrix.</p> <p>For the Gram matrix, we can save computations by computing only one matrix multiplication between the inputs and the scaled frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>A NxDN x DNxD array of inputs.</p> required <p>Returns:</p> Name Type Description <code>LinearOperator</code> <code>LinearOperator</code> <p>A dense linear operator representing the N\u00d7NN \\times NN\u00d7N Gram matrix.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features","title":"<code>compute_features(kernel: Kernel, x: Float[Array, 'N D']) -&gt; Float[Array, 'N L']</code>","text":"<p>Compute the features for the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>A N\u00d7DN \\times DN\u00d7D array of inputs.</p> required"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features--returns","title":"Returns","text":"<pre><code>Float[Array, \"N L\"]: A N\u00d7LN \\times LN\u00d7L array of features where L=2ML = 2ML=2M.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.scaling","title":"<code>scaling(kernel: Kernel)</code>","text":"<p>Compute the scaling factor for the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.scaling--returns","title":"Returns","text":"<pre><code>Float[Array, \"\"]: A scalar array.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/","title":"Constant Diagonal","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal","title":"<code>gpjax.kernels.computations.constant_diagonal</code>","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation","title":"<code>ConstantDiagonalKernelComputation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernelComputation</code></p>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Float[Array, 'N D']) -&gt; LinearOperator</code>","text":"<p>Compute the Gram matrix.</p> <p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: Gram covariance operator of the kernel function.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Float[Array, 'N D']) -&gt; Diagonal</code>","text":"<p>Compute the diagonal Gram matrix's entries.</p> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape N\u00d7DN\\times DN\u00d7D.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, 'N D'], y: Float[Array, 'M D']) -&gt; Float[Array, 'N M']</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the NxM covariance matrix on a pair of input matrices of shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, 'M D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed square Gram matrix.\n</code></pre>"},{"location":"api/kernels/computations/dense/","title":"Dense","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense","title":"<code>gpjax.kernels.computations.dense</code>","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation","title":"<code>DenseKernelComputation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernelComputation</code></p> <p>Dense kernel computation class. Operations with the kernel assume a dense gram matrix structure.</p>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Num[Array, 'N D']) -&gt; LinearOperator</code>","text":"<p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>x</code> <code>Num[Array, 'N N']</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: Gram covariance operator of the kernel function.\n</code></pre>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Num[Array, 'N D']) -&gt; Diagonal</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, 'N D'], y: Float[Array, 'M D']) -&gt; Float[Array, 'N M']</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the NxM covariance matrix on a pair of input matrices of shape NxDNxDNxD and MxDMxDMxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, 'M D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/","title":"Diagonal","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal","title":"<code>gpjax.kernels.computations.diagonal</code>","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation","title":"<code>DiagonalKernelComputation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernelComputation</code></p> <p>Diagonal kernel computation class. Operations with the kernel assume a diagonal Gram matrix.</p>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Num[Array, 'N D']) -&gt; Diagonal</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Float[Array, 'N D']) -&gt; LinearOperator</code>","text":"<p>Compute the Gram matrix.</p> <p>For a kernel with diagonal structure, compute the N\u00d7NN\\times NN\u00d7N Gram matrix on an input matrix of shape N\u00d7DN\\times DN\u00d7D.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: The computed square Gram matrix.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, 'N D'], y: Float[Array, 'M D']) -&gt; Float[Array, 'N M']</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the N\u00d7MN\\times MN\u00d7M covariance matrix on a pair of input matrices of shape N\u00d7DN\\times DN\u00d7D and M\u00d7DM\\times DM\u00d7D.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, 'M D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/eigen/","title":"Eigen","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen","title":"<code>gpjax.kernels.computations.eigen</code>","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation","title":"<code>EigenKernelComputation</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernelComputation</code></p> <p>Eigen kernel computation class. Kernels who operate on an eigen-decomposed structure should use this computation object.</p>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Num[Array, 'N D']) -&gt; LinearOperator</code>","text":"<p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>x</code> <code>Num[Array, 'N N']</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: Gram covariance operator of the kernel function.\n</code></pre>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Num[Array, 'N D']) -&gt; Diagonal</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, 'N D']</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>Diagonal: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.__init__","title":"<code>__init__() -&gt; None</code>","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Num[Array, 'N D'], y: Num[Array, 'M D']) -&gt; Float[Array, 'N M']</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For an N\u00d7DN\\times DN\u00d7D and M\u00d7DM\\times DM\u00d7D pair of matrices, evaluate the N\u00d7MN \\times MN\u00d7M cross-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Num[Array, 'N D']</code> <p>The input matrix.</p> required <code>y</code> <code>Num[Array, 'M D']</code> <p>The input matrix.</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>Float[Array, 'N M']</code> <p>description</p>"},{"location":"api/kernels/non_euclidean/graph/","title":"Graph","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph","title":"<code>gpjax.kernels.non_euclidean.graph</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.tfb","title":"<code>tfb = tfp.bijectors</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel","title":"<code>GraphKernel</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn graph kernel defined on the vertex set of a graph.</p> <p>A Mat\u00e9rn graph kernel defined on the vertices of a graph. The key reference for this object is borovitskiy et. al., (2020).</p> <p>Parameters:</p> Name Type Description Default <code>laplacian</code> <code>Float[Array]</code> <p>An N\u00d7NN \\times NN\u00d7N matrix representing the Laplacian matrix of a graph.</p> <code>static_field(None)</code>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.laplacian","title":"<code>laplacian: Union[Num[Array, 'N N'], None] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.lengthscale","title":"<code>lengthscale: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.smoothness","title":"<code>smoothness: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.eigenvalues","title":"<code>eigenvalues: Union[Float[Array, 'N 1'], None] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.eigenvectors","title":"<code>eigenvectors: Union[Float[Array, 'N N'], None] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.num_vertex","title":"<code>num_vertex: Union[ScalarInt, None] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(EigenKernelComputation(), repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.name","title":"<code>name: str = 'Graph Mat\u00e9rn'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(EigenKernelComputation(), repr=False), active_dims: Optional[List[int]] = static_field(None), name: str = 'Graph Mat\u00e9rn', laplacian: Union[Num[Array, 'N N'], None] = static_field(None), lengthscale: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), smoothness: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), eigenvalues: Union[Float[Array, 'N 1'], None] = static_field(None), eigenvectors: Union[Float[Array, 'N N'], None] = static_field(None), num_vertex: Union[ScalarInt, None] = static_field(None)) -&gt; None</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__call__","title":"<code>__call__(x: Int[Array, 'N 1'], y: Int[Array, 'N 1'], *, S, **kwargs)</code>","text":"<p>Compute the (co)variance between a vertex pair.</p> <p>For a graph G={V,E}\\mathcal{G} = \\{V, E\\}G={V,E} where V={v1,v2,\u2026vn}V = \\{v_1, v_2, \\ldots v_n \\}V={v1\u200b,v2\u200b,\u2026vn\u200b}, evaluate the graph kernel on a pair of vertices (vi,vj)(v_i, v_j)(vi\u200b,vj\u200b) for any i,j&lt;ni,j&lt;ni,j&lt;n.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'N 1']</code> <p>Index of the iiith vertex.</p> required <code>y</code> <code>Float[Array, 'N 1']</code> <p>Index of the jjjth vertex.</p> required"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(v_i, v_j)$.\n</code></pre>"},{"location":"api/kernels/non_euclidean/utils/","title":"Utils","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils","title":"<code>gpjax.kernels.non_euclidean.utils</code>","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd","title":"<code>jax_gather_nd(params: Float[Array, ' N *rest'], indices: Int[Array, ' M 1']) -&gt; Float[Array, ' M *rest']</code>","text":"<p>Slice a <code>params</code> array at a set of <code>indices</code>.</p> <p>This is a reimplementation of TensorFlow's <code>gather_nd</code> function: link</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Float[Array]</code> <p>An arbitrary array with leading axes of length $N$ upon which we shall slice.</p> required <code>indices</code> <code>Float[Int]</code> <p>An integer array of length $M$ with values in the range $[0, N)$ whose value at index $i$ will be used to slice <code>params</code> at index $i$.</p> required"},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd--returns","title":"Returns","text":"<pre><code>Float[Array: An arbitrary array with leading axes of length $M$.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/","title":"Arccosine","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine","title":"<code>gpjax.kernels.nonstationary.arccosine</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine","title":"<code>ArcCosine</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The ArCosine kernel.</p> <p>This kernel is non-stationary and resembles the behavior of neural networks. See Section 3.1 of Cho and Saul (2011) for additional details.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.order","title":"<code>order: ScalarInt = static_field(0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.weight_variance","title":"<code>weight_variance: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.bias_variance","title":"<code>bias_variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = static_field('AbstractKernel'), order: ScalarInt = static_field(0), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), weight_variance: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), bias_variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/","title":"Linear","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear","title":"<code>gpjax.kernels.nonstationary.linear</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear","title":"<code>Linear</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The linear kernel.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.name","title":"<code>name: str = 'Linear'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Linear', variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the linear kernel between a pair of arrays.</p> <p>For a pair of inputs x,y\u2208RDx, y \\in \\mathbb{R}^{D}x,y\u2208RD, let's evaluate the linear kernel k(x,y)=\u03c32x\u22a4yk(x, y)=\\sigma^2 x^{\\top}yk(x,y)=\u03c32x\u22a4y where \u03c3\u2208R&gt;0\\sigma^\\in \\mathbb{R}_{&gt;0}\u03c3\u2208R&gt;0\u200b is the kernel's variance parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function k(x,y)k(x, y)k(x,y) at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/","title":"Polynomial","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial","title":"<code>gpjax.kernels.nonstationary.polynomial</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial","title":"<code>Polynomial</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The Polynomial kernel with variable degree.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.degree","title":"<code>degree: ScalarInt = static_field(2)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.shift","title":"<code>shift: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = static_field('AbstractKernel'), degree: ScalarInt = static_field(2), shift: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the polynomial kernel of degree ddd between a pair of arrays.</p> <p>For a pair of inputs x,y\u2208RDx, y \\in \\mathbb{R}^{D}x,y\u2208RD, let's evaluate the polynomial kernel k(x,y)=(\u03b1+\u03c32xy)dk(x, y)=\\left( \\alpha + \\sigma^2 x y\\right)^{d}k(x,y)=(\u03b1+\u03c32xy)d where \u03c3\u2208R&gt;0\\sigma^\\in \\mathbb{R}_{&gt;0}\u03c3\u2208R&gt;0\u200b is the kernel's variance parameter, shift parameter \u03b1\\alpha\u03b1 and integer degree ddd.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/stationary/matern12/","title":"Mat\u00e9rn12","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12","title":"<code>gpjax.kernels.stationary.matern12</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12","title":"<code>Matern12</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 0.5.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.name","title":"<code>name: str = 'Mat\u00e9rn12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Mat\u00e9rn12', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 1/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32. <p>k(x,y)=\u03c32exp\u2061(\u2212\u2223x\u2212y\u22232\u21132) k(x, y) = \\sigma^2\\exp\\Bigg(-\\frac{\\lvert x-y \\rvert}{2\\ell^2}\\Bigg) k(x,y)=\u03c32exp(\u22122\u21132\u2223x\u2212y\u2223\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call</p> required <p>Returns:     ScalarFloat: The value of k(x,y)k(x, y)k(x,y)</p>"},{"location":"api/kernels/stationary/matern32/","title":"Mat\u00e9rn32","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32","title":"<code>gpjax.kernels.stationary.matern32</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32","title":"<code>Matern32</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 1.5.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.name","title":"<code>name: str = 'Mat\u00e9rn32'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Mat\u00e9rn32', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 3/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32.</p> <p>k(x,y)=\u03c32exp\u2061(1+3\u2223x\u2212y\u2223\u21132)exp\u2061(\u22123\u2223x\u2212y\u2223\u21132)     k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{3}\\lvert x-y \\rvert}{\\ell^2}  \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{3}\\lvert x-y\\rvert}{\\ell^2} \\Bigg) k(x,y)=\u03c32exp(1+\u211323\u200b\u2223x\u2212y\u2223\u200b)exp(\u2212\u211323\u200b\u2223x\u2212y\u2223\u200b)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/","title":"Mat\u00e9rn52","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52","title":"<code>gpjax.kernels.stationary.matern52</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52","title":"<code>Matern52</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 2.5.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.name","title":"<code>name: str = 'Mat\u00e9rn52'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Mat\u00e9rn52', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 5/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32. <p>k(x,y)=\u03c32exp\u2061(1+5\u2223x\u2212y\u2223\u21132+5\u2223x\u2212y\u222323\u21132)exp\u2061(\u22125\u2223x\u2212y\u2223\u21132) k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{5}\\lvert x-y \\rvert}{\\ell^2} + \\frac{5\\lvert x - y \\rvert^2}{3\\ell^2} \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{5}\\lvert x-y\\rvert}{\\ell^2} \\Bigg) k(x,y)=\u03c32exp(1+\u211325\u200b\u2223x\u2212y\u2223\u200b+3\u211325\u2223x\u2212y\u22232\u200b)exp(\u2212\u211325\u200b\u2223x\u2212y\u2223\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/stationary/periodic/","title":"Periodic","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic","title":"<code>gpjax.kernels.stationary.periodic</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic","title":"<code>Periodic</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The periodic kernel.</p> <p>Key reference is MacKay 1998 - \"Introduction to Gaussian processes\".</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.period","title":"<code>period: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.name","title":"<code>name: str = 'Periodic'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Periodic', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), period: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the Periodic kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with length-scale parameter \u2113\\ell\u2113, variance \u03c32\\sigma^2\u03c32 and period ppp. <p>k(x,y)=\u03c32exp\u2061(\u221212\u2211i=1D(sin\u2061(\u03c0(xi\u2212yi)/p)\u2113)2) k(x, y) = \\sigma^2 \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\left(\\frac{\\sin (\\pi (x_i - y_i)/p)}{\\ell}\\right)^2 \\right) k(x,y)=\u03c32exp(\u221221\u200bi=1\u2211D\u200b(\u2113sin(\u03c0(xi\u200b\u2212yi\u200b)/p)\u200b)2)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call</p> required <p>Returns:     ScalarFloat: The value of k(x,y)k(x, y)k(x,y).</p>"},{"location":"api/kernels/stationary/powered_exponential/","title":"Powered Exponential","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential","title":"<code>gpjax.kernels.stationary.powered_exponential</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential","title":"<code>PoweredExponential</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The powered exponential family of kernels. This also equivalent to the symmetric generalized normal distribution.</p> <p>See Diggle and Ribeiro (2007) - \"Model-based Geostatistics\". and https://en.wikipedia.org/wiki/Generalized_normal_distribution#Symmetric_version</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.power","title":"<code>power: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Sigmoid())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.name","title":"<code>name: str = 'Powered Exponential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Powered Exponential', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), power: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Sigmoid())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the Powered Exponential kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with length-scale parameter \u2113\\ell\u2113, \u03c3\\sigma\u03c3 and power \u03ba\\kappa\u03ba. <p>k(x,y)=\u03c32exp\u2061(\u2212(\u2225x\u2212y\u22252\u21132)\u03ba) k(x, y)=\\sigma^2\\exp\\Bigg(-\\Big(\\frac{\\lVert x-y\\rVert^2}{\\ell^2}\\Big)^\\kappa\\Bigg) k(x,y)=\u03c32exp(\u2212(\u21132\u2225x\u2212y\u22252\u200b)\u03ba)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/","title":"Rational Quadratic","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic","title":"<code>gpjax.kernels.stationary.rational_quadratic</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic","title":"<code>RationalQuadratic</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.alpha","title":"<code>alpha: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.name","title":"<code>name: str = 'Rational Quadratic'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'Rational Quadratic', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus()), alpha: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the Powered Exponential kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32. <p>k(x,y)=\u03c32exp\u2061(1+\u2225x\u2212y\u2225222\u03b1\u21132) k(x,y)=\\sigma^2\\exp\\Bigg(1+\\frac{\\lVert x-y\\rVert^2_2}{2\\alpha\\ell^2}\\Bigg) k(x,y)=\u03c32exp(1+2\u03b1\u21132\u2225x\u2212y\u222522\u200b\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of k(x,y)k(x, y)k(x,y).</p>"},{"location":"api/kernels/stationary/rbf/","title":"RBF","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf","title":"<code>gpjax.kernels.stationary.rbf</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF","title":"<code>RBF</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p> <p>The Radial Basis Function (RBF) kernel.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.name","title":"<code>name: str = 'RBF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.spectral_density","title":"<code>spectral_density: tfd.Normal</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation()), active_dims: Optional[List[int]] = static_field(None), name: str = 'RBF', lengthscale: Union[ScalarFloat, Float[Array, ' D']] = param_field(jnp.array(1.0), bijector=tfb.Softplus()), variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the RBF kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32: <p>k(x,y)=\u03c32exp\u2061(\u2212\u2225x\u2212y\u2225222\u21132) k(x,y)=\\sigma^2\\exp\\Bigg(- \\frac{\\lVert x - y \\rVert^2_2}{2 \\ell^2} \\Bigg) k(x,y)=\u03c32exp(\u22122\u21132\u2225x\u2212y\u222522\u200b\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of k(x,y)k(x, y)k(x,y).</p>"},{"location":"api/kernels/stationary/utils/","title":"Utils","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils","title":"<code>gpjax.kernels.stationary.utils</code>","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution","title":"<code>build_student_t_distribution(nu: int) -&gt; tfd.Distribution</code>","text":"<p>Build a Student's t distribution with a fixed smoothness parameter.</p> <p>For a fixed half-integer smoothness parameter, compute the spectral density of a Mat\u00e9rn kernel; a Student's t distribution.</p> <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>int</code> <p>The smoothness parameter of the Mat\u00e9rn kernel.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution--returns","title":"Returns","text":"<pre><code>tfp.Distribution: A Student's t distribution with the same smoothness parameter.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance","title":"<code>squared_distance(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the squared distance between a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>First input.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>Second input.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The squared distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance","title":"<code>euclidean_distance(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the euclidean distance between a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>First input.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>Second input.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The euclidean distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/white/","title":"White","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white","title":"<code>gpjax.kernels.stationary.white</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White","title":"<code>White</code>  <code>dataclass</code>","text":"<p>             Bases: <code>AbstractKernel</code></p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(ConstantDiagonalKernelComputation(), repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.name","title":"<code>name: str = 'White'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.cross_covariance","title":"<code>cross_covariance(x: Num[Array, 'N D'], y: Num[Array, 'M D'])</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.gram","title":"<code>gram(x: Num[Array, 'N D'])</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.slice_input","title":"<code>slice_input(x: Float[Array, '... D']) -&gt; Float[Array, '... Q']</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, '... D']</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together. Args:     other (AbstractKernel): The kernel to be added to the current kernel.</p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__init__","title":"<code>__init__(compute_engine: AbstractKernelComputation = static_field(ConstantDiagonalKernelComputation(), repr=False), active_dims: Optional[List[int]] = static_field(None), name: str = 'White', variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())) -&gt; None</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__call__","title":"<code>__call__(x: Float[Array, ' D'], y: Float[Array, ' D']) -&gt; ScalarFloat</code>","text":"<p>Compute the White noise kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with variance \u03c32\\sigma^2\u03c32: <p>k(x,y)=\u03c32\u03b4(x\u2212y) k(x, y) = \\sigma^2 \\delta(x-y) k(x,y)=\u03c32\u03b4(x\u2212y)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' D']</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, ' D']</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"examples/","title":"Where to find the docs","text":""},{"location":"examples/#where-to-find-the-docs","title":"Where to find the docs","text":"<p>The GPJax documentation can be found here: https://docs.jaxgaussianprocesses.com/</p>"},{"location":"examples/#how-to-build-the-docs","title":"How to build the docs","text":"<ol> <li>Ensure you have installed the requirements using <code>poetry install</code> in the root    directory.</li> <li>Make sure <code>pandoc</code> is installed</li> <li>Run the command <code>poetry run mkdocs serve</code> in the root directory.</li> </ol> <p>The documentation will then be served at an IP address printed, which can then be opened in a browser of you choice e.g. <code>Serving on http://127.0.0.1:8000/</code>.</p>"},{"location":"examples/#how-to-write-code-documentation","title":"How to write code documentation","text":"<p>Our documentation is generated using MkDocs. This automatically creates online documentation from docstrings, with full support for Markdown. Longer tutorial-style notebooks are also converted to webpages by MkDocs, with these notebooks being stored in the <code>docs/examples</code> directory. If you write a new notebook and wish to add it to the documentation website, add it to the <code>nav</code> section of the <code>mkdocs.yml</code> file found in the root directory.</p> <p>Below we provide some guidelines for writing docstrings.</p>"},{"location":"examples/#how-much-information-to-put-in-a-docstring","title":"How much information to put in a docstring","text":"<p>A docstring should be informative. If in doubt, then it is best to add more information to a docstring than less. Many users will skim documentation, so please ensure the opening sentence or two of a docstring contains the core information. Adding examples and mathematical descriptions to documentation is highly desirable.</p> <p>We are making an active effort within GPJax to improve our documentation. If you spot any areas where there is missing information within the existing documentation, then please either raise an issue or create a pull request.</p>"},{"location":"examples/#an-example-docstring","title":"An example docstring","text":"<p>An example docstring that adheres the principles of GPJax is given below. The docstring contains a simple, snappy introduction with links to auxiliary components. More detail is then provided in the form of a mathematical description and a code example. The docstring is concluded with a description of the objects attributes with corresponding types.</p> <pre><code>from gpjax.gps import AbstractPrior\nfrom gpjax.mean_functions import AbstractMeanFunction\nfrom gpjax.kernels import AbstractKernel\nfrom typing import Optional\nclass Prior(AbstractPrior):\nr\"\"\"A Gaussian process prior object.\n    The GP is parameterised by a\n    [mean](https://docs.jaxgaussianprocesses.com/api/mean_functions/)\n    and [kernel](https://docs.jaxgaussianprocesses.com/api/kernels/base/) function.\n    A Gaussian process prior parameterised by a mean function $`m(\\cdot)`$ and a kernel\n    function $`k(\\cdot, \\cdot)`$ is given by\n    $`p(f(\\cdot)) = \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))`$.\n    To invoke a `Prior` distribution, a kernel and mean function must be specified.\n    Example:\n        &gt;&gt;&gt; import gpjax as gpx\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n        &gt;&gt;&gt; kernel = gpx.kernels.RBF()\n        &gt;&gt;&gt; prior = gpx.gps.Prior(mean_function=meanf, kernel = kernel)\n    Attributes:\n        kernel (Kernel): The kernel function used to parameterise the prior.\n        mean_function (MeanFunction): The mean function used to parameterise the prior. Defaults to zero.\n        name (str): The name of the GP prior. Defaults to \"GP prior\".\n    \"\"\"\nkernel: AbstractKernel\nmean_function: AbstractMeanFunction\nname: Optional[str] = \"GP prior\"\n</code></pre>"},{"location":"examples/#documentation-syntax","title":"Documentation syntax","text":"<p>We adopt the following convention when documenting objects:</p> <ul> <li>Class attributes should be specified using the <code>Attributes:</code> tag.</li> <li>Method argument should be specified using the <code>Args:</code> tags.</li> <li>Values returned by a method should be specified using the <code>Returns:</code> tag.</li> <li>All attributes, arguments and returned values should have types.</li> </ul> <p>Note</p> <p>Inline math in docstrings needs to be rendered within both <code>$</code> and <code>symbols to be correctly rendered by MkDocs. For instance, where one would typically write `$k(x,y)$` in standard LaTeX, in docstrings you are required to write</code>k(x,y)k(x,y)k(x,y)`` in order for the math to be correctly rendered by MkDocs.</p>"},{"location":"examples/barycentres/","title":"Gaussian Processes Barycentres","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy.linalg as jsl\nfrom jaxtyping import install_import_hook\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax.distributions as tfd\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  import typing as tp  import jax import jax.numpy as jnp import jax.random as jr import jax.scipy.linalg as jsl from jaxtyping import install_import_hook import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax.distributions as tfd  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx   key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>n = 100\nn_test = 200\nn_datasets = 5\n\nx = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1)\nxtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1)\nf = lambda x, a, b: a + jnp.sin(b * x)\n\nys = []\nfor _i in range(n_datasets):\n    key, subkey = jr.split(key)\n    vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)\n    period = jr.uniform(subkey, minval=0.75, maxval=1.25)\n    noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)\n    noise = jr.normal(subkey, shape=x.shape) * noise_amount\n    ys.append(f(x, vertical_shift, period) + noise)\n\ny = jnp.hstack(ys)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\")\nplt.show()\n</pre> n = 100 n_test = 200 n_datasets = 5  x = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1) xtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1) f = lambda x, a, b: a + jnp.sin(b * x)  ys = [] for _i in range(n_datasets):     key, subkey = jr.split(key)     vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)     period = jr.uniform(subkey, minval=0.75, maxval=1.25)     noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)     noise = jr.normal(subkey, shape=x.shape) * noise_amount     ys.append(f(x, vertical_shift, period) + noise)  y = jnp.hstack(ys)  fig, ax = plt.subplots() ax.plot(x, y, \"x\") plt.show() In\u00a0[3]: Copied! <pre>def fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    D = gpx.Dataset(X=x, y=y)\n\n    likelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\n    posterior = (\n        gpx.gps.Prior(\n            mean_function=gpx.mean_functions.Constant(), kernel=gpx.kernels.RBF()\n        )\n        * likelihood\n    )\n\n    opt_posterior, _ = gpx.fit_scipy(\n        model=posterior,\n        objective=gpx.objectives.ConjugateMLL(negative=True),\n        train_data=D,\n    )\n    latent_dist = opt_posterior.predict(xtest, train_data=D)\n    return opt_posterior.likelihood(latent_dist)\n\n\nposterior_preds = [fit_gp(x, i) for i in ys]\n</pre> def fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:     if y.ndim == 1:         y = y.reshape(-1, 1)     D = gpx.Dataset(X=x, y=y)      likelihood = gpx.likelihoods.Gaussian(num_datapoints=n)     posterior = (         gpx.gps.Prior(             mean_function=gpx.mean_functions.Constant(), kernel=gpx.kernels.RBF()         )         * likelihood     )      opt_posterior, _ = gpx.fit_scipy(         model=posterior,         objective=gpx.objectives.ConjugateMLL(negative=True),         train_data=D,     )     latent_dist = opt_posterior.predict(xtest, train_data=D)     return opt_posterior.likelihood(latent_dist)   posterior_preds = [fit_gp(x, i) for i in ys] <pre>Optimization terminated successfully.\n         Current function value: 75.260512\n         Iterations: 14\n         Function evaluations: 19\n         Gradient evaluations: 19\n</pre> <pre>Optimization terminated successfully.\n         Current function value: -13.372859\n         Iterations: 12\n         Function evaluations: 17\n         Gradient evaluations: 17\n</pre> <pre>Optimization terminated successfully.\n         Current function value: 16.768109\n         Iterations: 10\n         Function evaluations: 19\n         Gradient evaluations: 19\n</pre> <pre>Optimization terminated successfully.\n         Current function value: 60.563367\n         Iterations: 12\n         Function evaluations: 18\n         Gradient evaluations: 18\n</pre> <pre>Optimization terminated successfully.\n         Current function value: 79.056458\n         Iterations: 13\n         Function evaluations: 18\n         Gradient evaluations: 18\n</pre> In\u00a0[4]: Copied! <pre>def sqrtm(A: jax.Array):\n    return jnp.real(jsl.sqrtm(A))\n\n\ndef wasserstein_barycentres(\n    distributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array\n):\n    covariances = [d.covariance() for d in distributions]\n    cov_stack = jnp.stack(covariances)\n    stack_sqrt = jax.vmap(sqrtm)(cov_stack)\n\n    def step(covariance_candidate: jax.Array, idx: None):\n        inner_term = jax.vmap(sqrtm)(\n            jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)\n        )\n        fixed_point = jnp.tensordot(weights, inner_term, axes=1)\n        return fixed_point, fixed_point\n\n    return step\n</pre> def sqrtm(A: jax.Array):     return jnp.real(jsl.sqrtm(A))   def wasserstein_barycentres(     distributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array ):     covariances = [d.covariance() for d in distributions]     cov_stack = jnp.stack(covariances)     stack_sqrt = jax.vmap(sqrtm)(cov_stack)      def step(covariance_candidate: jax.Array, idx: None):         inner_term = jax.vmap(sqrtm)(             jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)         )         fixed_point = jnp.tensordot(weights, inner_term, axes=1)         return fixed_point, fixed_point      return step <p>With a function defined for learning a barycentre, we'll now compute it using the <code>lax.scan</code> operator that drastically speeds up for loops in Jax (see the Jax documentation). The iterative update will be executed 100 times, with convergence measured by the difference between the previous and current iteration that we can confirm by inspecting the <code>sequence</code> array in the following cell.</p> In\u00a0[5]: Copied! <pre>weights = jnp.ones((n_datasets,)) / n_datasets\n\nmeans = jnp.stack([d.mean() for d in posterior_preds])\nbarycentre_mean = jnp.tensordot(weights, means, axes=1)\n\nstep_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights))\ninitial_covariance = jnp.eye(n_test)\n\nbarycentre_covariance, sequence = jax.lax.scan(\n    step_fn, initial_covariance, jnp.arange(100)\n)\nL = jnp.linalg.cholesky(barycentre_covariance)\n\nbarycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L)\n</pre> weights = jnp.ones((n_datasets,)) / n_datasets  means = jnp.stack([d.mean() for d in posterior_preds]) barycentre_mean = jnp.tensordot(weights, means, axes=1)  step_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights)) initial_covariance = jnp.eye(n_test)  barycentre_covariance, sequence = jax.lax.scan(     step_fn, initial_covariance, jnp.arange(100) ) L = jnp.linalg.cholesky(barycentre_covariance)  barycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L) In\u00a0[6]: Copied! <pre>def plot(\n    dist: tfd.MultivariateNormalTriL,\n    ax,\n    color: str,\n    label: str = None,\n    ci_alpha: float = 0.2,\n    linewidth: float = 1.0,\n    zorder: int = 0,\n):\n    mu = dist.mean()\n    sigma = dist.stddev()\n    ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)\n    ax.fill_between(\n        xtest.squeeze(),\n        mu - sigma,\n        mu + sigma,\n        alpha=ci_alpha,\n        color=color,\n        zorder=zorder,\n    )\n\n\nfig, ax = plt.subplots()\n[plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds]\nplot(\n    barycentre_process,\n    ax,\n    color=cols[0],\n    label=\"Barycentre\",\n    ci_alpha=0.5,\n    linewidth=2,\n    zorder=1,\n)\nax.legend()\n</pre> def plot(     dist: tfd.MultivariateNormalTriL,     ax,     color: str,     label: str = None,     ci_alpha: float = 0.2,     linewidth: float = 1.0,     zorder: int = 0, ):     mu = dist.mean()     sigma = dist.stddev()     ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)     ax.fill_between(         xtest.squeeze(),         mu - sigma,         mu + sigma,         alpha=ci_alpha,         color=color,         zorder=zorder,     )   fig, ax = plt.subplots() [plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds] plot(     barycentre_process,     ax,     color=cols[0],     label=\"Barycentre\",     ci_alpha=0.5,     linewidth=2,     zorder=1, ) ax.legend() Out[6]: <pre>&lt;matplotlib.legend.Legend at 0x7f0510dc34c0&gt;</pre> In\u00a0[7]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ngpjax                 : 0.8.0\ntensorflow_probability: 0.22.1\noptax                 : 0.1.9\nmatplotlib            : 3.8.3\njax                   : 0.4.25\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/barycentres/#gaussian-processes-barycentres","title":"Gaussian Processes Barycentres\u00b6","text":"<p>In this notebook we'll give an implementation of . In this work, the existence of a Wasserstein barycentre between a collection of Gaussian processes is proven. When faced with trying to average a set of probability distributions, the Wasserstein barycentre is an attractive choice as it enables uncertainty amongst the individual distributions to be incorporated into the averaged distribution. When compared to a naive mean of means and mean of variances approach to computing the average probability distributions, it can be seen that Wasserstein barycentres offer significantly more favourable uncertainty estimation.</p>"},{"location":"examples/barycentres/#background","title":"Background\u00b6","text":""},{"location":"examples/barycentres/#wasserstein-distance","title":"Wasserstein distance\u00b6","text":"<p>The 2-Wasserstein distance metric between two probability measures $\\mu$ and $\\nu$ quantifies the minimal cost required to transport the unit mass from $\\mu$ to $\\nu$, or vice-versa. Typically, computing this metric requires solving a linear program. However, when $\\mu$ and $\\nu$ both belong to the family of multivariate Gaussian distributions, the solution is analytically given by $$W_2^2(\\mu, \\nu) = \\lVert m_1- m_2 \\rVert^2_2 + \\operatorname{Tr}(S_1 + S_2 - 2(S_1^{1/2}S_2S_1^{1/2})^{1/2}),$$ where $\\mu \\sim \\mathcal{N}(m_1, S_1)$ and $\\nu\\sim\\mathcal{N}(m_2, S_2)$.</p>"},{"location":"examples/barycentres/#wasserstein-barycentre","title":"Wasserstein barycentre\u00b6","text":"<p>For a collection of $T$ measures $\\lbrace\\mu_i\\rbrace_{t=1}^T \\in \\mathcal{P}_2(\\theta)$, the Wasserstein barycentre $\\bar{\\mu}$ is the measure that minimises the average Wasserstein distance to all other measures in the set. More formally, the Wasserstein barycentre is the Fr\u00e9chet mean on a Wasserstein space that we can write as $$\\bar{\\mu} = \\operatorname{argmin}_{\\mu\\in\\mathcal{P}_2(\\theta)}\\sum_{t=1}^T \\alpha_t W_2^2(\\mu, \\mu_t),$$ where $\\alpha\\in\\mathbb{R}^T$ is a weight vector that sums to 1.</p> <p>As with the Wasserstein distance, identifying the Wasserstein barycentre $\\bar{\\mu}$ is often an computationally demanding optimisation problem. However, when all the measures admit a multivariate Gaussian density, the barycentre $\\bar{\\mu} = \\mathcal{N}(\\bar{m}, \\bar{S})$ has analytical solutions $$\\bar{m} = \\sum_{t=1}^T \\alpha_t m_t\\,, \\quad \\bar{S}=\\sum_{t=1}^T\\alpha_t (\\bar{S}^{1/2}S_t\\bar{S}^{1/2})^{1/2}\\,. \\qquad (\\star)$$ Identifying $\\bar{S}$ is achieved through a fixed-point iterative update.</p>"},{"location":"examples/barycentres/#barycentre-of-gaussian-processes","title":"Barycentre of Gaussian processes\u00b6","text":"<p>It was shown in  that the barycentre $\\bar{f}$ of a collection of Gaussian processes $\\lbrace f_i\\rbrace_{i=1}^T$ such that $f_i \\sim \\mathcal{GP}(m_i, K_i)$ can be found using the same solutions as in $(\\star)$. For a full theoretical understanding, we recommend reading the original paper. However, the central argument to this result is that one can first show that the barycentre GP $\\bar{f}\\sim\\mathcal{GP}(\\bar{m}, \\bar{S})$ is non-degenerate for any finite set of GPs $\\lbrace f_t\\rbrace_{t=1}^T$ i.e., $T&lt;\\infty$. With this established, one can show that for a $n$-dimensional finite Gaussian distribution $f_{i,n}$, the Wasserstein metric between any two Gaussian distributions $f_{i, n}, f_{j, n}$ converges to the Wasserstein metric between GPs as $n\\to\\infty$.</p> <p>In this notebook, we will demonstrate how this can be achieved in GPJax.</p>"},{"location":"examples/barycentres/#dataset","title":"Dataset\u00b6","text":"<p>We'll simulate five datasets and develop a Gaussian process posterior before identifying the Gaussian process barycentre at a set of test points. Each dataset will be a sine function with a different vertical shift, periodicity, and quantity of noise.</p>"},{"location":"examples/barycentres/#learning-a-posterior-distribution","title":"Learning a posterior distribution\u00b6","text":"<p>We'll now independently learn Gaussian process posterior distributions for each dataset. We won't spend any time here discussing how GP hyperparameters are optimised. For advice on achieving this, see the Regression notebook for advice on optimisation and the Kernels notebook for advice on selecting an appropriate kernel.</p>"},{"location":"examples/barycentres/#computing-the-barycentre","title":"Computing the barycentre\u00b6","text":"<p>In GPJax, the predictive distribution of a GP is given by a TensorFlow Probability distribution, making it straightforward to extract the mean vector and covariance matrix of each GP for learning a barycentre. We implement the fixed point scheme given in (3) in the following cell by utilising Jax's <code>vmap</code> operator to speed up large matrix operations using broadcasting in <code>tensordot</code>.</p>"},{"location":"examples/barycentres/#plotting-the-result","title":"Plotting the result\u00b6","text":"<p>With a barycentre learned, we can visualise the result. We can see that the result looks reasonable as it follows the sinusoidal curve of all the inferred GPs, and the uncertainty bands are sensible.</p>"},{"location":"examples/barycentres/#displacement-interpolation","title":"Displacement interpolation\u00b6","text":"<p>In the above example, we assigned uniform weights to each of the posteriors within the barycentre. In practice, we may have prior knowledge of which posterior is most likely to be the correct one. Regardless of the weights chosen, the barycentre remains a Gaussian process. We can interpolate between a pair of posterior distributions $\\mu_1$ and $\\mu_2$ to visualise the corresponding barycentre $\\bar{\\mu}$.</p> <p></p>"},{"location":"examples/barycentres/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/bayesian_optimisation/","title":"Introduction to Bayesian Optimisation","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float, Int\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom typing import List, Tuple\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\nfrom gpjax.typing import Array, FunctionalSample, ScalarFloat\nfrom jaxopt import ScipyBoundedMinimize\n\nkey = jr.key(42)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  import jax from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook, Float, Int import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib import cm import optax as ox import tensorflow_probability.substrates.jax as tfp from typing import List, Tuple  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx from gpjax.typing import Array, FunctionalSample, ScalarFloat from jaxopt import ScipyBoundedMinimize  key = jr.key(42) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>def standardised_forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:\n    mean = 0.45321\n    std = 4.4258\n    return ((6 * x - 2) ** 2 * jnp.sin(12 * x - 4) - mean) / std\n</pre> def standardised_forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:     mean = 0.45321     std = 4.4258     return ((6 * x - 2) ** 2 * jnp.sin(12 * x - 4) - mean) / std <p>We'll first go through one iteration of the BO loop step-by-step, before wrapping this up in a loop to perform the full optimisation.</p> <p>First we'll specify the domain over which we wish to optimise the function, as well as sampling some initial points for fitting our surrogate model using a space-filling design.</p> In\u00a0[3]: Copied! <pre>lower_bound = jnp.array([0.0])\nupper_bound = jnp.array([1.0])\ninitial_sample_num = 5\n\ninitial_x = tfp.mcmc.sample_halton_sequence(\n    dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = standardised_forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\n</pre> lower_bound = jnp.array([0.0]) upper_bound = jnp.array([1.0]) initial_sample_num = 5  initial_x = tfp.mcmc.sample_halton_sequence(     dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64 ).reshape(-1, 1) initial_y = standardised_forrester(initial_x) D = gpx.Dataset(X=initial_x, y=initial_y) <p>Next we'll define our GP model in the usual way, using a Mat\u00e9rn52 kernel, and fit the kernel parameters by minimising the negative log-marginal likelihood. We'll wrap this in a function as we'll be repeating this process at each iteration of the BO loop.</p> In\u00a0[4]: Copied! <pre>def return_optimised_posterior(\n    data: gpx.Dataset, prior: gpx.base.Module, key: Array\n) -&gt; gpx.base.Module:\n    likelihood = gpx.likelihoods.Gaussian(\n        num_datapoints=data.n, obs_stddev=jnp.array(1e-6)\n    )  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value\n    likelihood = likelihood.replace_trainable(obs_stddev=False)\n\n    posterior = prior * likelihood\n\n    negative_mll = gpx.objectives.ConjugateMLL(negative=True)\n    negative_mll(posterior, train_data=data)\n    negative_mll = jit(negative_mll)\n\n    opt_posterior, _ = gpx.fit(\n        model=posterior,\n        objective=negative_mll,\n        train_data=data,\n        optim=ox.adam(learning_rate=0.01),\n        num_iters=1000,\n        safe=True,\n        key=key,\n        verbose=False,\n    )\n\n    return opt_posterior\n\n\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, key)\n</pre> def return_optimised_posterior(     data: gpx.Dataset, prior: gpx.base.Module, key: Array ) -&gt; gpx.base.Module:     likelihood = gpx.likelihoods.Gaussian(         num_datapoints=data.n, obs_stddev=jnp.array(1e-6)     )  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value     likelihood = likelihood.replace_trainable(obs_stddev=False)      posterior = prior * likelihood      negative_mll = gpx.objectives.ConjugateMLL(negative=True)     negative_mll(posterior, train_data=data)     negative_mll = jit(negative_mll)      opt_posterior, _ = gpx.fit(         model=posterior,         objective=negative_mll,         train_data=data,         optim=ox.adam(learning_rate=0.01),         num_iters=1000,         safe=True,         key=key,         verbose=False,     )      return opt_posterior   mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Matern52() prior = gpx.gps.Prior(mean_function=mean, kernel=kernel) opt_posterior = return_optimised_posterior(D, prior, key) <p>We can then sample a function from the posterior distribution of the surrogate model. We will do this using the <code>sample_approx</code> method, which generates an approximate sample from the posterior using decoupled sampling introduced in (Wilson et al., 2020) and discussed in our Pathwise Sampling Notebook. This method is used as it enables us to sample from the posterior in a manner which scales linearly with the number of points sampled, $O(N)$, mitigating the cubic cost associated with drawing exact samples from a GP posterior, $O(N^3)$. It also generates more accurate samples than many other methods for drawing approximate samples from a GP posterior.</p> <p>Note that we also define a <code>utility_fn</code> which calls the approximate sample but returns the value returned as a scalar. This is because the <code>sample_approx</code> function returns an array of shape $[N, B]$, with $N$ being the number of points within each sample and $B$ being the number of samples drawn. We'll only be drawing (and optimising) one sample at a time, and our optimiser requires the function being optimised to return a scalar output (only querying it at $N=1$ points), so we'll remove the axes from the returned value.</p> In\u00a0[5]: Copied! <pre>approx_sample = opt_posterior.sample_approx(\n    num_samples=1, train_data=D, key=key, num_features=500\n)\nutility_fn = lambda x: approx_sample(x)[0][0]\n</pre> approx_sample = opt_posterior.sample_approx(     num_samples=1, train_data=D, key=key, num_features=500 ) utility_fn = lambda x: approx_sample(x)[0][0] <p>In order to minimise the sample, we'll be using the L-BFGS-B (Byrd et al., 1995) optimiser from the <code>jaxopt</code> library. This is a gradient-based optimiser which performs optimisation within a bounded domain. In order to perform optimisation, this optimiser requires a point to start from. Therefore, we will first query our sample from the posterior at a random set of points, and then use the lowest point from this set of points as the starting point for the optimiser. In this example we'll sample 100 points from the posterior, due to the simple nature of the Forrester function. However, in practice it can be beneficial to adopt a more sophisticated approach, and there are several heuristics available in the literature (see for example (Le Riche and Picheny, 2021)). For instance, one may randomly sample the posterior at a number of points proportional to the dimensionality of the input space, and one may run gradient-based optimisation from multiple of these points, to reduce the risk of converging upon local minima.</p> In\u00a0[6]: Copied! <pre>def optimise_sample(\n    sample: FunctionalSample,\n    key: Int[Array, \"\"],\n    lower_bound: Float[Array, \"D\"],\n    upper_bound: Float[Array, \"D\"],\n    num_initial_sample_points: int,\n) -&gt; ScalarFloat:\n    initial_sample_points = jr.uniform(\n        key,\n        shape=(num_initial_sample_points, lower_bound.shape[0]),\n        dtype=jnp.float64,\n        minval=lower_bound,\n        maxval=upper_bound,\n    )\n    initial_sample_y = sample(initial_sample_points)\n    best_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])\n\n    # We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.\n    negative_utility_fn = lambda x: sample(x)[0][0]\n    lbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")\n    bounds = (lower_bound, upper_bound)\n    x_star = lbfgsb.run(best_x, bounds=bounds).params\n    return x_star\n\n\nx_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100)\ny_star = standardised_forrester(x_star)\n</pre> def optimise_sample(     sample: FunctionalSample,     key: Int[Array, \"\"],     lower_bound: Float[Array, \"D\"],     upper_bound: Float[Array, \"D\"],     num_initial_sample_points: int, ) -&gt; ScalarFloat:     initial_sample_points = jr.uniform(         key,         shape=(num_initial_sample_points, lower_bound.shape[0]),         dtype=jnp.float64,         minval=lower_bound,         maxval=upper_bound,     )     initial_sample_y = sample(initial_sample_points)     best_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])      # We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.     negative_utility_fn = lambda x: sample(x)[0][0]     lbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")     bounds = (lower_bound, upper_bound)     x_star = lbfgsb.run(best_x, bounds=bounds).params     return x_star   x_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100) y_star = standardised_forrester(x_star) <p>Having found the minimum of the sample from the posterior, we can then evaluate the black-box objective function at this point, and append the new observation to our dataset.</p> <p>Below we plot the posterior distribution of the surrogate model, along with the sample drawn from the model, and the minimiser of this sample returned from the optimiser, which we denote with a star.</p> In\u00a0[7]: Copied! <pre>def plot_bayes_opt(\n    posterior: gpx.base.Module,\n    sample: FunctionalSample,\n    dataset: gpx.Dataset,\n    queried_x: ScalarFloat,\n) -&gt; None:\n    plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\n    forrester_y = standardised_forrester(plt_x)\n    sample_y = sample(plt_x)\n\n    latent_dist = posterior.predict(plt_x, train_data=dataset)\n    predictive_dist = posterior.likelihood(latent_dist)\n\n    predictive_mean = predictive_dist.mean()\n    predictive_std = predictive_dist.stddev()\n\n    fig, ax = plt.subplots()\n    ax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])\n    ax.fill_between(\n        plt_x.squeeze(),\n        predictive_mean - 2 * predictive_std,\n        predictive_mean + 2 * predictive_std,\n        alpha=0.2,\n        label=\"Two sigma\",\n        color=cols[1],\n    )\n    ax.plot(\n        plt_x,\n        predictive_mean - 2 * predictive_std,\n        linestyle=\"--\",\n        linewidth=1,\n        color=cols[1],\n    )\n    ax.plot(\n        plt_x,\n        predictive_mean + 2 * predictive_std,\n        linestyle=\"--\",\n        linewidth=1,\n        color=cols[1],\n    )\n    ax.plot(plt_x, sample_y, label=\"Posterior Sample\")\n    ax.plot(\n        plt_x,\n        forrester_y,\n        label=\"Forrester Function\",\n        color=cols[0],\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\n    ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\n    ax.scatter(\n        queried_x,\n        sample(queried_x),\n        label=\"Posterior Sample Optimum\",\n        marker=\"*\",\n        color=cols[3],\n        zorder=3,\n    )\n    ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n    plt.show()\n\n\nplot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n</pre> def plot_bayes_opt(     posterior: gpx.base.Module,     sample: FunctionalSample,     dataset: gpx.Dataset,     queried_x: ScalarFloat, ) -&gt; None:     plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)     forrester_y = standardised_forrester(plt_x)     sample_y = sample(plt_x)      latent_dist = posterior.predict(plt_x, train_data=dataset)     predictive_dist = posterior.likelihood(latent_dist)      predictive_mean = predictive_dist.mean()     predictive_std = predictive_dist.stddev()      fig, ax = plt.subplots()     ax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])     ax.fill_between(         plt_x.squeeze(),         predictive_mean - 2 * predictive_std,         predictive_mean + 2 * predictive_std,         alpha=0.2,         label=\"Two sigma\",         color=cols[1],     )     ax.plot(         plt_x,         predictive_mean - 2 * predictive_std,         linestyle=\"--\",         linewidth=1,         color=cols[1],     )     ax.plot(         plt_x,         predictive_mean + 2 * predictive_std,         linestyle=\"--\",         linewidth=1,         color=cols[1],     )     ax.plot(plt_x, sample_y, label=\"Posterior Sample\")     ax.plot(         plt_x,         forrester_y,         label=\"Forrester Function\",         color=cols[0],         linestyle=\"--\",         linewidth=2,     )     ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")     ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)     ax.scatter(         queried_x,         sample(queried_x),         label=\"Posterior Sample Optimum\",         marker=\"*\",         color=cols[3],         zorder=3,     )     ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))     plt.show()   plot_bayes_opt(opt_posterior, approx_sample, D, x_star) <p>At this point we can update our model with the newly augmented dataset, and repeat the whole process until some stopping criterion is met. Below we repeat this process for 10 iterations, printing out the queried point and the value of the black-box function at each iteration.</p> In\u00a0[8]: Copied! <pre>bo_iters = 5\n\n# Set up initial dataset\ninitial_x = tfp.mcmc.sample_halton_sequence(\n    dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = standardised_forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\n\nfor i in range(bo_iters):\n    key, subkey = jr.split(key)\n\n    # Generate optimised posterior using previously observed data\n    mean = gpx.mean_functions.Zero()\n    kernel = gpx.kernels.Matern52()\n    prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n    opt_posterior = return_optimised_posterior(D, prior, subkey)\n\n    # Draw a sample from the posterior, and find the minimiser of it\n    approx_sample = opt_posterior.sample_approx(\n        num_samples=1, train_data=D, key=subkey, num_features=500\n    )\n    x_star = optimise_sample(\n        approx_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100\n    )\n\n    plot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n\n    # Evaluate the black-box function at the best point observed so far, and add it to the dataset\n    y_star = standardised_forrester(x_star)\n    print(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")\n    D = D + gpx.Dataset(X=x_star, y=y_star)\n</pre> bo_iters = 5  # Set up initial dataset initial_x = tfp.mcmc.sample_halton_sequence(     dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64 ).reshape(-1, 1) initial_y = standardised_forrester(initial_x) D = gpx.Dataset(X=initial_x, y=initial_y)  for i in range(bo_iters):     key, subkey = jr.split(key)      # Generate optimised posterior using previously observed data     mean = gpx.mean_functions.Zero()     kernel = gpx.kernels.Matern52()     prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)     opt_posterior = return_optimised_posterior(D, prior, subkey)      # Draw a sample from the posterior, and find the minimiser of it     approx_sample = opt_posterior.sample_approx(         num_samples=1, train_data=D, key=subkey, num_features=500     )     x_star = optimise_sample(         approx_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100     )      plot_bayes_opt(opt_posterior, approx_sample, D, x_star)      # Evaluate the black-box function at the best point observed so far, and add it to the dataset     y_star = standardised_forrester(x_star)     print(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")     D = D + gpx.Dataset(X=x_star, y=y_star) <pre>Queried Point: [[0.730923]], Black-Box Function Value: [[-1.38601026]]\n</pre> <pre>Queried Point: [[0.75119497]], Black-Box Function Value: [[-1.45843127]]\n</pre> <pre>Queried Point: [[0.75533321]], Black-Box Function Value: [[-1.46233481]]\n</pre> <pre>Queried Point: [[0.75604723]], Black-Box Function Value: [[-1.46260151]]\n</pre> <pre>Queried Point: [[0.75616782]], Black-Box Function Value: [[-1.46263456]]\n</pre> <p>Below we plot the best observed black-box function value against the number of times the black-box function has been evaluated. Note that the first 5 samples are randomly sampled to fit the initial GP model, and we denote the start of using BO to sample with the dotted vertical line.</p> <p>We can see that the BO algorithm quickly converges to the global minimum of the black-box function!</p> In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\ncumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y)\nax.plot(fn_evaluations, cumulative_best_y)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.axhline(y=-1.463, linestyle=\"--\", label=\"True Minimum\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Best Observed Value\")\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() fn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1) cumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y) ax.plot(fn_evaluations, cumulative_best_y) ax.axvline(x=initial_sample_num, linestyle=\":\") ax.axhline(y=-1.463, linestyle=\"--\", label=\"True Minimum\") ax.set_xlabel(\"Number of Black-Box Function Evaluations\") ax.set_ylabel(\"Best Observed Value\") ax.legend() plt.show() <p>We'll now apply BO to a more challenging example, the Six-Hump Camel Function. This is a function of two inputs defined as follows:</p> <p>$$f(x_1, x_2) = (4 - 2.1x_1^2 + \\frac{x_1^4}{3})x_1^2 + x_1x_2 + (-4 + 4x_2^2)x_2^2$$</p> <p>We'll be evaluating it over the domain $x_1 \\in [-2, 2]$ and $x_2 \\in [-1, 1]$, and shall standardise it. The global minima of this function are located at $\\mathbf{x} = (0.0898, -0.7126)$ and $\\mathbf{x} = (-0.0898, 0.7126)$, where the standardised function takes the value $f(\\mathbf{x}) = -1.8377$.</p> In\u00a0[10]: Copied! <pre>def standardised_six_hump_camel(x: Float[Array, \"N 2\"]) -&gt; Float[Array, \"N 1\"]:\n    mean = 1.12767\n    std = 1.17500\n    x1 = x[..., :1]\n    x2 = x[..., 1:]\n    term1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2\n    term2 = x1 * x2\n    term3 = (-4 + 4 * x2**2) * x2**2\n    return (term1 + term2 + term3 - mean) / std\n</pre> def standardised_six_hump_camel(x: Float[Array, \"N 2\"]) -&gt; Float[Array, \"N 1\"]:     mean = 1.12767     std = 1.17500     x1 = x[..., :1]     x2 = x[..., 1:]     term1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2     term2 = x1 * x2     term3 = (-4 + 4 * x2**2) * x2**2     return (term1 + term2 + term3 - mean) / std <p>First, we'll visualise the function over the domain of interest:</p> In\u00a0[11]: Copied! <pre>x1 = jnp.linspace(-2, 2, 100)\nx2 = jnp.linspace(-1, 1, 100)\nx1, x2 = jnp.meshgrid(x1, x2)\nx = jnp.stack([x1.flatten(), x2.flatten()], axis=1)\ny = standardised_six_hump_camel(x)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurf = ax.plot_surface(\n    x1,\n    x2,\n    y.reshape(x1.shape[0], x2.shape[0]),\n    linewidth=0,\n    cmap=cm.coolwarm,\n    antialiased=False,\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nplt.show()\n</pre> x1 = jnp.linspace(-2, 2, 100) x2 = jnp.linspace(-1, 1, 100) x1, x2 = jnp.meshgrid(x1, x2) x = jnp.stack([x1.flatten(), x2.flatten()], axis=1) y = standardised_six_hump_camel(x)  fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}) surf = ax.plot_surface(     x1,     x2,     y.reshape(x1.shape[0], x2.shape[0]),     linewidth=0,     cmap=cm.coolwarm,     antialiased=False, ) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") plt.show() <p>For more clarity, we can generate a contour plot of the function which enables us to see the global minima of the function more clearly.</p> In\u00a0[12]: Copied! <pre>x_star_one = jnp.array([[0.0898, -0.7126]])\nx_star_two = jnp.array([[-0.0898, 0.7126]])\nfig, ax = plt.subplots()\ncontour_plot = ax.contourf(\n    x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\n    x_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\"\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2])\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\n</pre> x_star_one = jnp.array([[0.0898, -0.7126]]) x_star_two = jnp.array([[-0.0898, 0.7126]]) fig, ax = plt.subplots() contour_plot = ax.contourf(     x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40 ) ax.scatter(     x_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\" ) ax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2]) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") fig.colorbar(contour_plot) ax.legend() plt.show() <p>Next, we'll run the BO loop using Thompson sampling as before. This time we'll run the experiment 5 times in order to see how the algorithm performs on average, with different starting points for the initial GP model. This is good practice, as the performance obtained is likely to vary between runs depending on the initialisation samples used to fit the initial GP model.</p> In\u00a0[13]: Copied! <pre>lower_bound = jnp.array([-2.0, -1.0])\nupper_bound = jnp.array([2.0, 1.0])\ninitial_sample_num = 5\nbo_iters = 12\nnum_experiments = 5\nbo_experiment_results = []\n\nfor experiment in range(num_experiments):\n    print(f\"Starting Experiment: {experiment + 1}\")\n    # Set up initial dataset\n    initial_x = tfp.mcmc.sample_halton_sequence(\n        dim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n    )\n    initial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)\n    initial_y = standardised_six_hump_camel(initial_x)\n    D = gpx.Dataset(X=initial_x, y=initial_y)\n\n    for i in range(bo_iters):\n        key, subkey = jr.split(key)\n\n        # Generate optimised posterior\n        mean = gpx.mean_functions.Zero()\n        kernel = gpx.kernels.Matern52(\n            active_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0\n        )\n        prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n        opt_posterior = return_optimised_posterior(D, prior, subkey)\n\n        # Draw a sample from the posterior, and find the minimiser of it\n        approx_sample = opt_posterior.sample_approx(\n            num_samples=1, train_data=D, key=subkey, num_features=500\n        )\n        x_star = optimise_sample(\n            approx_sample,\n            subkey,\n            lower_bound,\n            upper_bound,\n            num_initial_sample_points=1000,\n        )\n\n        # Evaluate the black-box function at the best point observed so far, and add it to the dataset\n        y_star = standardised_six_hump_camel(x_star)\n        print(\n            f\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value:\"\n            f\" {y_star}\"\n        )\n        D = D + gpx.Dataset(X=x_star, y=y_star)\n    bo_experiment_results.append(D)\n</pre> lower_bound = jnp.array([-2.0, -1.0]) upper_bound = jnp.array([2.0, 1.0]) initial_sample_num = 5 bo_iters = 12 num_experiments = 5 bo_experiment_results = []  for experiment in range(num_experiments):     print(f\"Starting Experiment: {experiment + 1}\")     # Set up initial dataset     initial_x = tfp.mcmc.sample_halton_sequence(         dim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64     )     initial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)     initial_y = standardised_six_hump_camel(initial_x)     D = gpx.Dataset(X=initial_x, y=initial_y)      for i in range(bo_iters):         key, subkey = jr.split(key)          # Generate optimised posterior         mean = gpx.mean_functions.Zero()         kernel = gpx.kernels.Matern52(             active_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0         )         prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)         opt_posterior = return_optimised_posterior(D, prior, subkey)          # Draw a sample from the posterior, and find the minimiser of it         approx_sample = opt_posterior.sample_approx(             num_samples=1, train_data=D, key=subkey, num_features=500         )         x_star = optimise_sample(             approx_sample,             subkey,             lower_bound,             upper_bound,             num_initial_sample_points=1000,         )          # Evaluate the black-box function at the best point observed so far, and add it to the dataset         y_star = standardised_six_hump_camel(x_star)         print(             f\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value:\"             f\" {y_star}\"         )         D = D + gpx.Dataset(X=x_star, y=y_star)     bo_experiment_results.append(D) <pre>Starting Experiment: 1\n</pre> <pre>BO Iteration: 1, Queried Point: [[-0.45287328 -1.        ]], Black-Box Function Value: [[0.05116778]]\n</pre> <pre>BO Iteration: 2, Queried Point: [[ 0.49490465 -1.        ]], Black-Box Function Value: [[-0.65015781]]\n</pre> <pre>BO Iteration: 3, Queried Point: [[ 0.05584091 -1.        ]], Black-Box Function Value: [[-0.99664552]]\n</pre> <pre>BO Iteration: 4, Queried Point: [[-1.68015271  1.        ]], Black-Box Function Value: [[-0.64024239]]\n</pre> <pre>BO Iteration: 5, Queried Point: [[-0.19010096  0.53446244]], Black-Box Function Value: [[-1.62013756]]\n</pre> <pre>BO Iteration: 6, Queried Point: [[-0.01976805  0.5920224 ]], Black-Box Function Value: [[-1.74331746]]\n</pre> <pre>BO Iteration: 7, Queried Point: [[0.37326302 0.35639267]], Black-Box Function Value: [[-0.7836037]]\n</pre> <pre>BO Iteration: 8, Queried Point: [[-0.43551059  1.        ]], Black-Box Function Value: [[-0.74704233]]\n</pre> <pre>BO Iteration: 9, Queried Point: [[-0.0059713  -0.01797269]], Black-Box Function Value: [[-0.96060571]]\n</pre> <pre>BO Iteration: 10, Queried Point: [[0.07863875 0.77714229]], Black-Box Function Value: [[-1.70100254]]\n</pre> <pre>BO Iteration: 11, Queried Point: [[ 0.18718228 -0.7119549 ]], Black-Box Function Value: [[-1.80694538]]\n</pre> <pre>BO Iteration: 12, Queried Point: [[-0.0577367   0.80286595]], Black-Box Function Value: [[-1.76773162]]\nStarting Experiment: 2\n</pre> <pre>BO Iteration: 1, Queried Point: [[ 0.18588496 -1.        ]], Black-Box Function Value: [[-1.00241326]]\n</pre> <pre>BO Iteration: 2, Queried Point: [[0.01274286 1.        ]], Black-Box Function Value: [[-0.94832142]]\n</pre> <pre>BO Iteration: 3, Queried Point: [[-0.25472806 -1.        ]], Black-Box Function Value: [[-0.52948668]]\n</pre> <pre>BO Iteration: 4, Queried Point: [[ 0.02472471 -0.01713296]], Black-Box Function Value: [[-0.95899826]]\n</pre> <pre>BO Iteration: 5, Queried Point: [[-0.81096378  1.        ]], Black-Box Function Value: [[-0.10336913]]\n</pre> <pre>BO Iteration: 6, Queried Point: [[ 0.09530531 -0.4632344 ]], Black-Box Function Value: [[-1.54026804]]\n</pre> <pre>BO Iteration: 7, Queried Point: [[ 1.13229638 -0.75736414]], Black-Box Function Value: [[-0.49753681]]\n</pre> <pre>BO Iteration: 8, Queried Point: [[ 0.0983378  -0.66026871]], Black-Box Function Value: [[-1.8193251]]\n</pre> <pre>BO Iteration: 9, Queried Point: [[ 0.06178646 -0.67962639]], Black-Box Function Value: [[-1.82860694]]\n</pre> <pre>BO Iteration: 10, Queried Point: [[ 0.05028894 -0.68027625]], Black-Box Function Value: [[-1.82658325]]\n</pre> <pre>BO Iteration: 11, Queried Point: [[0.05201696 0.62814985]], Black-Box Function Value: [[-1.73593857]]\n</pre> <pre>BO Iteration: 12, Queried Point: [[0.28858031 0.52809973]], Black-Box Function Value: [[-1.24337734]]\nStarting Experiment: 3\n</pre> <pre>BO Iteration: 1, Queried Point: [[0.67361213 0.99945761]], Black-Box Function Value: [[0.81278707]]\n</pre> <pre>BO Iteration: 2, Queried Point: [[-1.90525628  1.        ]], Black-Box Function Value: [[-0.20456543]]\n</pre> <pre>BO Iteration: 3, Queried Point: [[-1.27430523  1.        ]], Black-Box Function Value: [[-0.01424552]]\n</pre> <pre>BO Iteration: 4, Queried Point: [[-1.66746145  0.85204852]], Black-Box Function Value: [[-1.09967229]]\n</pre> <pre>BO Iteration: 5, Queried Point: [[-0.04567181 -0.28192512]], Black-Box Function Value: [[-1.19073799]]\n</pre> <pre>BO Iteration: 6, Queried Point: [[-0.73927872  0.14701683]], Black-Box Function Value: [[0.24879844]]\n</pre> <pre>BO Iteration: 7, Queried Point: [[-0.39908059  0.08857643]], Black-Box Function Value: [[-0.51831114]]\n</pre> <pre>BO Iteration: 8, Queried Point: [[-1.52088303  0.9973661 ]], Black-Box Function Value: [[-0.44563386]]\n</pre> <pre>BO Iteration: 9, Queried Point: [[ 0.03364229 -0.99649079]], Black-Box Function Value: [[-1.00808323]]\n</pre> <pre>BO Iteration: 10, Queried Point: [[-1.6814623  -0.16674029]], Black-Box Function Value: [[0.93670447]]\n</pre> <pre>BO Iteration: 11, Queried Point: [[-1.09912132  0.59712233]], Black-Box Function Value: [[-0.29489825]]\n</pre> <pre>BO Iteration: 12, Queried Point: [[ 1.59106719 -0.39882686]], Black-Box Function Value: [[-0.18843489]]\nStarting Experiment: 4\n</pre> <pre>BO Iteration: 1, Queried Point: [[-0.46075076  0.59964392]], Black-Box Function Value: [[-1.33392761]]\n</pre> <pre>BO Iteration: 2, Queried Point: [[0.72418574 1.        ]], Black-Box Function Value: [[0.99130884]]\n</pre> <pre>BO Iteration: 3, Queried Point: [[0.19653963 0.24593557]], Black-Box Function Value: [[-0.98318352]]\n</pre> <pre>BO Iteration: 4, Queried Point: [[ 0.04646976 -1.        ]], Black-Box Function Value: [[-0.99192493]]\n</pre> <pre>BO Iteration: 5, Queried Point: [[-1.19248388 -0.49595401]], Black-Box Function Value: [[0.95486269]]\n</pre> <pre>BO Iteration: 6, Queried Point: [[-0.69522533 -1.        ]], Black-Box Function Value: [[0.89187536]]\n</pre> <pre>BO Iteration: 7, Queried Point: [[-0.3024268  1.       ]], Black-Box Function Value: [[-0.92047744]]\n</pre> <pre>BO Iteration: 8, Queried Point: [[ 0.17105159 -0.800855  ]], Black-Box Function Value: [[-1.76125265]]\n</pre> <pre>BO Iteration: 9, Queried Point: [[0.0877133  0.85534847]], Black-Box Function Value: [[-1.53821367]]\n</pre> <pre>BO Iteration: 10, Queried Point: [[ 0.41324169 -1.        ]], Black-Box Function Value: [[-0.78078045]]\n</pre> <pre>BO Iteration: 11, Queried Point: [[-1.04066222  0.78972619]], Black-Box Function Value: [[-0.50723527]]\n</pre> <pre>BO Iteration: 12, Queried Point: [[ 0.09750891 -0.6443376 ]], Black-Box Function Value: [[-1.80755097]]\nStarting Experiment: 5\n</pre> <pre>BO Iteration: 1, Queried Point: [[0.22628829 0.30761823]], Black-Box Function Value: [[-1.02246213]]\n</pre> <pre>BO Iteration: 2, Queried Point: [[1.84880328 0.56202353]], Black-Box Function Value: [[1.27311473]]\n</pre> <pre>BO Iteration: 3, Queried Point: [[-1.99471526 -1.        ]], Black-Box Function Value: [[3.85843995]]\n</pre> <pre>BO Iteration: 4, Queried Point: [[-0.17760737  1.        ]], Black-Box Function Value: [[-1.00525872]]\n</pre> <pre>BO Iteration: 5, Queried Point: [[-0.09783974  0.55916046]], Black-Box Function Value: [[-1.70544282]]\n</pre> <pre>BO Iteration: 6, Queried Point: [[ 1.24313328 -1.        ]], Black-Box Function Value: [[0.02189211]]\n</pre> <pre>BO Iteration: 7, Queried Point: [[ 2.        -0.7809877]], Black-Box Function Value: [[0.07832912]]\n</pre> <pre>BO Iteration: 8, Queried Point: [[-0.0011466  0.7647924]], Black-Box Function Value: [[-1.78698262]]\n</pre> <pre>BO Iteration: 9, Queried Point: [[-0.00435515  0.68787996]], Black-Box Function Value: [[-1.81081911]]\n</pre> <pre>BO Iteration: 10, Queried Point: [[0.01078227 0.71049828]], Black-Box Function Value: [[-1.80378873]]\n</pre> <pre>BO Iteration: 11, Queried Point: [[-0.38990677  0.69010267]], Black-Box Function Value: [[-1.5606326]]\n</pre> <pre>BO Iteration: 12, Queried Point: [[-0.08109417  0.70034084]], Black-Box Function Value: [[-1.83649915]]\n</pre> <p>We'll also run a random benchmark, whereby we randomly sample from the search space for 20 iterations. This is a useful benchmark to compare the performance of BO against in order to ascertain how much of an advantage BO provides over such a simple approach.</p> In\u00a0[14]: Copied! <pre>random_experiment_results = []\nfor i in range(num_experiments):\n    key, subkey = jr.split(key)\n    initial_x = bo_experiment_results[i].X[:5]\n    initial_y = bo_experiment_results[i].y[:5]\n    final_x = jr.uniform(\n        key,\n        shape=(bo_iters, 2),\n        dtype=jnp.float64,\n        minval=lower_bound,\n        maxval=upper_bound,\n    )\n    final_y = standardised_six_hump_camel(final_x)\n    random_x = jnp.concatenate([initial_x, final_x], axis=0)\n    random_y = jnp.concatenate([initial_y, final_y], axis=0)\n    random_experiment_results.append(gpx.Dataset(X=random_x, y=random_y))\n</pre> random_experiment_results = [] for i in range(num_experiments):     key, subkey = jr.split(key)     initial_x = bo_experiment_results[i].X[:5]     initial_y = bo_experiment_results[i].y[:5]     final_x = jr.uniform(         key,         shape=(bo_iters, 2),         dtype=jnp.float64,         minval=lower_bound,         maxval=upper_bound,     )     final_y = standardised_six_hump_camel(final_x)     random_x = jnp.concatenate([initial_x, final_x], axis=0)     random_y = jnp.concatenate([initial_y, final_y], axis=0)     random_experiment_results.append(gpx.Dataset(X=random_x, y=random_y)) <p>Finally, we'll process the experiment results to find the log regret at each iteration of the experiments. The regret is defined as the difference between the minimum value of the black-box function observed so far and the true global minimum of the black box function. Mathematically, at time $t$, with observations $\\mathcal{D}_t$, for function $f$ with global minimum $f^*$, the regret is defined as:</p> <p>$$\\text{regret}_t = \\min_{\\mathbf{x} \\in \\mathcal{D_t}}f(\\mathbf{x}) - f^*$$</p> <p>We'll then take the mean and standard deviation of the log of the regret values across the 5 experiments.</p> In\u00a0[15]: Copied! <pre>def obtain_log_regret_statistics(\n    experiment_results: List[gpx.Dataset],\n    global_minimum: ScalarFloat,\n) -&gt; Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:\n    log_regret_results = []\n    for exp_result in experiment_results:\n        observations = exp_result.y\n        cumulative_best_observations = jax.lax.associative_scan(\n            jax.numpy.minimum, observations\n        )\n        regret = cumulative_best_observations - global_minimum\n        log_regret = jnp.log(regret)\n        log_regret_results.append(log_regret)\n\n    log_regret_results = jnp.array(log_regret_results)\n    log_regret_mean = jnp.mean(log_regret_results, axis=0)\n    log_regret_std = jnp.std(log_regret_results, axis=0)\n    return log_regret_mean, log_regret_std\n\n\nbo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(\n    bo_experiment_results, -1.8377\n)\n(\n    random_log_regret_mean,\n    random_log_regret_std,\n) = obtain_log_regret_statistics(random_experiment_results, -1.8377)\n</pre> def obtain_log_regret_statistics(     experiment_results: List[gpx.Dataset],     global_minimum: ScalarFloat, ) -&gt; Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:     log_regret_results = []     for exp_result in experiment_results:         observations = exp_result.y         cumulative_best_observations = jax.lax.associative_scan(             jax.numpy.minimum, observations         )         regret = cumulative_best_observations - global_minimum         log_regret = jnp.log(regret)         log_regret_results.append(log_regret)      log_regret_results = jnp.array(log_regret_results)     log_regret_mean = jnp.mean(log_regret_results, axis=0)     log_regret_std = jnp.std(log_regret_results, axis=0)     return log_regret_mean, log_regret_std   bo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(     bo_experiment_results, -1.8377 ) (     random_log_regret_mean,     random_log_regret_std, ) = obtain_log_regret_statistics(random_experiment_results, -1.8377) <p>Now, when we plot the mean and standard deviation of the log regret at each iteration, we can see that BO outperforms random sampling!</p> In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\nax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\")\nax.fill_between(\n    fn_evaluations,\n    bo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],\n    bo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],\n    alpha=0.2,\n)\nax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\")\nax.fill_between(\n    fn_evaluations,\n    random_log_regret_mean[:, 0] - random_log_regret_std[:, 0],\n    random_log_regret_mean[:, 0] + random_log_regret_std[:, 0],\n    alpha=0.2,\n)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Log Regret\")\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() fn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1) ax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\") ax.fill_between(     fn_evaluations,     bo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],     bo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],     alpha=0.2, ) ax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\") ax.fill_between(     fn_evaluations,     random_log_regret_mean[:, 0] - random_log_regret_std[:, 0],     random_log_regret_mean[:, 0] + random_log_regret_std[:, 0],     alpha=0.2, ) ax.axvline(x=initial_sample_num, linestyle=\":\") ax.set_xlabel(\"Number of Black-Box Function Evaluations\") ax.set_ylabel(\"Log Regret\") ax.legend() plt.show() <p>It can also be useful to plot the queried points over the course of a single BO run, in order to gain some insight into how the algorithm queries the search space. Below we do this for one of the BO experiments, and can see that the algorithm initially performs some exploration of the search space whilst it is uncertain about the black-box function, but it then hones in one one of the global minima of the function, as we would hope!</p> In\u00a0[17]: Copied! <pre>fig, ax = plt.subplots()\ncontour_plot = ax.contourf(\n    x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\n    x_star_one[0][0],\n    x_star_one[0][1],\n    marker=\"*\",\n    color=cols[2],\n    label=\"Global Minimum\",\n    zorder=2,\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2)\nax.scatter(\n    bo_experiment_results[1].X[:, 0],\n    bo_experiment_results[1].X[:, 1],\n    marker=\"x\",\n    color=cols[1],\n    label=\"Bayesian Optimisation Queries\",\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() contour_plot = ax.contourf(     x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40 ) ax.scatter(     x_star_one[0][0],     x_star_one[0][1],     marker=\"*\",     color=cols[2],     label=\"Global Minimum\",     zorder=2, ) ax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2) ax.scatter(     bo_experiment_results[1].X[:, 0],     bo_experiment_results[1].X[:, 1],     marker=\"x\",     color=cols[1],     label=\"Bayesian Optimisation Queries\", ) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") fig.colorbar(contour_plot) ax.legend() plt.show() In\u00a0[18]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Christie' <pre>Author: Thomas Christie\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ngpjax                 : 0.8.0\nmatplotlib            : 3.8.3\noptax                 : 0.1.9\ntensorflow_probability: 0.22.1\njax                   : 0.4.25\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/bayesian_optimisation/#introduction-to-bayesian-optimisation","title":"Introduction to Bayesian Optimisation\u00b6","text":"<p>In this guide we introduce the Bayesian Optimisation (BO) paradigm for optimising black-box functions. We'll assume an understanding of Gaussian processes (GPs), so if you're not familiar with them, check out our GP introduction notebook.</p>"},{"location":"examples/bayesian_optimisation/#some-motivating-examples","title":"Some Motivating Examples\u00b6","text":"<p>Countless problems in the physical world involve optimising functions for which the explicit functional form is unknown, but which can be expensively queried throughout their domain. For example, within the domain of science the task of designing new molecules with optimised properties (Griffiths and Lobato, 2020) is incredibly useful. Here, the domain being optimised over is the space of possible molecules, with the objective function depending on the property being optimised, for instance within drug-design this may be the efficacy of the drug. The function from molecules to efficacy is unknown, but can be queried by synthesising a molecule and running an experiment to measure its efficacy. This is clearly an expensive procedure!</p> <p>Within the domain of machine learning, the task of optimising neural network architectures is another example of such a problem (commonly referred to as Neural Architecture Search (NAS)). Here, the domain is the space of possible neural network architectures, and the objective function is a metric such as the accuracy of the trained model. Again, the function from neural network architectures to accuracy is unknown, but can be queried by training a model with a given architecture and evaluating its accuracy. This is also an expensive procedure, as training models can be incredibly time consuming and computationally demanding.</p> <p>Finally, these problems are ubiquitous within the field of climate science, with (Hellan et al., 2023) providing several excellent examples. One such example is the task of deciding where to place wind turbines in a wind farm in order to maximise the energy generated. Here, the domain is the space of possible locations for the wind turbines, and the objective function is the energy generated by the wind farm. The function from locations to energy generated is unknown, but could be queried by running a simulation of the wind farm with the turbines placed at a given set of locations. Running such simulations can be expensive, particularly if they are high-fidelity.</p> <p>At the heart of all these problems is the task of optimising a function for which we don't have the explicit functional form, but which we can (expensively) query at any point in its domain. Bayesian optimisation provides a principled framework for solving such problems.</p>"},{"location":"examples/bayesian_optimisation/#what-is-bayesian-optimisation","title":"What is Bayesian Optimisation?\u00b6","text":"<p>Bayesian optimisation (BO) (Mo\u010dkus, 1974) provides a principled method for making decisions under uncertainty. The aim of BO is to find the global minimum of a black-box objective function, $\\min_{\\mathbf{x} \\in X} f(\\mathbf{x})$. The function $f$ is said to be a black-box function because its explicit functional form is unknown. However, it is assumed that one is able to ascertain information about the function by evaluating it at points in its domain, $X$. However, these evaluations are assumed to be expensive, as seen in the motivating examples. Therefore, the goal of BO is to minimise $f$ with as few evaluations of the black-box function as possible.</p> <p>As such, BO can be thought of as sequential decision-making problem. At each iteration one must choose which point (or batch of points) in a function's domain to evaluate next, drawing on previously observed values to make optimal decisions. In order to do this effectively, we need a way of representing our uncertainty about the black-box function $f$, which we can update in light of observing more data. Gaussian processes will be an ideal tool for this purpose!</p> <p>Surrogate models lie at the heart of BO, and are used to model the black-box function. GPs are a natural choice for this model, as they not only provide point estimates for the values taken by the function throughout its domain, but crucially provide a full predictive posterior distribution of the range of values the function may take. This rich quantification of uncertainty enables BO to balance exploration and exploitation in order to efficiently converge upon minima.</p> <p>Having chosen a surrogate model, which we can use to express our current beliefs about the black-box function, ideally we would like a method which can use the surrogate model's posterior distribution to automatically decide which point(s) in the black-box function's domain to query next. This is where acquisition functions come in. The acquisition function $\\alpha: X \\to \\mathbb{R}$ is defined over the same domain as the surrogate model, and uses the surrogate model's posterior distribution to quantify the expected utility, $U$, of evaluating the black-box function at a given point. Simply put, for each point in the black-box function's domain, $\\mathbf{x} \\in X$, the acquisition function quantifies how useful it would be to evaluate the black-box function at $\\mathbf{x}$ in order to find the minimum of the black-box function, whilst taking into consideration all the datapoints observed so far. Therefore, in order to decide which point to query next we simply choose the point which maximises the acquisition function, using an optimiser such as L-BFGS (Liu and Nocedal, 1989).</p> <p>The Bayesian optimisation loop can be summarised as follows, with $i$ denoting the current iteration:</p> <ol> <li>Select the next point to query, $\\mathbf{x}_{i}$, by maximising the acquisition function $\\alpha$, defined using the surrogate model $\\mathcal{M}_i$ conditioned on previously observed data $\\mathcal{D}_i$:</li> </ol> <p>$$\\mathbf{x}_{i} = \\arg\\max_{\\mathbf{x}} \\alpha (\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i)$$</p> <ol> <li><p>Evaluate the objective function at $\\mathbf{x}_i$, yielding observation $y_i = f(\\mathbf{x}_i)$.</p> </li> <li><p>Append the most recent observation to the dataset, $\\mathcal{D}_{i+1} = \\mathcal{D}_i \\cup \\{(\\mathbf{x}_i, y_i)\\}$.</p> </li> <li><p>Condition the model on the updated dataset to yield $\\mathcal{M}_{i+1}$.</p> </li> </ol> <p>This process is repeated until some stopping criterion is met, such as a function evaluation budget being exhausted.</p> <p>There are a plethora of acquisition functions to choose from, each with their own advantages and disadvantages, of which (Shahriari et al., 2015) provides an excellent overview.</p> <p>In this guide we will focus on Thompson sampling, a conceptually simple yet effective method for characterising the utility of querying points in a black-box function's domain, which will be useful in demonstrating the key aspects of BO.</p>"},{"location":"examples/bayesian_optimisation/#thompson-sampling","title":"Thompson Sampling\u00b6","text":"<p>Thompson sampling (Thompson, 1933) is a simple method which naturally balances exploration and exploitation. The core idea is to, at each iteration of the BO loop, sample a function, $g$, from the posterior distribution of the surrogate model $\\mathcal{M}_i$, and then evaluate the black-box function at the point(s) which minimise this sample. Given a sample $g$, from the posterior distribution given by the model $\\mathcal{M}_i$ the Thompson sampling utility function is defined as:</p> <p>$$U_{\\text{TS}}(\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i) = - g(\\mathbf{x})$$</p> <p>Note the negative sign; this is included as we want to maximise the utility of evaluating the black-box function $f$ at a given point. We interested in finding the minimum of $f$, so we maximise the negative of the sample from the posterior distribution $g$.</p> <p>As a toy example, we shall be applying BO to the widely used Forrester function:</p> <p>$$f(x) = (6x - 2)^2 \\sin(12x - 4)$$</p> <p>treating $f$ as a black-box function. Moreover, we shall restrict the domain of the function to $\\mathbf{x} \\in [0, 1]$. We shall also standardise the output of the function, such that it has a mean of 0 and standard deviation of 1. This is quite common practice when using GPs; we're using a zero mean prior, so ensuring that our data has a mean of zero aligns with this, and often we have scale parameters in the covariance function, which are frequently initialised, or have priors set on them, under the assumption that the function being modelled has unit variance. For similar reasons, it can also be useful to normalise the inputs to a GP. The global minimum of this (standardised) function is located at $x = 0.757$, where $f(x) = -1.463$.</p>"},{"location":"examples/bayesian_optimisation/#a-more-challenging-example-the-six-hump-camel-function","title":"A More Challenging Example - The Six-Hump Camel Function\u00b6","text":""},{"location":"examples/bayesian_optimisation/#other-acquisition-functions-and-further-reading","title":"Other Acquisition Functions and Further Reading\u00b6","text":"<p>As mentioned previously, there are many acquisition functions which one may use to characterise the expected utility of querying the black-box function at a given point. We list two of the most popular below:</p> <ul> <li><p>Probability of Improvement (PI) (Kushner, 1964): Given the lowest objective function observation so far, $f(\\mathbf{x}^*)$, PI calculates the probability that the objective function's value at a given point $\\mathbf{x}$ is lower than $f(\\mathbf{x}^*)$. Given a GP surrogate model $\\mathcal{M}_i$, PI is defined mathematically as: $$ \\alpha_{\\text{PI}}(\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i) = \\mathbb{P}[\\mathcal{M}_i (\\mathbf{x}) &lt; f(\\mathbf{x}^*)] = \\Phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right) $$</p> <p>with $\\Phi(\\cdot)$ denoting the standard normal cumulative distribution function.</p> </li> <li><p>Expected Improvement (EI) (Mo\u010dkus, 1974) - EI goes beyond PI by not only considering the probability of improving on the current best observed point, but also taking into account the \\textit{magnitude} of improvement. Mathematically, this is defined as follows: $$ \\begin{aligned} \\alpha_{\\text{EI}}(\\mathbf{x};\\mathcal{D}_i, \\mathcal{M}_i) &amp;= \\mathbb{E}[(f(\\mathbf{x}^*) - \\mathcal{M}_i(\\mathbf{x}))\\mathbb{I}(\\mathcal{M}_i(\\mathbf{x}) &lt; f(\\mathbf{x}^*))] \\\\ &amp;= \\underbrace{(f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x}))\\Phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)}_\\text{exploits areas with low mean} \\\\ &amp;+  \\underbrace{\\sigma_{\\mathcal{M}_i}(\\mathbf{x}) \\phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)}_\\text{explores areas with high variance} \\nonumber \\end{aligned} $$</p> <p>with $\\mathbb{I}(\\cdot)$ denoting the indicator function and $\\phi(\\cdot)$ being the standard normal probability density function.</p> </li> </ul> <p>For those particularly interested in diving deeper into Bayesian optimisation, be sure to check out Shahriari et al.'s \"Taking the Human Out of the Loop: A Review of Bayesian Optimization\", which includes a wide variety of acquisition functions, as well as some examples of more exotic BO problems, such as problems which also feature unknown constraints.</p>"},{"location":"examples/bayesian_optimisation/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/classification/","title":"Classification","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom time import time\nimport blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nimport jax.tree_util as jtu\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom tqdm import trange\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\ntfd = tfp.distributions\nidentity_matrix = jnp.eye\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from time import time import blackjax import jax import jax.numpy as jnp import jax.random as jr import jax.scipy as jsp import jax.tree_util as jtu from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax as tfp from tqdm import trange  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  tfd = tfp.distributions identity_matrix = jnp.eye key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>key, subkey = jr.split(key)\nx = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0)\ny = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\n</pre> key, subkey = jr.split(key) x = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0) y = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)  fig, ax = plt.subplots() ax.scatter(x, y) Out[2]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f03f0f30580&gt;</pre> In\u00a0[3]: Copied! <pre>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Constant()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\n</pre> kernel = gpx.kernels.RBF() meanf = gpx.mean_functions.Constant() prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n) <p>We construct the posterior through the product of our prior and likelihood.</p> In\u00a0[4]: Copied! <pre>posterior = prior * likelihood\nprint(type(posterior))\n</pre> posterior = prior * likelihood print(type(posterior)) <pre>&lt;class 'gpjax.gps.NonConjugatePosterior'&gt;\n</pre> <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. We begin with maximum a posteriori (MAP) estimation, a fast inference procedure to obtain point estimates for the latent function and the kernel's hyperparameters by maximising the marginal log-likelihood.</p> <p>We can obtain a MAP estimate by optimising the log-posterior density with Optax's optimisers.</p> In\u00a0[5]: Copied! <pre>negative_lpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=True))\n\noptimiser = ox.adam(learning_rate=0.01)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_lpd,\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.01),\n    num_iters=1000,\n    key=key,\n)\n</pre> negative_lpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=True))  optimiser = ox.adam(learning_rate=0.01)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_lpd,     train_data=D,     optim=ox.adamw(learning_rate=0.01),     num_iters=1000,     key=key, ) <p>From which we can make predictions at novel inputs, as illustrated below.</p> In\u00a0[6]: Copied! <pre>map_latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(map_latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\n\nax.legend()\n</pre> map_latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(map_latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.2,     color=cols[1],     label=\"One sigma\", ) ax.plot(     xtest,     predictive_mean - predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, )  ax.legend() Out[6]: <pre>&lt;matplotlib.legend.Legend at 0x7f03f06b4cd0&gt;</pre> <p>Here we projected the map estimates $\\hat{\\boldsymbol{f}}$ for the function values $\\boldsymbol{f}$ at the data points $\\boldsymbol{x}$ to get predictions over the whole domain,</p> <p>\\begin{align} p(f(\\cdot)| \\mathcal{D})  \\approx q_{map}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) \\delta(\\boldsymbol{f} - \\hat{\\boldsymbol{f}}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}</p> <p>However, as a point estimate, MAP estimation is severely limited for uncertainty quantification, providing only a single piece of information about the posterior.</p> In\u00a0[7]: Copied! <pre>import cola\nfrom gpjax.lower_cholesky import lower_cholesky\n\ngram, cross_covariance = (kernel.gram, kernel.cross_covariance)\njitter = 1e-6\n\n# Compute (latent) function value map estimates at training points:\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nKxx = cola.PSD(Kxx)\nLx = lower_cholesky(Kxx)\nf_hat = Lx @ opt_posterior.latent\n\n# Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f):\nH = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]\n\nL = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)\n\n# H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I\nL_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True)\nH_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\nLH = jnp.linalg.cholesky(H_inv)\nlaplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH)\n</pre> import cola from gpjax.lower_cholesky import lower_cholesky  gram, cross_covariance = (kernel.gram, kernel.cross_covariance) jitter = 1e-6  # Compute (latent) function value map estimates at training points: Kxx = opt_posterior.prior.kernel.gram(x) Kxx += identity_matrix(D.n) * jitter Kxx = cola.PSD(Kxx) Lx = lower_cholesky(Kxx) f_hat = Lx @ opt_posterior.latent  # Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f): H = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]  L = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)  # H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I L_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True) H_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False) LH = jnp.linalg.cholesky(H_inv) laplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH) <p>For novel inputs, we must project the above approximating distribution through the Gaussian conditional distribution $p(f(\\cdot)| \\boldsymbol{f})$,</p> <p>\\begin{align} p(f(\\cdot)| \\mathcal{D}) \\approx q_{Laplace}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) q(\\boldsymbol{f}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} (\\mathbf{K}_{\\boldsymbol{xx}} - [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1}) \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}</p> <p>This is the same approximate distribution $q_{map}(f(\\cdot))$, but we have perturbed the covariance by a curvature term of $\\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}$. We take the latent distribution computed in the previous section and add this term to the covariance to construct $q_{Laplace}(f(\\cdot))$.</p> In\u00a0[8]: Copied! <pre>def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:\n    map_latent_dist = opt_posterior.predict(xtest, train_data=D)\n\n    Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)\n    Kxx = opt_posterior.prior.kernel.gram(x)\n    Kxx += identity_matrix(D.n) * jitter\n    Kxx = cola.PSD(Kxx)\n\n    # Kxx\u207b\u00b9 Kxt\n    Kxx_inv_Kxt = cola.solve(Kxx, Kxt)\n\n    # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt\n    laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Kxt.T, H_inv), Kxx_inv_Kxt)\n\n    mean = map_latent_dist.mean()\n    covariance = map_latent_dist.covariance() + laplace_cov_term\n    L = jnp.linalg.cholesky(covariance)\n    return tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L)\n</pre> def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:     map_latent_dist = opt_posterior.predict(xtest, train_data=D)      Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)     Kxx = opt_posterior.prior.kernel.gram(x)     Kxx += identity_matrix(D.n) * jitter     Kxx = cola.PSD(Kxx)      # Kxx\u207b\u00b9 Kxt     Kxx_inv_Kxt = cola.solve(Kxx, Kxt)      # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt     laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Kxt.T, H_inv), Kxx_inv_Kxt)      mean = map_latent_dist.mean()     covariance = map_latent_dist.covariance() + laplace_cov_term     L = jnp.linalg.cholesky(covariance)     return tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L) <p>From this we can construct the predictive distribution at the test points.</p> In\u00a0[9]: Copied! <pre>laplace_latent_dist = construct_laplace(xtest)\npredictive_dist = opt_posterior.likelihood(laplace_latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> laplace_latent_dist = construct_laplace(xtest) predictive_dist = opt_posterior.likelihood(laplace_latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.2,     color=cols[1],     label=\"One sigma\", ) ax.plot(     xtest,     predictive_mean - predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x7f03f008a590&gt;</pre> <p>However, the Laplace approximation is still limited by considering information about the posterior at a single location. On the other hand, through approximate sampling, MCMC methods allow us to learn all information about the posterior distribution.</p> In\u00a0[10]: Copied! <pre>num_adapt = 500\nnum_samples = 500\n\nlpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n\n# Initialise the chain\nstart = time()\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\nprint(f\"Adaption time taken: {time() - start: .1f} seconds\")\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Sample from the posterior distribution\nstart = time()\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nprint(f\"Sampling time taken: {time() - start: .1f} seconds\")\n</pre> num_adapt = 500 num_samples = 500  lpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False)) unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))  adapt = blackjax.window_adaptation(     blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65 )  # Initialise the chain start = time() last_state, kernel, _ = adapt.run(key, posterior.unconstrain()) print(f\"Adaption time taken: {time() - start: .1f} seconds\")   def inference_loop(rng_key, kernel, initial_state, num_samples):     def one_step(state, rng_key):         state, info = kernel(rng_key, state)         return state, (state, info)      keys = jax.random.split(rng_key, num_samples)     _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)      return states, infos   # Sample from the posterior distribution start = time() states, infos = inference_loop(key, kernel, last_state, num_samples) print(f\"Sampling time taken: {time() - start: .1f} seconds\") <pre>Adaption time taken:  89.6 seconds\n</pre> <pre>Sampling time taken:  164.3 seconds\n</pre> In\u00a0[11]: Copied! <pre>acceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</pre> acceptance_rate = jnp.mean(infos.acceptance_probability) print(f\"Acceptance rate: {acceptance_rate:.2f}\") <pre>Acceptance rate: 0.66\n</pre> <p>Our acceptance rate is slightly too large, prompting an examination of the chain's trace plots. A well-mixing chain will have very few (if any) flat spots in its trace plot whilst also not having too many steps in the same direction. In addition to the model's hyperparameters, there will be 500 samples for each of the 100 latent function values in the <code>states.position</code> dictionary. We depict the chains that correspond to the model hyperparameters and the first value of the latent function for brevity.</p> In\u00a0[12]: Copied! <pre>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.prior.kernel.lengthscale)\nax1.plot(states.position.prior.kernel.variance)\nax2.plot(states.position.latent[:, 1, :])\nax0.set_title(\"Kernel Lengthscale\")\nax1.set_title(\"Kernel Variance\")\nax2.set_title(\"Latent Function (index = 1)\")\n</pre> fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3)) ax0.plot(states.position.prior.kernel.lengthscale) ax1.plot(states.position.prior.kernel.variance) ax2.plot(states.position.latent[:, 1, :]) ax0.set_title(\"Kernel Lengthscale\") ax1.set_title(\"Kernel Variance\") ax2.set_title(\"Latent Function (index = 1)\") Out[12]: <pre>Text(0.5, 1.0, 'Latent Function (index = 1)')</pre> In\u00a0[13]: Copied! <pre>thin_factor = 20\nposterior_samples = []\n\nfor i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):\n    sample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)\n    sample = sample.constrain()\n    latent_dist = sample.predict(xtest, train_data=D)\n    predictive_dist = sample.likelihood(latent_dist)\n    posterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n\nposterior_samples = jnp.vstack(posterior_samples)\nlower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(posterior_samples, axis=0)\n</pre> thin_factor = 20 posterior_samples = []  for i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):     sample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)     sample = sample.constrain()     latent_dist = sample.predict(xtest, train_data=D)     predictive_dist = sample.likelihood(latent_dist)     posterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))  posterior_samples = jnp.vstack(posterior_samples) lower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0) expected_val = jnp.mean(posterior_samples, axis=0) <pre>\rDrawing posterior samples:   0%|          | 0/25 [00:00&lt;?, ?it/s]</pre> <pre>\rDrawing posterior samples:   4%|\u258d         | 1/25 [00:00&lt;00:07,  3.06it/s]</pre> <pre>\rDrawing posterior samples:  12%|\u2588\u258f        | 3/25 [00:00&lt;00:02,  8.20it/s]</pre> <pre>\rDrawing posterior samples:  24%|\u2588\u2588\u258d       | 6/25 [00:00&lt;00:01, 12.52it/s]</pre> <pre>\rDrawing posterior samples:  32%|\u2588\u2588\u2588\u258f      | 8/25 [00:00&lt;00:01, 14.41it/s]</pre> <pre>\rDrawing posterior samples:  40%|\u2588\u2588\u2588\u2588      | 10/25 [00:00&lt;00:00, 15.84it/s]</pre> <pre>\rDrawing posterior samples:  48%|\u2588\u2588\u2588\u2588\u258a     | 12/25 [00:00&lt;00:00, 16.80it/s]</pre> <pre>\rDrawing posterior samples:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 14/25 [00:01&lt;00:00, 17.38it/s]</pre> <pre>\rDrawing posterior samples:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 16/25 [00:01&lt;00:00, 17.80it/s]</pre> <pre>\rDrawing posterior samples:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 18/25 [00:01&lt;00:00, 18.05it/s]</pre> <pre>\rDrawing posterior samples:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 20/25 [00:01&lt;00:00, 18.25it/s]</pre> <pre>\rDrawing posterior samples:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 22/25 [00:01&lt;00:00, 18.36it/s]</pre> <pre>\rDrawing posterior samples:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:01&lt;00:00, 18.58it/s]</pre> <pre>\rDrawing posterior samples: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:01&lt;00:00, 15.70it/s]</pre> <pre>\n</pre> <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots()\nax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7)\nax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=cols[1],\n    label=\"95\\\\% CI\",\n)\nax.plot(\n    xtest,\n    lower_ci.flatten(),\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    upper_ci.flatten(),\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> fig, ax = plt.subplots() ax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7) ax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1) ax.fill_between(     xtest.flatten(),     lower_ci.flatten(),     upper_ci.flatten(),     alpha=0.2,     color=cols[1],     label=\"95\\\\% CI\", ) ax.plot(     xtest,     lower_ci.flatten(),     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     upper_ci.flatten(),     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() Out[14]: <pre>&lt;matplotlib.legend.Legend at 0x7f03e879b6d0&gt;</pre> In\u00a0[15]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\"\n</pre> %load_ext watermark %watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\" <pre>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ntensorflow_probability: 0.22.1\nmatplotlib            : 3.8.3\nblackjax              : 0.9.6\ncola                  : 0.0.5\noptax                 : 0.1.9\njax                   : 0.4.25\ngpjax                 : 0.8.0\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/classification/#classification","title":"Classification\u00b6","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via maximum a posteriori (MAP) and Markov chain Monte Carlo (MCMC). We focus on a classification task here and use BlackJax for sampling.</p>"},{"location":"examples/classification/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-1., 1)$ and corresponding binary outputs</p> <p>$$\\boldsymbol{y} = 0.5 * \\text{sign}(\\cos(2 *  + \\boldsymbol{\\epsilon})) + 0.5, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} * (0.05)^{2} \\right).$$</p> <p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs for later.</p>"},{"location":"examples/classification/#map-inference","title":"MAP inference\u00b6","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. Since our observations are binary, we choose a Bernoulli likelihood with a probit link function.</p>"},{"location":"examples/classification/#laplace-approximation","title":"Laplace approximation\u00b6","text":"<p>The Laplace approximation improves uncertainty quantification by incorporating curvature induced by the marginal log-likelihood's Hessian to construct an approximate Gaussian distribution centered on the MAP estimate. Writing $\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})$ as the unormalised posterior for function values $\\boldsymbol{f}$ at the datapoints $\\boldsymbol{x}$, we can expand the log of this about the posterior mode $\\hat{\\boldsymbol{f}}$ via a Taylor expansion. This gives:</p> <p>\\begin{align} \\log\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) + \\left[\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})|_{\\hat{\\boldsymbol{f}}}\\right]^{T} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\mathcal{O}(\\lVert \\boldsymbol{f} - \\hat{\\boldsymbol{f}} \\rVert^3). \\end{align}</p> <p>Since $\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})$ is zero at the mode, this suggests the following approximation \\begin{align} \\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) \\approx \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) \\exp\\left\\{ \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) \\right\\} \\end{align},</p> <p>that we identify as a Gaussian distribution, $p(\\boldsymbol{f}| \\mathcal{D}) \\approx q(\\boldsymbol{f}) := \\mathcal{N}(\\hat{\\boldsymbol{f}}, [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} )$. Since the negative Hessian is positive definite, we can use the Cholesky decomposition to obtain the covariance matrix of the Laplace approximation at the datapoints below.</p>"},{"location":"examples/classification/#mcmc-inference","title":"MCMC inference\u00b6","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"examples/classification/#mcmc-through-blackjax","title":"MCMC through BlackJax\u00b6","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We'll use the No U-Turn Sampler (NUTS) implementation given in BlackJax for sampling. For the interested reader, NUTS is a Hamiltonian Monte Carlo sampling scheme where the number of leapfrog integration steps is computed at each step of the change according to the NUTS algorithm. In general, samplers constructed under this framework are very efficient.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 500 values from our Markov chain. In practice, drawing more samples will be necessary.</p>"},{"location":"examples/classification/#sampler-efficiency","title":"Sampler efficiency\u00b6","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain). For NUTS and Hamiltonian Monte Carlo sampling, we typically seek an acceptance rate of 60-70% to strike the right balance between having a chain which is stuck and rarely moves versus a chain that is too jumpy with frequent small steps.</p>"},{"location":"examples/classification/#prediction","title":"Prediction\u00b6","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every $n$th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>"},{"location":"examples/classification/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/collapsed_vi/","title":"Sparse Gaussian Process Regression","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from docs.examples.utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>n = 2500\nnoise = 0.5\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 2500 noise = 0.5  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise. We also plot an initial set of inducing points over the space.</p> In\u00a0[3]: Copied! <pre>n_inducing = 50\nz = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\nax.vlines(\n    x=z,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=0.5,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend(loc=\"best\")\nplt.show()\n</pre> n_inducing = 50 z = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1]) ax.vlines(     x=z,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=0.5,     label=\"Inducing point\",     color=cols[2], ) ax.legend(loc=\"best\") plt.show() <p>Next we define the true posterior model for the data - note that whilst we can define this, it is intractable to evaluate.</p> In\u00a0[4]: Copied! <pre>meanf = gpx.mean_functions.Constant()\nkernel = gpx.kernels.RBF()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior * likelihood\n</pre> meanf = gpx.mean_functions.Constant() kernel = gpx.kernels.RBF() likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n) prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel) posterior = prior * likelihood <p>We now define the SGPR model through <code>CollapsedVariationalGaussian</code>. Through a set of inducing points $\\boldsymbol{z}$ this object builds an approximation to the true posterior distribution. Consequently, we pass the true posterior and initial inducing points into the constructor as arguments.</p> In\u00a0[5]: Copied! <pre>q = gpx.variational_families.CollapsedVariationalGaussian(\n    posterior=posterior, inducing_inputs=z\n)\n</pre> q = gpx.variational_families.CollapsedVariationalGaussian(     posterior=posterior, inducing_inputs=z ) <p>We define our variational inference algorithm through <code>CollapsedVI</code>. This defines the collapsed variational free energy bound considered in Titsias (2009).</p> In\u00a0[6]: Copied! <pre>elbo = gpx.objectives.CollapsedELBO(negative=True)\n</pre> elbo = gpx.objectives.CollapsedELBO(negative=True) <p>For researchers, GPJax has the capacity to print the bibtex citation for objects such as the ELBO through the <code>cite()</code> function.</p> In\u00a0[7]: Copied! <pre>print(gpx.cite(elbo))\n</pre> print(gpx.cite(elbo)) <pre>@inproceedings{titsias2009variational,\nauthors = {Titsias, Michalis},\ntitle = {Variational learning of inducing variables in sparse Gaussian processes},\nyear = {2009},\nbooktitle = {International Conference on Artificial Intelligence and Statistics},\n}\n</pre> <p>JIT-compiling expensive-to-compute functions such as the ELBO is advisable. This can be achieved by wrapping the function in <code>jax.jit()</code>.</p> In\u00a0[8]: Copied! <pre>elbo = jit(elbo)\n</pre>  elbo = jit(elbo) <p>We now train our model akin to a Gaussian process regression model via the <code>fit</code> abstraction. Unlike the regression example given in the conjugate regression notebook, the inducing locations that induce our variational posterior distribution are now part of the model's parameters. Using a gradient-based optimiser, we can then optimise their location such that the evidence lower bound is maximised.</p> In\u00a0[9]: Copied! <pre>opt_posterior, history = gpx.fit(\n    model=q,\n    objective=elbo,\n    train_data=D,\n    optim=ox.adamw(learning_rate=1e-2),\n    num_iters=500,\n    key=key,\n)\n</pre> opt_posterior, history = gpx.fit(     model=q,     objective=elbo,     train_data=D,     optim=ox.adamw(learning_rate=1e-2),     num_iters=500,     key=key, ) In\u00a0[10]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")\n</pre> fig, ax = plt.subplots() ax.plot(history, color=cols[1]) ax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\") Out[10]: <pre>[Text(0.5, 0, 'Training iterate'), Text(0, 0.5, 'ELBO')]</pre> <p>We show predictions of our model with the learned inducing points overlaid in grey.</p> In\u00a0[11]: Copied! <pre>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\ninducing_points = opt_posterior.inducing_inputs\n\nsamples = latent_dist.sample(seed=key, sample_shape=(20,))\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\n\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\nax.plot(\n    xtest,\n    ytest,\n    label=\"Latent function\",\n    color=cols[1],\n    linestyle=\"-\",\n    linewidth=1,\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\n\n\nax.vlines(\n    x=inducing_points,\n    ymin=ytest.min(),\n    ymax=ytest.max(),\n    alpha=0.3,\n    linewidth=0.5,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nplt.show()\n</pre> latent_dist = opt_posterior(xtest, train_data=D) predictive_dist = opt_posterior.posterior.likelihood(latent_dist)  inducing_points = opt_posterior.inducing_inputs  samples = latent_dist.sample(seed=key, sample_shape=(20,))  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots()  ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1) ax.plot(     xtest,     ytest,     label=\"Latent function\",     color=cols[1],     linestyle=\"-\",     linewidth=1, ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])  ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     color=cols[1],     label=\"Two sigma\", ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=0.5, ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=0.5, )   ax.vlines(     x=inducing_points,     ymin=ytest.min(),     ymax=ytest.max(),     alpha=0.3,     linewidth=0.5,     label=\"Inducing point\",     color=cols[2], ) ax.legend() ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\") plt.show() In\u00a0[12]: Copied! <pre>full_rank_model = gpx.gps.Prior(\n    mean_function=gpx.mean_functions.Zero(), kernel=gpx.kernels.RBF()\n) * gpx.likelihoods.Gaussian(num_datapoints=D.n)\nnegative_mll = jit(gpx.objectives.ConjugateMLL(negative=True).step)\n%timeit negative_mll(full_rank_model, D).block_until_ready()\n</pre> full_rank_model = gpx.gps.Prior(     mean_function=gpx.mean_functions.Zero(), kernel=gpx.kernels.RBF() ) * gpx.likelihoods.Gaussian(num_datapoints=D.n) negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True).step) %timeit negative_mll(full_rank_model, D).block_until_ready() <pre>644 ms \u00b1 11.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[13]: Copied! <pre>negative_elbo = jit(gpx.objectives.CollapsedELBO(negative=True).step)\n%timeit negative_elbo(q, D).block_until_ready()\n</pre> negative_elbo = jit(gpx.objectives.CollapsedELBO(negative=True).step) %timeit negative_elbo(q, D).block_until_ready() <pre>2.11 ms \u00b1 351 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> <p>As we can see, the sparse approximation given here is around 50 times faster when compared against a full-rank model.</p> In\u00a0[14]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Daniel Dodd' <pre>Author: Daniel Dodd\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\njax       : 0.4.25\noptax     : 0.1.9\ngpjax     : 0.8.0\nmatplotlib: 3.8.3\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/collapsed_vi/#sparse-gaussian-process-regression","title":"Sparse Gaussian Process Regression\u00b6","text":"<p>In this notebook we consider sparse Gaussian process regression (SGPR) Titsias (2009). This is a solution for medium to large-scale conjugate regression problems. In order to arrive at a computationally tractable method, the approximate posterior is parameterized via a set of $m$ pseudo-points $\\boldsymbol{z}$. Critically, the approach leads to $\\mathcal{O}(nm^2)$ complexity for approximate maximum likelihood learning and $O(m^2)$ per test point for prediction.</p>"},{"location":"examples/collapsed_vi/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{500}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> <p>$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(7\\boldsymbol{x}) + x \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.5^2 \\right).$$</p> <p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/collapsed_vi/#runtime-comparison","title":"Runtime comparison\u00b6","text":"<p>Given the size of the data being considered here, inference in a GP with a full-rank covariance matrix is possible, albeit quite slow. We can therefore compare the speedup that we get from using the above sparse approximation with corresponding bound on the marginal log-likelihood against the marginal log-likelihood in the full model.</p>"},{"location":"examples/collapsed_vi/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/constructing_new_kernels/","title":"Kernel Guide","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom simple_pytree import static_field\nimport tensorflow_probability.substrates.jax as tfp\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base.param import param_field\n\nkey = jr.key(123)\ntfb = tfp.bijectors\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from dataclasses import dataclass from typing import Dict  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import numpy as np from simple_pytree import static_field import tensorflow_probability.substrates.jax as tfp  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base.param import param_field  key = jr.key(123) tfb = tfp.bijectors plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>kernels = [\n    gpx.kernels.Matern12(),\n    gpx.kernels.Matern32(),\n    gpx.kernels.Matern52(),\n    gpx.kernels.RBF(),\n    gpx.kernels.Polynomial(),\n    gpx.kernels.Polynomial(degree=2),\n]\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n\nmeanf = gpx.mean_functions.Zero()\n\nfor k, ax in zip(kernels, axes.ravel()):\n    prior = gpx.gps.Prior(mean_function=meanf, kernel=k)\n    rv = prior(x)\n    y = rv.sample(seed=key, sample_shape=(10,))\n    ax.plot(x, y.T, alpha=0.7)\n    ax.set_title(k.name)\n</pre> kernels = [     gpx.kernels.Matern12(),     gpx.kernels.Matern32(),     gpx.kernels.Matern52(),     gpx.kernels.RBF(),     gpx.kernels.Polynomial(),     gpx.kernels.Polynomial(degree=2), ] fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)  meanf = gpx.mean_functions.Zero()  for k, ax in zip(kernels, axes.ravel()):     prior = gpx.gps.Prior(mean_function=meanf, kernel=k)     rv = prior(x)     y = rv.sample(seed=key, sample_shape=(10,))     ax.plot(x, y.T, alpha=0.7)     ax.set_title(k.name) In\u00a0[3]: Copied! <pre>slice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3], lengthscale=jnp.ones((3,)))\n</pre> slice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3], lengthscale=jnp.ones((3,))) <p>The resulting kernel has one length-scale parameter per input dimension --- an ARD kernel.</p> In\u00a0[4]: Copied! <pre>print(f\"Lengthscales: {slice_kernel.lengthscale}\")\n</pre> print(f\"Lengthscales: {slice_kernel.lengthscale}\") <pre>Lengthscales: [1. 1. 1.]\n</pre> <p>We'll now simulate some data and evaluate the kernel on the previously selected input dimensions.</p> In\u00a0[5]: Copied! <pre># Inputs\nx_matrix = jr.normal(key, shape=(50, 5))\n\n# Compute the Gram matrix\nK = slice_kernel.gram(x_matrix)\nprint(K.shape)\n</pre> # Inputs x_matrix = jr.normal(key, shape=(50, 5))  # Compute the Gram matrix K = slice_kernel.gram(x_matrix) print(K.shape) <pre>(50, 50)\n</pre> In\u00a0[6]: Copied! <pre>k1 = gpx.kernels.RBF()\nk2 = gpx.kernels.Polynomial()\nsum_k = gpx.kernels.SumKernel(kernels=[k1, k2])\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(sum_k.gram(x).to_dense())\n\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\n</pre> k1 = gpx.kernels.RBF() k2 = gpx.kernels.Polynomial() sum_k = gpx.kernels.SumKernel(kernels=[k1, k2])  fig, ax = plt.subplots(ncols=3, figsize=(9, 3)) im0 = ax[0].matshow(k1.gram(x).to_dense()) im1 = ax[1].matshow(k2.gram(x).to_dense()) im2 = ax[2].matshow(sum_k.gram(x).to_dense())  fig.colorbar(im0, ax=ax[0], fraction=0.05) fig.colorbar(im1, ax=ax[1], fraction=0.05) fig.colorbar(im2, ax=ax[2], fraction=0.05) Out[6]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7faff46d6080&gt;</pre> <p>Similarly, products of kernels can be created through the <code>ProductKernel</code> class.</p> In\u00a0[7]: Copied! <pre>k3 = gpx.kernels.Matern32()\n\nprod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])\n\nfig, ax = plt.subplots(ncols=4, figsize=(12, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(k3.gram(x).to_dense())\nim3 = ax[3].matshow(prod_k.gram(x).to_dense())\n\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nfig.colorbar(im3, ax=ax[3], fraction=0.05)\n</pre> k3 = gpx.kernels.Matern32()  prod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])  fig, ax = plt.subplots(ncols=4, figsize=(12, 3)) im0 = ax[0].matshow(k1.gram(x).to_dense()) im1 = ax[1].matshow(k2.gram(x).to_dense()) im2 = ax[2].matshow(k3.gram(x).to_dense()) im3 = ax[3].matshow(prod_k.gram(x).to_dense())  fig.colorbar(im0, ax=ax[0], fraction=0.05) fig.colorbar(im1, ax=ax[1], fraction=0.05) fig.colorbar(im2, ax=ax[2], fraction=0.05) fig.colorbar(im3, ax=ax[3], fraction=0.05) Out[7]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7faff4984340&gt;</pre> In\u00a0[8]: Copied! <pre>def angular_distance(x, y, c):\n    return jnp.abs((x - y + c) % (c * 2) - c)\n\n\nbij = tfb.SoftClip(low=jnp.array(4.0, dtype=jnp.float64))\n\n\n@dataclass\nclass Polar(gpx.kernels.AbstractKernel):\n    period: float = static_field(2 * jnp.pi)\n    tau: float = param_field(jnp.array([5.0]), bijector=bij)\n\n    def __call__(\n        self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        c = self.period / 2.0\n        t = angular_distance(x, y, c)\n        K = (1 + self.tau * t / c) * jnp.clip(1 - t / c, 0, jnp.inf) ** self.tau\n        return K.squeeze()\n</pre> def angular_distance(x, y, c):     return jnp.abs((x - y + c) % (c * 2) - c)   bij = tfb.SoftClip(low=jnp.array(4.0, dtype=jnp.float64))   @dataclass class Polar(gpx.kernels.AbstractKernel):     period: float = static_field(2 * jnp.pi)     tau: float = param_field(jnp.array([5.0]), bijector=bij)      def __call__(         self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]     ) -&gt; Float[Array, \"1\"]:         c = self.period / 2.0         t = angular_distance(x, y, c)         K = (1 + self.tau * t / c) * jnp.clip(1 - t / c, 0, jnp.inf) ** self.tau         return K.squeeze() <p>We unpack this now to make better sense of it. In the kernel's initialiser we specify the length of a single period. As the underlying domain is a circle, this is $2\\pi$. We then define the kernel's <code>__call__</code> function which is a direct implementation of Equation (1) where we define <code>c</code> as half the value of <code>period</code>.</p> <p>To constrain $\\tau$ to be greater than 4, we use a <code>Softplus</code> bijector with a clipped lower bound of 4.0. This is done by specifying the <code>bijector</code> argument when we define the parameter field.</p> In\u00a0[9]: Copied! <pre># Simulate data\nangles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1)\nn = 20\nnoise = 0.2\n\nX = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0)\ny = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise\n\nD = gpx.Dataset(X=X, y=y)\n\n# Define polar Gaussian process\nPKern = Polar()\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\ncircular_posterior = gpx.gps.Prior(mean_function=meanf, kernel=PKern) * likelihood\n\n# Optimise GP's marginal log-likelihood using BFGS\nopt_posterior, history = gpx.fit_scipy(\n    model=circular_posterior,\n    objective=jit(gpx.objectives.ConjugateMLL(negative=True)),\n    train_data=D,\n)\n</pre> # Simulate data angles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1) n = 20 noise = 0.2  X = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0) y = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise  D = gpx.Dataset(X=X, y=y)  # Define polar Gaussian process PKern = Polar() meanf = gpx.mean_functions.Zero() likelihood = gpx.likelihoods.Gaussian(num_datapoints=n) circular_posterior = gpx.gps.Prior(mean_function=meanf, kernel=PKern) * likelihood  # Optimise GP's marginal log-likelihood using BFGS opt_posterior, history = gpx.fit_scipy(     model=circular_posterior,     objective=jit(gpx.objectives.ConjugateMLL(negative=True)),     train_data=D, ) <pre>Optimization terminated successfully.\n         Current function value: 29.785123\n         Iterations: 19\n         Function evaluations: 24\n         Gradient evaluations: 24\n</pre> In\u00a0[10]: Copied! <pre>posterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D))\nmu = posterior_rv.mean()\none_sigma = posterior_rv.stddev()\n</pre> posterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D)) mu = posterior_rv.mean() one_sigma = posterior_rv.stddev() In\u00a0[11]: Copied! <pre>fig = plt.figure(figsize=(7, 3.5))\ngridspec = fig.add_gridspec(1, 1)\nax = plt.subplot(gridspec[0], polar=True)\n\nax.fill_between(\n    angles.squeeze(),\n    mu - one_sigma,\n    mu + one_sigma,\n    alpha=0.3,\n    label=r\"1 Posterior s.d.\",\n    color=cols[1],\n    lw=0,\n)\nax.fill_between(\n    angles.squeeze(),\n    mu - 3 * one_sigma,\n    mu + 3 * one_sigma,\n    alpha=0.15,\n    label=r\"3 Posterior s.d.\",\n    color=cols[1],\n    lw=0,\n)\nax.plot(angles, mu, label=\"Posterior mean\")\nax.scatter(D.X, D.y, alpha=1, label=\"Observations\")\nax.legend()\n</pre> fig = plt.figure(figsize=(7, 3.5)) gridspec = fig.add_gridspec(1, 1) ax = plt.subplot(gridspec[0], polar=True)  ax.fill_between(     angles.squeeze(),     mu - one_sigma,     mu + one_sigma,     alpha=0.3,     label=r\"1 Posterior s.d.\",     color=cols[1],     lw=0, ) ax.fill_between(     angles.squeeze(),     mu - 3 * one_sigma,     mu + 3 * one_sigma,     alpha=0.15,     label=r\"3 Posterior s.d.\",     color=cols[1],     lw=0, ) ax.plot(angles, mu, label=\"Posterior mean\") ax.scatter(D.X, D.y, alpha=1, label=\"Observations\") ax.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x7fafe4417ca0&gt;</pre> In\u00a0[12]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ngpjax                 : 0.8.0\ntensorflow_probability: 0.22.1\nnumpy                 : 1.26.4\njax                   : 0.4.25\nmatplotlib            : 3.8.3\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/constructing_new_kernels/#kernel-guide","title":"Kernel Guide\u00b6","text":"<p>In this guide, we introduce the kernels available in GPJax and demonstrate how to create custom kernels.</p>"},{"location":"examples/constructing_new_kernels/#supported-kernels","title":"Supported Kernels\u00b6","text":"<p>The following kernels are natively supported in GPJax.</p> <ul> <li>Mat\u00e9rn 1/2, 3/2 and 5/2.</li> <li>RBF (or squared exponential).</li> <li>Rational quadratic.</li> <li>Powered exponential.</li> <li>Polynomial.</li> <li>White noise</li> <li>Linear.</li> <li>Polynomial.</li> <li>Graph kernels.</li> </ul> <p>While the syntax is consistent, each kernel's type influences the characteristics of the sample paths drawn. We visualise this below with 10 function draws per kernel.</p>"},{"location":"examples/constructing_new_kernels/#active-dimensions","title":"Active dimensions\u00b6","text":"<p>By default, kernels operate over every dimension of the supplied inputs. In some use cases, it is desirable to restrict kernels to specific dimensions of the input data. We can achieve this by the <code>active dims</code> argument, which determines which input index values the kernel evaluates.</p> <p>To see this, consider the following 5-dimensional dataset for which we would like our RBF kernel to act on the first, second and fourth dimensions.</p>"},{"location":"examples/constructing_new_kernels/#kernel-combinations","title":"Kernel combinations\u00b6","text":"<p>The product or sum of two positive definite matrices yields a positive definite matrix. Consequently, summing or multiplying sets of kernels is a valid operation that can give rich kernel functions. In GPJax, functionality for a sum kernel is provided by the <code>SumKernel</code> class.</p>"},{"location":"examples/constructing_new_kernels/#custom-kernel","title":"Custom kernel\u00b6","text":"<p>GPJax makes the process of implementing kernels of your choice straightforward with two key steps:</p> <ol> <li>Listing the kernel's parameters.</li> <li>Defining the kernel's pairwise operation.</li> </ol> <p>We'll demonstrate this process now for a circular kernel --- an adaption of the excellent guide given in the PYMC3 documentation. We encourage curious readers to visit their notebook here.</p>"},{"location":"examples/constructing_new_kernels/#circular-kernel","title":"Circular kernel\u00b6","text":"<p>When the underlying space is polar, typical Euclidean kernels such as Mat\u00e9rn kernels are insufficient at the boundary where discontinuities will present themselves. This is due to the fact that for a polar space $\\lvert 0, 2\\pi\\rvert=0$ i.e., the space wraps. Euclidean kernels have no mechanism in them to represent this logic and will instead treat $0$ and $2\\pi$ and elements far apart. Circular kernels do not exhibit this behaviour and instead wrap around the boundary points to create a smooth function. Such a kernel was given in Padonou &amp; Roustant (2015) where any two angles $\\theta$ and $\\theta'$ are written as $$W_c(\\theta, \\theta') = \\left\\lvert \\left(1 + \\tau \\frac{d(\\theta, \\theta')}{c} \\right) \\left(1 - \\frac{d(\\theta, \\theta')}{c} \\right)^{\\tau} \\right\\rvert \\quad \\tau \\geq 4 \\tag{1}.$$</p> <p>Here the hyperparameter $\\tau$ is analogous to a lengthscale for Euclidean stationary kernels, controlling the correlation between pairs of observations. While $d$ is an angular distance metric</p> <p>$$d(\\theta, \\theta') = \\lvert (\\theta-\\theta'+c) \\operatorname{mod} 2c - c \\rvert.$$</p> <p>To implement this, one must write the following class.</p>"},{"location":"examples/constructing_new_kernels/#using-our-polar-kernel","title":"Using our polar kernel\u00b6","text":"<p>We proceed to fit a GP with our custom circular kernel to a random sequence of points on a circle (see the Regression notebook for further details on this process).</p>"},{"location":"examples/constructing_new_kernels/#prediction","title":"Prediction\u00b6","text":"<p>We'll now query the GP's predictive posterior at linearly spaced novel inputs and illustrate the results.</p>"},{"location":"examples/constructing_new_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/decision_making/","title":"Introduction to Decision Making with GPJax","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\n\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\n\nimport gpjax as gpx\nfrom gpjax.decision_making.utility_functions import (\n    ThompsonSampling,\n)\nfrom gpjax.decision_making.utility_maximizer import (\n    ContinuousSinglePointUtilityMaximizer,\n)\nfrom gpjax.decision_making.decision_maker import UtilityDrivenDecisionMaker\nfrom gpjax.decision_making.utils import (\n    OBJECTIVE,\n    build_function_evaluator,\n)\nfrom gpjax.decision_making.posterior_handler import PosteriorHandler\nfrom gpjax.decision_making.search_space import ContinuousSearchSpace\nfrom gpjax.typing import (\n    Array,\n    Float,\n)\n\nkey = jr.key(42)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)   import jax.numpy as jnp import jax.random as jr import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox  import gpjax as gpx from gpjax.decision_making.utility_functions import (     ThompsonSampling, ) from gpjax.decision_making.utility_maximizer import (     ContinuousSinglePointUtilityMaximizer, ) from gpjax.decision_making.decision_maker import UtilityDrivenDecisionMaker from gpjax.decision_making.utils import (     OBJECTIVE,     build_function_evaluator, ) from gpjax.decision_making.posterior_handler import PosteriorHandler from gpjax.decision_making.search_space import ContinuousSearchSpace from gpjax.typing import (     Array,     Float, )  key = jr.key(42) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>def forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:\n    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\n</pre> def forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:     return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4) <p>Within the decision making loop, we'll be querying the black-box objective function many times, and will often use the observed values to fit some probabilistic model. Thereore, it would be useful to have some method to which we can pass a set of points which we wish to query the black-box function at, and which will return a GPJax <code>Dataset</code> object containing the observations. We can use the <code>build_function_evaluator</code> function provided in <code>decision_making.utils</code> to do this. This function takes as input a dictionary of labelled black-box functions, and will return a function evaluator, which can be called with a set of points to evaluate the black-box functions at. The function evaluator will return a dictionary of labelled <code>Dataset</code> objects containing the observations. Note that in our case we only have one black-box function of interest, but in general we may have multiple different black-box functions, such as if we also have constraint functions. The use of the labels inside the dictionary returned by the function evaluator enables us to easily distinguish between these different observations.</p> In\u00a0[3]: Copied! <pre>function_evaluator = build_function_evaluator({OBJECTIVE: forrester})\n</pre> function_evaluator = build_function_evaluator({OBJECTIVE: forrester}) In\u00a0[4]: Copied! <pre>lower_bounds = jnp.array([0.0])\nupper_bounds = jnp.array([1.0])\nsearch_space = ContinuousSearchSpace(\n    lower_bounds=lower_bounds, upper_bounds=upper_bounds\n)\n</pre> lower_bounds = jnp.array([0.0]) upper_bounds = jnp.array([1.0]) search_space = ContinuousSearchSpace(     lower_bounds=lower_bounds, upper_bounds=upper_bounds ) <p>The <code>ContinuousSearchSpace</code> class defines a <code>sample</code> method, which can be used to sample points from the search space using a space-filling design, in this case using the Halton sequence. This will be useful at many points throughout the decision making loop, but for now let's use it to create an initial set of points which we can use to fit our models:</p> In\u00a0[5]: Copied! <pre>initial_x = search_space.sample(5, key)\ninitial_datasets = function_evaluator(initial_x)\n</pre> initial_x = search_space.sample(5, key) initial_datasets = function_evaluator(initial_x) <p>Many sequential decision making algorithms are described as being model-based. With these algorithms, we use a probabilistic model, or multiple models, to drive the decision making process. In ordinary BO, a probabilistic model is used to model the objective function, and it is updated based on observations from the black-box objective function. These models are often referred to as surrogate models, and are used to approximate the functions of interest. We'll be using the Gaussian process functionality provided by GPJax to define our surrogate models, with some wrappers provided by the <code>decision_making</code> module to make it easier to use these models within the decision making loop. We can proceed as usual when defining our priors, choosing a suitable mean function and kernel for the job at hand:</p> In\u00a0[6]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Matern52() prior = gpx.gps.Prior(mean_function=mean, kernel=kernel) <p>One difference from GPJax is the way in which we define our likelihood. In GPJax, we construct our GP posteriors by defining a <code>likelihood</code> object and then multiplying it with our prior to get the posterior, <code>posterior = likelihood * prior</code>. However, the <code>AbstractLikelihood</code> objects takes <code>num_datapoints</code> as one of its arguments, and this is going to be changing in the case of BO, and decision making in general, as we keep updating our models having observed new data! In order to deal with this we'll define a <code>likelihood_builder</code>, which takes as an argument the number of datapoints used to condition our prior on, and returns a <code>likelihood</code> object. Below we use this to construct a <code>likelihood_builder</code> which will return a <code>Gaussian</code> likelihood, initialised with the correct number of datapoints:</p> In\u00a0[7]: Copied! <pre>likelihood_builder = lambda n: gpx.likelihoods.Gaussian(\n    num_datapoints=n, obs_stddev=jnp.array(1e-3)\n)\n</pre> likelihood_builder = lambda n: gpx.likelihoods.Gaussian(     num_datapoints=n, obs_stddev=jnp.array(1e-3) ) <p>Now we have all the components required for constructing our GP posterior. Since we'll be updating the posterior throughout the decision making loop as we observe more data, it would be useful to have an object which can handle all this logic for us. Fortunately, the <code>decision_making</code> module provides the <code>PosteriorHandler</code> class to do this for us. This class takes as input a <code>prior</code> and <code>likeligood_builder</code>, which we have defined above. We tend to also optimise the hyperparameters of the GP prior when \"fitting\" our GP, as demonstrated in the Regression notebook. This will be using the GPJax <code>fit</code> method under the hood, which requires an <code>optimization_objective</code>, <code>optimizer</code> and <code>num_optimization_iters</code>. Therefore, we also pass these to the <code>PosteriorHandler</code> as demonstrated below:</p> In\u00a0[8]: Copied! <pre>posterior_handler = PosteriorHandler(\n    prior,\n    likelihood_builder=likelihood_builder,\n    optimization_objective=gpx.objectives.ConjugateMLL(negative=True),\n    optimizer=ox.adam(learning_rate=0.01),\n    num_optimization_iters=1000,\n)\nposterior_handlers = {OBJECTIVE: posterior_handler}\n</pre> posterior_handler = PosteriorHandler(     prior,     likelihood_builder=likelihood_builder,     optimization_objective=gpx.objectives.ConjugateMLL(negative=True),     optimizer=ox.adam(learning_rate=0.01),     num_optimization_iters=1000, ) posterior_handlers = {OBJECTIVE: posterior_handler} <p>Note that we also create a labelled dictionary of <code>posterior_handlers</code>. This is a recurring theme with the decision making logic; we can have dictionaries containing datasets, posteriors and black box functions, and use labels to identify corresponding objects the dictionaries. For instance, here we have an \"OBJECTIVE\" posterior handler which is updated using the data in the \"OBJECTIVE\" dataset, which is in turn generated by the \"OBJECTIVE\" black-box function.</p> <p>Now, as the decision making loop progresses, we can use the <code>update_posterior</code> method of the <code>PosteriorHandler</code> to update our posterior as we observe more data. Note that we use the term posterior to refer to our GP posterior surrogate models in order to be consistent with the syntax used by GPJax. However, these GP posteriors are more widely referred to as models in the model-based decision making literature.</p> In\u00a0[9]: Copied! <pre>utility_function_builder = ThompsonSampling(num_features=500)\n</pre> utility_function_builder = ThompsonSampling(num_features=500) <p>We also need a method for maximising the utility function. Since <code>ThompsonSampling</code> is classed as a <code>SinglePointUtilityFunction</code>, we can use the <code>ContinuousSinglePointUtilityMaximizer</code> to maximise it. This requires the user to specify <code>num_initial_samples</code> and <code>num_restarts</code> when instantiating it. This first queries the utility function at <code>num_initial_samples</code> points, and then uses the best of these points as a starting point for L-BFGS-B, a gradient-based optimiser, to further refine. This is repeated <code>num_restarts</code> times, each time sampling a different initial set of <code>num_initial_samples</code> and the best point found is returned. We'll instantiate our maximiser below:</p> In\u00a0[10]: Copied! <pre>acquisition_maximizer = ContinuousSinglePointUtilityMaximizer(\n    num_initial_samples=100, num_restarts=1\n)\n</pre> acquisition_maximizer = ContinuousSinglePointUtilityMaximizer(     num_initial_samples=100, num_restarts=1 ) In\u00a0[11]: Copied! <pre>def plot_bo_iteration(\n    dm: UtilityDrivenDecisionMaker, last_queried_points: Float[Array, \"B D\"]\n):\n    posterior = dm.posteriors[OBJECTIVE]\n    dataset = dm.datasets[OBJECTIVE]\n    plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\n    forrester_y = forrester(plt_x.squeeze(axis=-1))\n    utility_fn = dm.current_utility_functions[0]\n    sample_y = -utility_fn(plt_x)\n\n    latent_dist = posterior.predict(plt_x, train_data=dataset)\n    predictive_dist = posterior.likelihood(latent_dist)\n\n    predictive_mean = predictive_dist.mean()\n    predictive_std = predictive_dist.stddev()\n\n    fig, ax = plt.subplots()\n    ax.plot(plt_x.squeeze(), predictive_mean, label=\"Predictive Mean\", color=cols[1])\n    ax.fill_between(\n        plt_x.squeeze(),\n        predictive_mean - 2 * predictive_std,\n        predictive_mean + 2 * predictive_std,\n        alpha=0.2,\n        label=\"Two sigma\",\n        color=cols[1],\n    )\n    ax.plot(\n        plt_x.squeeze(),\n        predictive_mean - 2 * predictive_std,\n        linestyle=\"--\",\n        linewidth=1,\n        color=cols[1],\n    )\n    ax.plot(\n        plt_x.squeeze(),\n        predictive_mean + 2 * predictive_std,\n        linestyle=\"--\",\n        linewidth=1,\n        color=cols[1],\n    )\n    ax.plot(plt_x.squeeze(), sample_y, label=\"Posterior Sample\")\n    ax.plot(\n        plt_x.squeeze(),\n        forrester_y,\n        label=\"Forrester Function\",\n        color=cols[0],\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\n    ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\n    ax.scatter(\n        last_queried_points[0],\n        -utility_fn(last_queried_points[0][None, ...]),\n        label=\"Posterior Sample Optimum\",\n        marker=\"*\",\n        color=cols[3],\n        zorder=3,\n    )\n    ax.legend(loc=\"center left\", bbox_to_anchor=(0.950, 0.5))\n    plt.show()\n</pre> def plot_bo_iteration(     dm: UtilityDrivenDecisionMaker, last_queried_points: Float[Array, \"B D\"] ):     posterior = dm.posteriors[OBJECTIVE]     dataset = dm.datasets[OBJECTIVE]     plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)     forrester_y = forrester(plt_x.squeeze(axis=-1))     utility_fn = dm.current_utility_functions[0]     sample_y = -utility_fn(plt_x)      latent_dist = posterior.predict(plt_x, train_data=dataset)     predictive_dist = posterior.likelihood(latent_dist)      predictive_mean = predictive_dist.mean()     predictive_std = predictive_dist.stddev()      fig, ax = plt.subplots()     ax.plot(plt_x.squeeze(), predictive_mean, label=\"Predictive Mean\", color=cols[1])     ax.fill_between(         plt_x.squeeze(),         predictive_mean - 2 * predictive_std,         predictive_mean + 2 * predictive_std,         alpha=0.2,         label=\"Two sigma\",         color=cols[1],     )     ax.plot(         plt_x.squeeze(),         predictive_mean - 2 * predictive_std,         linestyle=\"--\",         linewidth=1,         color=cols[1],     )     ax.plot(         plt_x.squeeze(),         predictive_mean + 2 * predictive_std,         linestyle=\"--\",         linewidth=1,         color=cols[1],     )     ax.plot(plt_x.squeeze(), sample_y, label=\"Posterior Sample\")     ax.plot(         plt_x.squeeze(),         forrester_y,         label=\"Forrester Function\",         color=cols[0],         linestyle=\"--\",         linewidth=2,     )     ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")     ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)     ax.scatter(         last_queried_points[0],         -utility_fn(last_queried_points[0][None, ...]),         label=\"Posterior Sample Optimum\",         marker=\"*\",         color=cols[3],         zorder=3,     )     ax.legend(loc=\"center left\", bbox_to_anchor=(0.950, 0.5))     plt.show() <p>Now let's put it all together and run our decision making loop for 6 iterations, with a batch size of 1:</p> In\u00a0[12]: Copied! <pre>dm = UtilityDrivenDecisionMaker(\n    search_space=search_space,\n    posterior_handlers=posterior_handlers,\n    datasets=initial_datasets,\n    utility_function_builder=utility_function_builder,\n    utility_maximizer=acquisition_maximizer,\n    batch_size=1,\n    key=key,\n    post_ask=[plot_bo_iteration],\n    post_tell=[],\n)\n\nresults = dm.run(\n    6,\n    black_box_function_evaluator=function_evaluator,\n)\n</pre> dm = UtilityDrivenDecisionMaker(     search_space=search_space,     posterior_handlers=posterior_handlers,     datasets=initial_datasets,     utility_function_builder=utility_function_builder,     utility_maximizer=acquisition_maximizer,     batch_size=1,     key=key,     post_ask=[plot_bo_iteration],     post_tell=[], )  results = dm.run(     6,     black_box_function_evaluator=function_evaluator, ) <p>We can see that our <code>DecisionMaker</code> is successfully able to find the minimimizer of the black box function!</p> <p>In this notebook we have provided an introduction to the new <code>decision_making</code> module of GPJax. We have demonstrated how one may use the abstractions provided by this module to implement a Bayesian optimisation loop, and have also highlighted some of the flexibility provided by the module. We hope that this module will provide a useful framework for solving a wide range of sequential decision making problems, and that it will be easy for users to extend the functionality provided by the module to suit their needs!</p> <p>We should note that the <code>decision_making</code> module is still in its early stages, and so whilst we hope to avoid making breaking changes to it, they may occur as the module evolves and more advanced functionality is implemented. If people have any feedback or features they would like to implement/see implemented, feel free to open an issue on the GPJax GitHub page.</p> In\u00a0[13]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Christie' <pre>Author: Thomas Christie\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\njax       : 0.4.25\ngpjax     : 0.8.0\noptax     : 0.1.9\nmatplotlib: 3.8.3\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/decision_making/#introduction-to-decision-making-with-gpjax","title":"Introduction to Decision Making with GPJax\u00b6","text":"<p>In this notebook we provide an introduction to the decision making module of GPJax, which can be used to solve sequential decision making problems. Common examples of such problems include Bayesian optimisation (BO) and experimental design. For an in-depth introduction to Bayesian optimisation itself, be sure to checkout out our Introduction to BO Notebook.</p> <p>We'll be using BO as a case study to demonstrate how one may use the decision making module to solve sequential decision making problems. The goal of the decision making module is to provide a set of tools that can easily be used to solve a wide range of sequential decision making problems. The module is designed to be modular, and so it is easy to swap out different components of the decision making pipeline. Whilst it provides the functionality for quickly implementing a typical deicision making loop out of the box, we also hope that it will provide sufficient flexibility to allow users to define their own, more exotic, decision making pipelines.</p>"},{"location":"examples/decision_making/#the-black-box-objective-function","title":"The Black-Box Objective Function\u00b6","text":"<p>We'll be using the same problem as in the Introduction to BO Notebook, but rather than focussing on the mechanics of BO we'll be looking at how one may use the abstractions provided by the decision making module to implement the BO loop.</p> <p>In BO, and sequential decision making in general, we will often have a black-box function of interest which we can evaluate. In this notebook we'll be using the Forrester function as our objective to minimise:</p> <p>$$f(x) = (6x - 2)^2\\sin(12x-4)$$</p>"},{"location":"examples/decision_making/#the-search-space","title":"The Search Space\u00b6","text":"<p>Having defined a method for evaluating the black-box function, we now need to define the search space over which we wish to optimise. In this case we'll be optimising over the interval $[0, 1]$. We can use the <code>ContinuousSearchSpace</code> class provided in <code>decision_making.search_space</code> to define this search space, as seen below:</p>"},{"location":"examples/decision_making/#the-surrogate-models","title":"The Surrogate Models\u00b6","text":""},{"location":"examples/decision_making/#the-utility-function","title":"The Utility Function\u00b6","text":"<p>Now all that remains for us to define is the utiliy function, and a way of maximising it. Within the utility-driven decision making framework, we define a utility function, often using our GP surrogates, which characterises the utility, or usefulness, of querying the black-box function at any point within the domain of interest. We can then maximise this function to decide which point to query next. In this case we'll be using Thompson sampling as a utility function for determining where to query next. With this function we simply draw a sample from the GP posterior, and choose the minimizer of the sample as the point to query next. In the <code>decision_making</code> framework we create <code>UtilityFunctionBuilder</code> objects. Currently, we only support <code>SinglePointUtilityFunction</code>s, which are utility functions which characterise the utility of querying a single point. Thompson sampling is somewhat of a special case, as we can draw $B$ independent samples from the GP posterior and optimise each of these samples in order to obtain a batch of points to query next. We'll see an example of this later on.</p> <p>Within the <code>ThompsonSampling</code> utility function builder class we implement the <code>build_utility_function</code> method, which takes as input a dictionary containing lablled GP posteriors, as well as the corresponding datasets for these posteriors, and draws an approximate sample from the GP posterior which is a surrogate for the objective function. We instantiate our utility function builder below, specifying the number of Random Fourier features to use when constructing the approximate samples from the GP posterior:</p>"},{"location":"examples/decision_making/#putting-it-all-together-with-the-decision-maker","title":"Putting it All Together with the Decision Maker\u00b6","text":"<p>We now have all the ingredients ready for our Bayesian optimisation loop, so let's put all the logic together using the <code>UtilityDrivenDecisionMaker</code> class provided by the <code>decision_making</code> module. This class has 3 core methods:</p> <ol> <li><code>ask</code> - This method is used to decide which point(s) to query next.</li> <li><code>tell</code> - This method is used to tell the decision maker the results from querying the black-box function at the points returned by <code>ask</code>, and will often update GP posteriors in light of this data.</li> <li><code>run</code> - This is used to run the decision making loop for a specified number of iterations, alternating between <code>ask</code> and <code>tell</code>.</li> </ol> <p>For many decision making problems, the logic provided in the <code>UtilityDrivenDecisionMaker</code> will be sufficient, and is a convenient way of gluing the various bits of machinery involved in sequential decision making together. However, for more exotic decision making loops, it is easy for the user to define their own decision maker class by inheriting from the <code>AbstractDecisionMaker</code> class and defining their own <code>ask</code>, <code>tell</code> and <code>run</code> methods.</p> <p>However, we do also provide the user with some additional flexibility when using the <code>UtilityDrivenDecisionMaker</code> class. Often we may wish to perform certain actions after the <code>ask</code> step and the <code>tell</code> step, such as plotting the acquisition function and the point chosen to be queried for debugging purposes. We can do this by passing a list of functions to be called at each of these points as the <code>post_ask</code> and <code>post_tell</code> attributes of the <code>UtilityDrivenDecisionMaker</code>. Both sets of functions are called with the <code>UtilityDrivenDecisionMaker</code> as an argument, and so have access to all the attributes of the decision maker. The <code>post_ask</code> functions are additionally passed the most recently queried points too. We'll use this functionality to plot the acquisition function and the point chosen to be queried at each step of the decision making loop:</p>"},{"location":"examples/decision_making/#conclusions","title":"Conclusions\u00b6","text":""},{"location":"examples/decision_making/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/deep_kernels/","title":"Deep Kernel Learning","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import (\n    dataclass,\n    field,\n)\nfrom typing import Any\n\nimport flax\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom scipy.signal import sawtooth\nfrom gpjax.base import static_field\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base import param_field\n    import gpjax.kernels as jk\n    from gpjax.kernels import DenseKernelComputation\n    from gpjax.kernels.base import AbstractKernel\n    from gpjax.kernels.computations import AbstractKernelComputation\n\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from dataclasses import (     dataclass,     field, ) from typing import Any  import flax from flax import linen as nn import jax import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from scipy.signal import sawtooth from gpjax.base import static_field  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base import param_field     import gpjax.kernels as jk     from gpjax.kernels import DenseKernelComputation     from gpjax.kernels.base import AbstractKernel     from gpjax.kernels.computations import AbstractKernelComputation  key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>n = 500\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nytest = f(xtest)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\nax.plot(xtest, ytest, label=\"True function\")\nax.legend(loc=\"best\")\n</pre> n = 500 noise = 0.2  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x)) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1) ytest = f(xtest)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5) ax.plot(xtest, ytest, label=\"True function\") ax.legend(loc=\"best\") Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x7f16b482d8a0&gt;</pre> In\u00a0[3]: Copied! <pre>@dataclass\nclass DeepKernelFunction(AbstractKernel):\n    base_kernel: AbstractKernel = None\n    network: nn.Module = static_field(None)\n    dummy_x: jax.Array = static_field(None)\n    key: jax.Array = static_field(jr.key(123))\n    nn_params: Any = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if self.base_kernel is None:\n            raise ValueError(\"base_kernel must be specified\")\n        if self.network is None:\n            raise ValueError(\"network must be specified\")\n        self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))\n\n    def __call__(\n        self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        state = self.network.init(self.key, x)\n        xt = self.network.apply(state, x)\n        yt = self.network.apply(state, y)\n        return self.base_kernel(xt, yt)\n</pre> @dataclass class DeepKernelFunction(AbstractKernel):     base_kernel: AbstractKernel = None     network: nn.Module = static_field(None)     dummy_x: jax.Array = static_field(None)     key: jax.Array = static_field(jr.key(123))     nn_params: Any = field(init=False, repr=False)      def __post_init__(self):         if self.base_kernel is None:             raise ValueError(\"base_kernel must be specified\")         if self.network is None:             raise ValueError(\"network must be specified\")         self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))      def __call__(         self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]     ) -&gt; Float[Array, \"1\"]:         state = self.network.init(self.key, x)         xt = self.network.apply(state, x)         yt = self.network.apply(state, y)         return self.base_kernel(xt, yt) In\u00a0[4]: Copied! <pre>feature_space_dim = 3\n\n\nclass Network(nn.Module):\n\"\"\"A simple MLP.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=32)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=64)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=feature_space_dim)(x)\n        return x\n\n\nforward_linear = Network()\n</pre> feature_space_dim = 3   class Network(nn.Module):     \"\"\"A simple MLP.\"\"\"      @nn.compact     def __call__(self, x):         x = nn.Dense(features=32)(x)         x = nn.relu(x)         x = nn.Dense(features=64)(x)         x = nn.relu(x)         x = nn.Dense(features=feature_space_dim)(x)         return x   forward_linear = Network() In\u00a0[5]: Copied! <pre>base_kernel = gpx.kernels.Matern52(\n    active_dims=list(range(feature_space_dim)),\n    lengthscale=jnp.ones((feature_space_dim,)),\n)\nkernel = DeepKernelFunction(\n    network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x\n)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n</pre> base_kernel = gpx.kernels.Matern52(     active_dims=list(range(feature_space_dim)),     lengthscale=jnp.ones((feature_space_dim,)), ) kernel = DeepKernelFunction(     network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x ) meanf = gpx.mean_functions.Zero() prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n) posterior = prior * likelihood In\u00a0[6]: Copied! <pre>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.01,\n    warmup_steps=75,\n    decay_steps=700,\n    end_value=0.0,\n)\n\noptimiser = ox.chain(\n    ox.clip(1.0),\n    ox.adamw(learning_rate=schedule),\n)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=jax.jit(gpx.objectives.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=optimiser,\n    num_iters=800,\n    key=key,\n)\n</pre> schedule = ox.warmup_cosine_decay_schedule(     init_value=0.0,     peak_value=0.01,     warmup_steps=75,     decay_steps=700,     end_value=0.0, )  optimiser = ox.chain(     ox.clip(1.0),     ox.adamw(learning_rate=schedule), )  opt_posterior, history = gpx.fit(     model=posterior,     objective=jax.jit(gpx.objectives.ConjugateMLL(negative=True)),     train_data=D,     optim=optimiser,     num_iters=800,     key=key, ) In\u00a0[7]: Copied! <pre>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> latent_dist = opt_posterior(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     color=cols[1],     label=\"Two sigma\", ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() Out[7]: <pre>&lt;matplotlib.legend.Legend at 0x7f16acb2ce50&gt;</pre> In\u00a0[8]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ngpjax     : 0.8.0\nflax      : 0.8.1\nmatplotlib: 3.8.3\noptax     : 0.1.9\njax       : 0.4.25\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/deep_kernels/#deep-kernel-learning","title":"Deep Kernel Learning\u00b6","text":"<p>In this notebook we demonstrate how GPJax can be used in conjunction with Flax to build deep kernel Gaussian processes. Modelling data with discontinuities is a challenging task for regular Gaussian process models. However, as shown in , transforming the inputs to our Gaussian process model's kernel through a neural network can offer a solution to this.</p>"},{"location":"examples/deep_kernels/#dataset","title":"Dataset\u00b6","text":"<p>As previously mentioned, deep kernels are particularly useful when the data has discontinuities. To highlight this, we will use a sawtooth function as our data.</p>"},{"location":"examples/deep_kernels/#deep-kernels","title":"Deep kernels\u00b6","text":""},{"location":"examples/deep_kernels/#details","title":"Details\u00b6","text":"<p>Instead of applying a kernel $k(\\cdot, \\cdot')$ directly on some data, we seek to apply a feature map $\\phi(\\cdot)$ that projects the data to learn more meaningful representations beforehand. In deep kernel learning, $\\phi$ is a neural network whose parameters are learned jointly with the GP model's hyperparameters. The corresponding kernel is then computed by $k(\\phi(\\cdot), \\phi(\\cdot'))$. Here $k(\\cdot,\\cdot')$ is referred to as the base kernel.</p>"},{"location":"examples/deep_kernels/#implementation","title":"Implementation\u00b6","text":"<p>Although deep kernels are not currently supported natively in GPJax, defining one is straightforward as we now demonstrate. Inheriting from the base <code>AbstractKernel</code> in GPJax, we create the <code>DeepKernelFunction</code> object that allows the user to supply the neural network and base kernel of their choice. Kernel matrices are then computed using the regular <code>gram</code> and <code>cross_covariance</code> functions.</p>"},{"location":"examples/deep_kernels/#defining-a-network","title":"Defining a network\u00b6","text":"<p>With a deep kernel object created, we proceed to define a neural network. Here we consider a small multi-layer perceptron with two linear hidden layers and ReLU activation functions between the layers. The first hidden layer contains 64 units, while the second layer contains 32 units. Finally, we'll make the output of our network a three units wide. The corresponding kernel that we define will then be of ARD form to allow for different lengthscales in each dimension of the feature space. Users may wish to design more intricate network structures for more complex tasks, which functionality is supported well in Haiku.</p>"},{"location":"examples/deep_kernels/#defining-a-model","title":"Defining a model\u00b6","text":"<p>Having characterised the feature extraction network, we move to define a Gaussian process parameterised by this deep kernel. We consider a third-order Mat\u00e9rn base kernel and assume a Gaussian likelihood.</p>"},{"location":"examples/deep_kernels/#optimisation","title":"Optimisation\u00b6","text":"<p>We train our model via maximum likelihood estimation of the marginal log-likelihood. The parameters of our neural network are learned jointly with the model's hyperparameter set.</p> <p>With the inclusion of a neural network, we take this opportunity to highlight the additional benefits gleaned from using Optax for optimisation. In particular, we showcase the ability to use a learning rate scheduler that decays the optimiser's learning rate throughout the inference. We decrease the learning rate according to a half-cosine curve over 700 iterations, providing us with large step sizes early in the optimisation procedure before approaching more conservative values, ensuring we do not step too far. We also consider a linear warmup, where the learning rate is increased from 0 to 1 over 50 steps to get a reasonable initial learning rate value.</p>"},{"location":"examples/deep_kernels/#prediction","title":"Prediction\u00b6","text":"<p>With a set of learned parameters, the only remaining task is to predict the output of the model. We can do this by simply applying the model to a test data set.</p>"},{"location":"examples/deep_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/graph_kernels/","title":"Graph Kernels","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport random\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport optax as ox\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  import random  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import networkx as nx import optax as ox  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>vertex_per_side = 20\nn_edges_to_remove = 30\np = 0.8\n\nG = nx.barbell_graph(vertex_per_side, 0)\n\nrandom.seed(123)\n[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\n\npos = nx.spring_layout(G, seed=123)  # positions for all nodes\n\nnx.draw(\n    G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n)\n</pre> vertex_per_side = 20 n_edges_to_remove = 30 p = 0.8  G = nx.barbell_graph(vertex_per_side, 0)  random.seed(123) [G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]  pos = nx.spring_layout(G, seed=123)  # positions for all nodes  nx.draw(     G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False ) <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/IPython/core/events.py:82: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/usr/share/miniconda/envs/test/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[3]: Copied! <pre>L = nx.laplacian_matrix(G).toarray()\n</pre> L = nx.laplacian_matrix(G).toarray() In\u00a0[4]: Copied! <pre>x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\n\ntrue_kernel = gpx.kernels.GraphKernel(\n    laplacian=L,\n    lengthscale=2.3,\n    variance=3.2,\n    smoothness=6.1,\n)\nprior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=true_kernel)\n\nfx = prior(x)\ny = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)\n\nD = gpx.Dataset(X=x, y=y)\n</pre> x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)  true_kernel = gpx.kernels.GraphKernel(     laplacian=L,     lengthscale=2.3,     variance=3.2,     smoothness=6.1, ) prior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=true_kernel)  fx = prior(x) y = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)  D = gpx.Dataset(X=x, y=y) <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/jaxtyping/_decorator.py:450: UserWarning: X is not of type float64. Got X.dtype=int64. This may lead to numerical instability. \n  out = fn(*args, **kwargs)\n</pre> <p>We can visualise this signal in the following cell.</p> In\u00a0[5]: Copied! <pre>nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\n\nvmin, vmax = y.min(), y.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\nax = plt.gca()\ncbar = plt.colorbar(sm, ax=ax)\n</pre> nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)  vmin, vmax = y.min(), y.max() sm = plt.cm.ScalarMappable(     cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax) ) sm.set_array([]) ax = plt.gca() cbar = plt.colorbar(sm, ax=ax) <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/IPython/core/events.py:82: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n</pre> <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[6]: Copied! <pre>likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\nkernel = gpx.kernels.GraphKernel(laplacian=L)\nprior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=kernel)\nposterior = prior * likelihood\n</pre> likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n) kernel = gpx.kernels.GraphKernel(laplacian=L) prior = gpx.gps.Prior(mean_function=gpx.mean_functions.Zero(), kernel=kernel) posterior = prior * likelihood <p>For researchers and the curious reader, GPJax provides the ability to print the bibtex citation for objects such as the graph kernel through the <code>cite()</code> function.</p> In\u00a0[7]: Copied! <pre>print(gpx.cite(kernel))\n</pre> print(gpx.cite(kernel)) <pre>@inproceedings{borovitskiy2021matern,\nauthors = {Borovitskiy, Viacheslav and Azangulov, Iskander and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc and Durrande, Nicolas},\ntitle = {Mat\u00e9rn Gaussian Processes on Graphs},\nyear = {2021},\nbooktitle = {International Conference on Artificial Intelligence and Statistics},\n}\n</pre> <p>With a posterior defined, we can now optimise the model's hyperparameters.</p> In\u00a0[8]: Copied! <pre>opt_posterior, training_history = gpx.fit_scipy(\n    model=posterior,\n    objective=gpx.objectives.ConjugateMLL(negative=True),\n    train_data=D,\n)\n</pre> opt_posterior, training_history = gpx.fit_scipy(     model=posterior,     objective=gpx.objectives.ConjugateMLL(negative=True),     train_data=D, ) <pre>Optimization terminated successfully.\n         Current function value: -114.126696\n         Iterations: 118\n         Function evaluations: 187\n         Gradient evaluations: 179\n</pre> In\u00a0[9]: Copied! <pre>initial_dist = likelihood(posterior(x, D))\npredictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\n\ninitial_mean = initial_dist.mean()\nlearned_mean = predictive_dist.mean()\n\nrmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\n\ninitial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\nlearned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\nprint(\n    f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\n    f\" {learned_rmse: .2f}\"\n)\n</pre> initial_dist = likelihood(posterior(x, D)) predictive_dist = opt_posterior.likelihood(opt_posterior(x, D))  initial_mean = initial_dist.mean() learned_mean = predictive_dist.mean()  rmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))  initial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean))) learned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean))) print(     f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"     f\" {learned_rmse: .2f}\" ) <pre>RMSE with initial parameters:  6.80\nRMSE with learned parameters:  0.11\n</pre> <p>We can also plot the source of error in our model's predictions on the graph by the following.</p> In\u00a0[10]: Copied! <pre>error = jnp.abs(learned_mean - y.squeeze())\n\nnx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\n\nvmin, vmax = error.min(), error.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nax = plt.gca()\ncbar = plt.colorbar(sm, ax=ax)\n</pre> error = jnp.abs(learned_mean - y.squeeze())  nx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)  vmin, vmax = error.min(), error.max() sm = plt.cm.ScalarMappable(     cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax) ) ax = plt.gca() cbar = plt.colorbar(sm, ax=ax) <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/IPython/core/events.py:82: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n</pre> <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <p>Reassuringly, our model seems to provide equally good predictions in each cluster.</p> In\u00a0[11]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\nmatplotlib: 3.8.3\njax       : 0.4.25\noptax     : 0.1.9\nnetworkx  : 3.2.1\ngpjax     : 0.8.0\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/graph_kernels/#graph-kernels","title":"Graph Kernels\u00b6","text":"<p>This notebook demonstrates how regression models can be constructed on the vertices of a graph using a Gaussian process with a Mat\u00e9rn kernel presented in . For a general discussion of the kernels supported within GPJax, see the kernels notebook.</p>"},{"location":"examples/graph_kernels/#graph-construction","title":"Graph construction\u00b6","text":"<p>Our graph $\\mathcal{G}=\\lbrace V, E \\rbrace$ comprises a set of vertices $V = \\lbrace v_1, v_2, \\ldots, v_n\\rbrace$ and edges $E=\\lbrace (v_i, v_j)\\in V \\ : \\ i \\neq j\\rbrace$. In particular, we will consider a barbell graph that is an undirected graph containing two clusters of vertices with a single shared edge between the two clusters.</p> <p>Contrary to the typical barbell graph, we'll randomly remove a subset of 30 edges within each of the two clusters. Given the 40 vertices within the graph, this results in 351 edges as shown below.</p>"},{"location":"examples/graph_kernels/#computing-the-graph-laplacian","title":"Computing the graph Laplacian\u00b6","text":"<p>Graph kernels use the Laplacian matrix $L$ to quantify the smoothness of a signal (or function) on a graph $$L=D-A,$$ where $D$ is the diagonal degree matrix containing each vertices' degree and $A$ is the adjacency matrix that has an $(i,j)^{\\text{th}}$ entry of 1 if $v_i, v_j$ are connected and 0 otherwise. Networkx gives us an easy way to compute this.</p>"},{"location":"examples/graph_kernels/#simulating-a-signal-on-the-graph","title":"Simulating a signal on the graph\u00b6","text":"<p>Our task is to construct a Gaussian process $f(\\cdot)$ that maps from the graph's vertex set $V$ onto the real line. To that end, we begin by simulating a signal on the graph's vertices that we will go on to try and predict. We use a single draw from a Gaussian process prior to draw our response values $\\boldsymbol{y}$ where we hardcode parameter values. The corresponding input value set for this model, denoted $\\boldsymbol{x}$, is the index set of the graph's vertices.</p>"},{"location":"examples/graph_kernels/#constructing-a-graph-gaussian-process","title":"Constructing a graph Gaussian process\u00b6","text":"<p>With our dataset created, we proceed to define our posterior Gaussian process and optimise the model's hyperparameters. Whilst our underlying space is the graph's vertex set and is therefore non-Euclidean, our likelihood is still Gaussian and the model is still conjugate. For this reason, we simply perform gradient descent on the GP's marginal log-likelihood term as in the regression notebook. We do this using the BFGS optimiser provided in <code>scipy</code> via 'jaxopt'.</p>"},{"location":"examples/graph_kernels/#making-predictions","title":"Making predictions\u00b6","text":"<p>Having optimised our hyperparameters, we can now make predictions on the graph. Though we haven't defined a training and testing dataset here, we'll simply query the predictive posterior for the full graph to compare the root-mean-squared error (RMSE) of the model for the initialised parameters vs the optimised set.</p>"},{"location":"examples/graph_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/intro_to_gps/","title":"New to Gaussian Processes?","text":"In\u00a0[1]: Copied! <pre>import warnings\n\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow_probability.substrates.jax as tfp\nfrom docs.examples.utils import confidence_ellipse\n\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ntfd = tfp.distributions\n\nud1 = tfd.Normal(0.0, 1.0)\nud2 = tfd.Normal(-1.0, 0.5)\nud3 = tfd.Normal(0.25, 1.5)\n\nxs = jnp.linspace(-5.0, 5.0, 500)\n\nfig, ax = plt.subplots()\nfor d in [ud1, ud2, ud3]:\n    ax.plot(\n        xs,\n        jnp.exp(d.log_prob(xs)),\n        label=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",\n    )\n    ax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2)\nax.legend(loc=\"best\")\n</pre> import warnings  import jax.numpy as jnp import jax.random as jr import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import tensorflow_probability.substrates.jax as tfp from docs.examples.utils import confidence_ellipse  plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] tfd = tfp.distributions  ud1 = tfd.Normal(0.0, 1.0) ud2 = tfd.Normal(-1.0, 0.5) ud3 = tfd.Normal(0.25, 1.5)  xs = jnp.linspace(-5.0, 5.0, 500)  fig, ax = plt.subplots() for d in [ud1, ud2, ud3]:     ax.plot(         xs,         jnp.exp(d.log_prob(xs)),         label=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",     )     ax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2) ax.legend(loc=\"best\") Out[1]: <pre>&lt;matplotlib.legend.Legend at 0x7fa46c4f3d60&gt;</pre> <p>A Gaussian random variable is uniquely defined in distribution by its mean $\\mu$ and variance $\\sigma^2$ and we therefore write $y\\sim\\mathcal{N}(\\mu, \\sigma^2)$ when describing a Gaussian random variable. We can compute these two quantities by $$ \\begin{align}     \\mathbb{E}[y] = \\mu\\,, \\quad \\quad \\mathbb{E}\\left[(y-\\mu)^2\\right] =\\sigma^2\\,. \\end{align} $$ Extending this concept to vector-valued random variables reveals the multivariate Gaussian random variables which brings us closer to the full definition of a GP.</p> <p>Let $\\mathbf{y}$ be a $D$-dimensional random variable, $\\boldsymbol{\\mu}$ be a $D$-dimensional mean vector and $\\boldsymbol{\\Sigma}$ be a $D\\times D$ covariance matrix. If $\\mathbf{y}$ is a Gaussian random variable, then the density of $\\mathbf{y}$ is $$ \\begin{align}     \\mathcal{N}(\\mathbf{y}\\,|\\, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2\\pi}^{D/2} \\lvert\\boldsymbol{\\Sigma}\\rvert^{1/2}} \\exp\\left(-\\frac{1}{2} \\left(\\mathbf{y} - \\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{y}-\\boldsymbol{\\mu}\\right) \\right) \\,. \\end{align} $$ Three example parameterisations of this can be visualised below where $\\rho$ determines the correlation of the multivariate Gaussian.</p> In\u00a0[2]: Copied! <pre>key = jr.key(123)\n\nd1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2))\nd2 = tfd.MultivariateNormalTriL(\n    jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]]))\n)\nd3 = tfd.MultivariateNormalTriL(\n    jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]]))\n)\n\ndists = [d1, d2, d3]\n\nxvals = jnp.linspace(-5.0, 5.0, 500)\nyvals = jnp.linspace(-5.0, 5.0, 500)\n\nxx, yy = jnp.meshgrid(xvals, yvals)\n\npos = jnp.empty(xx.shape + (2,))\npos.at[:, :, 0].set(xx)\npos.at[:, :, 1].set(yy)\n\nfig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\ntitles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\n\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)\n\nfor a, t, d in zip([ax0, ax1, ax2], titles, dists):\n    d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(\n        xx.shape\n    )\n    cntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)\n    for c in cntf.collections:\n        c.set_edgecolor(\"face\")\n    a.set_xlim(-2.75, 2.75)\n    a.set_ylim(-2.75, 2.75)\n    samples = d.sample(seed=key, sample_shape=(5000,))\n    xsample, ysample = samples[:, 0], samples[:, 1]\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n    )\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"\n    )\n    a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)\n    a.set(xlabel=\"x\", ylabel=\"y\", title=t)\n</pre> key = jr.key(123)  d1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2)) d2 = tfd.MultivariateNormalTriL(     jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]])) ) d3 = tfd.MultivariateNormalTriL(     jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]])) )  dists = [d1, d2, d3]  xvals = jnp.linspace(-5.0, 5.0, 500) yvals = jnp.linspace(-5.0, 5.0, 500)  xx, yy = jnp.meshgrid(xvals, yvals)  pos = jnp.empty(xx.shape + (2,)) pos.at[:, :, 0].set(xx) pos.at[:, :, 1].set(yy)  fig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True) titles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]  cmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)  for a, t, d in zip([ax0, ax1, ax2], titles, dists):     d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(         xx.shape     )     cntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)     for c in cntf.collections:         c.set_edgecolor(\"face\")     a.set_xlim(-2.75, 2.75)     a.set_ylim(-2.75, 2.75)     samples = d.sample(seed=key, sample_shape=(5000,))     xsample, ysample = samples[:, 0], samples[:, 1]     confidence_ellipse(         xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8     )     confidence_ellipse(         xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"     )     a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)     a.set(xlabel=\"x\", ylabel=\"y\", title=t) <pre>/tmp/ipykernel_27566/1315003044.py:32: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n  for c in cntf.collections:\n</pre> <pre>/tmp/ipykernel_27566/1315003044.py:32: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n  for c in cntf.collections:\n</pre> <p>Extending the intuition given for the moments of a univariate Gaussian random variables, we can obtain the mean and covariance by $$ \\begin{align}    \\mathbb{E}[\\mathbf{y}] = \\mathbf{\\mu}, \\quad \\operatorname{Cov}(\\mathbf{y}) &amp; = \\mathbf{E}\\left[(\\mathbf{y} - \\mathbf{\\mu)}(\\mathbf{y} - \\mathbf{\\mu)}^{\\top} \\right]\\\\       &amp; =\\mathbb{E}[\\mathbf{y}\\mathbf{y}^{\\top}] - \\mathbb{E}[\\mathbf{y}]\\mathbb{E}[\\mathbf{y}]^{\\top} \\\\       &amp; =\\mathbf{\\Sigma}\\,. \\end{align} $$ The covariance matrix is a symmetric positive definite matrix that generalises the notion of variance to multiple dimensions. The matrix's diagonal entries contain the variance of each element, whilst the off-diagonal entries quantify the degree to which the respective pair of random variables are linearly related; this quantity is called the covariance.</p> <p>Assuming a Gaussian likelihood function in a Bayesian model is attractive as the mean and variance parameters are highly interpretable. This makes prior elicitation straightforward as the parameters' value can be intuitively contextualised within the scope of the problem at hand. Further, in models where the posterior distribution is Gaussian, we again use the distribution's mean and variance to describe our prediction and corresponding uncertainty around a given event occurring.</p> <p>Not only are Gaussian random variables highly interpretable, but linear operations involving them lead to analytical solutions. An example of this that will be useful in the sequel is the marginalisation and conditioning property of sets of Gaussian random variables. We will present these two results now for a pair of Gaussian random variables, but it should be stressed that these results hold for any finite set of Gaussian random variables.</p> <p>For a pair of random variables $\\mathbf{x}$ and $\\mathbf{y}$ defined on the same support, the distribution over them both is known as the joint distribution. The joint distribution $p(\\mathbf{x}, \\mathbf{y})$ quantifies the probability of two events, one from $p(\\mathbf{x})$ and another from $p(\\mathbf{y})$, occurring at the same time. We visualise this idea below.</p> In\u00a0[3]: Copied! <pre>n = 1000\nx = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,))\nkey, subkey = jr.split(key)\ny = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,))\nkey, subkey = jr.split(subkey)\nxfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\nyfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\ndf = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    g = sns.jointplot(\n        data=df,\n        x=\"x\",\n        y=\"y\",\n        hue=\"idx\",\n        marker=\".\",\n        space=0.0,\n        xlim=(-4.0, 4.0),\n        ylim=(-4.0, 4.0),\n        height=4,\n        marginal_ticks=False,\n        legend=False,\n        palette=\"inferno\",\n        marginal_kws={\n            \"fill\": True,\n            \"linewidth\": 1,\n            \"color\": cols[1],\n            \"alpha\": 0.3,\n            \"bw_adjust\": 2,\n            \"cmap\": cmap,\n        },\n        joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},\n    )\n    g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\n    g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\n    g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=1.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=2.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=3.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n</pre> n = 1000 x = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,)) key, subkey = jr.split(key) y = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,)) key, subkey = jr.split(subkey) xfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,)) key, subkey = jr.split(subkey) yfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,)) key, subkey = jr.split(subkey) df = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})  with warnings.catch_warnings():     warnings.simplefilter(\"ignore\")     g = sns.jointplot(         data=df,         x=\"x\",         y=\"y\",         hue=\"idx\",         marker=\".\",         space=0.0,         xlim=(-4.0, 4.0),         ylim=(-4.0, 4.0),         height=4,         marginal_ticks=False,         legend=False,         palette=\"inferno\",         marginal_kws={             \"fill\": True,             \"linewidth\": 1,             \"color\": cols[1],             \"alpha\": 0.3,             \"bw_adjust\": 2,             \"cmap\": cmap,         },         joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},     )     g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))     g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))     g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=1.0,         linestyle=\"--\",         linewidth=0.5,     )     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=2.0,         linestyle=\"--\",         linewidth=0.5,     )     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=3.0,         linestyle=\"--\",         linewidth=0.5,     ) <p>Formally, we can define this by letting $p(\\mathbf{x}, \\mathbf{y})$ be the joint probability distribution defined over $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}}, \\boldsymbol{\\Sigma}_{\\mathbf{xx}})$ and $\\mathbf{y}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}})$. We define the joint distribution as $$ \\begin{align}     p\\left(\\begin{bmatrix}         \\mathbf{x} \\\\ \\mathbf{y}     \\end{bmatrix}\\right) = \\mathcal{N}\\left(\\begin{bmatrix}         \\boldsymbol{\\mu}_{\\mathbf{x}} \\\\ \\boldsymbol{\\mu}_{\\mathbf{y}}     \\end{bmatrix}, \\begin{bmatrix}         \\boldsymbol{\\Sigma}_{\\mathbf{xx}} &amp; \\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\\\         \\boldsymbol{\\Sigma}_{\\mathbf{yx}} &amp; \\boldsymbol{\\Sigma}_{\\mathbf{yy}}     \\end{bmatrix} \\right)\\,, \\end{align} $$ where $\\boldsymbol{\\Sigma}_{\\mathbf{x}\\mathbf{y}}$ is the cross-covariance matrix of $\\mathbf{x}$ and $\\mathbf{y}$.</p> <p>When presented with a joint distribution, two tasks that we may wish to perform are marginalisation and conditioning. For a joint distribution $p(\\mathbf{x}, \\mathbf{y})$ where we are interested only in $p(\\mathbf{x})$, we must integrate over all possible values of $\\mathbf{y}$ to obtain $p(\\mathbf{x})$. This process is marginalisation. Conditioning allows us to evaluate the probability of one random variable, given that the other random variable is fixed. For a joint Gaussian distribution, marginalisation and conditioning have analytical expressions where the resulting distribution is also a Gaussian random variable.</p> <p>For a joint Gaussian random variable, the marginalisation of $\\mathbf{x}$ or $\\mathbf{y}$ is given by $$ \\begin{alignat}{3}     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{y} &amp;&amp; = p(\\mathbf{x})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}},\\boldsymbol{\\Sigma}_{\\mathbf{xx}})\\\\     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{x} &amp;&amp; = p(\\mathbf{y})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}},     \\boldsymbol{\\Sigma}_{\\mathbf{yy}})\\,. \\end{alignat} $$ The conditional distributions are given by $$ \\begin{align}     p(\\mathbf{y}\\,|\\, \\mathbf{x}) &amp; = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{y}} + \\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{\\mathbf{x}}), \\boldsymbol{\\Sigma}_{\\mathbf{yy}}-\\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\right)\\,. \\end{align} $$</p> <p>Within this section, we have introduced the idea of multivariate Gaussian random variables and presented some key results concerning their properties. In the following section, we will lift our presentation of Gaussian random variables to GPs.</p> In\u00a0[4]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ntensorflow_probability: 0.22.1\npandas                : 1.5.3\njax                   : 0.4.25\nseaborn               : 0.12.2\nmatplotlib            : 3.8.3\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/intro_to_gps/#new-to-gaussian-processes","title":"New to Gaussian Processes?\u00b6","text":"<p>Fantastic that you're here! This notebook is designed to be a gentle introduction to the mathematics of Gaussian processes (GPs). No prior knowledge of Bayesian inference or GPs is assumed, and this notebook is self-contained. At a high level, we begin by introducing Bayes' theorem and its implications within probabilistic modelling. We then proceed to introduce the Gaussian random variable along with its multivariate form. We conclude by showing how this notion can be extended to GPs.</p>"},{"location":"examples/intro_to_gps/#bayes-theorem","title":"Bayes' Theorem\u00b6","text":"<p>A probabilistic modelling task is comprised of an observed dataset $\\mathbf{y}$ for which we construct a model. The parameters $\\theta$ of our model are unknown, and our goal is to conduct inference to determine their range of likely values. To achieve this, we apply Bayes' theorem $$ \\begin{align}     \\label{eq:BayesTheorem}     p(\\theta\\,|\\, \\mathbf{y}) = \\frac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{p(\\mathbf{y})} = \\frac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{\\int_{\\theta}p(\\mathbf{y}, \\theta)\\mathrm{d}\\theta}\\,, \\end{align} $$ where $p(\\mathbf{y}\\,|\\,\\theta)$ denotes the likelihood, or model, and quantifies how likely the observed dataset $\\mathbf{y}$ is, given the parameter estimate $\\theta$. The prior distribution $p(\\theta)$ reflects our initial beliefs about the value of $\\theta$ before observing data, whilst the posterior $p(\\theta\\,|\\, \\mathbf{y})$ gives an updated estimate of the parameters' value, after observing $\\mathbf{y}$. The marginal likelihood, or Bayesian model evidence, $p(\\mathbf{y})$ is the probability of the observed data under all possible hypotheses that our prior model can generate. Within Bayesian model selection, this property makes the marginal log-likelihood an indispensable tool. Selecting models under this criterion places a higher emphasis on models that can generalise better to new data points.</p> <p>When the posterior distribution belongs to the same family of probability distributions as the prior, we describe the prior and the likelihood as conjugate to each other. Such a scenario is convenient in Bayesian inference as it allows us to derive closed-form expressions for the posterior distribution. When the likelihood function is a member of the exponential family, then there exists a conjugate prior. However, the conjugate prior may not have a form that precisely reflects the practitioner's belief surrounding the parameter. For this reason, conjugate models seldom appear; one exception to this is GP regression that we present fully in our Regression notebook.</p> <p>For models that do not contain a conjugate prior, the marginal log-likelihood must be calculated to normalise the posterior distribution and ensure it integrates to 1. For models with a single, 1-dimensional parameter, it may be possible to compute this integral analytically or through a quadrature scheme, such as Gauss-Hermite. However, in machine learning, the dimensionality of $\\theta$ is often large and the corresponding integral required to compute $p(\\mathbf{y})$ quickly becomes intractable as the dimension grows. Techniques such as Markov Chain Monte Carlo and variational inference allow us to approximate integrals such as the one seen in $p(\\mathbf{y})$.</p> <p>Once a posterior distribution has been obtained, we can make predictions at new points $\\mathbf{y}^{\\star}$ through the posterior predictive distribution. This is achieved by integrating out the parameter set $\\theta$ from our posterior distribution through $$ \\begin{align}     p(\\mathbf{y}^{\\star}\\mid \\mathbf{y}) = \\int p(\\mathbf{y}^{\\star} \\,|\\, \\theta, \\mathbf{y} ) p(\\theta\\,|\\, \\mathbf{y})\\mathrm{d}\\theta\\,. \\end{align} $$ As with the marginal log-likelihood, evaluating this quantity requires computing an integral which may not be tractable, particularly when $\\theta$ is high-dimensional.</p> <p>It is difficult to communicate statistics directly through a posterior distribution, so we often compute and report moments of the posterior distribution. Most commonly, we report the first moment and the centred second moment $$ \\begin{alignat}{2}     \\mu  = \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]  &amp; = \\int \\theta p(\\theta\\mid\\mathbf{y})\\mathrm{d}\\theta\\\\     \\sigma^2  = \\mathbb{V}[\\theta\\,|\\,\\mathbf{y}] &amp; = \\int \\left(\\theta -     \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]\\right)^2p(\\theta\\,|\\,\\mathbf{y})\\mathrm{d}\\theta&amp;\\,. \\end{alignat} $$ Through this pair of statistics, we can communicate our beliefs about the most likely value of $\\theta$ i.e., $\\mu$, and the uncertainty $\\sigma$ around the expected value. However, as with the marginal log-likelihood and predictive posterior distribution, computing these statistics again requires a potentially intractable integral.</p>"},{"location":"examples/intro_to_gps/#gaussian-random-variables","title":"Gaussian random variables\u00b6","text":"<p>We begin our review with the simplest case; a univariate Gaussian random variable. For a random variable $y$, let $\\mu\\in\\mathbb{R}$ be a mean scalar and $\\sigma^2\\in\\mathbb{R}_{&gt;0}$ a variance scalar. If $y$ is a Gaussian random variable, then the density of $y$ is $$ \\begin{align}     \\mathcal{N}(y\\,|\\, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^{2}}\\right)\\,. \\end{align} $$ We can plot three different parameterisations of this density.</p>"},{"location":"examples/intro_to_gps/#gaussian-processes","title":"Gaussian processes\u00b6","text":"<p>When transitioning from Gaussian random variables to GP there is a shift in thought required to parse the forthcoming material. Firstly, to be consistent with the general literature, we hereon use $\\mathbf{X}$ to denote an observed vector of data points, not a random variable as has been true up until now. To distinguish between matrices and vectors, we use bold upper case characters e.g., $\\mathbf{X}$ for matrices, and bold lower case characters for vectors e.g., $\\mathbf{x}$.</p> <p>We are interested in modelling supervised learning problems, where we have $n$ observations $\\mathbf{y}=\\{y_1, y_2,\\ldots ,y_n\\}\\subset\\mathcal{Y}$ at corresponding inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\subset\\mathcal{X}$. We aim to capture the relationship between $\\mathbf{X}$ and $\\mathbf{y}$ using a model $f$ with which we may make predictions at an unseen set of test points $\\mathbf{X}^{\\star}\\subset\\mathcal{X}$. We formalise this by $$ \\begin{align}     y = f(\\mathbf{X}) + \\varepsilon\\,, \\end{align} $$ where $\\varepsilon$ is an observational noise term. We collectively refer to $(\\mathbf{X}, \\mathbf{y})$ as the training data and $\\mathbf{X}^{\\star}$ as the set of test points. This process is visualised below</p> <p></p> <p>As we shall go on to see, GPs offer an appealing workflow for scenarios such as this, all under a Bayesian framework.</p> <p>We write a GP $f(\\cdot) \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot))$ with mean function $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ and $\\boldsymbol{\\theta}$-parameterised kernel $k: \\mathcal{X} \\times \\mathcal{X}\\rightarrow \\mathbb{R}$. When evaluating the GP on a finite set of points $\\mathbf{X}\\subset\\mathcal{X}$, $k$ gives rise to the Gram matrix $\\mathbf{K}_{ff}$ such that the $(i, j)^{\\text{th}}$ entry of the matrix is given by $[\\mathbf{K}_{ff}]_{i, j} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. As is conventional within the literature, we centre our training data and assume $\\mu(\\mathbf{X}):= 0$ for all $\\mathbf{X}\\in\\mathcal{X}$. We further drop dependency on $\\boldsymbol{\\theta}$ and $\\mathbf{X}$ for notational convenience in the remainder of this article.</p> <p>We define a joint GP prior over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}) = \\mathcal{N}\\left(\\mathbf{0}, \\begin{bmatrix}         \\mathbf{K}_{xf} &amp; \\mathbf{K}_{xx}     \\end{bmatrix}\\right)\\,, \\end{align} $$ where $\\mathbf{f}^{\\star} = f(\\mathbf{X}^{\\star})$. Conditional on the GP's latent function $f$, we assume a factorising likelihood generates our observations $$ \\begin{align}     p(\\mathbf{y}\\,|\\,\\mathbf{f}) = \\prod_{i=1}^n p(y_i\\,|\\, f_i)\\,. \\end{align} $$ Strictly speaking, the likelihood function is $p(\\mathbf{y}\\,|\\,\\phi(\\mathbf{f}))$ where $\\phi$ is the likelihood function's associated link function. Example link functions include the probit or logistic functions for a Bernoulli likelihood and the identity function for a Gaussian likelihood. We eschew this notation for now as this section primarily considers Gaussian likelihood functions where the role of $\\phi$ is superfluous. However, this intuition will be helpful for models with a non-Gaussian likelihood, such as those encountered in classification.</p> <p>Applying Bayes' theorem \\eqref{eq:BayesTheorem} yields the joint posterior distribution over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}\\,|\\,\\mathbf{y}) = \\frac{p(\\mathbf{y}\\,|\\,\\mathbf{f})p(\\mathbf{f},\\mathbf{f}^{\\star})}{p(\\mathbf{y})}\\,. \\end{align} $$</p> <p>The choice of kernel function that we use to parameterise our GP is an important modelling decision as the choice of kernel dictates properties such as differentiability, variance and characteristic lengthscale of the functions that are admissible under the GP prior. A kernel is a positive-definite function with parameters $\\boldsymbol{\\theta}$ that maps pairs of inputs $\\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}$ onto the real line. We dedicate the entirety of the Introduction to Kernels notebook to exploring the different GPs each kernel can yield.</p>"},{"location":"examples/intro_to_gps/#gaussian-process-regression","title":"Gaussian process regression\u00b6","text":"<p>When the likelihood function is a Gaussian distribution $p(y_i\\,|\\, f_i) = \\mathcal{N}(y_i\\,|\\, f_i, \\sigma_n^2)$, marginalising $\\mathbf{f}$ from the joint posterior to obtain the posterior predictive distribution is exact $$ \\begin{align}     p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) = \\mathcal{N}(\\mathbf{f}^{\\star}\\,|\\,\\boldsymbol{\\mu}_{\\,|\\,\\mathbf{y}}, \\Sigma_{\\,|\\,\\mathbf{y}})\\,, \\end{align} $$ where $$ \\begin{align}     \\mathbf{\\mu}_{\\mid \\mathbf{y}} &amp; = \\mathbf{K}_{\\star f}\\left( \\mathbf{K}_{ff}+\\sigma^2_n\\mathbf{I}_n\\right)^{-1}\\mathbf{y} \\\\     \\Sigma_{\\,|\\,\\mathbf{y}} &amp; = \\mathbf{K}_{\\star\\star} - \\mathbf{K}_{xf}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n\\right)^{-1}\\mathbf{K}_{fx} \\,. \\end{align} $$ Further, the log of the  marginal likelihood of the GP can be analytically expressed as $$ \\begin{align}         &amp; = 0.5\\left(-\\underbrace{\\mathbf{y}^{\\top}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n \\right)^{-1}\\mathbf{y}}_{\\text{Data fit}} -\\underbrace{\\log\\lvert \\mathbf{K}_{ff} + \\sigma^2_n\\rvert}_{\\text{Complexity}} -\\underbrace{n\\log 2\\pi}_{\\text{Constant}} \\right)\\,. \\end{align} $$</p> <p>Model selection can be performed for a GP through gradient-based optimisation of $\\log p(\\mathbf{y})$ with respect to the kernel's parameters $\\boldsymbol{\\theta}$ and the observational noise $\\sigma^2_n$. Collectively, we call these terms the model hyperparameters $\\boldsymbol{\\xi} = \\{\\boldsymbol{\\theta},\\sigma_n^2\\}$ from which the maximum likelihood estimate is given by $$ \\begin{align*}     \\boldsymbol{\\xi}^{\\star} = \\operatorname{argmax}_{\\boldsymbol{\\xi} \\in \\Xi} \\log p(\\mathbf{y})\\,. \\end{align*} $$</p> <p>Observing the individual terms in the marginal log-likelihood can help understand exactly why optimising the marginal log-likelihood gives reasonable solutions. The data fit term is the only component of the marginal log-likelihood that includes the observed response $\\mathbf{y}$ and will therefore encourage solutions that model the data well. Conversely, the complexity term contains a determinant operator and therefore measures the volume of the function space covered by the GP. Whilst a more complex function has a better chance of modelling the observed data well, this is only true to a point and functions that are overly complex will overfit the data. Optimising with respect to the marginal log-likelihood balances these two objectives when identifying the optimal solution, as visualised below.</p> <p></p>"},{"location":"examples/intro_to_gps/#conclusions","title":"Conclusions\u00b6","text":"<p>Within this notebook we have built up the concept of a GP, starting from Bayes' theorem and the definition of a Gaussian random variable. Using the ideas presented in this notebook, the user should be in a position to dive into our Regression notebook and start getting their hands on some code. For those looking to learn more about the underling theory of GPs, an excellent starting point is the Gaussian Processes for Machine Learning textbook. Alternatively, the thesis of Alexander Terenin provides a rigorous exposition of GPs that served as the inspiration for this notebook.</p>"},{"location":"examples/intro_to_gps/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/intro_to_kernels/","title":"Introduction to Kernels","text":"<p>In this guide we provide an introduction to kernels, and the role they play in Gaussian process models.</p> In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nfrom docs.examples.utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\nfrom gpjax.typing import Array\nfrom sklearn.preprocessing import StandardScaler\n\nkey = jr.key(42)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook, Float import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox import pandas as pd from docs.examples.utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx from gpjax.typing import Array from sklearn.preprocessing import StandardScaler  key = jr.key(42) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <p>Using Gaussian Processes (GPs) to model functions can offer several advantages over alternative methods, such as deep neural networks. One key advantage is their rich quantification of uncertainty; not only do they provide point estimates for the values taken by a function throughout its domain, but they provide a full predictive posterior distribution over the range of values the function may take. This rich quantification of uncertainty is useful in many applications, such as Bayesian optimisation, which relies on being able to make uncertainty-aware decisions.</p> <p>However, another advantage of GPs is the ability for one to place priors on the functions being modelled. For instance, one may know that the underlying function being modelled observes certain characteristics, such as being periodic or having a certain level of smoothness. The kernel, or covariance function, is the primary means through which one is able to encode such prior knowledge about the function being modelled. This enables one to equip the GP with inductive biases which enable it to learn from data more efficiently, whilst generalising to unseen data more effectively.</p> <p>In this notebook we'll develop some intuition for what kinds of priors are encoded through the use of different kernels, and how this can be useful when modelling different types of functions.</p> <p>One of the most widely used families of kernels is the Mat\u00e9rn family (Mat\u00e9rn, 1960). These kernels take on the following form:</p> <p>$$k_{\\nu}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2\\nu} \\frac{|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right)^{\\nu} K_{\\nu} \\left(\\sqrt{2\\nu} \\frac{|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right)$$</p> <p>where $K_{\\nu}$ is a modified Bessel function, $\\nu$, $\\kappa$ and $\\sigma^2$ are hyperparameters specifying the mean-square differentiability, lengthscale and variability respectively, and $|\\cdot|$ is used to denote the Euclidean norm. Note that for those of you less interested in the mathematical underpinnings of kernels, it isn't necessary to understand the exact functional form of the Mat\u00e9rn kernels to gain an understanding of how they behave. The key takeaway is that they are parameterised by several hyperparameters, and that these hyperparameters dictate the behaviour of functions sampled from the corresponding GP. The plots below will provide some more intuition for how these hyperparameters affect the behaviour of functions sampled from the corresponding GP.</p> <p>Some commonly used Mat\u00e9rn kernels use half-integer values of $\\nu$, such as $\\nu = 1/2$ or $\\nu = 3/2$. The fraction is sometimes omitted when naming the kernel, so that $\\nu = 1/2$ is referred to as the Mat\u00e9rn12 kernel, and $\\nu = 3/2$ is referred to as the Mat\u00e9rn32 kernel. When $\\nu$ takes in a half-integer value, $\\nu = k + 1/2$, the kernel can be expressed as the product of a polynomial of order $k$ and an exponential:</p> <p>$$k_{k + 1/2}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\exp\\left(-\\frac{\\sqrt{2\\nu}|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right) \\frac{\\Gamma(k+1)}{\\Gamma(2k+1)} \\times \\sum_{i= 0}^k \\frac{(k+i)!}{i!(k-i)!} \\left(\\frac{(\\sqrt{8\\nu}|\\mathbf{x} - \\mathbf{x'}|)}{\\kappa}\\right)^{k-i}$$</p> <p>In the limit of $\\nu \\to \\infty$ this yields the squared-exponential, or radial basis function (RBF), kernel, which is infinitely mean-square differentiable:</p> <p>$$k_{\\infty}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\exp\\left(-\\frac{|\\mathbf{x} - \\mathbf{x'}|^2}{2\\kappa^2}\\right)$$</p> <p>But what kind of functions does this kernel encode prior knowledge about? Let's take a look at some samples from GP priors defined used Mat\u00e9rn kernels with different values of $\\nu$:</p> In\u00a0[2]: Copied! <pre>kernels = [\n    gpx.kernels.Matern12(),\n    gpx.kernels.Matern32(),\n    gpx.kernels.Matern52(),\n    gpx.kernels.RBF(),\n]\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n\nmeanf = gpx.mean_functions.Zero()\n\nfor k, ax in zip(kernels, axes.ravel()):\n    prior = gpx.gps.Prior(mean_function=meanf, kernel=k)\n    rv = prior(x)\n    y = rv.sample(seed=key, sample_shape=(10,))\n    ax.plot(x, y.T, alpha=0.7)\n    ax.set_title(k.name)\n</pre> kernels = [     gpx.kernels.Matern12(),     gpx.kernels.Matern32(),     gpx.kernels.Matern52(),     gpx.kernels.RBF(), ] fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)  meanf = gpx.mean_functions.Zero()  for k, ax in zip(kernels, axes.ravel()):     prior = gpx.gps.Prior(mean_function=meanf, kernel=k)     rv = prior(x)     y = rv.sample(seed=key, sample_shape=(10,))     ax.plot(x, y.T, alpha=0.7)     ax.set_title(k.name) <p>The plots above clearly show that the choice of $\\nu$ has a large impact on the smoothness of the functions being modelled by the GP, with functions drawn from GPs defined with the Mat\u00e9rn kernel becoming increasingly smooth as $\\nu \\to \\infty$. More formally, this notion of smoothness is captured through the mean-square differentiability of the function being modelled. Functions sampled from GPs using a Mat\u00e9rn kernel are $k$-times mean-square differentiable, if and only if $\\nu &gt; k$. For instance, functions sampled from a GP using a Mat\u00e9rn12 kernel are zero times mean-square differentiable, and functions sampled from a GP using the RBF kernel are infinitely mean-square differentiable.</p> <p>As an important aside, a general property of the Mat\u00e9rn family of kernels is that they are examples of stationary kernels. This means that they only depend on the displacement of the two points being compared, $\\mathbf{x} - \\mathbf{x}'$, and not on their absolute values. This is a useful property to have, as it means that the kernel is invariant to translations in the input space. They also go beyond this, as they only depend on the Euclidean distance between the two points being compared, $|\\mathbf{x} - \\mathbf{x}'|$. Kernels which satisfy this property are known as isotropic kernels. This makes the function invariant to all rigid motions in the input space, such as rotations.</p> <p>Most kernels have several hyperparameters, which we denote $\\mathbf{\\theta}$, which encode different assumptions about the underlying function being modelled. For the Mat\u00e9rn family described above, $\\mathbf{\\theta} = \\{\\nu, \\kappa, \\sigma\\}$. A fully Bayesian approach to dealing with hyperparameters would be to place a prior over them, and marginalise over the posterior derived from the data in order to perform predictions. However, this is often computationally very expensive, and so a common approach is to instead optimise the hyperparameters by maximising the log marginal likelihood of the data. Given training data $\\mathbf{D} = (\\mathbf{X}, \\mathbf{y})$, assumed to contain some additive Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, the log marginal likelihood of the dataset is defined as:</p> <p>$$ \\begin{aligned} \\log(p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\theta})) &amp;= \\log\\left(\\int p(\\mathbf{y} | \\mathbf{f}, \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{f} | \\mathbf{X}, \\boldsymbol{\\theta}) d\\mathbf{f}\\right) \\nonumber \\\\ &amp;= - \\frac{1}{2} \\mathbf{y} ^ \\top \\left(K(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I} \\right)^{-1} \\mathbf{y} - \\frac{1}{2} \\log |K(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}| - \\frac{n}{2} \\log 2 \\pi \\end{aligned}$$</p> <p>This expression can then be maximised with respect to the hyperparameters using a gradient-based approach such as Adam or L-BFGS. Note that we may choose to fix some hyperparameters, and in GPJax the parameter $\\nu$ is set by the user, and not inferred though optimisation. For more details on using the log marginal likelihood to optimise kernel hyperparameters, see our GP introduction notebook.</p> <p>We'll demonstrate the advantages of being able to infer kernel parameters from the training data by fitting a GP to the widely used Forrester function:</p> <p>$$f(x) = (6x - 2)^2 \\sin(12x - 4)$$</p> In\u00a0[3]: Copied! <pre># Forrester function\ndef forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:\n    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\n\n\nn = 13\n\ntraining_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1)\ntraining_y = forrester(training_x)\nD = gpx.Dataset(X=training_x, y=training_y)\n\ntest_x = jnp.linspace(0, 1, 100).reshape(-1, 1)\ntest_y = forrester(test_x)\n</pre> # Forrester function def forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:     return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)   n = 13  training_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1) training_y = forrester(training_x) D = gpx.Dataset(X=training_x, y=training_y)  test_x = jnp.linspace(0, 1, 100).reshape(-1, 1) test_y = forrester(test_x) <p>First we define our model, using the Mat\u00e9rn52 kernel, and construct our posterior without optimising the kernel hyperparameters:</p> In\u00a0[4]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\n    lengthscale=jnp.array(0.1)\n)  # Initialise our kernel lengthscale to 0.1\n\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n\nlikelihood = gpx.likelihoods.Gaussian(\n    num_datapoints=D.n, obs_stddev=jnp.array(1e-3)\n)  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value\nlikelihood = likelihood.replace_trainable(obs_stddev=False)\n\nno_opt_posterior = prior * likelihood\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Matern52(     lengthscale=jnp.array(0.1) )  # Initialise our kernel lengthscale to 0.1  prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)  likelihood = gpx.likelihoods.Gaussian(     num_datapoints=D.n, obs_stddev=jnp.array(1e-3) )  # Our function is noise-free, so we set the observation noise's standard deviation to a very small value likelihood = likelihood.replace_trainable(obs_stddev=False)  no_opt_posterior = prior * likelihood <p>We can then optimise the hyperparameters by minimising the negative log marginal likelihood of the data:</p> In\u00a0[5]: Copied! <pre>negative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(no_opt_posterior, train_data=D)\n</pre> negative_mll = gpx.objectives.ConjugateMLL(negative=True) negative_mll(no_opt_posterior, train_data=D) Out[5]: <pre>Array(132.18608226, dtype=float64)</pre> In\u00a0[6]: Copied! <pre>opt_posterior, history = gpx.fit_scipy(\n    model=no_opt_posterior,\n    objective=negative_mll,\n    train_data=D,\n)\n</pre> opt_posterior, history = gpx.fit_scipy(     model=no_opt_posterior,     objective=negative_mll,     train_data=D, ) <pre>Optimization terminated successfully.\n         Current function value: 21.179313\n         Iterations: 29\n         Function evaluations: 31\n         Gradient evaluations: 31\n</pre> <p>Having optimised the hyperparameters, we can now make predictions using the posterior with the optimised hyperparameters, and compare them to the predictions made using the posterior with the default hyperparameters:</p> In\u00a0[7]: Copied! <pre>def plot_ribbon(ax, x, dist, color):\n    mean = dist.mean()\n    std = dist.stddev()\n    ax.plot(x, mean, label=\"Predictive mean\", color=color)\n    ax.fill_between(\n        x.squeeze(),\n        mean - 2 * std,\n        mean + 2 * std,\n        alpha=0.2,\n        label=\"Two sigma\",\n        color=color,\n    )\n    ax.plot(x, mean - 2 * std, linestyle=\"--\", linewidth=1, color=color)\n    ax.plot(x, mean + 2 * std, linestyle=\"--\", linewidth=1, color=color)\n</pre> def plot_ribbon(ax, x, dist, color):     mean = dist.mean()     std = dist.stddev()     ax.plot(x, mean, label=\"Predictive mean\", color=color)     ax.fill_between(         x.squeeze(),         mean - 2 * std,         mean + 2 * std,         alpha=0.2,         label=\"Two sigma\",         color=color,     )     ax.plot(x, mean - 2 * std, linestyle=\"--\", linewidth=1, color=color)     ax.plot(x, mean + 2 * std, linestyle=\"--\", linewidth=1, color=color) In\u00a0[8]: Copied! <pre>opt_latent_dist = opt_posterior.predict(test_x, train_data=D)\nopt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)\n\nopt_predictive_mean = opt_predictive_dist.mean()\nopt_predictive_std = opt_predictive_dist.stddev()\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6))\nax1.plot(\n    test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5)\nplot_ribbon(ax1, test_x, opt_predictive_dist, color=cols[1])\nax1.set_title(\"Posterior with Hyperparameter Optimisation\")\nax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n\nno_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D)\nno_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)\n\nax2.plot(\n    test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5)\nplot_ribbon(ax2, test_x, no_opt_predictive_dist, color=cols[1])\nax2.set_title(\"Posterior without Hyperparameter Optimisation\")\nax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> opt_latent_dist = opt_posterior.predict(test_x, train_data=D) opt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)  opt_predictive_mean = opt_predictive_dist.mean() opt_predictive_std = opt_predictive_dist.stddev()  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6)) ax1.plot(     test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5) plot_ribbon(ax1, test_x, opt_predictive_dist, color=cols[1]) ax1.set_title(\"Posterior with Hyperparameter Optimisation\") ax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))  no_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D) no_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)  ax2.plot(     test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=\"k\", zorder=5) plot_ribbon(ax2, test_x, no_opt_predictive_dist, color=cols[1]) ax2.set_title(\"Posterior without Hyperparameter Optimisation\") ax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[8]: <pre>&lt;matplotlib.legend.Legend at 0x7f85c822f820&gt;</pre> <p>We can see that optimising the hyperparameters by minimising the negative log marginal likelihood of the data results in a more faithful fit of the GP to the data. In particular, we can observe that the GP using optimised hyperparameters is more accurately able to reflect uncertainty in its predictions, as opposed to the GP using the default parameters, which is overconfident in its predictions.</p> <p>The lengthscale, $\\kappa$, and variance, $\\sigma^2$, are shown below, both before and after optimisation:</p> In\u00a0[9]: Copied! <pre>no_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale\nno_opt_variance = no_opt_posterior.prior.kernel.variance\nopt_lengthscale = opt_posterior.prior.kernel.lengthscale\nopt_variance = opt_posterior.prior.kernel.variance\n\nprint(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\")\nprint(\n    f\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\"\n)\n</pre> no_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale no_opt_variance = no_opt_posterior.prior.kernel.variance opt_lengthscale = opt_posterior.prior.kernel.lengthscale opt_variance = opt_posterior.prior.kernel.variance  print(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\") print(     f\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\" ) <pre>Optimised Lengthscale: 0.4139936874165975 and Variance: 315.3903089792944\nNon-Optimised Lengthscale: 0.1 and Variance: 1.0\n</pre> <p>Whilst the Mat\u00e9rn kernels are often used as a first choice of kernel, and they often perform well due to their smoothing properties often being well-aligned with the properties of the underlying function being modelled, sometimes more prior knowledge is known about the function being modelled. For instance, it may be known that the function being modelled is periodic. In this case, a suitable kernel choice would be the periodic kernel:</p> <p>$$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\left(\\frac{\\sin (\\pi (\\mathbf{x}_i - \\mathbf{x}_i')/p)}{\\ell}\\right)^2 \\right)$$</p> <p>with $D$ being the dimensionality of the inputs.</p> <p>Below we show $10$ samples drawn from a GP prior using the periodic kernel:</p> In\u00a0[10]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Periodic()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\n\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Periodic Kernel\")\nplt.show()\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Periodic() prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,))  fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from the Periodic Kernel\") plt.show() <p>In other scenarios, it may be known that the underlying function is linear, in which case the linear kernel would be a suitable choice:</p> <p>$$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\mathbf{x}^\\top \\mathbf{x}'$$</p> <p>Unlike the kernels shown above, the linear kernel is not stationary, and so it is not invariant to translations in the input space.</p> <p>Below we show $10$ samples drawn from a GP prior using the linear kernel:</p> In\u00a0[11]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Linear()\nprior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\n\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Linear Kernel\")\nplt.show()\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Linear() prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,))  fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from the Linear Kernel\") plt.show() <p>It is also mathematically valid to compose kernels through operations such as addition and multiplication in order to produce more expressive kernels. For the mathematically interested amongst you, this is valid as the resulting kernel functions still satisfy the necessary conditions introduced at the start of this notebook. Adding or multiplying kernel functions is equivalent to performing elementwise addition or multiplication of the corresponding covariance matrices, and fortunately symmetric, positive semi-definite kernels are closed under these operations. This means that kernels produced by adding or multiplying other kernels will also be symmetric and positive semi-definite, and so will also be valid kernels. GPJax provides the functionality required to easily compose kernels via addition and multiplication, which we'll demonstrate below.</p> <p>First, we'll take a look at some samples drawn from a GP prior using a kernel which is composed of the sum of a linear kernel and a periodic kernel:</p> In\u00a0[12]: Copied! <pre>kernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\")\nplt.show()\n</pre> kernel_one = gpx.kernels.Linear() kernel_two = gpx.kernels.Periodic() sum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two]) mean = gpx.mean_functions.Zero() prior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,)) fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\") plt.show() <p>We can see that the samples drawn behave as one would naturally expect through adding the two kernels together. In particular, the samples are still periodic, as with the periodic kernel, but their mean also linearly increases/decreases as they move away from the origin, as seen with the linear kernel.</p> <p>Below we take a look at some samples drawn from a GP prior using a kernel which is composed of the same two kernels, but this time multiplied together:</p> In\u00a0[13]: Copied! <pre>kernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\")\nplt.show()\n</pre> kernel_one = gpx.kernels.Linear() kernel_two = gpx.kernels.Periodic() sum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two]) mean = gpx.mean_functions.Zero() prior = gpx.gps.Prior(mean_function=mean, kernel=sum_kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,)) fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\") plt.show() <p>Once again, the samples drawn behave as one would naturally expect through multiplying the two kernels together. In particular, the samples are still periodic but their mean linearly increases/decreases as they move away from the origin, and the amplitude of the oscillations also linearly increases with increasing distance from the origin.</p> <p>We'll put together some of the ideas we've discussed in this notebook by fitting a GP to the Mauna Loa CO2 dataset. This dataset measures atmospheric CO2 concentration at the Mauna Loa Observatory in Hawaii, and is widely used in the GP literature. It contains monthly CO2 readings starting in March 1958. Interestingly, there was an eruption at the Mauna Loa volcano in November 2022, so readings from December 2022 have changed to a site roughly 21 miles North of the Mauna Loa Observatory. We'll use the data from March 1958 to November 2022, and see how our GP extrapolates to 8 years before and after the data in the training set.</p> <p>First we'll load the data and plot it:</p> In\u00a0[14]: Copied! <pre>co2_data = pd.read_csv(\n    \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\"\n)\nco2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12]\ntrain_x = co2_data[\"decimal date\"].values[:, None]\ntrain_y = co2_data[\"average\"].values[:, None]\n\nfig, ax = plt.subplots()\nax.plot(train_x, train_y)\nax.set_title(\"CO2 Concentration in the Atmosphere\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO2 Concentration (ppm)\")\nplt.show()\n</pre> co2_data = pd.read_csv(     \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\" ) co2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12] train_x = co2_data[\"decimal date\"].values[:, None] train_y = co2_data[\"average\"].values[:, None]  fig, ax = plt.subplots() ax.plot(train_x, train_y) ax.set_title(\"CO2 Concentration in the Atmosphere\") ax.set_xlabel(\"Year\") ax.set_ylabel(\"CO2 Concentration (ppm)\") plt.show() <p>Looking at the data, we can see that there is clearly a periodic trend, with a period of roughly 1 year. We can also see that the data is increasing over time, which is also expected. This looks roughly linear, although it may have a non-linear component. This information will be useful when we come to choose our kernel.</p> <p>First, we'll construct our GPJax dataset, and will standardise the outputs, to match our assumption that the data has zero mean.</p> In\u00a0[15]: Copied! <pre>test_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1)\ny_scaler = StandardScaler().fit(train_y)\nstandardised_train_y = y_scaler.transform(train_y)\n\nD = gpx.Dataset(X=train_x, y=standardised_train_y)\n</pre> test_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1) y_scaler = StandardScaler().fit(train_y) standardised_train_y = y_scaler.transform(train_y)  D = gpx.Dataset(X=train_x, y=standardised_train_y) <p>Having constructed our dataset, we'll now define our kernel. We'll use a kernel which is composed of the sum of a linear kernel and a periodic kernel, as we saw in the previous section that this kernel is able to capture both the periodic and linear trends in the data. We'll also add an RBF kernel to the sum, which will allow us to capture any non-linear trends in the data:</p> <p>$$\\text{Kernel = Linear + Periodic + RBF}$$</p> In\u00a0[16]: Copied! <pre>mean = gpx.mean_functions.Zero()\nrbf_kernel = gpx.kernels.RBF(lengthscale=100.0)\nperiodic_kernel = gpx.kernels.Periodic()\nlinear_kernel = gpx.kernels.Linear(variance=0.001)\nsum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel])\nfinal_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])\n\nprior = gpx.gps.Prior(mean_function=mean, kernel=final_kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n\nposterior = prior * likelihood\n</pre> mean = gpx.mean_functions.Zero() rbf_kernel = gpx.kernels.RBF(lengthscale=100.0) periodic_kernel = gpx.kernels.Periodic() linear_kernel = gpx.kernels.Linear(variance=0.001) sum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel]) final_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])  prior = gpx.gps.Prior(mean_function=mean, kernel=final_kernel) likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)  posterior = prior * likelihood <p>With our model constructed, let's now fit it to the data, by minimising the negative log marginal likelihood of the data:</p> In\u00a0[17]: Copied! <pre>negative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=ox.adamw(learning_rate=1e-2),\n    num_iters=500,\n    key=key,\n)\n</pre> negative_mll = gpx.objectives.ConjugateMLL(negative=True) negative_mll(posterior, train_data=D)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=ox.adamw(learning_rate=1e-2),     num_iters=500,     key=key, ) <p>Now we can obtain the model's prediction over a period of time which includes the training data, as well as 8 years before and after the training data:</p> In\u00a0[18]: Copied! <pre>latent_dist = opt_posterior.predict(test_x, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean().reshape(-1, 1)\npredictive_std = predictive_dist.stddev().reshape(-1, 1)\n</pre> latent_dist = opt_posterior.predict(test_x, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean().reshape(-1, 1) predictive_std = predictive_dist.stddev().reshape(-1, 1) <p>Let's plot the model's predictions over this period of time:</p> In\u00a0[19]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\n    train_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5\n)\nax.fill_between(\n    test_x.squeeze(),\n    predictive_mean.squeeze() - 2 * predictive_std.squeeze(),\n    predictive_mean.squeeze() + 2 * predictive_std.squeeze(),\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    test_x,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(10, 5)) ax.plot(     train_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5 ) ax.fill_between(     test_x.squeeze(),     predictive_mean.squeeze() - 2 * predictive_std.squeeze(),     predictive_mean.squeeze() + 2 * predictive_std.squeeze(),     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     test_x,     predictive_mean - 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     test_x,     predictive_mean + 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.set_xlabel(\"Year\") ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[19]: <pre>&lt;matplotlib.legend.Legend at 0x7f85c05e22c0&gt;</pre> <p>We can see that the model seems to have captured the periodic trend in the data, as well as the (roughly) linear trend. This enables our model to make reasonable seeming predictions over the 8 years before and after the training data. Let's zoom in on the period from 2010 onwards:</p> In\u00a0[20]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\n    train_x[train_x &gt;= 2010],\n    standardised_train_y[train_x &gt;= 2010],\n    \"x\",\n    label=\"Observations\",\n    color=cols[0],\n    alpha=0.5,\n)\nax.fill_between(\n    test_x[test_x &gt;= 2010].squeeze(),\n    predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010],\n    label=\"Predictive mean\",\n    color=cols[1],\n)\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(10, 5)) ax.plot(     train_x[train_x &gt;= 2010],     standardised_train_y[train_x &gt;= 2010],     \"x\",     label=\"Observations\",     color=cols[0],     alpha=0.5, ) ax.fill_between(     test_x[test_x &gt;= 2010].squeeze(),     predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     test_x[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     test_x[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     test_x[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010],     label=\"Predictive mean\",     color=cols[1], ) ax.set_xlabel(\"Year\") ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[20]: <pre>&lt;matplotlib.legend.Legend at 0x7f85c06a72b0&gt;</pre> <p>This certainly looks like a reasonable fit to the data, with sensible extrapolation beyond the training data, which finishes in November 2022. Moreover, the learned parameters of the kernel are interpretable. Let's take a look at the learned period of the periodic kernel:</p> In\u00a0[21]: Copied! <pre>print(\n    \"Periodic Kernel Period:\"\n    f\" {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\"\n)\n</pre> print(     \"Periodic Kernel Period:\"     f\" {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\" ) <pre>Periodic Kernel Period: 0.9995240117826185\n</pre> <p>This tells us that the periodic trend learned has a period of $\\approx 1$. This makes intuitive sense, as the unit of the input data is years, and we can see that the periodic trend tends to repeat itself roughly every year!</p> In\u00a0[22]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Christie' <pre>Author: Thomas Christie\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\npandas    : 1.5.3\ngpjax     : 0.8.0\njax       : 0.4.25\nmatplotlib: 3.8.3\noptax     : 0.1.9\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/intro_to_kernels/#introduction-to-kernels","title":"Introduction to Kernels\u00b6","text":""},{"location":"examples/intro_to_kernels/#what-is-a-kernel","title":"What is a Kernel?\u00b6","text":"<p>Intuitively, for a function $f$, the kernel defines the notion of similarity between the value of the function at two points, $f(\\mathbf{x})$ and $f(\\mathbf{x}')$, and will be denoted as $k(\\mathbf{x}, \\mathbf{x}')$:</p> <p>$$\\begin{aligned} k(\\mathbf{x}, \\mathbf{x}') &amp;= \\text{Cov}[f(\\mathbf{x}), f(\\mathbf{x}')] \\\\ &amp;= \\mathbb{E}[(f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{x})])(f(\\mathbf{x}') - \\mathbb{E}[f(\\mathbf{x}')])] \\end{aligned}$$</p> <p>One would expect that, given a previously unobserved test point $\\mathbf{x}^*$, the training points which are closest to this unobserved point will be most similar to it. As such, the kernel is used to define this notion of similarity within the GP framework. It is up to the user to select a kernel function which is appropriate for the function being modelled. In this notebook we are going to give some examples of commonly used kernels, and try to develop an understanding of when one may wish to use one kernel over another. However, before we do this, it is worth discussing the necessary conditions for a function to be a valid kernel/covariance function. This requires a little bit of maths, so for those of you who just wish to obtain an intuitive understanding, feel free to skip to the section introducing the Mat\u00e9rn family of kernels.</p>"},{"location":"examples/intro_to_kernels/#what-are-the-necessary-conditions-for-a-function-to-be-a-valid-kernel","title":"What are the necessary conditions for a function to be a valid kernel?\u00b6","text":"<p>Whilst intuitively the kernel function is used to define the notion of similarity within the GP framework, it is important to note that there are two necessary conditions that a kernel function must satisfy in order to be a valid covariance function. For clarity, we will refer to any function mapping two inputs to a scalar output as a kernel function, and we will refer to a valid kernel function satisfying the two necessary conditions as a covariance function. However, it is worth noting that the GP community often uses the terms kernel function and covariance function interchangeably.</p> <p>The first necessary condition is that the covariance function must be symmetric, i.e. $k(\\mathbf{x}, \\mathbf{x}') = k(\\mathbf{x}', \\mathbf{x})$. This is because the covariance between two random variables $X$ and $X'$ is symmetric; if one looks at the definition of covariance given above, it is clear that it is invariant to swapping the order of the inputs $\\mathbf{x}$ and $\\mathbf{x}'$.</p> <p>The second necessary condition is that the covariance function must be positive semi-definite (PSD). In order to understand this condition, it is useful to first introduce the concept of a Gram matrix. We'll use the same notation as the GP introduction notebook, and denote $n$ input points as $\\mathbf{X} = \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}$. Given these input points and a kernel function $k$ the Gram matrix stores the pairwise kernel evaluations between all input points. Mathematically, this leads to the Gram matrix being defined as:</p> <p>$$K(\\mathbf{X}, \\mathbf{X}) = \\begin{bmatrix} k(\\mathbf{x}_1, \\mathbf{x}_1) &amp; \\cdots &amp; k(\\mathbf{x}_1, \\mathbf{x}_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ k(\\mathbf{x}_n, \\mathbf{x}_1) &amp; \\cdots &amp; k(\\mathbf{x}_n, \\mathbf{x}_n) \\end{bmatrix}$$</p> <p>such that $K(\\mathbf{X}, \\mathbf{X})_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$.</p> <p>In order for $k$ to be a valid covariance function, the corresponding Gram matrix must be positive semi-definite. In this case the Gram matrix is referred to as a covariance matrix. A real $n \\times n$ matrix $K$ is positive semi-definite if and only if for all vectors $\\mathbf{z} \\in \\mathbb{R}^n$:</p> <p>$$\\mathbf{z}^\\top K \\mathbf{z} \\geq 0$$</p> <p>Alternatively, a real $n \\times n$ matrix $K$ is positive semi-definite if and only if all of its eigenvalues are non-negative.</p> <p>Therefore, the two necessary conditions for a function to be a valid covariance function are that it must be symmetric and positive semi-definite. In this section we have referred to any function from two inputs to a scalar output as a kernel function, with its corresponding matrix of pairwise evaluations referred to as the Gram matrix, and a function satisfying the two necessary conditions as a covariance function, with its corresponding matrix of pairwise evaluations referred to as the covariance matrix. This enabled us to easily define the necessary conditions for a function to be a valid covariance function. However, as noted previously, the GP community often uses these terms interchangeably, and so we will for the remainder of this notebook.</p>"},{"location":"examples/intro_to_kernels/#introducing-a-common-family-of-kernels-the-matern-family","title":"Introducing a Common Family of Kernels - The Mat\u00e9rn Family\u00b6","text":""},{"location":"examples/intro_to_kernels/#inferring-kernel-hyperparameters","title":"Inferring Kernel Hyperparameters\u00b6","text":""},{"location":"examples/intro_to_kernels/#expressing-other-priors-with-different-kernels","title":"Expressing Other Priors with Different Kernels\u00b6","text":""},{"location":"examples/intro_to_kernels/#composing-kernels","title":"Composing Kernels\u00b6","text":""},{"location":"examples/intro_to_kernels/#putting-it-all-together-on-a-real-world-dataset","title":"Putting it All Together on a Real-World Dataset\u00b6","text":""},{"location":"examples/intro_to_kernels/#mauna-loa-co2-dataset","title":"Mauna Loa CO2 Dataset\u00b6","text":""},{"location":"examples/intro_to_kernels/#defining-kernels-on-non-euclidean-spaces","title":"Defining Kernels on Non-Euclidean Spaces\u00b6","text":"<p>In this notebook, we have focused solely on kernels whose domain resides in Euclidean space. However, what if one wished to work with data whose domain is non-Euclidean? For instance, one may wish to work with graph-structured data, or data which lies on a manifold, or even strings. Fortunately, kernels exist for a wide variety of domains. Whilst this is beyond the scope of this notebook, feel free to checkout out our notebook on graph kernels for an introduction on how to define the Mat\u00e9rn kernel on graph-structured data, and there are a wide variety of resources online for learning about defining kernels in other domains. In terms of open-source libraries, the Geometric Kernels library could be a good place to start if you're interested in looking at how these kernels may be implemented, with the additional benefit that it is compatible with GPJax.</p>"},{"location":"examples/intro_to_kernels/#further-reading","title":"Further Reading\u00b6","text":"<p>Congratulations on making it this far! We hope that this guide has given you a good introduction to kernels and how they can be used in GPJax. If you're interested in learning more about kernels, we recommend the following resources, which have also been used as inspiration for this guide:</p> <ul> <li>Gaussian Processes for Machine Learning - Chapter 4 provides a comprehensive overview of kernels, diving deep into some of the technical details and also providing some kernels defined on non-Euclidean spaces such as strings.</li> <li>David Duvenaud's Kernel Cookbook is a great resource for learning about kernels, and also provides some information about some of the pitfalls people commonly encounter when using the Mat\u00e9rn family of kernels. His PhD thesis, Automatic Model Construction with Gaussian Processes, also provides some in-depth recipes for how one may incorporate their prior knowledge when constructing kernels.</li> <li>Finally, please check out our more advanced kernel guide, which details some more kernels available in GPJax as well as how one may combine kernels together to form more complex kernels.</li> </ul>"},{"location":"examples/intro_to_kernels/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/likelihoods_guide/","title":"Likelihood guide","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport gpjax as gpx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\n\ntfd = tfp.distributions\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey = jr.key(123)\n\n\nn = 50\nx = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0)\nxtest = jnp.linspace(-3, 3, 100)[:, None]\nf = lambda x: jnp.sin(x)\ny = f(x) + 0.1 * jr.normal(key, shape=x.shape)\nD = gpx.Dataset(x, y)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\")\nax.plot(x, f(x), label=\"Latent function\")\nax.legend()\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  import gpjax as gpx import jax import jax.numpy as jnp import jax.random as jr import matplotlib.pyplot as plt import tensorflow_probability.substrates.jax as tfp  tfd = tfp.distributions plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] key = jr.key(123)   n = 50 x = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0) xtest = jnp.linspace(-3, 3, 100)[:, None] f = lambda x: jnp.sin(x) y = f(x) + 0.1 * jr.normal(key, shape=x.shape) D = gpx.Dataset(x, y)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\") ax.plot(x, f(x), label=\"Latent function\") ax.legend() Out[1]: <pre>&lt;matplotlib.legend.Legend at 0x7fbcbee2f040&gt;</pre> <p>In this example, our observations have support $[-3, 3]$ and are generated from a sinusoidal function with Gaussian noise. As such, our response values $\\mathbf{y}$ range between $-1$ and $1$, subject to Gaussian noise. Due to this, a Gaussian likelihood is appropriate for this dataset as it allows for negative values.</p> <p>As we see in \\eqref{eq:likelihood_fn}, the likelihood function factorises over the $n$ observations. As such, we must provide this information to GPJax when instantiating a likelihood object. We do this by specifying the <code>num_datapoints</code> argument.</p> In\u00a0[2]: Copied! <pre>gpx.likelihoods.Gaussian(num_datapoints=D.n)\n</pre> gpx.likelihoods.Gaussian(num_datapoints=D.n) Out[2]: <pre>Gaussian(num_datapoints=50, integrator=AnalyticalGaussianIntegrator(), obs_stddev=Array(1., dtype=float64, weak_type=True))</pre> In\u00a0[3]: Copied! <pre>gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.5)\n</pre> gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.5) Out[3]: <pre>Gaussian(num_datapoints=50, integrator=AnalyticalGaussianIntegrator(), obs_stddev=0.5)</pre> <p>To control other properties of the observation noise such as trainability and value constraints, see our PyTree guide.</p> In\u00a0[4]: Copied! <pre>kernel = gpx.kernels.Matern32()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(kernel=kernel, mean_function=meanf)\n\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.1)\n\nposterior = prior * likelihood\n\nlatent_dist = posterior.predict(xtest, D)\n\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\n\nfor ax in axes.ravel():\n    subkey, _ = jr.split(subkey)\n    ax.plot(\n        latent_dist.sample(sample_shape=(1,), seed=subkey).T,\n        lw=1,\n        color=cols[0],\n        label=\"Latent samples\",\n    )\n    ax.plot(\n        likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n        \"o\",\n        markersize=5,\n        alpha=0.3,\n        color=cols[1],\n        label=\"Predictive samples\",\n    )\n</pre> kernel = gpx.kernels.Matern32() meanf = gpx.mean_functions.Zero() prior = gpx.gps.Prior(kernel=kernel, mean_function=meanf)  likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_stddev=0.1)  posterior = prior * likelihood  latent_dist = posterior.predict(xtest, D)  fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2)) key, subkey = jr.split(key)  for ax in axes.ravel():     subkey, _ = jr.split(subkey)     ax.plot(         latent_dist.sample(sample_shape=(1,), seed=subkey).T,         lw=1,         color=cols[0],         label=\"Latent samples\",     )     ax.plot(         likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,         \"o\",         markersize=5,         alpha=0.3,         color=cols[1],         label=\"Predictive samples\",     ) <p>Similarly, for a Bernoulli likelihood function, the samples of $y$ would be binary.</p> In\u00a0[5]: Copied! <pre>likelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)\n\n\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\n\nfor ax in axes.ravel():\n    subkey, _ = jr.split(subkey)\n    ax.plot(\n        latent_dist.sample(sample_shape=(1,), seed=subkey).T,\n        lw=1,\n        color=cols[0],\n        label=\"Latent samples\",\n    )\n    ax.plot(\n        likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n        \"o\",\n        markersize=3,\n        alpha=0.5,\n        color=cols[1],\n        label=\"Predictive samples\",\n    )\n</pre> likelihood = gpx.likelihoods.Bernoulli(num_datapoints=D.n)   fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2)) key, subkey = jr.split(key)  for ax in axes.ravel():     subkey, _ = jr.split(subkey)     ax.plot(         latent_dist.sample(sample_shape=(1,), seed=subkey).T,         lw=1,         color=cols[0],         label=\"Latent samples\",     )     ax.plot(         likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,         \"o\",         markersize=3,         alpha=0.5,         color=cols[1],         label=\"Predictive samples\",     ) In\u00a0[6]: Copied! <pre>z = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1)\nq = gpx.variational_families.VariationalGaussian(posterior=posterior, inducing_inputs=z)\n\n\ndef q_moments(x):\n    qx = q(x)\n    return qx.mean(), qx.variance()\n\n\nmean, variance = jax.vmap(q_moments)(x[:, None])\n</pre> z = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1) q = gpx.variational_families.VariationalGaussian(posterior=posterior, inducing_inputs=z)   def q_moments(x):     qx = q(x)     return qx.mean(), qx.variance()   mean, variance = jax.vmap(q_moments)(x[:, None]) <p>Now that we have the variational mean and variational (co)variance, we can compute the expected log-likelihood using the <code>expected_log_likelihood</code> method of the likelihood object.</p> In\u00a0[7]: Copied! <pre>jnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance))\n</pre> jnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance)) Out[7]: <pre>Array(-47.73779086, dtype=float64)</pre> <p>However, had we wanted to do this using quadrature, then we would have done the following:</p> In\u00a0[8]: Copied! <pre>lquad = gpx.likelihoods.Gaussian(\n    num_datapoints=D.n,\n    obs_stddev=jnp.array([0.1]),\n    integrator=gpx.integrators.GHQuadratureIntegrator(num_points=20),\n)\n</pre> lquad = gpx.likelihoods.Gaussian(     num_datapoints=D.n,     obs_stddev=jnp.array([0.1]),     integrator=gpx.integrators.GHQuadratureIntegrator(num_points=20), ) <p>However, this is not recommended for the Gaussian likelihood given that the expectation can be computed analytically.</p> In\u00a0[9]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\nmatplotlib            : 3.8.3\ngpjax                 : 0.8.0\ntensorflow_probability: 0.22.1\njax                   : 0.4.25\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/likelihoods_guide/#likelihood-guide","title":"Likelihood guide\u00b6","text":"<p>In this notebook, we will walk users through the process of creating a new likelihood in GPJax.</p>"},{"location":"examples/likelihoods_guide/#background","title":"Background\u00b6","text":"<p>In this section we'll provide a short introduction to likelihoods and why they are important. For users who are already familiar with likelihoods, feel free to skip to the next section, and for users who would like more information than is provided here, please see our introduction to Gaussian processes notebook.</p>"},{"location":"examples/likelihoods_guide/#what-is-a-likelihood","title":"What is a likelihood?\u00b6","text":"<p>We adopt the notation of our introduction to Gaussian processes notebook where we have a Gaussian process (GP) $f(\\cdot)\\sim\\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))$ and a dataset $\\mathbf{y} = \\{y_n\\}_{n=1}^N$ observed at corresponding inputs $\\mathbf{x} = \\{x_n\\}_{n=1}^N$. The evaluation of $f$ at $\\mathbf{x}$ is denoted by $\\mathbf{f} = \\{f(x_n)\\}_{n=1}^N$. The likelihood function of the GP is then given by $$ \\begin{align}     \\label{eq:likelihood_fn}     p(\\mathbf{y}\\mid \\mathbf{f}) = \\prod_{n=1}^N p(y_n\\mid f(x_n))\\,. \\end{align} $$ Conceptually, this conditional distribution describes the probability of the observed data, conditional on the latent function values.</p>"},{"location":"examples/likelihoods_guide/#why-is-the-likelihood-important","title":"Why is the likelihood important?\u00b6","text":"<p>Choosing the correct likelihood function when building a GP, or any Bayesian model for that matter, is crucial. The likelihood function encodes our assumptions about the data and the noise that we expect to observe. For example, if we are modelling air pollution, then we would not expect to observe negative values of pollution. In this case, we would choose a likelihood function that is only defined for positive values. Similarly, if our data is the proportion of people who voted for a particular political party, then we would expect to observe values between 0 and 1. In this case, we would choose a likelihood function that is only defined for values between 0 and 1.</p>"},{"location":"examples/likelihoods_guide/#likelihoods-in-gpjax","title":"Likelihoods in GPJax\u00b6","text":"<p>In GPJax, all likelihoods are a subclass of the <code>AbstractLikelihood</code> class. This base abstract class contains the three core methods that all likelihoods must implement: <code>predict</code>, <code>link_function</code>, and <code>expected_log_likelihood</code>. We will discuss each of these methods in the forthcoming sections, but first, we will show how to instantiate a likelihood object. To do this, we'll need a dataset.</p>"},{"location":"examples/likelihoods_guide/#likelihood-parameters","title":"Likelihood parameters\u00b6","text":"<p>Some likelihoods, such as the Gaussian likelihood, contain parameters that we seek to infer. In the case of the Gaussian likelihood, we have a single parameter $\\sigma^2$ that determines the observation noise. In GPJax, we can specify the value of $\\sigma$ when instantiating the likelihood object. If we do not specify a value, then the likelihood will be initialised with a default value. In the case of the Gaussian likelihood, the default value is $1.0$. If we instead wanted to initialise the likelihood standard deviation with a value of $0.5$, then we would do this as follows:</p>"},{"location":"examples/likelihoods_guide/#prediction","title":"Prediction\u00b6","text":"<p>The <code>predict</code> method of a likelihood object transforms the latent distribution of the Gaussian process. In the case of a Gaussian likelihood, this simply applies the observational noise value to the diagonal values of the covariance matrix. For other likelihoods, this may be a more complex transformation. For example, the Bernoulli likelihood transforms the latent distribution of the Gaussian process into a distribution over binary values.</p> <p>We visualise this below for the Gaussian likelihood function. In blue we can see samples of $\\mathbf{f}^{\\star}$, whilst in red we see samples of $\\mathbf{y}^{\\star}$.</p>"},{"location":"examples/likelihoods_guide/#link-functions","title":"Link functions\u00b6","text":"<p>In the above figure, we can see the latent samples being constrained to be either 0 or 1 when a Bernoulli likelihood is specified. This is achieved by the <code>inverse link_function</code> $\\eta(\\cdot)$ of the likelihood. The link function is a deterministic function that maps the latent distribution of the Gaussian process to the support of the likelihood function. For example, the link function of the Bernoulli likelihood that is used in GPJax is the inverse probit function $$ \\eta(x) = 0.5\\left(1 + \\Phi\\left(\\frac{x}{\\sqrt{2}}\\right) * (1-2)\\right)\\,, $$ where $\\Phi$ is the cumulative distribution function of the standard normal distribution.</p> <p>A table of commonly used link functions and their corresponding likelihood can be found here.</p>"},{"location":"examples/likelihoods_guide/#expected-log-likelihood","title":"Expected log likelihood\u00b6","text":"<p>The final method that is associated with a likelihood function in GPJax is the expected log-likelihood. This term is evaluated in the stochastic variational Gaussian process in the ELBO term. For a variational approximation $q(f)= \\mathcal{N}(f\\mid m, S)$, the ELBO can be written as $$ \\begin{align}     \\label{eq:elbo}     \\mathcal{L}(q) = \\mathbb{E}_{f\\sim q(f)}\\left[ p(\\mathbf{y}\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,. \\end{align} $$ As both $q(f)$ and $p(f)$ are Gaussian distributions, the Kullback-Leibler term can be analytically computed. However, the expectation term is not always so easy to compute. Fortunately, the bound in \\eqref{eq:elbo} can be decomposed as a sum of the datapoints $$ \\begin{align}     \\label{eq:elbo_decomp}     \\mathcal{L}(q) = \\sum_{n=1}^N \\mathbb{E}_{f\\sim q(f)}\\left[ p(y_n\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,. \\end{align} $$ This simplifies computation of the expectation as it is now a series of $N$ 1-dimensional integrals. As such, GPJax by default uses quadrature to compute these integrals. However, for some likelihoods, such as the Gaussian likelihood, the expectation can be computed analytically. In these cases, we can supply an object that inherits from <code>AbstractIntegrator</code> to the likelihood upon instantiation. To see this, let us consider a Gaussian likelihood where we'll first define a variational approximation to the posterior.</p>"},{"location":"examples/likelihoods_guide/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/oceanmodelling/","title":"Gaussian Processes for Vector Fields and Ocean Current Modelling","text":"In\u00a0[1]: Copied! <pre>from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import dataclass, field\n\nfrom jax import hessian\nfrom jax import config\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow_probability as tfp\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncolors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> from jax import config  config.update(\"jax_enable_x64\", True) from dataclasses import dataclass, field  from jax import hessian from jax import config import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) from matplotlib import rcParams import matplotlib.pyplot as plt import pandas as pd import tensorflow_probability as tfp  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre># function to place data from csv into correct array shape\ndef prepare_data(df):\n    pos = jnp.array([df[\"lon\"], df[\"lat\"]])\n    vel = jnp.array([df[\"ubar\"], df[\"vbar\"]])\n    # extract shape stored as 'metadata' in the test data\n    try:\n        shape = (int(df[\"shape\"][1]), int(df[\"shape\"][0]))  # shape = (34,16)\n        return pos, vel, shape\n    except KeyError:\n        return pos, vel\n\n\n# loading in data\n\ngulf_data_train = pd.read_csv(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_train.csv\"\n)\ngulf_data_test = pd.read_csv(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_test.csv\"\n)\n\n\npos_test, vel_test, shape = prepare_data(gulf_data_test)\npos_train, vel_train = prepare_data(gulf_data_train)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\nax.quiver(\n    pos_test[0],\n    pos_test[1],\n    vel_test[0],\n    vel_test[1],\n    color=colors[0],\n    label=\"Ocean Current\",\n    angles=\"xy\",\n    scale=10,\n)\nax.quiver(\n    pos_train[0],\n    pos_train[1],\n    vel_train[0],\n    vel_train[1],\n    color=colors[1],\n    alpha=0.7,\n    label=\"Drifter\",\n    angles=\"xy\",\n    scale=10,\n)\n\nax.set(\n    xlabel=\"Longitude\",\n    ylabel=\"Latitude\",\n)\nax.legend(\n    framealpha=0.0,\n    ncols=2,\n    fontsize=\"medium\",\n    bbox_to_anchor=(0.5, -0.3),\n    loc=\"lower center\",\n)\nplt.show()\n</pre> # function to place data from csv into correct array shape def prepare_data(df):     pos = jnp.array([df[\"lon\"], df[\"lat\"]])     vel = jnp.array([df[\"ubar\"], df[\"vbar\"]])     # extract shape stored as 'metadata' in the test data     try:         shape = (int(df[\"shape\"][1]), int(df[\"shape\"][0]))  # shape = (34,16)         return pos, vel, shape     except KeyError:         return pos, vel   # loading in data  gulf_data_train = pd.read_csv(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_train.csv\" ) gulf_data_test = pd.read_csv(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/static/main/data/gulfdata_test.csv\" )   pos_test, vel_test, shape = prepare_data(gulf_data_test) pos_train, vel_train = prepare_data(gulf_data_train)  fig, ax = plt.subplots(1, 1, figsize=(6, 3)) ax.quiver(     pos_test[0],     pos_test[1],     vel_test[0],     vel_test[1],     color=colors[0],     label=\"Ocean Current\",     angles=\"xy\",     scale=10, ) ax.quiver(     pos_train[0],     pos_train[1],     vel_train[0],     vel_train[1],     color=colors[1],     alpha=0.7,     label=\"Drifter\",     angles=\"xy\",     scale=10, )  ax.set(     xlabel=\"Longitude\",     ylabel=\"Latitude\", ) ax.legend(     framealpha=0.0,     ncols=2,     fontsize=\"medium\",     bbox_to_anchor=(0.5, -0.3),     loc=\"lower center\", ) plt.show() In\u00a0[3]: Copied! <pre># Change vectors x -&gt; X = (x,z), and vectors y -&gt; Y = (y,z) via the artificial z label\ndef label_position(data):\n    # introduce alternating z label\n    n_points = len(data[0])\n    label = jnp.tile(jnp.array([0.0, 1.0]), n_points)\n    return jnp.vstack((jnp.repeat(data, repeats=2, axis=1), label)).T\n\n\n# change vectors y -&gt; Y by reshaping the velocity measurements\ndef stack_velocity(data):\n    return data.T.flatten().reshape(-1, 1)\n\n\ndef dataset_3d(pos, vel):\n    return gpx.Dataset(label_position(pos), stack_velocity(vel))\n\n\n# label and place the training data into a Dataset object to be used by GPJax\ndataset_train = dataset_3d(pos_train, vel_train)\n\n# we also require the testing data to be relabelled for later use, such that we can query the 2Nx2N GP at the test points\ndataset_ground_truth = dataset_3d(pos_test, vel_test)\n</pre> # Change vectors x -&gt; X = (x,z), and vectors y -&gt; Y = (y,z) via the artificial z label def label_position(data):     # introduce alternating z label     n_points = len(data[0])     label = jnp.tile(jnp.array([0.0, 1.0]), n_points)     return jnp.vstack((jnp.repeat(data, repeats=2, axis=1), label)).T   # change vectors y -&gt; Y by reshaping the velocity measurements def stack_velocity(data):     return data.T.flatten().reshape(-1, 1)   def dataset_3d(pos, vel):     return gpx.Dataset(label_position(pos), stack_velocity(vel))   # label and place the training data into a Dataset object to be used by GPJax dataset_train = dataset_3d(pos_train, vel_train)  # we also require the testing data to be relabelled for later use, such that we can query the 2Nx2N GP at the test points dataset_ground_truth = dataset_3d(pos_test, vel_test) In\u00a0[4]: Copied! <pre>@dataclass\nclass VelocityKernel(gpx.kernels.AbstractKernel):\n    kernel0: gpx.kernels.AbstractKernel = field(\n        default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n    )\n    kernel1: gpx.kernels.AbstractKernel = field(\n        default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n    )\n\n    def __call__(\n        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        # standard RBF-SE kernel is x and x' are on the same output, otherwise returns 0\n\n        z = jnp.array(X[2], dtype=int)\n        zp = jnp.array(Xp[2], dtype=int)\n\n        # achieve the correct value via 'switches' that are either 1 or 0\n        k0_switch = ((z + 1) % 2) * ((zp + 1) % 2)\n        k1_switch = z * zp\n\n        return k0_switch * self.kernel0(X, Xp) + k1_switch * self.kernel1(X, Xp)\n</pre>   @dataclass class VelocityKernel(gpx.kernels.AbstractKernel):     kernel0: gpx.kernels.AbstractKernel = field(         default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])     )     kernel1: gpx.kernels.AbstractKernel = field(         default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])     )      def __call__(         self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]     ) -&gt; Float[Array, \"1\"]:         # standard RBF-SE kernel is x and x' are on the same output, otherwise returns 0          z = jnp.array(X[2], dtype=int)         zp = jnp.array(Xp[2], dtype=int)          # achieve the correct value via 'switches' that are either 1 or 0         k0_switch = ((z + 1) % 2) * ((zp + 1) % 2)         k1_switch = z * zp          return k0_switch * self.kernel0(X, Xp) + k1_switch * self.kernel1(X, Xp) In\u00a0[5]: Copied! <pre>def initialise_gp(kernel, mean, dataset):\n    prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)\n    likelihood = gpx.likelihoods.Gaussian(\n        num_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)\n    )\n    posterior = prior * likelihood\n    return posterior\n\n\n# Define the velocity GP\nmean = gpx.mean_functions.Zero()\nkernel = VelocityKernel()\nvelocity_posterior = initialise_gp(kernel, mean, dataset_train)\n</pre> def initialise_gp(kernel, mean, dataset):     prior = gpx.gps.Prior(mean_function=mean, kernel=kernel)     likelihood = gpx.likelihoods.Gaussian(         num_datapoints=dataset.n, obs_stddev=jnp.array([1.0e-3], dtype=jnp.float64)     )     posterior = prior * likelihood     return posterior   # Define the velocity GP mean = gpx.mean_functions.Zero() kernel = VelocityKernel() velocity_posterior = initialise_gp(kernel, mean, dataset_train) <p>With a model now defined, we can proceed to optimise the hyperparameters of our likelihood over $D_0$. This is done by minimising the MLL using <code>BFGS</code>. We also plot its value at each step to visually confirm that we have found the minimum. See the  introduction to Gaussian Processes notebook for more information on optimising the MLL.</p> In\u00a0[6]: Copied! <pre>def optimise_mll(posterior, dataset, NIters=1000, key=key):\n    # define the MLL using dataset_train\n    objective = gpx.objectives.ConjugateMLL(negative=True)\n    # Optimise to minimise the MLL\n    opt_posterior, history = gpx.fit_scipy(\n        model=posterior,\n        objective=objective,\n        train_data=dataset,\n    )\n    return opt_posterior\n\n\nopt_velocity_posterior = optimise_mll(velocity_posterior, dataset_train)\n</pre> def optimise_mll(posterior, dataset, NIters=1000, key=key):     # define the MLL using dataset_train     objective = gpx.objectives.ConjugateMLL(negative=True)     # Optimise to minimise the MLL     opt_posterior, history = gpx.fit_scipy(         model=posterior,         objective=objective,         train_data=dataset,     )     return opt_posterior   opt_velocity_posterior = optimise_mll(velocity_posterior, dataset_train) <pre>Optimization terminated successfully.\n         Current function value: -26.620707\n         Iterations: 42\n         Function evaluations: 70\n         Gradient evaluations: 70\n</pre> In\u00a0[7]: Copied! <pre>def latent_distribution(opt_posterior, pos_3d, dataset_train):\n    latent = opt_posterior.predict(pos_3d, train_data=dataset_train)\n    latent_mean = latent.mean()\n    latent_std = latent.stddev()\n    return latent_mean, latent_std\n\n\n# extract latent mean and std of g, redistribute into vectors to model F\nvelocity_mean, velocity_std = latent_distribution(\n    opt_velocity_posterior, dataset_ground_truth.X, dataset_train\n)\n\ndataset_latent_velocity = dataset_3d(pos_test, velocity_mean)\n</pre> def latent_distribution(opt_posterior, pos_3d, dataset_train):     latent = opt_posterior.predict(pos_3d, train_data=dataset_train)     latent_mean = latent.mean()     latent_std = latent.stddev()     return latent_mean, latent_std   # extract latent mean and std of g, redistribute into vectors to model F velocity_mean, velocity_std = latent_distribution(     opt_velocity_posterior, dataset_ground_truth.X, dataset_train )  dataset_latent_velocity = dataset_3d(pos_test, velocity_mean) <p>We now replot the ground truth (testing data) $D_0$, the predicted latent vector field $\\mathbf{F}_{\\text{latent}}(\\mathbf{x_i})$, and a heatmap of the residuals at each location $\\mathbf{R}(\\mathbf{x}_{0,i}) = \\mathbf{y}_{0,i} - \\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i}) $, as well as $\\left|\\left|\\mathbf{R}(\\mathbf{x}_{0,i})\\right|\\right|$.</p> In\u00a0[8]: Copied! <pre># Residuals between ground truth and estimate\n\n\ndef plot_vector_field(ax, dataset, **kwargs):\n    ax.quiver(\n        dataset.X[::2][:, 0],\n        dataset.X[::2][:, 1],\n        dataset.y[::2],\n        dataset.y[1::2],\n        **kwargs,\n    )\n\n\ndef prepare_ax(ax, X, Y, title, **kwargs):\n    ax.set(\n        xlim=[X.min() - 0.1, X.max() + 0.1],\n        ylim=[Y.min() + 0.1, Y.max() + 0.1],\n        aspect=\"equal\",\n        title=title,\n        ylabel=\"latitude\",\n        **kwargs,\n    )\n\n\ndef residuals(dataset_latent, dataset_ground_truth):\n    return jnp.sqrt(\n        (dataset_latent.y[::2] - dataset_ground_truth.y[::2]) ** 2\n        + (dataset_latent.y[1::2] - dataset_ground_truth.y[1::2]) ** 2\n    )\n\n\ndef plot_fields(\n    dataset_ground_truth, dataset_trajectory, dataset_latent, shape=shape, scale=10\n):\n    X = dataset_ground_truth.X[:, 0][::2]\n    Y = dataset_ground_truth.X[:, 1][::2]\n    # make figure\n    fig, ax = plt.subplots(1, 3, figsize=(12.0, 3.0), sharey=True)\n\n    # ground truth\n    plot_vector_field(\n        ax[0],\n        dataset_ground_truth,\n        color=colors[0],\n        label=\"Ocean Current\",\n        angles=\"xy\",\n        scale=scale,\n    )\n    plot_vector_field(\n        ax[0],\n        dataset_trajectory,\n        color=colors[1],\n        label=\"Drifter\",\n        angles=\"xy\",\n        scale=scale,\n    )\n    prepare_ax(ax[0], X, Y, \"Ground Truth\", xlabel=\"Longitude\")\n\n    # Latent estimate of vector field F\n    plot_vector_field(ax[1], dataset_latent, color=colors[0], angles=\"xy\", scale=scale)\n    plot_vector_field(\n        ax[1], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale\n    )\n    prepare_ax(ax[1], X, Y, \"GP Estimate\", xlabel=\"Longitude\")\n\n    # residuals\n    residuals_vel = jnp.flip(\n        residuals(dataset_latent, dataset_ground_truth).reshape(shape), axis=0\n    )\n    im = ax[2].imshow(\n        residuals_vel,\n        extent=[X.min(), X.max(), Y.min(), Y.max()],\n        cmap=\"jet\",\n        vmin=0,\n        vmax=1.0,\n        interpolation=\"spline36\",\n    )\n    plot_vector_field(\n        ax[2], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale\n    )\n    prepare_ax(ax[2], X, Y, \"Residuals\", xlabel=\"Longitude\")\n    fig.colorbar(im, fraction=0.027, pad=0.04, orientation=\"vertical\")\n\n    fig.legend(\n        framealpha=0.0,\n        ncols=2,\n        fontsize=\"medium\",\n        bbox_to_anchor=(0.5, -0.03),\n        loc=\"lower center\",\n    )\n    plt.show()\n\n\nplot_fields(dataset_ground_truth, dataset_train, dataset_latent_velocity)\n</pre> # Residuals between ground truth and estimate   def plot_vector_field(ax, dataset, **kwargs):     ax.quiver(         dataset.X[::2][:, 0],         dataset.X[::2][:, 1],         dataset.y[::2],         dataset.y[1::2],         **kwargs,     )   def prepare_ax(ax, X, Y, title, **kwargs):     ax.set(         xlim=[X.min() - 0.1, X.max() + 0.1],         ylim=[Y.min() + 0.1, Y.max() + 0.1],         aspect=\"equal\",         title=title,         ylabel=\"latitude\",         **kwargs,     )   def residuals(dataset_latent, dataset_ground_truth):     return jnp.sqrt(         (dataset_latent.y[::2] - dataset_ground_truth.y[::2]) ** 2         + (dataset_latent.y[1::2] - dataset_ground_truth.y[1::2]) ** 2     )   def plot_fields(     dataset_ground_truth, dataset_trajectory, dataset_latent, shape=shape, scale=10 ):     X = dataset_ground_truth.X[:, 0][::2]     Y = dataset_ground_truth.X[:, 1][::2]     # make figure     fig, ax = plt.subplots(1, 3, figsize=(12.0, 3.0), sharey=True)      # ground truth     plot_vector_field(         ax[0],         dataset_ground_truth,         color=colors[0],         label=\"Ocean Current\",         angles=\"xy\",         scale=scale,     )     plot_vector_field(         ax[0],         dataset_trajectory,         color=colors[1],         label=\"Drifter\",         angles=\"xy\",         scale=scale,     )     prepare_ax(ax[0], X, Y, \"Ground Truth\", xlabel=\"Longitude\")      # Latent estimate of vector field F     plot_vector_field(ax[1], dataset_latent, color=colors[0], angles=\"xy\", scale=scale)     plot_vector_field(         ax[1], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale     )     prepare_ax(ax[1], X, Y, \"GP Estimate\", xlabel=\"Longitude\")      # residuals     residuals_vel = jnp.flip(         residuals(dataset_latent, dataset_ground_truth).reshape(shape), axis=0     )     im = ax[2].imshow(         residuals_vel,         extent=[X.min(), X.max(), Y.min(), Y.max()],         cmap=\"jet\",         vmin=0,         vmax=1.0,         interpolation=\"spline36\",     )     plot_vector_field(         ax[2], dataset_trajectory, color=colors[1], angles=\"xy\", scale=scale     )     prepare_ax(ax[2], X, Y, \"Residuals\", xlabel=\"Longitude\")     fig.colorbar(im, fraction=0.027, pad=0.04, orientation=\"vertical\")      fig.legend(         framealpha=0.0,         ncols=2,         fontsize=\"medium\",         bbox_to_anchor=(0.5, -0.03),         loc=\"lower center\",     )     plt.show()   plot_fields(dataset_ground_truth, dataset_train, dataset_latent_velocity) <p>From the latent estimate we can see the velocity GP struggles to reconstruct features of the ground truth. This is because our construction of the kernel placed an independent prior on each physical dimension, which cannot be assumed. Therefore, we need a different approach that can implicitly incorporate this dependence at a fundamental level. To achieve this we will require a Helmholtz Decomposition.</p> In\u00a0[9]: Copied! <pre>@dataclass\nclass HelmholtzKernel(gpx.kernels.AbstractKernel):\n    # initialise Phi and Psi kernels as any stationary kernel in gpJax\n    potential_kernel: gpx.kernels.AbstractKernel = field(\n        default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n    )\n    stream_kernel: gpx.kernels.AbstractKernel = field(\n        default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])\n    )\n\n    def __call__(\n        self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        # obtain indices for k_helm, implement in the correct sign between the derivatives\n        z = jnp.array(X[2], dtype=int)\n        zp = jnp.array(Xp[2], dtype=int)\n        sign = (-1) ** (z + zp)\n\n        # convert to array to correctly index, -ve sign due to exchange symmetry (only true for stationary kernels)\n        potential_dvtve = -jnp.array(\n            hessian(self.potential_kernel)(X, Xp), dtype=jnp.float64\n        )[z][zp]\n        stream_dvtve = -jnp.array(\n            hessian(self.stream_kernel)(X, Xp), dtype=jnp.float64\n        )[1 - z][1 - zp]\n\n        return potential_dvtve + sign * stream_dvtve\n</pre> @dataclass class HelmholtzKernel(gpx.kernels.AbstractKernel):     # initialise Phi and Psi kernels as any stationary kernel in gpJax     potential_kernel: gpx.kernels.AbstractKernel = field(         default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])     )     stream_kernel: gpx.kernels.AbstractKernel = field(         default_factory=lambda: gpx.kernels.RBF(active_dims=[0, 1])     )      def __call__(         self, X: Float[Array, \"1 D\"], Xp: Float[Array, \"1 D\"]     ) -&gt; Float[Array, \"1\"]:         # obtain indices for k_helm, implement in the correct sign between the derivatives         z = jnp.array(X[2], dtype=int)         zp = jnp.array(Xp[2], dtype=int)         sign = (-1) ** (z + zp)          # convert to array to correctly index, -ve sign due to exchange symmetry (only true for stationary kernels)         potential_dvtve = -jnp.array(             hessian(self.potential_kernel)(X, Xp), dtype=jnp.float64         )[z][zp]         stream_dvtve = -jnp.array(             hessian(self.stream_kernel)(X, Xp), dtype=jnp.float64         )[1 - z][1 - zp]          return potential_dvtve + sign * stream_dvtve In\u00a0[10]: Copied! <pre># Redefine Gaussian process with Helmholtz kernel\nkernel = HelmholtzKernel()\nhelmholtz_posterior = initialise_gp(kernel, mean, dataset_train)\n# Optimise hyperparameters using BFGS\nopt_helmholtz_posterior = optimise_mll(helmholtz_posterior, dataset_train)\n</pre> # Redefine Gaussian process with Helmholtz kernel kernel = HelmholtzKernel() helmholtz_posterior = initialise_gp(kernel, mean, dataset_train) # Optimise hyperparameters using BFGS opt_helmholtz_posterior = optimise_mll(helmholtz_posterior, dataset_train) <pre>Optimization terminated successfully.\n         Current function value: -28.611975\n         Iterations: 35\n         Function evaluations: 58\n         Gradient evaluations: 58\n</pre> In\u00a0[11]: Copied! <pre># obtain latent distribution, extract x and y values over g\nhelmholtz_mean, helmholtz_std = latent_distribution(\n    opt_helmholtz_posterior, dataset_ground_truth.X, dataset_train\n)\ndataset_latent_helmholtz = dataset_3d(pos_test, helmholtz_mean)\n\nplot_fields(dataset_ground_truth, dataset_train, dataset_latent_helmholtz)\n</pre> # obtain latent distribution, extract x and y values over g helmholtz_mean, helmholtz_std = latent_distribution(     opt_helmholtz_posterior, dataset_ground_truth.X, dataset_train ) dataset_latent_helmholtz = dataset_3d(pos_test, helmholtz_mean)  plot_fields(dataset_ground_truth, dataset_train, dataset_latent_helmholtz) <p>Visually, the Helmholtz model performs better than the velocity model, preserving the local structure of the $\\mathbf{F}$. Since we placed priors on $\\Phi$ and $\\Psi$, the construction of $\\mathbf{F}$ allows for correlations between the dimensions (non-zero off-diagonal elements in the Gram matrix populated by $k_\\text{Helm}\\left(\\mathbf{X},\\mathbf{X}^{\\prime}\\right)$ ).</p> In\u00a0[12]: Copied! <pre># ensure testing data alternates between x0 and x1 components\ndef nlpd(mean, std, vel_test):\n    vel_query = jnp.column_stack((vel_test[0], vel_test[1])).flatten()\n    normal = tfp.substrates.jax.distributions.Normal(loc=mean, scale=std)\n    return -jnp.sum(normal.log_prob(vel_query))\n\n\n# compute nlpd for velocity and helmholtz\nnlpd_vel = nlpd(velocity_mean, velocity_std, vel_test)\nnlpd_helm = nlpd(helmholtz_mean, helmholtz_std, vel_test)\n\nprint(\"NLPD for Velocity: %.2f \\nNLPD for Helmholtz: %.2f\" % (nlpd_vel, nlpd_helm))\n</pre> # ensure testing data alternates between x0 and x1 components def nlpd(mean, std, vel_test):     vel_query = jnp.column_stack((vel_test[0], vel_test[1])).flatten()     normal = tfp.substrates.jax.distributions.Normal(loc=mean, scale=std)     return -jnp.sum(normal.log_prob(vel_query))   # compute nlpd for velocity and helmholtz nlpd_vel = nlpd(velocity_mean, velocity_std, vel_test) nlpd_helm = nlpd(helmholtz_mean, helmholtz_std, vel_test)  print(\"NLPD for Velocity: %.2f \\nNLPD for Helmholtz: %.2f\" % (nlpd_vel, nlpd_helm)) <pre>NLPD for Velocity: 730.13 \nNLPD for Helmholtz: -280.59\n</pre> <p>The Helmholtz model outperforms the velocity model, as indicated by the lower NLPD score.</p> <p></p> In\u00a0[13]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Ivan Shalashilin'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Ivan Shalashilin' <pre>Author: Ivan Shalashilin\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ntensorflow_probability: 0.22.1\nmatplotlib            : 3.8.3\ngpjax                 : 0.8.0\njax                   : 0.4.25\npandas                : 1.5.3\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/oceanmodelling/#gaussian-processes-for-vector-fields-and-ocean-current-modelling","title":"Gaussian Processes for Vector Fields and Ocean Current Modelling\u00b6","text":"<p>In this notebook, we use Gaussian processes to learn vector-valued functions. We will be recreating the results by Berlinghieri et al. (2023) by an application to real-world ocean surface velocity data, collected via surface drifters.</p> <p>Surface drifters are measurement devices that measure the dynamics and circulation patterns of the world's oceans. Studying and predicting ocean currents are important to climate research, for example, forecasting and predicting oil spills, oceanographic surveying of eddies and upwelling, or providing information on the distribution of biomass in ecosystems. We will be using the Gulf Drifters Open dataset, which contains all publicly available surface drifter trajectories from the Gulf of Mexico spanning 28 years.</p>"},{"location":"examples/oceanmodelling/#data-loading-and-preprocessing","title":"Data loading and preprocessing\u00b6","text":"<p>The real dataset has been binned into an $N=34\\times16$ grid, equally spaced over the longitude-latitude interval $[-90.8,-83.8] \\times [24.0,27.5]$. Each bin has a size $\\approx 0.21\\times0.21$, and contains the average velocity across all measurements that fall inside it.</p> <p>We will call this binned ocean data the ground truth, and label it with the vector field $$ \\mathbf{F} \\equiv \\mathbf{F}(\\mathbf{x}), $$ where $\\mathbf{x} = (x^{(0)}$,$x^{(1)})^\\text{T}$, with a vector basis in the standard Cartesian directions (dimensions will be indicated by superscripts).</p> <p>We shall label the ground truth $D_0=\\left\\{ \\left(\\mathbf{x}_{0,i} , \\mathbf{y}_{0,i} \\right)\\right\\}_{i=1}^N$, where $\\mathbf{y}_{0,i}$ is the 2-dimensional velocity vector at the $i$-th location, $\\mathbf{x}_{0,i}$. The training dataset contains simulated measurements from ocean drifters $D_T=\\left\\{\\left(\\mathbf{x}_{T,i}, \\mathbf{y}_{T,i} \\right)\\right\\}_{i=1}^{N_T}$, $N_T = 20$ in this case (the subscripts indicate the ground truth and the simulated measurements respectively).</p>"},{"location":"examples/oceanmodelling/#problem-setting","title":"Problem Setting\u00b6","text":"<p>We aim to obtain estimates for $\\mathbf{F}$ at the set of points $\\left\\{ \\mathbf{x}_{0,i} \\right\\}_{i=1}^N$ using Gaussian processes, followed by a comparison of the latent model to the ground truth $D_0$. Note that $D_0$ is not passed into any functions used  by GPJax, and is only used to compare against the two GP models at the end of the notebook.</p> <p>Since $\\mathbf{F}$ is a vector-valued function, we require GPs that can directly learn vector-valued functions<sup>1</sup>. To implement this in GPJax, the problem can be changed to learn a scalar-valued function by 'massaging' the data into a  $2N\\times2N$ problem, such that each dimension of our GP is associated with a component of $\\mathbf{y}_{T,i}$.</p> <p>For a particular measurement $\\mathbf{y}$ (training or testing) at location $\\mathbf{x}$, the components $(y^{(0)}, y^{(1)})$ are described by the latent vector field $\\mathbf{F}$, such that</p> <p>$$ \\mathbf{y} = \\mathbf{F}(\\mathbf{x}) = \\left(\\begin{array}{l} f^{(0)}\\left(\\mathbf{x}\\right) \\\\ f^{(1)}\\left(\\mathbf{x}\\right) \\end{array}\\right), $$</p> <p>where each $f^{(z)}\\left(\\mathbf{x}\\right), z \\in \\{0,1\\}$ is a scalar-valued function.</p> <p>Now consider the scalar-valued function $g: \\mathbb{R}^2 \\times\\{0,1\\} \\rightarrow \\mathbb{R}$, such that</p> <p>$$ g \\left(\\mathbf{x} , 0 \\right) = f^{(0)} ( \\mathbf{x} ), \\text{and } g \\left( \\mathbf{x}, 1 \\right)=f^{(1)}\\left(\\mathbf{x}\\right). $$</p> <p>We have increased the input dimension by 1, from the 2D $\\mathbf{x}$ to the 3D $\\mathbf{X} = \\left(\\mathbf{x}, 0\\right)$ or $\\mathbf{X} = \\left(\\mathbf{x}, 1\\right)$.</p> <p>By choosing the value of the third dimension, 0 or 1, we may now incorporate this information into the computation of the kernel. We therefore make new 3D datasets $D_{T,3D} = \\left\\{\\left( \\mathbf{X}_{T,i},\\mathbf{Y}_{T,i} \\right) \\right\\} _{i=0}^{2N_T}$ and $D_{0,3D} = \\left\\{\\left( \\mathbf{X}_{0,i},\\mathbf{Y}_{0,i} \\right) \\right\\} _{i=0}^{2N}$ that incorporates this new labelling, such that for each dataset (indicated by the subscript $D = 0$ or $D=T$),</p> <p>$$ X_{D,i} = \\left( \\mathbf{x}_{D,i}, z \\right), $$ and $$ Y_{D,i} = y_{D,i}^{(z)}, $$</p> <p>where $z = 0$ if $i$ is odd and $z=1$ if $i$ is even.</p>"},{"location":"examples/oceanmodelling/#velocity-dimension-decomposition","title":"Velocity (dimension) decomposition\u00b6","text":"<p>Having labelled the data, we are now in a position to use GPJax to learn the function $g$, and hence $\\mathbf{F}$. A naive approach to the problem is to apply a GP prior directly to the velocities of each dimension independently, which is called the velocity GP. For our prior, we choose an isotropic mean 0 over all dimensions of the GP, and a piecewise kernel that depends on the $z$ labels of the inputs, such that for two inputs $\\mathbf{X} = \\left( \\mathbf{x}, z \\right )$ and $\\mathbf{X}^\\prime = \\left( \\mathbf{x}^\\prime, z^\\prime \\right )$,</p> <p>$$ k_{\\text{vel}} \\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right)= \\begin{cases}k^{(z)}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right) &amp; \\text { if } z=z^{\\prime} \\\\ 0 &amp; \\text { if } z \\neq z^{\\prime}, \\end{cases} $$</p> <p>where $k^{(z)}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)$ are the user chosen kernels for each dimension. What this means is that there are no correlations between the $x^{(0)}$ and $x^{(1)}$ dimensions for all choices $\\mathbf{X}$ and $\\mathbf{X}^{\\prime}$, since there are no off-diagonal elements in the Gram matrix populated by this choice.</p> <p>To implement this approach in GPJax, we define <code>VelocityKernel</code> in the following cell, following the steps outlined in the custom kernels notebook. This modular implementation takes the choice of user kernels as its class attributes: <code>kernel0</code> and <code>kernel1</code>. We must additionally pass the argument <code>active_dims = [0,1]</code>, which is an attribute of the base class <code>AbstractKernel</code>, into the chosen kernels. This is necessary such that the subsequent likelihood optimisation does not optimise over the artificial label dimension.</p>"},{"location":"examples/oceanmodelling/#gpjax-implementation","title":"GPJax implementation\u00b6","text":"<p>Next, we define the model in GPJax. The prior is defined using $k_{\\text{vel}}\\left(\\mathbf{X}, \\mathbf{X}^\\prime \\right)$ and 0 mean and 0 observation noise. We choose a Gaussian marginal log-likelihood (MLL).</p>"},{"location":"examples/oceanmodelling/#comparison","title":"Comparison\u00b6","text":"<p>We next obtain the latent distribution of the GP of $g$ at $\\mathbf{x}_{0,i}$, then extract its mean and standard at the test locations, $\\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})$, as well as the standard deviation (we will use it at the very end).</p>"},{"location":"examples/oceanmodelling/#helmholtz-decomposition","title":"Helmholtz decomposition\u00b6","text":"<p>In 2 dimensions, a twice continuously differentiable and compactly supported vector field $\\mathbf{F}: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ can be expressed as the sum of the gradient of a scalar potential $\\Phi: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$, called the potential function, and the vorticity operator of another scalar potential $\\Psi: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$, called the stream function (Berlinghieri et al. (2023)) such that $$ \\mathbf{F}=\\operatorname{grad} \\Phi+\\operatorname{rot} \\Psi, $$ where $$ \\operatorname{grad} \\Phi:=\\left[\\begin{array}{l} \\partial \\Phi / \\partial x^{(0)} \\\\ \\partial \\Phi / \\partial x^{(1)} \\end{array}\\right] \\text { and } \\operatorname{rot} \\Psi:=\\left[\\begin{array}{c} \\partial \\Psi / \\partial x^{(1)} \\\\ -\\partial \\Psi / \\partial x^{(0)} \\end{array}\\right]. $$</p> <p>This is reminiscent of a 3 dimensional Helmholtz decomposition.</p> <p>The 2 dimensional decomposition motivates a different approach: placing priors on $\\Psi$ and $\\Phi$, allowing us to make assumptions directly about fundamental properties of $\\mathbf{F}$. If we choose independent GP priors such that $\\Phi \\sim \\mathcal{G P}\\left(0, k_{\\Phi}\\right)$ and $\\Psi \\sim \\mathcal{G P}\\left(0, k_{\\Psi}\\right)$, then $\\mathbf{F} \\sim \\mathcal{G P} \\left(0, k_\\text{Helm}\\right)$ (since acting linear operations on a GPs give GPs).</p> <p>For $\\mathbf{X}, \\mathbf{X}^{\\prime} \\in \\mathbb{R}^2 \\times \\left\\{0,1\\right\\}$ and $z, z^\\prime \\in \\{0,1\\}$,</p> <p>$$ \\boxed{ k_{\\mathrm{Helm}}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)_{z,z^\\prime} =  \\frac{\\partial^2 k_{\\Phi}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)}{\\partial x^{(z)} \\partial\\left(x^{\\prime}\\right)^{(z^\\prime)}}+(-1)^{z+z^\\prime} \\frac{\\partial^2 k_{\\Psi}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)}{\\partial x^{(1-z)} \\partial\\left(x^{\\prime}\\right)^{(1-z^\\prime)}}}. $$</p> <p>where $x^{(z)}$ and $(x^\\prime)^{(z^\\prime)}$ are the $z$ and $z^\\prime$ components of $\\mathbf{X}$ and ${\\mathbf{X}}^{\\prime}$ respectively.</p> <p>We compute the second derivatives using <code>jax.hessian</code>. In the following implementation, for a kernel $k(\\mathbf{x}, \\mathbf{x}^{\\prime})$, this computes the Hessian matrix with respect to the components of $\\mathbf{x}$</p> <p>$$ \\frac{\\partial^2 k\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)}{\\partial x^{(z)} \\partial x^{(z^\\prime)}}. $$</p> <p>Note that we have operated $\\dfrac{\\partial}{\\partial x^{(z)}}$, not $\\dfrac{\\partial}{\\partial \\left(x^\\prime \\right)^{(z)}}$, as the boxed equation suggests. This is not an issue if we choose stationary kernels $k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = k(\\mathbf{x} - \\mathbf{x}^{\\prime})$ , as the partial derivatives with respect to the components have the following exchange symmetry:</p> <p>$$ \\frac{\\partial}{\\partial x^{(z)}} = - \\frac{\\partial}{\\partial \\left( x^\\prime \\right)^{(z)}}, $$</p> <p>for either $z$.</p>"},{"location":"examples/oceanmodelling/#gpjax-implementation","title":"GPJax implementation\u00b6","text":"<p>We repeat the same steps as with the velocity GP model, replacing <code>VelocityKernel</code> with <code>HelmholtzKernel</code>.</p>"},{"location":"examples/oceanmodelling/#comparison","title":"Comparison\u00b6","text":"<p>We again plot the ground truth (testing data) $D_0$, the predicted latent vector field $\\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})$, and a heatmap of the residuals at each location $R(\\mathbf{x}_{0,i}) = \\mathbf{y}_{0,i} - \\mathbf{F}_{\\text{latent}}(\\mathbf{x}_{0,i})$ and $\\left|\\left|R(\\mathbf{x}_{0,i})  \\right|\\right|$.</p>"},{"location":"examples/oceanmodelling/#negative-log-predictive-densities","title":"Negative log predictive densities\u00b6","text":"<p>Lastly, we directly compare the velocity and Helmholtz models by computing the negative log predictive densities for each model. This is a quantitative metric that measures the probability of the ground truth given the data.</p> <p>$$ \\mathrm{NLPD}=-\\sum_{i=1}^{2N} \\log \\left(  p\\left(\\mathcal{Y}_i = Y_{0,i} \\mid \\mathbf{X}_{i}\\right) \\right), $$</p> <p>where each $p\\left(\\mathcal{Y}_i \\mid \\mathbf{X}_i \\right)$ is the marginal Gaussian distribution over $\\mathcal{Y}_i$ at each test location, and $Y_{i,0}$ is the $i$-th component of the (massaged) test data that we reserved at the beginning of the notebook in $D_0$. A smaller value is better, since the deviation of the ground truth and the model are small in this case.</p>"},{"location":"examples/oceanmodelling/#footnote","title":"Footnote\u00b6","text":"<p>Kernels for vector-valued functions have been studied in the literature, see Alvarez et al. (2012)</p>"},{"location":"examples/oceanmodelling/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/poisson/","title":"Count data regression","text":"In\u00a0[1]: Copied! <pre>import blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\nfrom jax import config\nfrom jaxtyping import install_import_hook\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\ntfd = tfp.distributions\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> import blackjax import jax import jax.numpy as jnp import jax.random as jr import jax.tree_util as jtu import matplotlib as mpl import matplotlib.pyplot as plt import tensorflow_probability.substrates.jax as tfp from jax import config from jaxtyping import install_import_hook  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. config.update(\"jax_enable_x64\", True) tfd = tfp.distributions key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>key, subkey = jr.split(key)\nn = 50\nx = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\nf = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\ny = jr.poisson(key, jnp.exp(f(x)))\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[1])\nax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\nax.legend()\n</pre> key, subkey = jr.split(key) n = 50 x = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0) f = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function y = jr.poisson(key, jnp.exp(f(x)))  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[1]) ax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\") ax.legend() <pre>/usr/share/miniconda/envs/test/lib/python3.10/site-packages/jaxtyping/_decorator.py:450: UserWarning: y is not of type float64.Got y.dtype=int64. This may lead to numerical instability.\n  out = fn(*args, **kwargs)\n</pre> Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x7f4f7dcd42e0&gt;</pre> In\u00a0[3]: Copied! <pre>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Constant()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.likelihoods.Poisson(num_datapoints=D.n)\n</pre> kernel = gpx.kernels.RBF() meanf = gpx.mean_functions.Constant() prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.likelihoods.Poisson(num_datapoints=D.n) <p>We construct the posterior through the product of our prior and likelihood.</p> In\u00a0[4]: Copied! <pre>posterior = prior * likelihood\nprint(type(posterior))\n</pre> posterior = prior * likelihood print(type(posterior)) <pre>&lt;class 'gpjax.gps.NonConjugatePosterior'&gt;\n</pre> <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. Here, we show how to use MCMC methods.</p> In\u00a0[5]: Copied! <pre># Adapted from BlackJax's introduction notebook.\nnum_adapt = 100\nnum_samples = 200\n\nlpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n\n# Initialise the chain\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Sample from the posterior distribution\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\n</pre> # Adapted from BlackJax's introduction notebook. num_adapt = 100 num_samples = 200  lpd = jax.jit(gpx.objectives.LogPosteriorDensity(negative=False)) unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))  adapt = blackjax.window_adaptation(     blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65 )  # Initialise the chain last_state, kernel, _ = adapt.run(key, posterior.unconstrain())   def inference_loop(rng_key, kernel, initial_state, num_samples):     def one_step(state, rng_key):         state, info = kernel(rng_key, state)         return state, (state, info)      keys = jax.random.split(rng_key, num_samples)     _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)      return states, infos   # Sample from the posterior distribution states, infos = inference_loop(key, kernel, last_state, num_samples) In\u00a0[6]: Copied! <pre>acceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</pre> acceptance_rate = jnp.mean(infos.acceptance_probability) print(f\"Acceptance rate: {acceptance_rate:.2f}\") <pre>Acceptance rate: 0.68\n</pre> In\u00a0[7]: Copied! <pre>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.constrain().prior.kernel.variance)\nax1.plot(states.position.constrain().prior.kernel.lengthscale)\nax2.plot(states.position.constrain().prior.mean_function.constant)\nax0.set_title(\"Kernel variance\")\nax1.set_title(\"Kernel lengthscale\")\nax2.set_title(\"Mean function constant\")\n</pre> fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3)) ax0.plot(states.position.constrain().prior.kernel.variance) ax1.plot(states.position.constrain().prior.kernel.lengthscale) ax2.plot(states.position.constrain().prior.mean_function.constant) ax0.set_title(\"Kernel variance\") ax1.set_title(\"Kernel lengthscale\") ax2.set_title(\"Mean function constant\") Out[7]: <pre>Text(0.5, 1.0, 'Mean function constant')</pre> In\u00a0[8]: Copied! <pre>thin_factor = 10\nsamples = []\n\nfor i in range(num_adapt, num_samples + num_adapt, thin_factor):\n    sample = jtu.tree_map(lambda samples: samples[i], states.position)\n    sample = sample.constrain()\n    latent_dist = sample.predict(xtest, train_data=D)\n    predictive_dist = sample.likelihood(latent_dist)\n    samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n\nsamples = jnp.vstack(samples)\n\nlower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(samples, axis=0)\n</pre> thin_factor = 10 samples = []  for i in range(num_adapt, num_samples + num_adapt, thin_factor):     sample = jtu.tree_map(lambda samples: samples[i], states.position)     sample = sample.constrain()     latent_dist = sample.predict(xtest, train_data=D)     predictive_dist = sample.likelihood(latent_dist)     samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))  samples = jnp.vstack(samples)  lower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0) expected_val = jnp.mean(samples, axis=0) <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(\n    x, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7\n)\nax.plot(\n    xtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1\n)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=cols[0],\n    label=\"95% CI\",\n)\n</pre> fig, ax = plt.subplots() ax.plot(     x, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7 ) ax.plot(     xtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1 ) ax.fill_between(     xtest.flatten(),     lower_ci.flatten(),     upper_ci.flatten(),     alpha=0.2,     color=cols[0],     label=\"95% CI\", ) Out[9]: <pre>&lt;matplotlib.collections.PolyCollection at 0x7f4f744369e0&gt;</pre> In\u00a0[10]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Francesco Zanetta\"\n</pre> %load_ext watermark %watermark -n -u -v -iv -w -a \"Francesco Zanetta\" <pre>Author: Francesco Zanetta\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\ntensorflow_probability: 0.22.1\nblackjax              : 0.9.6\njax                   : 0.4.25\nmatplotlib            : 3.8.3\ngpjax                 : 0.8.0\n\nWatermark: 2.4.3\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/poisson/#count-data-regression","title":"Count data regression\u00b6","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via Markov chain Monte Carlo (MCMC). We focus on a count data regression task here and use BlackJax for sampling.</p>"},{"location":"examples/poisson/#dataset","title":"Dataset\u00b6","text":"<p>For count data regression, the Poisson distribution is a natural choice for the likelihood function. The probability mass function of the Poisson distribution is given by</p> <p>$$ p(y \\,|\\, \\lambda) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!},$$</p> <p>where $y$ is the count and the parameter $\\lambda \\in \\mathbb{R}_{&gt;0}$ is the rate of the Poisson distribution.</p> <p>We than set $\\lambda = \\exp(f)$ where $f$ is the latent Gaussian process. The exponential function is the link function for the Poisson distribution: it maps the output of a GP to the positive real line, which is suitable for modeling count data.</p> <p>We simulate a dataset $\\mathcal{D} = \\{(\\mathbf{X}, \\mathbf{y})\\}$ with inputs $\\mathbf{X} \\in \\mathbb{R}^d$ and corresponding count outputs $\\mathbf{y}$. We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code>.</p>"},{"location":"examples/poisson/#gaussian-process-definition","title":"Gaussian Process definition\u00b6","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. We adopt the Poisson likelihood available in GPJax.</p>"},{"location":"examples/poisson/#mcmc-inference","title":"MCMC inference\u00b6","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"examples/poisson/#mcmc-through-blackjax","title":"MCMC through BlackJax\u00b6","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 200 values from our Markov chain. In practice, drawing more samples will be necessary.</p>"},{"location":"examples/poisson/#sampler-efficiency","title":"Sampler efficiency\u00b6","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain).</p>"},{"location":"examples/poisson/#prediction","title":"Prediction\u00b6","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every $n$-th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>"},{"location":"examples/poisson/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/pytrees/","title":"\ud83c\udf33 GPJax PyTrees","text":""},{"location":"examples/pytrees/#gpjax-module","title":"\ud83c\udf33 GPJax Module","text":"<p><code>GPJax</code> represents all objects as JAX PyTrees, giving</p> <ul> <li>A simple API with a TensorFlow / PyTorch feel ...</li> <li>... whilst fully compatible with JAX's functional paradigm ...</li> <li>... And works out of the box (no filtering) with JAX's transformations   such as <code>grad</code>.</li> </ul> <p>We achieve this through providing a base <code>Module</code> abstraction to cleanly handle parameter trainability and optimising transformations of JAX models.</p>"},{"location":"examples/pytrees/#gaussian-process-objects-as-data","title":"Gaussian process objects as data","text":"<p>Our abstraction is inspired by the Equinox library and aims to offer a Bayesian/Gaussian process extension to their neural network abstractions. Our approach enables users to easily create Python classes and define parameter domains and training statuses for optimisation within a single model object. This object can be used with JAX autogradients without any filtering.</p> <p>The fundamental concept is to describe every model object as an immutable tree structure, where every method is a function of the state (represented by the tree's leaves).</p> <p>To help you understand how to create custom objects in GPJax, we will look at a simple example in the following section.</p>"},{"location":"examples/pytrees/#the-rbf-kernel","title":"The RBF kernel","text":"<p>The kernel in a Gaussian process model is a mathematical function that defines the covariance structure between data points, allowing us to model complex relationships and make predictions based on the observed data. The radial basis function (RBF, or squared exponential) kernel is a popular choice. For any pair of vectors x,y\u2208Rdx, y \\in \\mathbb{R}^dx,y\u2208Rd, its form is given by</p> <p>k(x,y)=\u03c32exp\u2061(\u2225x\u2212y\u2225222\u21132) k(x, y) = \\sigma^2\\exp\\left(\\frac{\\lVert x-y\\rVert_{2}^2}{2\\ell^2} \\right) k(x,y)=\u03c32exp(2\u21132\u2225x\u2212y\u222522\u200b\u200b)</p> <p>where \u03c32\u2208R&gt;0\\sigma^2\\in\\mathbb{R}_{&gt;0}\u03c32\u2208R&gt;0\u200b is a variance parameter and \u21132\u2208R&gt;0\\ell^2\\in\\mathbb{R}_{&gt;0}\u21132\u2208R&gt;0\u200b a lengthscale parameter. Terming the evaluation of k(x,y)k(x, y)k(x,y) the covariance, we can represent this object as a Python <code>dataclass</code> as follows:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom dataclasses import dataclass, field\n@dataclass\nclass RBF:\nlengthscale: float = field(default=1.0)\nvariance: float = field(default=1.0)\ndef covariance(self, x: float, y: float) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>Here, the Python <code>dataclass</code> is a class that simplifies the process of creating classes that primarily store data. It reduces boilerplate code and provides convenient methods for initialising and representing the data. An equivalent class could be written as:</p> <pre><code>class RBF:\ndef __init__(self, lengthscale: float = 1.0, variance: float = 1.0) -&gt; None:\nself.lengthscale = lengthscale\nself.variance = variance\ndef covariance(self, x: float, y: float) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x-y) / self.lengthscale)**2)\n</code></pre> <p>To establish some terminology, within the above RBF <code>dataclass</code>, we refer to the lengthscale and variance as fields. Further, the <code>RBF.covariance</code> is a method. So far so good. However, if we wanted to take the gradient of the kernel with respect to its parameters $<code>\\nabla_{\\ell, \\sigma^2} k(1.0, 2.0; \\ell, \\sigma^2)</code>$ at inputs x=1.0x=1.0x=1.0 and y=2.0y=2.0y=2.0, then we encounter a problem:</p> <p><pre><code>kernel = RBF()\ntry:\njax.grad(lambda kern: kern.covariance(1.0, 2.0))(kernel)\nexcept TypeError as e:\nprint(e)\n</code></pre> <pre><code>Argument 'RBF(lengthscale=1.0, variance=1.0)' of type &lt;class '__main__.RBF'&gt; is not a valid JAX type.\n</code></pre></p> <p>This issues arises as the object we have defined is not yet compatible with JAX. To achieve this we must consider JAX's PyTree abstraction.</p>"},{"location":"examples/pytrees/#pytrees","title":"PyTrees","text":"<p>JAX PyTrees are a powerful tool in the JAX library that enable users to work with complex data structures in a way that is efficient, flexible, and easy to use. A PyTree is a data structure that is composed of other data structures, and it can be thought of as a tree where each 'node' is either a leaf (a simple data structure) or another PyTree. By default, the set of 'node' types that are regarded a PyTree are Python lists, tuples, and dicts.</p> <p>For instance:</p> <p><pre><code>tree = [3.14, {\"Monte\": object(), \"Carlo\": False}]\nprint(tree)\n</code></pre> <pre><code>[3.14, {'Monte': &lt;object object at 0x1188ba6b0&gt;, 'Carlo': False}]\n</code></pre> is a PyTree with structure</p> <p><pre><code>import jax.tree_util as jtu\nprint(jtu.tree_structure(tree))\n</code></pre> <pre><code>PyTreeDef([*, {'Carlo': *, 'Monte': *}])\n</code></pre> with the following leaves</p> <p><pre><code>print(jtu.tree_leaves(tree))\n</code></pre> <pre><code>[3.14, False, &lt;object object at 0x1188ba6b0&gt;]\n</code></pre></p> <p>Consider a second example, a PyTree of JAX arrays</p> <pre><code>tree = (\njnp.array([1.0, 2.0, 3.0]),\njnp.array([4.0, 5.0, 6.0]),\njnp.array([7.0, 8.0, 9.0]),\n)\n</code></pre> <p>You can use this template to perform various operations on the data, such as applying a function to each leaf of the PyTree.</p> <p>For example, suppose you want to square each element of the arrays. You can then apply this using the <code>tree_map</code> function from the <code>jax.tree_util</code> module:</p> <p><pre><code>print(jtu.tree_map(lambda x: x**2, tree))\n</code></pre> <pre><code>(Array([1., 4., 9.], dtype=float32), Array([16., 25., 36.], dtype=float32), Array([49., 64., 81.], dtype=float32))\n</code></pre></p> <p>In this example, the PyTree makes it easy to apply a function to each leaf of a complex data structure, without having to manually traverse the data structure and handle each leaf individually. JAX PyTrees, therefore, are a powerful tool that can simplify many tasks in machine learning and scientific computing. As such, most JAX functions operate over PyTrees of JAX arrays. For instance, <code>jax.lax.scan</code>, accepts as input and produces as output a PyTree of JAX arrays.</p> <p>Another key advantages of using JAX PyTrees is that they are designed to work efficiently with JAX's automatic differentiation and compilation features. For example, suppose you have a function that takes a PyTree as input and returns a scalar value:</p> <p><pre><code>def sum_squares(x):\nreturn jnp.sum(x[0] ** 2 + x[1] ** 2 + x[2] ** 2)\nsum_squares(tree)\n</code></pre> <pre><code>Array(285., dtype=float32)\n</code></pre></p> <p>You can use JAX's <code>grad</code> function to automatically compute the gradient of this function with respect to the input PyTree:</p> <p><pre><code>gradient = jax.grad(sum_squares)(tree)\nprint(gradient)\n</code></pre> <pre><code>(Array([2., 4., 6.], dtype=float32), Array([ 8., 10., 12.], dtype=float32), Array([14., 16., 18.], dtype=float32))\n</code></pre></p> <p>This computes the gradient of the <code>sum_squares</code> function with respect to the input PyTree, and returns a new PyTree with the same shape and structure.</p> <p>JAX PyTrees are also designed to be highly extensible, where custom types can be readily registered through a global registry with the values of such traversed recursively (i.e., as a tree!). This means we can define our own custom data structures and use them as PyTrees. This is the functionality that we exploit, whereby we construct all Gaussian process models via a tree-structure through our <code>Module</code> object.</p>"},{"location":"examples/pytrees/#module","title":"Module","text":"<p>Our design, first and foremost, minimises additional abstractions on top of standard JAX: everything is just PyTrees and transformations on PyTrees, and secondly, provides full compatibility with the main JAX library itself, enhancing integrability with the broader ecosystem of third-party JAX libraries. To achieve this, our core idea is represent all model objects via an immutable PyTree. Here the leaves of the PyTree represent the parameters that are to be trained, and we describe their domain and trainable status as <code>dataclass</code> metadata.</p> <p>For our RBF kernel we have two parameters; the lengthscale and the variance. Both of these have positive domains, and by default we want to train both of these parameters. To encode this we use a <code>param_field</code>, where we can define the domain of both parameters via a <code>Softplus</code> bijector (that restricts them to the positive domain), and set their trainable status to <code>True</code>.</p> <pre><code>import tensorflow_probability.substrates.jax.bijectors as tfb\nfrom gpjax.base import Module, param_field\n@dataclass\nclass RBF(Module):\nlengthscale: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\nvariance: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>Here <code>param_field</code> is just a special type of <code>dataclasses.field</code>. As such the following:</p> <pre><code>param_field(1.0, bijector= tfb.Identity(), trainable=False)\n</code></pre> <p>is equivalent to the following <code>dataclasses.field</code></p> <pre><code>field(default=1.0, metadata={\"trainable\": False, \"bijector\": tfb.Identity()})\n</code></pre> <p>By default unmarked leaf attributes default to an <code>Identity</code> bijector and trainablility set to <code>True</code>.</p>"},{"location":"examples/pytrees/#replacing-values","title":"Replacing values","text":"<p>For consistency with JAX\u2019s functional programming principles, <code>Module</code> instances are immutable. PyTree nodes can be changed out-of-place via the <code>replace</code> method.</p> <p><pre><code>kernel = RBF()\nkernel = kernel.replace(lengthscale=3.14)  # Update e.g., the lengthscale.\nprint(kernel)\n</code></pre> <pre><code>RBF(lengthscale=3.14, variance=1.0)\n</code></pre></p>"},{"location":"examples/pytrees/#transformations","title":"Transformations \ud83e\udd16","text":"<p>Use <code>constrain</code> / <code>unconstrain</code> to return a <code>Module</code> with each parameter's bijector <code>forward</code> / <code>inverse</code> operation applied!</p> <p><pre><code># Transform kernel to unconstrained space\nunconstrained_kernel = kernel.unconstrain()\nprint(unconstrained_kernel)\n# Transform kernel back to constrained space\nkernel = unconstrained_kernel.constrain()\nprint(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(3.0957527, dtype=float32), variance=Array(0.54132485, dtype=float32))\nRBF(lengthscale=Array(3.14, dtype=float32), variance=Array(1., dtype=float32))\n</code></pre></p> <p>Default transformations can be replaced on an instance via the <code>replace_bijector</code> method.</p> <p><pre><code>new_kernel = kernel.replace_bijector(lengthscale=tfb.Identity())\n# Transform kernel to unconstrained space\nunconstrained_kernel = new_kernel.unconstrain()\nprint(unconstrained_kernel)\n# Transform kernel back to constrained space\nnew_kernel = unconstrained_kernel.constrain()\nprint(new_kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(3.14, dtype=float32), variance=Array(0.54132485, dtype=float32))\nRBF(lengthscale=Array(3.14, dtype=float32), variance=Array(1., dtype=float32))\n</code></pre></p>"},{"location":"examples/pytrees/#trainability","title":"Trainability \ud83d\ude82","text":"<p>Recall the example earlier, where we wanted to take the gradient of the kernel with respect to its parameters \u2207\u2113,\u03c32k(1.0,2.0;\u2113,\u03c32)\\nabla_{\\ell, \\sigma^2} k(1.0, 2.0; \\ell,\\sigma^2)\u2207\u2113,\u03c32\u200bk(1.0,2.0;\u2113,\u03c32) at inputs x=1.0x=1.0x=1.0 and y=2.0y=2.0y=2.0. We can now confirm we can do this with the new <code>Module</code>.</p> <p><pre><code>kernel = RBF()\njax.grad(lambda kern: kern.covariance(1.0, 2.0))(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0.60653067, dtype=float32, weak_type=True), variance=Array(0.60653067, dtype=float32, weak_type=True))\n</code></pre></p> <p>During gradient learning of models, it can sometimes be useful to fix certain parameters during the optimisation routine. For this, JAX provides a <code>stop_gradient</code> operand to prevent the flow of gradients during forward or reverse-mode automatic differentiation, as illustrated below for a function f(x)=x2f(x) = x^2f(x)=x2.</p> <p><pre><code>from jax import lax\ndef f(x):\nx = lax.stop_gradient(x)\nreturn x**2\njax.grad(f)(1.0)\n</code></pre> <pre><code>Array(0., dtype=float32, weak_type=True)\n</code></pre></p> <p>We see that gradient return is <code>0.0</code> instead of <code>2.0</code> due to the stopping of the gradient. Analogous to this, we provide this functionality to gradient flows on our <code>Module</code> class, via a <code>stop_gradient</code> method.</p> <p>Setting a (leaf) parameter's trainability to false can be achieved via the <code>replace_trainable</code> method.</p> <p><pre><code>kernel = RBF()\nkernel = kernel.replace_trainable(lengthscale=False)\njax.grad(lambda kern: kern.stop_gradient().covariance(1.0, 2.0))(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0., dtype=float32, weak_type=True), variance=Array(0.60653067, dtype=float32, weak_type=True))\n</code></pre></p> <p>As expected, the gradient is zero for the lengthscale parameter.</p>"},{"location":"examples/pytrees/#static-fields","title":"Static fields","text":"<p>In machine learning, initialising model parameters from random points is a common practice because it helps to break the symmetry in the model and allows the optimization algorithm to explore different regions of the parameter space.</p> <p>We could cleanly do this within the RBF class via a <code>post_init</code> method as follows:</p> <p><pre><code>import jax.random as jr\nimport tensorflow_probability.substrates.jax.distributions as tfd\nfrom dataclasses import field\n@dataclass\nclass RBF(Module):\nlengthscale: float = param_field(\ninit=False, bijector=tfb.Softplus(), trainable=True\n)\nvariance: float = param_field(init=False, bijector=tfb.Softplus(), trainable=True)\nkey: jax.Array = field(default_factory = lambda: jr.key(42))\n# Note, for Python &lt;3.11 you may use the following:\n# key: jax.Array = jr.key(42)\ndef __post_init__(self):\n# Split key into two keys\nkey1, key2 = jr.split(self.key)\n# Sample from Gamma distribution to initialise lengthscale and variance\nself.lengthscale = tfd.Gamma(1.0, 0.1).sample(seed=key1)\nself.variance = tfd.Gamma(1.0, 0.1).sample(seed=key2)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\nkernel = RBF()\nprint(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0.54950446, dtype=float32), variance=Array(2.8077831, dtype=float32), key=Array([ 0, 42], dtype=uint32))\n</code></pre></p> <p>So far so good. But however, if we now took our gradient again</p> <p><pre><code>try:\njax.grad(lambda kern: kern.stop_gradient().covariance(1.0, 2.0))(kernel)\nexcept TypeError as e:\nprint(e)\n</code></pre> <pre><code>grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got uint32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.\n</code></pre></p> <p>We observe that we get a TypeError because the key is not differentiable. We can fix this by using a <code>static_field</code> for defining our key attribute.</p> <p><pre><code>from gpjax.base import static_field\n@dataclass\nclass RBF(Module):\nlengthscale: float = param_field(\ninit=False, bijector=tfb.Softplus(), trainable=True\n)\nvariance: float = param_field(init=False, bijector=tfb.Softplus(), trainable=True)\nkey: jax.Array = static_field(default_factory=lambda: jr.key(42))\ndef __post_init__(self):\n# Split key into two keys\nkey1, key2 = jr.split(self.key)\n# Sample from Gamma distribution to initialise lengthscale and variance\nself.lengthscale = tfd.Gamma(1.0, 0.1).sample(seed=key1)\nself.variance = tfd.Gamma(1.0, 0.1).sample(seed=key2)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\nfixed_kernel = RBF()\nprint(fixed_kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0.54950446, dtype=float32), variance=Array(2.8077831, dtype=float32), key=Array([ 0, 42], dtype=uint32))\n</code></pre></p> <p>So we get the same class as before. But this time</p> <p><pre><code>jax.grad(lambda kern: kern.stop_gradient().covariance(1.0, 2.0))(fixed_kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(3.230818, dtype=float32), variance=Array(0.19092491, dtype=float32), key=Array([ 0, 42], dtype=uint32))\n</code></pre></p> <p>What happened to get the result we wanted? The difference lies in the treatment of the key attribute as a PyTree leaf in the first example, which caused the gradient computation to fail. Examining the flattened PyTree's of both cases:</p> <p><pre><code>print(jax.tree_util.tree_flatten(fixed_kernel))\nprint(jax.tree_util.tree_flatten(kernel))\n</code></pre> <pre><code>([Array(0.54950446, dtype=float32), Array(2.8077831, dtype=float32)], PyTreeDef(CustomNode(RBF[(['lengthscale', 'variance'], [('key', Array([ 0, 42], dtype=uint32))])], [*, *])))\n([Array([ 0, 42], dtype=uint32), Array(0.54950446, dtype=float32), Array(2.8077831, dtype=float32)], PyTreeDef(CustomNode(RBF[(['key', 'lengthscale', 'variance'], [])], [*, *, *])))\n</code></pre></p> <p>We see that assigning <code>static_field</code> tells JAX not to regard the attribute as leaf of the PyTree.</p>"},{"location":"examples/pytrees/#metadata","title":"Metadata","text":"<p>To determine the parameter domain and trainable statuses of each parameter, the <code>Module</code> stores metadata for each leaf of the PyTree. This metadata is defined through a <code>dataclasses.field</code>. Thus, under the hood, we can define our <code>RBF</code> kernel object (equivalent to before) manually as follows:</p> <pre><code>from dataclasses import field\n@dataclass\nclass RBF(Module):\nlengthscale: float = field(\ndefault=1.0, metadata={\"bijector\": tfb.Softplus(), \"trainable\": True}\n)\nvariance: float = field(\ndefault=1.0, metadata={\"bijector\": tfb.Softplus(), \"trainable\": True}\n)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>Here the <code>metadata</code> in the <code>dataclasses.field</code>, defines the metadata we associate with each PyTree leaf. This metadata can be a dictionary of any attributes we wish to store about each leaf. For example, we could extend this further by introducing a <code>name</code> attribute:</p> <pre><code>from dataclasses import field\n@dataclass\nclass RBF(Module):\nlengthscale: float = field(\ndefault=1.0,\nmetadata={\"bijector\": tfb.Softplus(), \"trainable\": True, \"name\": \"lengthscale\"},\n)\nvariance: float = field(\ndefault=1.0,\nmetadata={\"bijector\": tfb.Softplus(), \"trainable\": True, \"name\": \"variance\"},\n)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>We can trace the metadata defined on the class via <code>meta_leaves</code>.</p> <p><pre><code>from gpjax.base import meta_leaves\nrbf = RBF()\nmeta_leaves(rbf)\n</code></pre> <pre><code>[({'bijector': &lt;tfp.bijectors.Softplus 'softplus' batch_shape=[] forward_min_event_ndims=0 inverse_min_event_ndims=0 dtype_x=? dtype_y=?&gt;,\n   'trainable': True,\n   'name': 'lengthscale'},\n  1.0),\n ({'bijector': &lt;tfp.bijectors.Softplus 'softplus' batch_shape=[] forward_min_event_ndims=0 inverse_min_event_ndims=0 dtype_x=? dtype_y=?&gt;,\n   'trainable': True,\n   'name': 'variance'},\n  1.0)]\n</code></pre></p> <p>Similar to <code>jax.tree_utils.tree_leaves</code>, this function returns a flattened PyTree. However, instead of just the values, it returns a list of tuples that contain both the metadata and value of each PyTree leaf. This traced metadata can be utilised for applying maps (how <code>constrain</code>, <code>unconstrain</code>, <code>stop_gradient</code> work), as described in the next section.</p>"},{"location":"examples/pytrees/#metamap","title":"Metamap","text":"<p>The <code>constrain</code>, <code>unconstrain</code>, and <code>stop_gradient</code> methods on the <code>Module</code> use a <code>meta_map</code> function under the hood. This function enables us to apply metadata functions to the PyTree leaves, making it a powerful tool.</p> <p>To achieve this, the function involves the same tracing as <code>meta_leaves</code> to create a flattened list of tuples consisting of (metadata, leaf value). However, it also allows us to apply a function to this list and return a new transformed PyTree, as demonstrated in the examples that follow.</p>"},{"location":"examples/pytrees/#filter-example","title":"Filter example:","text":"<p>A <code>meta_map</code> works similarly to <code>jax.tree_utils.tree_map</code>. However, it differs in that it allows us to define a function that operates on the tuple (metadata, leaf value). For example, we could use a function to filter based on a <code>name</code> attribute.</p> <p><pre><code>from gpjax.base import meta_map\ndef filter_lengthscale(meta_leaf):\nmeta, leaf = meta_leaf\nif meta.get(\"name\", None) == \"lengthscale\":\nreturn 3.14\nelse:\nreturn leaf\nprint(meta_map(filter_lengthscale, rbf))\n</code></pre> <pre><code>RBF(lengthscale=3.14, variance=1.0)\n</code></pre></p>"},{"location":"examples/pytrees/#how-constrain-works","title":"How <code>constrain</code> works:","text":"<p>To apply a constrain, we filter on the attribute \"bijector\", and apply a forward transformation to the PyTree leaf:</p> <p><pre><code># This is how constrain works! \u26cf\ndef _apply_constrain(meta_leaf):\nmeta, leaf = meta_leaf\nif meta is None:\nreturn leaf\nreturn meta.get(\"bijector\", tfb.Identity()).forward(leaf)\nmeta_map(_apply_constrain, rbf)\n</code></pre> <pre><code>RBF(lengthscale=Array(1.3132617, dtype=float32), variance=Array(1.3132617, dtype=float32))\n</code></pre></p> <p>As expected, we find the same result as calling <code>rbf.constrain()</code>.</p>"},{"location":"examples/regression/","title":"Regression","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from docs.examples.utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>n = 100\nnoise = 0.3\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 100 noise = 0.3  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise.</p> In\u00a0[3]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\n</pre> fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1]) ax.legend(loc=\"best\") Out[3]: <pre>&lt;matplotlib.legend.Legend at 0x7f0fe109f550&gt;</pre> <p>Our aim in this tutorial will be to reconstruct the latent function from our noisy observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a Gaussian process prior in the next section.</p> In\u00a0[4]: Copied! <pre>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n</pre> kernel = gpx.kernels.RBF() meanf = gpx.mean_functions.Zero() prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel) <p>The above construction forms the foundation for GPJax's models. Moreover, the GP prior we have just defined can be represented by a TensorFlow Probability multivariate Gaussian distribution. Such functionality enables trivial sampling, and the evaluation of the GP's mean and covariance .</p> In\u00a0[5]: Copied! <pre>prior_dist = prior.predict(xtest)\n\nprior_mean = prior_dist.mean()\nprior_std = prior_dist.variance()\nsamples = prior_dist.sample(seed=key, sample_shape=(20,))\n\n\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\n    xtest.flatten(),\n    prior_mean - prior_std,\n    prior_mean + prior_std,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\n</pre> prior_dist = prior.predict(xtest)  prior_mean = prior_dist.mean() prior_std = prior_dist.variance() samples = prior_dist.sample(seed=key, sample_shape=(20,))   fig, ax = plt.subplots() ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\") ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\") ax.fill_between(     xtest.flatten(),     prior_mean - prior_std,     prior_mean + prior_std,     alpha=0.3,     color=cols[1],     label=\"Prior variance\", ) ax.legend(loc=\"best\") ax = clean_legend(ax) In\u00a0[6]: Copied! <pre>likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n</pre> likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n) <p>The posterior is proportional to the prior multiplied by the likelihood, written as</p> <p>$$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$</p> <p>Mimicking this construct, the posterior is established in GPJax through the <code>*</code> operator.</p> In\u00a0[7]: Copied! <pre>posterior = prior * likelihood\n</pre> posterior = prior * likelihood In\u00a0[8]: Copied! <pre>negative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\n\n\n# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n# optim = ox.chain(\n#     ox.adam(learning_rate=0.01),\n#     ox.masked(ox.set_to_zero(), static_tree)\n#     )\n</pre> negative_mll = gpx.objectives.ConjugateMLL(negative=True) negative_mll(posterior, train_data=D)   # static_tree = jax.tree_map(lambda x: not(x), posterior.trainables) # optim = ox.chain( #     ox.adam(learning_rate=0.01), #     ox.masked(ox.set_to_zero(), static_tree) #     ) Out[8]: <pre>Array(124.80517341, dtype=float64)</pre> <p>For researchers, GPJax has the capacity to print the bibtex citation for objects such as the marginal log-likelihood through the <code>cite()</code> function.</p> In\u00a0[9]: Copied! <pre>print(gpx.cite(negative_mll))\n</pre> print(gpx.cite(negative_mll)) <pre>@book{rasmussen2006gaussian,\nauthors = {Rasmussen, Carl Edward and Williams, Christopher K},\ntitle = {Gaussian Processes for Machine Learning},\nyear = {2006},\npublisher = {MIT press Cambridge, MA},\nvolume = {2},\n}\n</pre> <p>JIT-compiling expensive-to-compute functions such as the marginal log-likelihood is advisable. This can be achieved by wrapping the function in <code>jax.jit()</code>.</p> In\u00a0[10]: Copied! <pre>negative_mll = jit(negative_mll)\n</pre> negative_mll = jit(negative_mll) <p>Since most optimisers (including here) minimise a given function, we have realised the negative marginal log-likelihood and just-in-time (JIT) compiled this to accelerate training.</p> <p>We can now define an optimiser. For this example we'll use the <code>bfgs</code> optimiser.</p> In\u00a0[11]: Copied! <pre>opt_posterior, history = gpx.fit_scipy(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n)\n</pre> opt_posterior, history = gpx.fit_scipy(     model=posterior,     objective=negative_mll,     train_data=D, ) <pre>Optimization terminated successfully.\n         Current function value: 55.103686\n         Iterations: 14\n         Function evaluations: 17\n         Gradient evaluations: 17\n</pre> In\u00a0[12]: Copied! <pre>latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev() <p>With the predictions and their uncertainty acquired, we illustrate the GP's performance at explaining the data $\\mathcal{D}$ and recovering the underlying latent function of interest.</p> In\u00a0[13]: Copied! <pre>fig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(7.5, 2.5)) ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[13]: <pre>&lt;matplotlib.legend.Legend at 0x7f103d3bd840&gt;</pre> In\u00a0[14]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd' <pre>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\nmatplotlib: 3.8.3\ngpjax     : 0.8.0\noptax     : 0.1.9\njax       : 0.4.25\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/regression/#regression","title":"Regression\u00b6","text":"<p>In this notebook we demonstate how to fit a Gaussian process regression model.</p>"},{"location":"examples/regression/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> <p>$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$</p> <p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/regression/#defining-the-prior","title":"Defining the prior\u00b6","text":"<p>A zero-mean Gaussian process (GP) places a prior distribution over real-valued functions $f(\\cdot)$ where $f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$ for any finite collection of inputs $\\boldsymbol{x}$.</p> <p>Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$ with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$. The choice of kernel function is critical as, among other things, it governs the smoothness of the outputs that our GP can generate.</p> <p>For simplicity, we consider a radial basis function (RBF) kernel: $$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$</p> <p>On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$, we can reciprocate this process in GPJax via defining a <code>Prior</code> with our chosen <code>RBF</code> kernel.</p>"},{"location":"examples/regression/#constructing-the-posterior","title":"Constructing the posterior\u00b6","text":"<p>Having defined our GP, we proceed to define a description of our data $\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of likelihood is a critical in Bayesian modelling, for simplicity we consider a Gaussian with noise parameter $\\alpha$ $$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$ This is defined in GPJax through calling a <code>Gaussian</code> instance.</p>"},{"location":"examples/regression/#parameter-state","title":"Parameter state\u00b6","text":"<p>As outlined in the PyTrees documentation, parameters are contained within the model and for the leaves of the PyTree. Consequently, in this particular model, we have three parameters: the kernel lengthscale, kernel variance and the observation noise variance. Whilst we have initialised each of these to 1, we can learn Type 2 MLEs for each of these parameters by optimising the marginal log-likelihood (MLL).</p>"},{"location":"examples/regression/#prediction","title":"Prediction\u00b6","text":"<p>Equipped with the posterior and a set of optimised hyperparameter values, we are now in a position to query our GP's predictive distribution at novel test inputs. To do this, we use our defined <code>posterior</code> and <code>likelihood</code> at our test inputs to obtain the predictive distribution as a <code>Distrax</code> multivariate Gaussian upon which <code>mean</code> and <code>stddev</code> can be used to extract the predictive mean and standard deviatation.</p>"},{"location":"examples/regression/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/uncollapsed_vi/","title":"Sparse Stochastic Variational Inference","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    import gpjax.kernels as jk\n\nkey = jr.key(123)\ntfb = tfp.bijectors\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax as tfp  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     import gpjax.kernels as jk  key = jr.key(123) tfb = tfp.bijectors plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>n = 50000\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)\n</pre> n = 50000 noise = 0.2  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1) In\u00a0[3]: Copied! <pre>z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.vlines(\n    z,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\nax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\n</pre> z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)  fig, ax = plt.subplots() ax.vlines(     z,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=1,     label=\"Inducing point\",     color=cols[2], ) ax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\") ax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\") ax.legend() ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\") Out[3]: <pre>[Text(0.5, 0, '$x$'), Text(0, 0.5, '$f(x)$')]</pre> <p>The inducing inputs will summarise our dataset, and since they are treated as variational parameters, their locations will be optimised. The next step to SVGP is to define a variational family.</p> In\u00a0[4]: Copied! <pre>meanf = gpx.mean_functions.Zero()\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n)\nprior = gpx.gps.Prior(mean_function=meanf, kernel=jk.RBF())\np = prior * likelihood\nq = gpx.variational_families.VariationalGaussian(posterior=p, inducing_inputs=z)\n</pre> meanf = gpx.mean_functions.Zero() likelihood = gpx.likelihoods.Gaussian(num_datapoints=n) prior = gpx.gps.Prior(mean_function=meanf, kernel=jk.RBF()) p = prior * likelihood q = gpx.variational_families.VariationalGaussian(posterior=p, inducing_inputs=z) <p>Here, the variational process $q(\\cdot)$ depends on the prior through $p(f(\\cdot)|f(\\boldsymbol{z}))$ in $(\\times)$.</p> In\u00a0[5]: Copied! <pre>negative_elbo = gpx.objectives.ELBO(negative=True)\n</pre> negative_elbo = gpx.objectives.ELBO(negative=True) <p>For researchers, GPJax has the capacity to print the bibtex citation for objects such as the ELBO through the <code>cite()</code> function.</p> In\u00a0[6]: Copied! <pre>print(gpx.cite(negative_elbo))\n</pre> print(gpx.cite(negative_elbo)) <pre>@article{hensman2013gaussian,\nauthors = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},\ntitle = {Gaussian Processes for Big Data},\nyear = {2013},\nbooktitle = {Uncertainty in Artificial Intelligence},\n}\n</pre> <p>JIT-compiling expensive-to-compute functions such as the ELBO is advisable. This can be achieved by wrapping the function in <code>jax.jit()</code>.</p> In\u00a0[7]: Copied! <pre>negative_elbo = jit(negative_elbo)\n</pre> negative_elbo = jit(negative_elbo) In\u00a0[8]: Copied! <pre>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.01,\n    warmup_steps=75,\n    decay_steps=1500,\n    end_value=0.001,\n)\n\nopt_posterior, history = gpx.fit(\n    model=q,\n    objective=negative_elbo,\n    train_data=D,\n    optim=ox.adam(learning_rate=schedule),\n    num_iters=3000,\n    key=jr.key(42),\n    batch_size=128,\n)\n</pre> schedule = ox.warmup_cosine_decay_schedule(     init_value=0.0,     peak_value=0.01,     warmup_steps=75,     decay_steps=1500,     end_value=0.001, )  opt_posterior, history = gpx.fit(     model=q,     objective=negative_elbo,     train_data=D,     optim=ox.adam(learning_rate=schedule),     num_iters=3000,     key=jr.key(42),     batch_size=128, ) In\u00a0[9]: Copied! <pre>latent_dist = opt_posterior(xtest)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.vlines(\n    opt_posterior.inducing_inputs,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\n</pre> latent_dist = opt_posterior(xtest) predictive_dist = opt_posterior.posterior.likelihood(latent_dist)  meanf = predictive_dist.mean() sigma = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0]) ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1]) ax.fill_between(     xtest.flatten(),     meanf - 2 * sigma,     meanf + 2 * sigma,     alpha=0.3,     color=cols[1],     label=\"Two sigma\", ) ax.vlines(     opt_posterior.inducing_inputs,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=1,     label=\"Inducing point\",     color=cols[2], ) ax.legend() Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x7fb0b86dfee0&gt;</pre> In\u00a0[10]: Copied! <pre>triangular_transform = tfb.FillScaleTriL(\n    diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter)\n)\nreparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform)\n</pre> triangular_transform = tfb.FillScaleTriL(     diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter) ) reparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform) In\u00a0[11]: Copied! <pre>opt_rep, history = gpx.fit(\n    model=reparameterised_q,\n    objective=negative_elbo,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=3000,\n    key=jr.key(42),\n    batch_size=128,\n)\n</pre> opt_rep, history = gpx.fit(     model=reparameterised_q,     objective=negative_elbo,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=3000,     key=jr.key(42),     batch_size=128, ) In\u00a0[12]: Copied! <pre>latent_dist = opt_rep(xtest)\npredictive_dist = opt_rep.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.vlines(\n    opt_rep.inducing_inputs,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\n</pre> latent_dist = opt_rep(xtest) predictive_dist = opt_rep.posterior.likelihood(latent_dist)  meanf = predictive_dist.mean() sigma = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0]) ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1]) ax.fill_between(     xtest.flatten(),     meanf - 2 * sigma,     meanf + 2 * sigma,     alpha=0.3,     color=cols[1],     label=\"Two sigma\", ) ax.vlines(     opt_rep.inducing_inputs,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=1,     label=\"Inducing point\",     color=cols[2], ) ax.legend() Out[12]: <pre>&lt;matplotlib.legend.Legend at 0x7fb0b88ebf70&gt;</pre> <p>We can see that <code>Square</code> transformation is able to get relatively better fit compared to <code>Softplus</code> with the same number of iterations, but <code>Softplus</code> is recommended over <code>Square</code> for stability of optimisation.</p> In\u00a0[13]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel' <pre>Author: Thomas Pinder, Daniel Dodd &amp; Zeel B Patel\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\njax                   : 0.4.25\ngpjax                 : 0.8.0\ntensorflow_probability: 0.22.1\nmatplotlib            : 3.8.3\noptax                 : 0.1.9\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/uncollapsed_vi/#sparse-stochastic-variational-inference","title":"Sparse Stochastic Variational Inference\u00b6","text":"<p>In this notebook we demonstrate how to implement sparse variational Gaussian processes (SVGPs) of Hensman et al. (2015). In particular, this approximation framework provides a tractable option for working with non-conjugate Gaussian processes with more than ~5000 data points. However, for conjugate models of less than 5000 data points, we recommend using the marginal log-likelihood approach presented in the regression notebook. Though we illustrate SVGPs here with a conjugate regression example, the same GPJax code works for general likelihoods, such as a Bernoulli for classification.</p>"},{"location":"examples/uncollapsed_vi/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{5000}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-5, 5)$ and corresponding binary outputs</p> <p>$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4 * \\boldsymbol{x}) + \\sin(2 * \\boldsymbol{x}), \\textbf{I} * (0.2)^{2} \\right).$$</p> <p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs for later.</p>"},{"location":"examples/uncollapsed_vi/#sparse-gps-via-inducing-inputs","title":"Sparse GPs via inducing inputs\u00b6","text":"<p>Despite their endowment with elegant theoretical properties, GPs are burdened with prohibitive $\\mathcal{O}(n^3)$ inference and $\\mathcal{O}(n^2)$ memory costs in the number of data points $n$ due to the necessity of computing inverses and determinants of the kernel Gram matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ during inference and hyperparameter learning. Sparse GPs seek to resolve tractability through low-rank approximations.</p> <p>Their name originates with the idea of using subsets of the data to approximate the kernel matrix, with sparseness occurring through the selection of the data points. Given inputs $\\boldsymbol{x}$ and outputs $\\boldsymbol{y}$ the task was to select an $m&lt;n$ lower-dimensional dataset $(\\boldsymbol{z},\\boldsymbol{\\tilde{y}}) \\subset (\\boldsymbol{x}, \\boldsymbol{y})$ to train a Gaussian process on instead. By generalising the set of selected points $\\boldsymbol{z}$, known as inducing inputs, to remove the restriction of being part of the dataset, we can arrive at a flexible low-rank approximation framework of the model using functions of $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$ to replace the true covariance matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ at significantly lower costs. For example,  review many popular approximation schemes in this vein. However, because the model and the approximation are intertwined, assigning performance and faults to one or the other becomes tricky.</p> <p>On the other hand, sparse variational Gaussian processes (SVGPs) approximate the posterior, not the model. These provide a low-rank approximation scheme via variational inference. Here we posit a family of densities parameterised by \u201cvariational parameters\u201d. We then seek to find the closest family member to the posterior by minimising the Kullback-Leibler divergence over the variational parameters. The fitted variational density then serves as a proxy for the exact posterior. This procedure makes variational methods efficiently solvable via off-the-shelf optimisation techniques whilst retaining the true-underlying model. Furthermore, SVGPs offer further cost reductions with mini-batch stochastic gradient descent   and address non-conjugacy . We show a cost comparison between the approaches below, where $b$ is the mini-batch size.</p> GPs sparse GPs SVGP Inference cost $\\mathcal{O}(n^3)$ $\\mathcal{O}(n m^2)$ $\\mathcal{O}(b m^2 + m^3)$ Memory cost $\\mathcal{O}(n^2)$ $\\mathcal{O}(n m)$ $\\mathcal{O}(b m + m^2)$ <p>To apply SVGP inference to our dataset, we begin by initialising $m = 50$ equally spaced inducing inputs $\\boldsymbol{z}$ across our observed data's support. These are depicted below via horizontal black lines.</p>"},{"location":"examples/uncollapsed_vi/#defining-the-variational-process","title":"Defining the variational process\u00b6","text":"<p>We begin by considering the form of the posterior distribution for all function values $f(\\cdot)$</p> <p>\\begin{align} p(f(\\cdot) | \\mathcal{D}) = \\int p(f(\\cdot)|f(\\boldsymbol{x})) p(f(\\boldsymbol{x})|\\mathcal{D}) \\text{d}f(\\boldsymbol{x}). \\qquad (\\dagger) \\end{align}</p> <p>To arrive at an approximation framework, we assume some redundancy in the data. Instead of predicting $f(\\cdot)$ with function values at the datapoints $f(\\boldsymbol{x})$, we assume this can be achieved with only function values at $m$ inducing inputs $\\boldsymbol{z}$</p> <p>$$ p(f(\\cdot) | \\mathcal{D}) \\approx \\int p(f(\\cdot)|f(\\boldsymbol{z})) p(f(\\boldsymbol{z})|\\mathcal{D}) \\text{d}f(\\boldsymbol{z}). \\qquad (\\star) $$</p> <p>This lower dimensional integral results in computational savings in the model's predictive component from $p(f(\\cdot)|f(\\boldsymbol{x}))$ to $p(f(\\cdot)|f(\\boldsymbol{z}))$ where inverting $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is replaced with inverting $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$. However, since we did not observe our data $\\mathcal{D}$ at $\\boldsymbol{z}$ we ask, what exactly is the posterior $p(f(\\boldsymbol{z})|\\mathcal{D})$?</p> <p>Notice this is simply obtained by substituting $\\boldsymbol{z}$ into $(\\dagger)$, but we arrive back at square one with computing the expensive integral. To side-step this, we consider replacing $p(f(\\boldsymbol{z})|\\mathcal{D})$ in $(\\star)$ with a cheap-to-compute approximate distribution $q(f(\\boldsymbol{z}))$</p> <p>$$ q(f(\\cdot)) = \\int p(f(\\cdot)|f(\\boldsymbol{z})) q(f(\\boldsymbol{z})) \\text{d}f(\\boldsymbol{z}). \\qquad (\\times) $$</p> <p>To measure the quality of the approximation, we consider the Kullback-Leibler divergence $\\operatorname{KL}(\\cdot || \\cdot)$ from our approximate process $q(f(\\cdot))$ to the true process $p(f(\\cdot)|\\mathcal{D})$. By parametrising $q(f(\\boldsymbol{z}))$ over a variational family of distributions, we can optimise Kullback-Leibler divergence with respect to the variational parameters. Moreover, since inducing input locations $\\boldsymbol{z}$ augment the model, they themselves can be treated as variational parameters without altering the true underlying model $p(f(\\boldsymbol{z})|\\mathcal{D})$. This is exactly what gives SVGPs great flexibility whilst retaining robustness to overfitting.</p> <p>It is popular to elect a Gaussian variational distribution $q(f(\\boldsymbol{z})) = \\mathcal{N}(f(\\boldsymbol{z}); \\mathbf{m}, \\mathbf{S})$ with parameters $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S}\\}$, since conjugacy is provided between $q(f(\\boldsymbol{z}))$ and $p(f(\\cdot)|f(\\boldsymbol{z}))$ so that the resulting variational process $q(f(\\cdot))$ is a GP. We can implement this in GPJax by the following.</p>"},{"location":"examples/uncollapsed_vi/#inference","title":"Inference\u00b6","text":""},{"location":"examples/uncollapsed_vi/#evidence-lower-bound","title":"Evidence lower bound\u00b6","text":"<p>With our model defined, we seek to infer the optimal inducing inputs $\\boldsymbol{z}$, variational mean $\\mathbf{m}$ and covariance $\\mathbf{S}$ that define our approximate posterior. To achieve this, we maximise the evidence lower bound (ELBO) with respect to $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S} \\}$, a proxy for minimising the Kullback-Leibler divergence. Moreover, as hinted by its name, the ELBO is a lower bound to the marginal log-likelihood, providing a tractable objective to optimise the model's hyperparameters akin to the conjugate setting. For further details on this, see Sections 3.1 and 4.1 of the excellent review paper .</p> <p>Since Optax's optimisers work to minimise functions, to maximise the ELBO we return its negative.</p>"},{"location":"examples/uncollapsed_vi/#mini-batching","title":"Mini-batching\u00b6","text":"<p>Despite introducing inducing inputs into our model, inference can still be intractable with large datasets. To circumvent this, optimisation can be done using stochastic mini-batches.</p>"},{"location":"examples/uncollapsed_vi/#predictions","title":"Predictions\u00b6","text":"<p>With optimisation complete, we can use our inferred parameter set to make predictions at novel inputs akin to all other models within GPJax on our variational process object $q(\\cdot)$ (for example, see the regression notebook).</p>"},{"location":"examples/uncollapsed_vi/#custom-transformations","title":"Custom transformations\u00b6","text":"<p>To train a covariance matrix, GPJax uses <code>tfb.FillScaleTriL</code> transformation by default. <code>tfb.FillScaleTriL</code> fills a 1d vector into a lower triangular matrix and then applies <code>Softplus</code> transformation on the diagonal to satisfy the necessary conditions for a valid Cholesky matrix. Users can change this default transformation with another valid transformation of their choice. For example, <code>Square</code> transformation on the diagonal can also serve the purpose.</p>"},{"location":"examples/uncollapsed_vi/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/yacht/","title":"UCI Data Benchmarking","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nkey = jr.key(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.metrics import (     mean_squared_error,     r2_score, ) from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. key = jr.key(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] In\u00a0[2]: Copied! <pre>try:\n    yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\nexcept FileNotFoundError:\n    yacht = pd.read_fwf(\n        \"docs/examples/data/yacht_hydrodynamics.data\", header=None\n    ).values[:-1, :]\n\nX = yacht[:, :-1]\ny = yacht[:, -1].reshape(-1, 1)\n</pre> try:     yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :] except FileNotFoundError:     yacht = pd.read_fwf(         \"docs/examples/data/yacht_hydrodynamics.data\", header=None     ).values[:-1, :]  X = yacht[:, :-1] y = yacht[:, -1].reshape(-1, 1) In\u00a0[3]: Copied! <pre>Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\n</pre> Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) In\u00a0[4]: Copied! <pre>log_ytr = np.log(ytr)\nlog_yte = np.log(yte)\n\ny_scaler = StandardScaler().fit(log_ytr)\nscaled_ytr = y_scaler.transform(log_ytr)\nscaled_yte = y_scaler.transform(log_yte)\n</pre> log_ytr = np.log(ytr) log_yte = np.log(yte)  y_scaler = StandardScaler().fit(log_ytr) scaled_ytr = y_scaler.transform(log_ytr) scaled_yte = y_scaler.transform(log_yte) <p>We can see the effect of these transformations in the below three panels.</p> In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\nax[0].hist(ytr, bins=30, color=cols[1])\nax[0].set_title(\"y\")\nax[1].hist(log_ytr, bins=30, color=cols[1])\nax[1].set_title(\"log(y)\")\nax[2].hist(scaled_ytr, bins=30, color=cols[1])\nax[2].set_title(\"scaled log(y)\")\n</pre> fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5)) ax[0].hist(ytr, bins=30, color=cols[1]) ax[0].set_title(\"y\") ax[1].hist(log_ytr, bins=30, color=cols[1]) ax[1].set_title(\"log(y)\") ax[2].hist(scaled_ytr, bins=30, color=cols[1]) ax[2].set_title(\"scaled log(y)\") Out[5]: <pre>Text(0.5, 1.0, 'scaled log(y)')</pre> In\u00a0[6]: Copied! <pre>x_scaler = StandardScaler().fit(Xtr)\nscaled_Xtr = x_scaler.transform(Xtr)\nscaled_Xte = x_scaler.transform(Xte)\n</pre> x_scaler = StandardScaler().fit(Xtr) scaled_Xtr = x_scaler.transform(Xtr) scaled_Xte = x_scaler.transform(Xte) In\u00a0[7]: Copied! <pre>n_train, n_covariates = scaled_Xtr.shape\nkernel = gpx.kernels.RBF(\n    active_dims=list(range(n_covariates)),\n    variance=np.var(scaled_ytr),\n    lengthscale=0.1 * np.ones((n_covariates,)),\n)\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train)\n\nposterior = prior * likelihood\n</pre> n_train, n_covariates = scaled_Xtr.shape kernel = gpx.kernels.RBF(     active_dims=list(range(n_covariates)),     variance=np.var(scaled_ytr),     lengthscale=0.1 * np.ones((n_covariates,)), ) meanf = gpx.mean_functions.Zero() prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)  likelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train)  posterior = prior * likelihood In\u00a0[8]: Copied! <pre>training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\n\nnegative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))\n\nopt_posterior, history = gpx.fit_scipy(\n    model=posterior,\n    objective=negative_mll,\n    train_data=training_data,\n)\n</pre> training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)  negative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))  opt_posterior, history = gpx.fit_scipy(     model=posterior,     objective=negative_mll,     train_data=training_data, ) <pre>Optimization terminated successfully.\n         Current function value: -6.805748\n         Iterations: 22\n         Function evaluations: 33\n         Gradient evaluations: 33\n</pre> In\u00a0[9]: Copied! <pre>latent_dist = opt_posterior(scaled_Xte, training_data)\npredictive_dist = likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_stddev = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior(scaled_Xte, training_data) predictive_dist = likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_stddev = predictive_dist.stddev() In\u00a0[10]: Copied! <pre>rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nr2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nprint(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")\n</pre> rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean) r2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean) print(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\") <pre>Results:\n\tRMSE:  0.0102\n\tR2:  0.99\n</pre> <p>Both of these metrics seem very promising, so, based off these, we can be quite happy that our first attempt at modelling the Yacht data is promising.</p> In\u00a0[11]: Copied! <pre>residuals = scaled_yte.squeeze() - predictive_mean\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\n\nax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\nax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\nax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\n\nax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\nax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\nax[1].set_ylim([-1.0, 1.0])\nax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n\nax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\nax[2].set_title(\"Residuals\")\n</pre> residuals = scaled_yte.squeeze() - predictive_mean  fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)  ax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1]) ax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes) ax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")  ax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1]) ax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes) ax[1].set_ylim([-1.0, 1.0]) ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")  ax[2].hist(np.asarray(residuals), bins=30, color=cols[1]) ax[2].set_title(\"Residuals\") Out[11]: <pre>Text(0.5, 1.0, 'Residuals')</pre> <p>From this, we can see that our model is struggling to predict the smallest values of the Yacht's hydrodynamic and performs increasingly well as the Yacht's hydrodynamic performance increases. This is likely due to the original data's heavy right-skew, and successive modelling attempts may wish to introduce a heteroscedastic likelihood function that would enable more flexible modelling of the smaller response values.</p> In\u00a0[12]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Tue Mar 12 2024\n\nPython implementation: CPython\nPython version       : 3.10.13\nIPython version      : 8.22.2\n\npandas    : 1.5.3\nmatplotlib: 3.8.3\nnumpy     : 1.26.4\ngpjax     : 0.8.0\njax       : 0.4.25\n\nWatermark: 2.4.3\n\n</pre>"},{"location":"examples/yacht/#uci-data-benchmarking","title":"UCI Data Benchmarking\u00b6","text":"<p>In this notebook, we will show how to apply GPJax on a benchmark UCI regression problem. These kind of tasks are often used in the research community to benchmark and assess new techniques against those already in the literature. Much of the code contained in this notebook can be adapted to applied problems concerning datasets other than the one presented here.</p>"},{"location":"examples/yacht/#data-loading","title":"Data Loading\u00b6","text":"<p>We'll be using the Yacht dataset from the UCI machine learning data repository. Each observation describes the hydrodynamic performance of a yacht through its resistance. The dataset contains 6 covariates and a single positive, real valued response variable. There are 308 observations in the dataset, so we can comfortably use a conjugate regression Gaussian process here (for more more details, checkout the Regression notebook).</p>"},{"location":"examples/yacht/#preprocessing","title":"Preprocessing\u00b6","text":"<p>With a dataset loaded, we'll now preprocess it such that it is more amenable to modelling with a Gaussian process.</p>"},{"location":"examples/yacht/#data-partitioning","title":"Data Partitioning\u00b6","text":"<p>We'll first partition our data into a training and testing split. We'll fit our Gaussian process to the training data and evaluate its performance on the test data. This allows us to investigate how effectively our Gaussian process generalises to out-of-sample datapoints and ensure that we are not overfitting. We'll hold 30% of our data back for testing purposes.</p>"},{"location":"examples/yacht/#response-variable","title":"Response Variable\u00b6","text":"<p>We'll now process our response variable $\\mathbf{y}$. As the below plots show, the data has a very long tail and is certainly not Gaussian. However, we would like to model a Gaussian response variable so that we can adopt a Gaussian likelihood function and leverage the model's conjugacy. To achieve this, we'll first log-scale the data, to bring the long right tail in closer to the data's mean. We'll then standardise the data such that is distributed according to a unit normal distribution. Both of these transformations are invertible through the log-normal expectation and variance formulae and the the inverse standardisation identity, should we ever need our model's predictions to be back on the scale of the original dataset.</p> <p>For transforming both the input and response variable, all transformations will be done with respect to the training data where relevant.</p>"},{"location":"examples/yacht/#input-variable","title":"Input Variable\u00b6","text":"<p>We'll now transform our input variable $\\mathbf{X}$ to be distributed according to a unit Gaussian.</p>"},{"location":"examples/yacht/#model-fitting","title":"Model fitting\u00b6","text":"<p>With data now loaded and preprocessed, we'll proceed to defining a Gaussian process model and optimising its parameters. This notebook purposefully does not go into great detail on this process, so please see notebooks such as the Regression notebook and Classification notebook for further information.</p>"},{"location":"examples/yacht/#model-specification","title":"Model specification\u00b6","text":"<p>We'll use a radial basis function kernel to parameterise the Gaussian process in this notebook. As we have 5 covariates, we'll assign each covariate its own lengthscale parameter. This form of kernel is commonly known as an automatic relevance determination (ARD) kernel.</p> <p>In practice, the exact form of kernel used should be selected such that it represents your understanding of the data. For example, if you were to model temperature; a process that we know to be periodic, then you would likely wish to select a periodic kernel. Having Gaussian-ised our data somewhat, we'll also adopt a Gaussian likelihood function.</p>"},{"location":"examples/yacht/#model-optimisation","title":"Model Optimisation\u00b6","text":"<p>With a model now defined, we can proceed to optimise the hyperparameters of our model using Scipy.</p>"},{"location":"examples/yacht/#prediction","title":"Prediction\u00b6","text":"<p>With an optimal set of parameters learned, we can make predictions on the set of data that we held back right at the start. We'll do this in the usual way by first computing the latent function's distribution before computing the predictive posterior distribution.</p>"},{"location":"examples/yacht/#evaluation","title":"Evaluation\u00b6","text":"<p>We'll now show how the performance of our Gaussian process can be evaluated by numerically and visually.</p>"},{"location":"examples/yacht/#metrics","title":"Metrics\u00b6","text":"<p>To numerically assess the performance of our model, two commonly used metrics are root mean squared error (RMSE) and the R2 coefficient. RMSE is simply the square root of the squared difference between predictions and actuals. A value of 0 for this metric implies that our model has 0 generalisation error on the test set. R2 measures the amount of variation within the data that is explained by the model. This can be useful when designing variance reduction methods such as control variates as it allows you to understand what proportion of the data's variance will be soaked up. A perfect model here would score 1 for R2 score, whereas predicting the data's mean would score 0 and models doing worse than simple mean predictions can score less than 0.</p>"},{"location":"examples/yacht/#diagnostic-plots","title":"Diagnostic plots\u00b6","text":"<p>To accompany the above metrics, we can also produce residual plots to explore exactly where our model's shortcomings lie. If we define a residual as the true value minus the prediction, then we can produce three plots:</p> <ol> <li>Predictions vs. actuals.</li> <li>Predictions vs. residuals.</li> <li>Residual density.</li> </ol> <p>The first plot allows us to explore if our model struggles to predict well for larger or smaller values by observing where the model deviates more from the line $y=x$. In the second plot we can inspect whether or not there were outliers or structure within the errors of our model. A well-performing model would have predictions close to and symmetrically distributed either side of $y=0$. Such a plot can be useful for diagnosing heteroscedasticity. Finally, by plotting a histogram of our residuals we can observe whether or not there is any skew to our residuals.</p>"},{"location":"examples/yacht/#system-configuration","title":"System configuration\u00b6","text":""}]}