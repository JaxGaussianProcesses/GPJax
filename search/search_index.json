{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe1 Home","text":""},{"location":"#welcome-to-gpjax","title":"Welcome to GPJax!","text":"<p>GPJax is a didactic Gaussian process (GP) library in JAX, supporting GPU acceleration and just-in-time compilation. We seek to provide a flexible API to enable researchers to rapidly prototype and develop new ideas.</p> <p></p>"},{"location":"#hello-gp","title":"\"Hello, GP!\"","text":"<p>Typing GP models is as simple as the maths we would write on paper, as shown below.</p> PythonMath <pre><code>import gpjax as gpx\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.RBF()\nprior = gpx.gps.Prior(mean_function = mean, kernel = kernel)\nlikelihood = gpx.likelihoods.Gaussian(num_datapoints = 123)\nposterior = prior * likelihood\n</code></pre> <p>k(\u22c5,\u22c5\u2032)=\u03c32exp\u2061(\u2212\u2225\u22c5\u2212\u22c5\u2032\u2225222\u21132)p(f(\u22c5))=GP(0,k(\u22c5,\u22c5\u2032))p(y \u2223 f(\u22c5))=N(y \u2223 f(\u22c5),\u03c3n2)p(f(\u22c5) \u2223 y)\u221dp(f(\u22c5))p(y \u2223 f(\u22c5)) . \\begin{align} k(\\cdot, \\cdot') &amp; = \\sigma^2\\exp\\left(-\\frac{\\lVert \\cdot- \\cdot'\\rVert_2^2}{2\\ell^2}\\right)\\\\ p(f(\\cdot)) &amp; = \\mathcal{GP}(\\mathbf{0}, k(\\cdot, \\cdot')) \\\\ p(y\\,|\\, f(\\cdot)) &amp; = \\mathcal{N}(y\\,|\\, f(\\cdot), \\sigma_n^2) \\\\ \\\\ p(f(\\cdot) \\,|\\, y) &amp; \\propto p(f(\\cdot))p(y\\,|\\, f(\\cdot))\\,. \\end{align} k(\u22c5,\u22c5\u2032)p(f(\u22c5))p(y\u2223f(\u22c5))p(f(\u22c5)\u2223y)\u200b=\u03c32exp(\u22122\u21132\u2225\u22c5\u2212\u22c5\u2032\u222522\u200b\u200b)=GP(0,k(\u22c5,\u22c5\u2032))=N(y\u2223f(\u22c5),\u03c3n2\u200b)\u221dp(f(\u22c5))p(y\u2223f(\u22c5)).\u200b\u200b</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Install</p> <p>GPJax can be installed via pip. See our installation guide for further details.</p> <pre><code>pip install gpjax\n</code></pre> <p>New</p> <p>New to GPs? Then why not check out our introductory notebook that starts from Bayes' theorem and univariate Gaussian distributions.</p> <p>Begin</p> <p>Looking for a good place to start? Then why not begin with our regression notebook.</p>"},{"location":"#citing-gpjax","title":"Citing GPJax","text":"<p>If you use GPJax in your research, please cite our JOSS paper.</p> <pre><code>@article{Pinder2022,\n  doi = {10.21105/joss.04455},\n  url = {https://doi.org/10.21105/joss.04455},\n  year = {2022},\n  publisher = {The Open Journal},\n  volume = {7},\n  number = {75},\n  pages = {4455},\n  author = {Thomas Pinder and Daniel Dodd},\n  title = {GPJax: A Gaussian Process Framework in JAX},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"CODE OF CONDUCT","text":"<p>Like the technical community as a whole, the GPJax team and community is made up of a mixture of professionals and volunteers from all over the world, working on every aspect of the mission - including mentorship, teaching and connecting people.</p> <p>Diversity is one of our huge strengths, but it can also lead to communication issues and unhappiness. To that end, we have a few ground rules that we ask people to adhere to when they're participating within this community and project. These rules apply equally to founders, mentors and those seeking help and guidance.</p> <p>This isn't an exhaustive list of things that you can't do. Rather, take it in the spirit in which it's intended - a guide to make it easier to enrich all of us, the technical community and the conferences and usergroups we hope to guide new speakers to.</p> <p>This code of conduct applies to all communication: this includes IRC, the mailing list, and other forums such as Skype, Google+ Hangouts, etc.</p> <ul> <li>Be welcoming, friendly, and patient.</li> <li>Be considerate. Your work will be used by other people, and you in turn will depend on the work of others. Any decision you make will affect users and colleagues, and you should take those consequences into account when making decisions.</li> <li>Be respectful. Not all of us will agree all the time, but disagreement is no excuse for poor behaviour and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It's important to remember that a community where people feel uncomfortable or threatened is not a productive one. Members of the GPJax community should be respectful when dealing with other members as well as with people outside the GPJax community and with user groups/conferences, usergroup/conference organizers.</li> <li>Be careful in the words that you choose. Remember that sexist, racist, and other exclusionary jokes can be offensive to those around you. Be kind and welcoming to others. Do not insult or put down other participants. Behave professionally. The harassment and exclusionary behaviour towards others is unacceptable. This includes, but is not limited to:</li> <li>Violent threats or language directed against another person.</li> <li>Discriminatory jokes and language.</li> <li>Posting sexually explicit or violent material.</li> <li>Posting (or threatening to post) other people's personally identifying information (\"doxing\").</li> <li>Personal insults, especially those using racist or sexist terms.</li> <li>Unwelcome sexual attention.</li> <li>Advocating for, or encouraging, any of the above behaviour.</li> <li>Repeated harassment of others. In general, if someone asks you to stop, then stop.</li> <li>When we disagree, we try to understand why. Disagreements, both social and technical, happen all the time and GPJax is no exception. It is important that we resolve disagreements and differing views constructively. Remember that we're different. The strength of GPJax comes from its varied community, people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn't mean that they're wrong. Don't forget that it is human to err and blaming each other doesn't get us anywhere, rather offer to help resolving issues and to help learn from mistakes.</li> </ul> <p>Text adapted from the Speak Up! project and Django</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#how-can-i-contribute","title":"How can I contribute?","text":"<p>GPJax welcomes contributions from interested individuals or groups. There are many ways to contribute, including:</p> <ul> <li>Answering questions on our discussions   page.</li> <li>Raising issues related to bugs   or desired enhancements.</li> <li>Contributing or improving the   docs or   examples.</li> <li>Fixing outstanding issues   (bugs).</li> <li>Extending or improving our codebase.</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of conduct","text":"<p>As a contributor to GPJax, you can help us keep the community open and inclusive. Please read and follow our Code of Conduct.</p>"},{"location":"contributing/#opening-issues-and-getting-support","title":"Opening issues and getting support","text":"<p>Please open issues on Github Issue Tracker. Here you can mention</p> <p>You can ask a question or start a discussion in the Discussion section on Github.</p>"},{"location":"contributing/#contributing-to-the-source-code","title":"Contributing to the source code","text":"<p>Submitting code contributions to GPJax is done via a GitHub pull request. Our preferred workflow is to first fork the GitHub repository, clone it to your local machine, and develop on a feature branch. Once you're happy with your changes, install our <code>pre-commit hooks</code>, <code>commit</code> and <code>push</code> your code.</p> <p>New to this? Don't panic, our guide below will walk you through every detail!</p> <p>Note</p> <p>Before opening a pull request we recommend you check our pull request checklist.</p>"},{"location":"contributing/#step-by-step-guide","title":"Step-by-step guide:","text":"<ol> <li> <p>Click here to Fork GPJax's   codebase (alternatively, click the 'Fork' button towards the top right of   the main repository page). This   adds a copy of the codebase to your GitHub user account.</p> </li> <li> <p>Clone your GPJax fork from your GitHub account to your local disk, and add   the base repository as a remote:   <pre><code>$ git clone git@github.com:&lt;your GitHub handle&gt;/GPJax.git\n$ cd GPJax\n$ git remote add upstream git@github.com:GPJax.git\n</code></pre></p> </li> <li> <p>Create a <code>feature</code> branch to hold your development changes:</p> </li> </ol> <p><pre><code>$ git checkout -b my-feature\n</code></pre>   Always use a <code>feature</code> branch. It's good practice to avoid   work on the <code>main</code> branch of any repository.</p> <ol> <li>We use Poetry for packaging and dependency management, and project requirements are in       <code>pyproject.toml</code>. We suggest using a virtual environment for development. For those using Apple Silicon chips, we advise using Conda miniforge. Once the virtual environment is activated, run:</li> </ol> <pre><code>$ poetry install\n</code></pre> <p>At this point we recommend you check your installation passes the supplied unit tests:</p> <pre><code>$ poetry run pytest\n</code></pre> <ol> <li>Install the pre-commit hooks.</li> </ol> <pre><code>$ pre-commit install\n</code></pre> <p>Please ensure you have done this before committing any files. If   successful, this will print the following output <code>pre-commit installed at   .git/hooks/pre-commit</code>.</p> <ol> <li>At this point you can manually run the pre-commit hooks with the following command:</li> </ol> <pre><code>poetry run pre-commit run --all-files\n</code></pre> <ol> <li>Add changed files using <code>git add</code> and then <code>git commit</code> files to record your   changes locally:</li> </ol> <p><pre><code>$ git add modified_files\n$ git commit\n</code></pre>   After committing, it is a good idea to sync with the base repository in case   there have been any changes:</p> <pre><code>$ git fetch upstream\n$ git rebase upstream/main\n</code></pre> <p>Then push the changes to your GitHub account with:</p> <pre><code>$ git push -u origin my-feature\n</code></pre> <ol> <li>Go to the GitHub web page of your fork of the GPJax repo. Click the 'Pull   request' button to send your changes to the project's maintainers for   review.</li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<p>We welcome both complete or \"work in progress\" pull requests. Before opening one, we recommended you check the following guidelines to ensure a smooth review process.</p> <p>My contribution is a \"work in progress\":</p> <p>Please prefix the title of incomplete contributions with <code>[WIP]</code> (to indicate a work in progress). WIPs are useful to:</p> <ol> <li>Indicate you are working on something to avoid duplicated work.</li> <li>Request broad review of functionality or API.</li> <li>Seek collaborators.</li> </ol> <p>In the description of the pull request, we recommend you outline where work needs doing. For example, do some tests need writing?</p> <p>My contribution is complete:</p> <p>If addressing an issue, please use the pull request title to describe the issue and mention the issue number in the pull request description. This will make sure a link back to the original issue is created. Then before making your pull request, we recommend you check the following:</p> <ul> <li>Do all public methods have informative docstrings that describe their   function, input(s) and output(s)?</li> <li>Do the pre-commit hooks pass?</li> <li>Do the tests pass when everything is rebuilt from scratch?</li> <li> <p>Documentation and high-coverage tests are necessary for enhancements to be   accepted. Test coverage can be checked with:</p> <pre><code>$ poetry run pytest tests --cov=./ --cov-report=html\n</code></pre> </li> </ul> <p>Navigate to the newly created folder <code>htmlcov</code> and open <code>index.html</code> to view   the coverage report.</p> <p>This guide was derived from PyMC's guide to contributing.</p>"},{"location":"design/","title":"\ud83c\udfa8 Design principles","text":""},{"location":"design/#design-principles","title":"Design Principles","text":"<p><code>GPJax</code> is designed to be a Gaussian process package that provides an accurate representation of the underlying maths. Variable names are chosen to closely match the notation in (Rasmussen and Williams, 2006)1. We here list the notation used in <code>GPJax</code> with its corresponding mathematical quantity.</p>"},{"location":"design/#gaussian-process-notation","title":"Gaussian process notation","text":"On paper GPJax code Description nnn n Number of train inputs x=(x1,\u2026,xn)\\boldsymbol{x} = (x_1,\\dotsc,x_{n})x=(x1\u200b,\u2026,xn\u200b) x Train inputs y=(y1,\u2026,yn)\\boldsymbol{y} = (y_1,\\dotsc,y_{n})y=(y1\u200b,\u2026,yn\u200b) y Train labels t\\boldsymbol{t}t t Test inputs f(\u22c5)f(\\cdot)f(\u22c5) f Latent function modelled as a GP f(x)f({\\boldsymbol{x}})f(x) fx Latent function at inputs x\\boldsymbol{x}x \u03bcx\\boldsymbol{\\mu}_{\\boldsymbol{x}}\u03bcx\u200b mux Prior mean at inputs x\\boldsymbol{x}x Kxx\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}Kxx\u200b Kxx Kernel Gram matrix at inputs x\\boldsymbol{x}x Lx\\mathbf{L}_{\\boldsymbol{x}}Lx\u200b Lx Lower Cholesky decomposition of Kxx\\boldsymbol{K}_{\\boldsymbol{x}\\boldsymbol{x}}Kxx\u200b Ktx\\mathbf{K}_{\\boldsymbol{t}\\boldsymbol{x}}Ktx\u200b Ktx Cross-covariance between inputs t\\boldsymbol{t}t and x\\boldsymbol{x}x"},{"location":"design/#sparse-gaussian-process-notation","title":"Sparse Gaussian process notation","text":"On paper GPJax code Description mmm m Number of inducing inputs z=(z1,\u2026,zm)\\boldsymbol{z} = (z_1,\\dotsc,z_{m})z=(z1\u200b,\u2026,zm\u200b) z Inducing inputs u=(u1,\u2026,um)\\boldsymbol{u} = (u_1,\\dotsc,u_{m})u=(u1\u200b,\u2026,um\u200b) u Inducing outputs"},{"location":"design/#package-style","title":"Package style","text":"<p>Prior to building GPJax, the developers of GPJax have benefited greatly from the GPFlow and GPyTorch packages. As such, many of the design principles in GPJax are inspired by the excellent precursory packages. Documentation designs have been greatly inspired by the exceptional Flax docs.</p> <ol> <li> <p>Rasmussen, C. E. and Williams, C. K. (2006) Gaussian Processes for Machine Learning. 3. MIT press Cambridge, MA.\u00a0\u21a9</p> </li> </ol>"},{"location":"give_me_the_code/","title":"Give me the code","text":""},{"location":"give_me_the_code/#count-data-regression","title":"Count data regression","text":"<pre><code>import blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\nfrom jax.config import config\nfrom jaxtyping import install_import_hook\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\ntfd = tfp.distributions\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey, subkey = jr.split(key)\nn = 50\nx = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\nf = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\ny = jr.poisson(key, jnp.exp(f(x)))\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[1])\nax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\nax.legend()\nkernel = gpx.RBF()\nmeanf = gpx.Constant()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Poisson(num_datapoints=D.n)\nposterior = prior * likelihood\nprint(type(posterior))\n# Adapted from BlackJax's introduction notebook.\nnum_adapt = 100\nnum_samples = 200\nlpd = jax.jit(gpx.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\nadapt = blackjax.window_adaptation(\nblackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n# Initialise the chain\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\ndef one_step(state, rng_key):\nstate, info = kernel(rng_key, state)\nreturn state, (state, info)\nkeys = jax.random.split(rng_key, num_samples)\n_, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\nreturn states, infos\n# Sample from the posterior distribution\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nacceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\nfig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.constrain().prior.kernel.variance)\nax1.plot(states.position.constrain().prior.kernel.lengthscale)\nax2.plot(states.position.constrain().prior.mean_function.constant)\nax0.set_title(\"Kernel variance\")\nax1.set_title(\"Kernel lengthscale\")\nax2.set_title(\"Mean function constant\")\nthin_factor = 10\nsamples = []\nfor i in range(num_adapt, num_samples + num_adapt, thin_factor):\nsample = jtu.tree_map(lambda samples: samples[i], states.position)\nsample = sample.constrain()\nlatent_dist = sample.predict(xtest, train_data=D)\npredictive_dist = sample.likelihood(latent_dist)\nsamples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\nsamples = jnp.vstack(samples)\nlower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(samples, axis=0)\nfig, ax = plt.subplots()\nax.plot(\nx, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7\n)\nax.plot(\nxtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1\n)\nax.fill_between(\nxtest.flatten(),\nlower_ci.flatten(),\nupper_ci.flatten(),\nalpha=0.2,\ncolor=cols[0],\nlabel=\"95% CI\",\n)\n%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Francesco Zanetta\"\n</code></pre>"},{"location":"give_me_the_code/#new-to-gaussian-processes","title":"New to Gaussian Processes?","text":"<pre><code>import warnings\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow_probability.substrates.jax as tfp\nfrom docs.examples.utils import confidence_ellipse\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ntfd = tfp.distributions\nud1 = tfd.Normal(0.0, 1.0)\nud2 = tfd.Normal(-1.0, 0.5)\nud3 = tfd.Normal(0.25, 1.5)\nxs = jnp.linspace(-5.0, 5.0, 500)\nfig, ax = plt.subplots()\nfor d in [ud1, ud2, ud3]:\nax.plot(\nxs,\njnp.exp(d.log_prob(xs)),\nlabel=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",\n)\nax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2)\nax.legend(loc=\"best\")\nkey = jr.PRNGKey(123)\nd1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2))\nd2 = tfd.MultivariateNormalTriL(\njnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]]))\n)\nd3 = tfd.MultivariateNormalTriL(\njnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]]))\n)\ndists = [d1, d2, d3]\nxvals = jnp.linspace(-5.0, 5.0, 500)\nyvals = jnp.linspace(-5.0, 5.0, 500)\nxx, yy = jnp.meshgrid(xvals, yvals)\npos = jnp.empty(xx.shape + (2,))\npos.at[:, :, 0].set(xx)\npos.at[:, :, 1].set(yy)\nfig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\ntitles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)\nfor a, t, d in zip([ax0, ax1, ax2], titles, dists):\nd_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(\nxx.shape\n)\ncntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)\nfor c in cntf.collections:\nc.set_edgecolor(\"face\")\na.set_xlim(-2.75, 2.75)\na.set_ylim(-2.75, 2.75)\nsamples = d.sample(seed=key, sample_shape=(5000,))\nxsample, ysample = samples[:, 0], samples[:, 1]\nconfidence_ellipse(\nxsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n)\nconfidence_ellipse(\nxsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"\n)\na.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)\na.set(xlabel=\"x\", ylabel=\"y\", title=t)\nn = 1000\nx = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,))\nkey, subkey = jr.split(key)\ny = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,))\nkey, subkey = jr.split(subkey)\nxfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\nyfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\ndf = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\ng = sns.jointplot(\ndata=df,\nx=\"x\",\ny=\"y\",\nhue=\"idx\",\nmarker=\".\",\nspace=0.0,\nxlim=(-4.0, 4.0),\nylim=(-4.0, 4.0),\nheight=4,\nmarginal_ticks=False,\nlegend=False,\npalette=\"inferno\",\nmarginal_kws={\n\"fill\": True,\n\"linewidth\": 1,\n\"color\": cols[1],\n\"alpha\": 0.3,\n\"bw_adjust\": 2,\n\"cmap\": cmap,\n},\njoint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},\n)\ng.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\ng.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\ng.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\nconfidence_ellipse(\nxfull,\nyfull,\ng.ax_joint,\nedgecolor=\"#3f3f3f\",\nn_std=1.0,\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nconfidence_ellipse(\nxfull,\nyfull,\ng.ax_joint,\nedgecolor=\"#3f3f3f\",\nn_std=2.0,\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nconfidence_ellipse(\nxfull,\nyfull,\ng.ax_joint,\nedgecolor=\"#3f3f3f\",\nn_std=3.0,\nlinestyle=\"--\",\nlinewidth=0.5,\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#sparse-gaussian-process-regression","title":"Sparse Gaussian Process Regression","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 2500\nnoise = 0.5\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\nytest = f(xtest)\nn_inducing = 50\nz = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\nax.vlines(\nx=z,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=0.5,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend(loc=\"best\")\nplt.show()\nmeanf = gpx.Constant()\nkernel = gpx.RBF()\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior * likelihood\nq = gpx.CollapsedVariationalGaussian(posterior=posterior, inducing_inputs=z)\nelbo = gpx.CollapsedELBO(negative=True)\nprint(gpx.cite(elbo))\nelbo = jit(elbo)\nopt_posterior, history = gpx.fit(\nmodel=q,\nobjective=elbo,\ntrain_data=D,\noptim=ox.adamw(learning_rate=1e-2),\nnum_iters=500,\nkey=key,\n)\nfig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")\nlatent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\ninducing_points = opt_posterior.inducing_inputs\nsamples = latent_dist.sample(seed=key, sample_shape=(20,))\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\nax.plot(\nxtest,\nytest,\nlabel=\"Latent function\",\ncolor=cols[1],\nlinestyle=\"-\",\nlinewidth=1,\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nax.plot(\nxtest,\npredictive_mean + 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=0.5,\n)\nax.vlines(\nx=inducing_points,\nymin=ytest.min(),\nymax=ytest.max(),\nalpha=0.3,\nlinewidth=0.5,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nplt.show()\nfull_rank_model = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.RBF()) * gpx.Gaussian(\nnum_datapoints=D.n\n)\nnegative_mll = jit(gpx.ConjugateMLL(negative=True))\n%timeit negative_mll(full_rank_model, D).block_until_ready()\nnegative_elbo = jit(gpx.CollapsedELBO(negative=True))\n%timeit negative_elbo(q, D).block_until_ready()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Daniel Dodd'\n</code></pre>"},{"location":"give_me_the_code/#pathwise-sampling-for-spatial-modelling","title":"Pathwise Sampling for Spatial Modelling","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import dataclass\nimport fsspec\nimport geopandas as gpd\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nimport planetary_computer\nimport pystac_client\nimport rioxarray as rio\nfrom rioxarray.merge import merge_arrays\nimport xarray as xr\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.base import param_field\nfrom gpjax.dataset import Dataset\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n# Observed temperature data\ntry:\ntemperature = pd.read_csv(\"data/max_tempeature_switzerland.csv\")\nexcept FileNotFoundError:\ntemperature = pd.read_csv(\"docs/examples/data/max_tempeature_switzerland.csv\")\ntemperature = gpd.GeoDataFrame(\ntemperature,\ngeometry=gpd.points_from_xy(temperature.longitude, temperature.latitude),\n).dropna(how=\"any\")\n# Country borders shapefile\npath = \"simplecache::https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\"\nwith fsspec.open(path) as file:\nch_shp = gpd.read_file(file).query(\"ADMIN == 'Switzerland'\")\n# Read DEM data and clip it to switzerland\ncatalog = pystac_client.Client.open(\n\"https://planetarycomputer.microsoft.com/api/stac/v1\",\nmodifier=planetary_computer.sign_inplace,\n)\nsearch = catalog.search(collections=[\"cop-dem-glo-90\"], bbox=[5.5, 45.5, 10.0, 48.5])\nitems = list(search.get_all_items())\ntiles = [rio.open_rasterio(i.assets[\"data\"].href).squeeze().drop(\"band\") for i in items]\ndem = merge_arrays(tiles).coarsen(x=10, y=10).mean().rio.clip(ch_shp[\"geometry\"])\nfig, ax = plt.subplots(figsize=(8, 5), layout=\"constrained\")\ndem.plot(\ncmap=\"terrain\", cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Elevation [m]\"}\n)\ntemperature.plot(\"t_max\", ax=ax, cmap=\"RdBu_r\", vmin=-15, vmax=15, edgecolor=\"k\", s=50)\nax.set(title=\"Switzerland's topography and SwissMetNet stations\", aspect=\"auto\")\ncb = fig.colorbar(ax.collections[-1], aspect=50, pad=0.02)\ncb.set_label(\"Max. daily temperature [\u00b0C]\", labelpad=-2)\nx = temperature[[\"latitude\", \"longitude\", \"elevation\"]].values\ny = temperature[[\"t_max\"]].values\nD = Dataset(\nX=jnp.array(x),\ny=jnp.array(y),\n)\nkernel = gpx.kernels.RBF(\nactive_dims=[0, 1, 2],\nlengthscale=jnp.array([0.1, 0.1, 100.0]),\n)\n@dataclass\nclass MeanFunction(gpx.gps.AbstractMeanFunction):\nw: Float[Array, \"1\"] = param_field(jnp.array([0.0]))\nb: Float[Array, \"1\"] = param_field(jnp.array([0.0]))\ndef __call__(self, x: Float[Array, \"N D\"]) -&gt; Float[Array, \"N 1\"]:\nelevation = x[:, 2:3]\nout = elevation * self.w + self.b\nreturn out\nmean_function = MeanFunction()\nprior = gpx.Prior(kernel=kernel, mean_function=mean_function)\nlikelihood = gpx.Gaussian(D.n)\nposterior = prior * likelihood\nnegative_mll = jax.jit(gpx.objectives.ConjugateMLL(negative=True))\nnegative_mll(posterior, train_data=D)\noptim = ox.chain(ox.adam(learning_rate=0.1), ox.clip(1.0))\nposterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=D,\noptim=optim,\nnum_iters=3000,\nsafe=True,\nkey=key,\n)\nposterior: gpx.gps.ConjugatePosterior\n# select the target pixels and exclude nans\nxtest = dem.drop(\"spatial_ref\").stack(p=[\"y\", \"x\"]).to_dataframe(name=\"dem\")\nmask = jnp.any(jnp.isnan(xtest.values), axis=-1)\n# generate 50 samples\nytest = posterior.sample_approx(50, D, key, num_features=200)(\njnp.array(xtest.values[~mask])\n)\npredtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan\npredtest[~mask] = ytest.mean(axis=-1)\npredtest = predtest.unstack()\npredtest.plot(\nvmin=-15.0,\nvmax=15.0,\ncmap=\"RdBu_r\",\ncbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"},\n)\nplt.gca().set_title(\"Interpolated maximum daily temperature\")\npredtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan\npredtest[~mask] = ytest.std(axis=-1)\npredtest = predtest.unstack()\n# plot\npredtest.plot(\ncbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Standard deviation [\u00b0C]\"},\n)\nplt.gca().set_title(\"Standard deviation\")\npredtest = (\nxr.zeros_like(dem.stack(p=[\"y\", \"x\"]))\n.expand_dims(realization=range(9))\n.transpose(\"p\", \"realization\")\n.copy()\n)\npredtest[~mask] = ytest[:, :9]\npredtest = predtest.unstack()\npredtest.plot(\ncol=\"realization\",\ncol_wrap=3,\ncbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"},\n)\nprint(posterior.prior.mean_function)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Francesco Zanetta'\n</code></pre>"},{"location":"give_me_the_code/#deep-kernel-learning","title":"Deep Kernel Learning","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import (\ndataclass,\nfield,\n)\nfrom typing import Any\nimport flax\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom scipy.signal import sawtooth\nfrom gpjax.base import static_field\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.base import param_field\nimport gpjax.kernels as jk\nfrom gpjax.kernels import DenseKernelComputation\nfrom gpjax.kernels.base import AbstractKernel\nfrom gpjax.kernels.computations import AbstractKernelComputation\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 500\nnoise = 0.2\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nytest = f(xtest)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\nax.plot(xtest, ytest, label=\"True function\")\nax.legend(loc=\"best\")\n@dataclass\nclass DeepKernelFunction(AbstractKernel):\nbase_kernel: AbstractKernel = None\nnetwork: nn.Module = static_field(None)\ndummy_x: jax.Array = static_field(None)\nkey: jr.PRNGKeyArray = static_field(jr.PRNGKey(123))\nnn_params: Any = field(init=False, repr=False)\ndef __post_init__(self):\nif self.base_kernel is None:\nraise ValueError(\"base_kernel must be specified\")\nif self.network is None:\nraise ValueError(\"network must be specified\")\nself.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))\ndef __call__(\nself, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n) -&gt; Float[Array, \"1\"]:\nstate = self.network.init(self.key, x)\nxt = self.network.apply(state, x)\nyt = self.network.apply(state, y)\nreturn self.base_kernel(xt, yt)\nfeature_space_dim = 3\nclass Network(nn.Module):\n\"\"\"A simple MLP.\"\"\"\n@nn.compact\ndef __call__(self, x):\nx = nn.Dense(features=32)(x)\nx = nn.relu(x)\nx = nn.Dense(features=64)(x)\nx = nn.relu(x)\nx = nn.Dense(features=feature_space_dim)(x)\nreturn x\nforward_linear = Network()\nbase_kernel = gpx.Matern52(active_dims=list(range(feature_space_dim)))\nkernel = DeepKernelFunction(\nnetwork=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x\n)\nmeanf = gpx.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\nschedule = ox.warmup_cosine_decay_schedule(\ninit_value=0.0,\npeak_value=0.01,\nwarmup_steps=75,\ndecay_steps=700,\nend_value=0.0,\n)\noptimiser = ox.chain(\nox.clip(1.0),\nox.adamw(learning_rate=schedule),\n)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=jax.jit(gpx.ConjugateMLL(negative=True)),\ntrain_data=D,\noptim=optimiser,\nnum_iters=800,\nkey=key,\n)\nlatent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\npredictive_mean + 2 * predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#introduction-to-bayesian-optimisation","title":"Introduction to Bayesian Optimisation","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float, Int\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom typing import List, Tuple\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.typing import Array, FunctionalSample, ScalarFloat\nfrom jaxopt import ScipyBoundedMinimize\nkey = jr.PRNGKey(42)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ndef forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:\nreturn (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\nlower_bound = jnp.array([0.0])\nupper_bound = jnp.array([1.0])\ninitial_sample_num = 5\ninitial_x = tfp.mcmc.sample_halton_sequence(\ndim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\ndef return_optimised_posterior(\ndata: gpx.Dataset, prior: gpx.Module, key: Array\n) -&gt; gpx.Module:\nlikelihood = gpx.Gaussian(\nnum_datapoints=data.n, obs_noise=jnp.array(1e-6)\n)  # Our function is noise-free, so we set the observation noise to a very small value\nlikelihood = likelihood.replace_trainable(obs_noise=False)\nposterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=data)\nnegative_mll = jit(negative_mll)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=data,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=1000,\nsafe=True,\nkey=key,\nverbose=False,\n)\nreturn opt_posterior\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, key)\napprox_sample = opt_posterior.sample_approx(\nnum_samples=1, train_data=D, key=key, num_features=500\n)\nutility_fn = lambda x: approx_sample(x)[0][0]\ndef optimise_sample(\nsample: FunctionalSample,\nkey: Int[Array, \"\"],\nlower_bound: Float[Array, \"D\"],\nupper_bound: Float[Array, \"D\"],\nnum_initial_sample_points: int,\n) -&gt; ScalarFloat:\ninitial_sample_points = jr.uniform(\nkey,\nshape=(num_initial_sample_points, lower_bound.shape[0]),\ndtype=jnp.float64,\nminval=lower_bound,\nmaxval=upper_bound,\n)\ninitial_sample_y = sample(initial_sample_points)\nbest_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])\n# We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.\nnegative_utility_fn = lambda x: sample(x)[0][0]\nlbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")\nbounds = (lower_bound, upper_bound)\nx_star = lbfgsb.run(best_x, bounds=bounds).params\nreturn x_star\nx_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100)\ny_star = forrester(x_star)\ndef plot_bayes_opt(\nposterior: gpx.Module,\nsample: FunctionalSample,\ndataset: gpx.Dataset,\nqueried_x: ScalarFloat,\n) -&gt; None:\nplt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\nforrester_y = forrester(plt_x)\nsample_y = sample(plt_x)\nlatent_dist = posterior.predict(plt_x, train_data=dataset)\npredictive_dist = posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])\nax.fill_between(\nplt_x.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\nplt_x,\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nplt_x,\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(plt_x, sample_y, label=\"Posterior Sample\")\nax.plot(\nplt_x,\nforrester_y,\nlabel=\"Forrester Function\",\ncolor=cols[0],\nlinestyle=\"--\",\nlinewidth=2,\n)\nax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\nax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\nax.scatter(\nqueried_x,\nsample(queried_x),\nlabel=\"Posterior Sample Optimum\",\nmarker=\"*\",\ncolor=cols[3],\nzorder=3,\n)\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nplt.show()\nplot_bayes_opt(opt_posterior, approx_sample, D, x_star)\nbo_iters = 5\n# Set up initial dataset\ninitial_x = tfp.mcmc.sample_halton_sequence(\ndim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\nfor i in range(bo_iters):\nkey, subkey = jr.split(key)\n# Generate optimised posterior using previously observed data\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, subkey)\n# Draw a sample from the posterior, and find the minimiser of it\napprox_sample = opt_posterior.sample_approx(\nnum_samples=1, train_data=D, key=subkey, num_features=500\n)\nx_star = optimise_sample(\napprox_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100\n)\nplot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n# Evaluate the black-box function at the best point observed so far, and add it to the dataset\ny_star = forrester(x_star)\nprint(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")\nD = D + gpx.Dataset(X=x_star, y=y_star)\nfig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\ncumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y)\nax.plot(fn_evaluations, cumulative_best_y)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.axhline(y=-6.0207, linestyle=\"--\", label=\"True Minimum\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Best Observed Value\")\nax.legend()\nplt.show()\ndef six_hump_camel(x: Float[Array, \"N 2\"]) -&gt; Float[Array, \"N 1\"]:\nx1 = x[..., :1]\nx2 = x[..., 1:]\nterm1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2\nterm2 = x1 * x2\nterm3 = (-4 + 4 * x2**2) * x2**2\nreturn term1 + term2 + term3\nx1 = jnp.linspace(-2, 2, 100)\nx2 = jnp.linspace(-1, 1, 100)\nx1, x2 = jnp.meshgrid(x1, x2)\nx = jnp.stack([x1.flatten(), x2.flatten()], axis=1)\ny = six_hump_camel(x)\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurf = ax.plot_surface(\nx1,\nx2,\ny.reshape(x1.shape[0], x2.shape[0]),\nlinewidth=0,\ncmap=cm.coolwarm,\nantialiased=False,\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nplt.show()\nx_star_one = jnp.array([[0.0898, -0.7126]])\nx_star_two = jnp.array([[-0.0898, 0.7126]])\nfig, ax = plt.subplots()\ncontour_plot = ax.contourf(\nx1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\nx_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\"\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2])\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\nlower_bound = jnp.array([-2.0, -1.0])\nupper_bound = jnp.array([2.0, 1.0])\ninitial_sample_num = 5\nbo_iters = 11\nnum_experiments = 5\nbo_experiment_results = []\nfor experiment in range(num_experiments):\nprint(f\"Starting Experiment: {experiment + 1}\")\n# Set up initial dataset\ninitial_x = tfp.mcmc.sample_halton_sequence(\ndim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n)\ninitial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)\ninitial_y = six_hump_camel(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\nfor i in range(bo_iters):\nkey, subkey = jr.split(key)\n# Generate optimised posterior\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\nactive_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0\n)\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, subkey)\n# Draw a sample from the posterior, and find the minimiser of it\napprox_sample = opt_posterior.sample_approx(\nnum_samples=1, train_data=D, key=subkey, num_features=500\n)\nx_star = optimise_sample(\napprox_sample,\nsubkey,\nlower_bound,\nupper_bound,\nnum_initial_sample_points=1000,\n)\n# Evaluate the black-box function at the best point observed so far, and add it to the dataset\ny_star = six_hump_camel(x_star)\nprint(\nf\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value: {y_star}\"\n)\nD = D + gpx.Dataset(X=x_star, y=y_star)\nbo_experiment_results.append(D)\nrandom_experiment_results = []\nfor i in range(num_experiments):\nkey, subkey = jr.split(key)\ninitial_x = bo_experiment_results[i].X[:5]\ninitial_y = bo_experiment_results[i].y[:5]\nfinal_x = jr.uniform(\nkey,\nshape=(bo_iters, 2),\ndtype=jnp.float64,\nminval=lower_bound,\nmaxval=upper_bound,\n)\nfinal_y = six_hump_camel(final_x)\nrandom_x = jnp.concatenate([initial_x, final_x], axis=0)\nrandom_y = jnp.concatenate([initial_y, final_y], axis=0)\nrandom_experiment_results.append(gpx.Dataset(X=random_x, y=random_y))\ndef obtain_log_regret_statistics(\nexperiment_results: List[gpx.Dataset],\nglobal_minimum: ScalarFloat,\n) -&gt; Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:\nlog_regret_results = []\nfor exp_result in experiment_results:\nobservations = exp_result.y\ncumulative_best_observations = jax.lax.associative_scan(\njax.numpy.minimum, observations\n)\nregret = cumulative_best_observations - global_minimum\nlog_regret = jnp.log(regret)\nlog_regret_results.append(log_regret)\nlog_regret_results = jnp.array(log_regret_results)\nlog_regret_mean = jnp.mean(log_regret_results, axis=0)\nlog_regret_std = jnp.std(log_regret_results, axis=0)\nreturn log_regret_mean, log_regret_std\nbo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(\nbo_experiment_results, -1.031625\n)\n(\nrandom_log_regret_mean,\nrandom_log_regret_std,\n) = obtain_log_regret_statistics(random_experiment_results, -1.031625)\nfig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\nax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\")\nax.fill_between(\nfn_evaluations,\nbo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],\nbo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],\nalpha=0.2,\n)\nax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\")\nax.fill_between(\nfn_evaluations,\nrandom_log_regret_mean[:, 0] - random_log_regret_std[:, 0],\nrandom_log_regret_mean[:, 0] + random_log_regret_std[:, 0],\nalpha=0.2,\n)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Log Regret\")\nax.legend()\nplt.show()\nfig, ax = plt.subplots()\ncontour_plot = ax.contourf(\nx1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\nx_star_one[0][0],\nx_star_one[0][1],\nmarker=\"*\",\ncolor=cols[2],\nlabel=\"Global Minimum\",\nzorder=2,\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2)\nax.scatter(\nbo_experiment_results[0].X[:, 0],\nbo_experiment_results[0].X[:, 1],\nmarker=\"x\",\ncolor=cols[1],\nlabel=\"Bayesian Optimisation Queries\",\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</code></pre>"},{"location":"give_me_the_code/#sparse-stochastic-variational-inference","title":"Sparse Stochastic Variational Inference","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nimport gpjax.kernels as jk\nkey = jr.PRNGKey(123)\ntfb = tfp.bijectors\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 50000\nnoise = 0.2\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)\nz = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.vlines(\nz,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=1,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\nax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.Gaussian(num_datapoints=n)\nprior = gpx.Prior(mean_function=meanf, kernel=jk.RBF())\np = prior * likelihood\nq = gpx.VariationalGaussian(posterior=p, inducing_inputs=z)\nnegative_elbo = gpx.ELBO(negative=True)\nprint(gpx.cite(negative_elbo))\nnegative_elbo = jit(negative_elbo)\nschedule = ox.warmup_cosine_decay_schedule(\ninit_value=0.0,\npeak_value=0.01,\nwarmup_steps=75,\ndecay_steps=1500,\nend_value=0.001,\n)\nopt_posterior, history = gpx.fit(\nmodel=q,\nobjective=negative_elbo,\ntrain_data=D,\noptim=ox.adam(learning_rate=schedule),\nnum_iters=3000,\nkey=jr.PRNGKey(42),\nbatch_size=128,\n)\nlatent_dist = opt_posterior(xtest)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\nxtest.flatten(),\nmeanf - 2 * sigma,\nmeanf + 2 * sigma,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.vlines(\nopt_posterior.inducing_inputs,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=1,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend()\ntriangular_transform = tfb.FillScaleTriL(\ndiag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter)\n)\nreparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform)\nopt_rep, history = gpx.fit(\nmodel=reparameterised_q,\nobjective=negative_elbo,\ntrain_data=D,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=3000,\nkey=jr.PRNGKey(42),\nbatch_size=128,\n)\nlatent_dist = opt_rep(xtest)\npredictive_dist = opt_rep.posterior.likelihood(latent_dist)\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\nxtest.flatten(),\nmeanf - 2 * sigma,\nmeanf + 2 * sigma,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Two sigma\",\n)\nax.vlines(\nopt_rep.inducing_inputs,\nymin=y.min(),\nymax=y.max(),\nalpha=0.3,\nlinewidth=1,\nlabel=\"Inducing point\",\ncolor=cols[2],\n)\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'\n</code></pre>"},{"location":"give_me_the_code/#gaussian-processes-barycentres","title":"Gaussian Processes Barycentres","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport typing as tp\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy.linalg as jsl\nfrom jaxtyping import install_import_hook\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax.distributions as tfd\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 100\nn_test = 200\nn_datasets = 5\nx = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1)\nxtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1)\nf = lambda x, a, b: a + jnp.sin(b * x)\nys = []\nfor _i in range(n_datasets):\nkey, subkey = jr.split(key)\nvertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)\nperiod = jr.uniform(subkey, minval=0.75, maxval=1.25)\nnoise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)\nnoise = jr.normal(subkey, shape=x.shape) * noise_amount\nys.append(f(x, vertical_shift, period) + noise)\ny = jnp.hstack(ys)\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\")\nplt.show()\ndef fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:\nif y.ndim == 1:\ny = y.reshape(-1, 1)\nD = gpx.Dataset(X=x, y=y)\nlikelihood = gpx.Gaussian(num_datapoints=n)\nposterior = gpx.Prior(mean_function=gpx.Constant(), kernel=gpx.RBF()) * likelihood\nopt_posterior, _ = gpx.fit(\nmodel=posterior,\nobjective=jax.jit(gpx.ConjugateMLL(negative=True)),\ntrain_data=D,\noptim=ox.adamw(learning_rate=0.01),\nnum_iters=500,\nkey=key,\n)\nlatent_dist = opt_posterior.predict(xtest, train_data=D)\nreturn opt_posterior.likelihood(latent_dist)\nposterior_preds = [fit_gp(x, i) for i in ys]\ndef sqrtm(A: jax.Array):\nreturn jnp.real(jsl.sqrtm(A))\ndef wasserstein_barycentres(\ndistributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array\n):\ncovariances = [d.covariance() for d in distributions]\ncov_stack = jnp.stack(covariances)\nstack_sqrt = jax.vmap(sqrtm)(cov_stack)\ndef step(covariance_candidate: jax.Array, idx: None):\ninner_term = jax.vmap(sqrtm)(\njnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)\n)\nfixed_point = jnp.tensordot(weights, inner_term, axes=1)\nreturn fixed_point, fixed_point\nreturn step\nweights = jnp.ones((n_datasets,)) / n_datasets\nmeans = jnp.stack([d.mean() for d in posterior_preds])\nbarycentre_mean = jnp.tensordot(weights, means, axes=1)\nstep_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights))\ninitial_covariance = jnp.eye(n_test)\nbarycentre_covariance, sequence = jax.lax.scan(\nstep_fn, initial_covariance, jnp.arange(100)\n)\nL = jnp.linalg.cholesky(barycentre_covariance)\nbarycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L)\ndef plot(\ndist: tfd.MultivariateNormalTriL,\nax,\ncolor: str,\nlabel: str = None,\nci_alpha: float = 0.2,\nlinewidth: float = 1.0,\nzorder: int = 0,\n):\nmu = dist.mean()\nsigma = dist.stddev()\nax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)\nax.fill_between(\nxtest.squeeze(),\nmu - sigma,\nmu + sigma,\nalpha=ci_alpha,\ncolor=color,\nzorder=zorder,\n)\nfig, ax = plt.subplots()\n[plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds]\nplot(\nbarycentre_process,\nax,\ncolor=cols[0],\nlabel=\"Barycentre\",\nci_alpha=0.5,\nlinewidth=2,\nzorder=1,\n)\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#likelihood-guide","title":"Likelihood guide","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport gpjax as gpx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey = jr.PRNGKey(123)\nn = 50\nx = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0)\nxtest = jnp.linspace(-3, 3, 100)[:, None]\nf = lambda x: jnp.sin(x)\ny = f(x) + 0.1 * jr.normal(key, shape=x.shape)\nD = gpx.Dataset(x, y)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\")\nax.plot(x, f(x), label=\"Latent function\")\nax.legend()\ngpx.likelihoods.Gaussian(num_datapoints=D.n)\ngpx.likelihoods.Gaussian(num_datapoints=D.n, obs_noise=0.5)\nkernel = gpx.Matern32()\nmeanf = gpx.Zero()\nprior = gpx.Prior(kernel=kernel, mean_function=meanf)\nlikelihood = gpx.Gaussian(num_datapoints=D.n, obs_noise=0.1)\nposterior = prior * likelihood\nlatent_dist = posterior.predict(xtest, D)\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\nfor ax in axes.ravel():\nsubkey, _ = jr.split(subkey)\nax.plot(\nlatent_dist.sample(sample_shape=(1,), seed=subkey).T,\nlw=1,\ncolor=cols[0],\nlabel=\"Latent samples\",\n)\nax.plot(\nlikelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n\"o\",\nmarkersize=5,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Predictive samples\",\n)\nlikelihood = gpx.Bernoulli(num_datapoints=D.n)\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\nfor ax in axes.ravel():\nsubkey, _ = jr.split(subkey)\nax.plot(\nlatent_dist.sample(sample_shape=(1,), seed=subkey).T,\nlw=1,\ncolor=cols[0],\nlabel=\"Latent samples\",\n)\nax.plot(\nlikelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n\"o\",\nmarkersize=3,\nalpha=0.5,\ncolor=cols[1],\nlabel=\"Predictive samples\",\n)\nz = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1)\nq = gpx.VariationalGaussian(posterior=posterior, inducing_inputs=z)\ndef q_moments(x):\nqx = q(x)\nreturn qx.mean(), qx.variance()\nmean, variance = jax.vmap(q_moments)(x[:, None])\njnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance))\nlquad = gpx.Gaussian(\nnum_datapoints=D.n,\nobs_noise=jnp.array([0.1]),\nintegrator=gpx.integrators.GHQuadratureIntegrator(num_points=20),\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#uci-data-benchmarking","title":"UCI Data Benchmarking","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax as ox\nimport pandas as pd\nfrom sklearn.metrics import (\nmean_squared_error,\nr2_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\n# Enable Float64 for more stable matrix inversions.\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ntry:\nyacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\nexcept FileNotFoundError:\nyacht = pd.read_fwf(\n\"docs/examples/data/yacht_hydrodynamics.data\", header=None\n).values[:-1, :]\nX = yacht[:, :-1]\ny = yacht[:, -1].reshape(-1, 1)\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\nlog_ytr = np.log(ytr)\nlog_yte = np.log(yte)\ny_scaler = StandardScaler().fit(log_ytr)\nscaled_ytr = y_scaler.transform(log_ytr)\nscaled_yte = y_scaler.transform(log_yte)\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\nax[0].hist(ytr, bins=30, color=cols[1])\nax[0].set_title(\"y\")\nax[1].hist(log_ytr, bins=30, color=cols[1])\nax[1].set_title(\"log(y)\")\nax[2].hist(scaled_ytr, bins=30, color=cols[1])\nax[2].set_title(\"scaled log(y)\")\nx_scaler = StandardScaler().fit(Xtr)\nscaled_Xtr = x_scaler.transform(Xtr)\nscaled_Xte = x_scaler.transform(Xte)\nn_train, n_covariates = scaled_Xtr.shape\nkernel = gpx.RBF(active_dims=list(range(n_covariates)))\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=n_train)\nposterior = prior * likelihood\ntraining_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\nnegative_mll = jit(gpx.ConjugateMLL(negative=True))\noptimiser = ox.adamw(0.05)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=training_data,\noptim=ox.adamw(learning_rate=0.05),\nnum_iters=500,\nkey=key,\n)\nlatent_dist = opt_posterior(scaled_Xte, training_data)\npredictive_dist = likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_stddev = predictive_dist.stddev()\nrmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nr2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nprint(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")\nresiduals = scaled_yte.squeeze() - predictive_mean\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\nax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\nax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\nax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\nax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\nax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\nax[1].set_ylim([-1.0, 1.0])\nax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\nax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\nax[2].set_title(\"Residuals\")\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#graph-kernels","title":"Graph Kernels","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport random\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport optax as ox\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nvertex_per_side = 20\nn_edges_to_remove = 30\np = 0.8\nG = nx.barbell_graph(vertex_per_side, 0)\nrandom.seed(123)\n[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\npos = nx.spring_layout(G, seed=123)  # positions for all nodes\nnx.draw(\nG, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n)\nL = nx.laplacian_matrix(G).toarray()\nx = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\ntrue_kernel = gpx.GraphKernel(\nlaplacian=L,\nlengthscale=2.3,\nvariance=3.2,\nsmoothness=6.1,\n)\nprior = gpx.Prior(mean_function=gpx.Zero(), kernel=true_kernel)\nfx = prior(x)\ny = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)\nD = gpx.Dataset(X=x, y=y)\nnx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\nvmin, vmax = y.min(), y.max()\nsm = plt.cm.ScalarMappable(\ncmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\ncbar = plt.colorbar(sm)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nkernel = gpx.GraphKernel(laplacian=L)\nprior = gpx.Prior(mean_function=gpx.Zero(), kernel=kernel)\nposterior = prior * likelihood\nprint(gpx.cite(kernel))\nopt_posterior, training_history = gpx.fit(\nmodel=posterior,\nobjective=jit(gpx.ConjugateMLL(negative=True)),\ntrain_data=D,\noptim=ox.adamw(learning_rate=0.01),\nnum_iters=1000,\nkey=key,\n)\ninitial_dist = likelihood(posterior(x, D))\npredictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\ninitial_mean = initial_dist.mean()\nlearned_mean = predictive_dist.mean()\nrmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\ninitial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\nlearned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\nprint(\nf\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\nf\" {learned_rmse: .2f}\"\n)\nerror = jnp.abs(learned_mean - y.squeeze())\nnx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\nvmin, vmax = error.min(), error.max()\nsm = plt.cm.ScalarMappable(\ncmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\ncbar = plt.colorbar(sm)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#kernel-guide","title":"Kernel Guide","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom dataclasses import dataclass\nfrom typing import Dict\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax as ox\nfrom simple_pytree import static_field\nimport tensorflow_probability.substrates.jax as tfp\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.base.param import param_field\nkey = jr.PRNGKey(123)\ntfb = tfp.bijectors\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkernels = [\ngpx.kernels.Matern12(),\ngpx.kernels.Matern32(),\ngpx.kernels.Matern52(),\ngpx.kernels.RBF(),\ngpx.kernels.Polynomial(),\ngpx.kernels.Polynomial(degree=2),\n]\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nmeanf = gpx.mean_functions.Zero()\nfor k, ax in zip(kernels, axes.ravel()):\nprior = gpx.Prior(mean_function=meanf, kernel=k)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nax.plot(x, y.T, alpha=0.7)\nax.set_title(k.name)\nslice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3])\nprint(f\"Lengthscales: {slice_kernel.lengthscale}\")\n# Inputs\nx_matrix = jr.normal(key, shape=(50, 5))\n# Compute the Gram matrix\nK = slice_kernel.gram(x_matrix)\nprint(K.shape)\nk1 = gpx.kernels.RBF()\nk2 = gpx.kernels.Polynomial()\nsum_k = gpx.kernels.SumKernel(kernels=[k1, k2])\nfig, ax = plt.subplots(ncols=3, figsize=(9, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(sum_k.gram(x).to_dense())\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nk3 = gpx.kernels.Matern32()\nprod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])\nfig, ax = plt.subplots(ncols=4, figsize=(12, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(k3.gram(x).to_dense())\nim3 = ax[3].matshow(prod_k.gram(x).to_dense())\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nfig.colorbar(im3, ax=ax[3], fraction=0.05)\ndef angular_distance(x, y, c):\nreturn jnp.abs((x - y + c) % (c * 2) - c)\nbij = tfb.Chain([tfb.Softplus(), tfb.Shift(np.array(4.0).astype(np.float64))])\n@dataclass\nclass Polar(gpx.kernels.AbstractKernel):\nperiod: float = static_field(2 * jnp.pi)\ntau: float = param_field(jnp.array([4.0]), bijector=bij)\ndef __call__(\nself, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]\n) -&gt; Float[Array, \"1\"]:\nc = self.period / 2.0\nt = angular_distance(x, y, c)\nK = (1 + self.tau * t / c) * jnp.clip(1 - t / c, 0, jnp.inf) ** self.tau\nreturn K.squeeze()\n# Simulate data\nangles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1)\nn = 20\nnoise = 0.2\nX = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0)\ny = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise\nD = gpx.Dataset(X=X, y=y)\n# Define polar Gaussian process\nPKern = Polar()\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.Gaussian(num_datapoints=n)\ncircular_posterior = gpx.Prior(mean_function=meanf, kernel=PKern) * likelihood\n# Optimise GP's marginal log-likelihood using Adam\nopt_posterior, history = gpx.fit(\nmodel=circular_posterior,\nobjective=jit(gpx.ConjugateMLL(negative=True)),\ntrain_data=D,\noptim=ox.adamw(learning_rate=0.05),\nnum_iters=500,\nkey=key,\n)\nposterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D))\nmu = posterior_rv.mean()\none_sigma = posterior_rv.stddev()\nfig = plt.figure(figsize=(7, 3.5))\ngridspec = fig.add_gridspec(1, 1)\nax = plt.subplot(gridspec[0], polar=True)\nax.fill_between(\nangles.squeeze(),\nmu - one_sigma,\nmu + one_sigma,\nalpha=0.3,\nlabel=r\"1 Posterior s.d.\",\ncolor=cols[1],\nlw=0,\n)\nax.fill_between(\nangles.squeeze(),\nmu - 3 * one_sigma,\nmu + 3 * one_sigma,\nalpha=0.15,\nlabel=r\"3 Posterior s.d.\",\ncolor=cols[1],\nlw=0,\n)\nax.plot(angles, mu, label=\"Posterior mean\")\nax.scatter(D.X, D.y, alpha=1, label=\"Observations\")\nax.legend()\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</code></pre>"},{"location":"give_me_the_code/#introduction-to-kernels","title":"Introduction to Kernels","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nfrom docs.examples.utils import clean_legend\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nfrom gpjax.typing import Array\nfrom sklearn.preprocessing import StandardScaler\nkey = jr.PRNGKey(42)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkernels = [\ngpx.kernels.Matern12(),\ngpx.kernels.Matern32(),\ngpx.kernels.Matern52(),\ngpx.kernels.RBF(),\n]\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nmeanf = gpx.mean_functions.Zero()\nfor k, ax in zip(kernels, axes.ravel()):\nprior = gpx.Prior(mean_function=meanf, kernel=k)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nax.plot(x, y.T, alpha=0.7)\nax.set_title(k.name)\n# Forrester function\ndef forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:\nreturn (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\nn = 5\ntraining_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1)\ntraining_y = forrester(training_x)\nD = gpx.Dataset(X=training_x, y=training_y)\ntest_x = jnp.linspace(0, 1, 100).reshape(-1, 1)\ntest_y = forrester(test_x)\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\nlengthscale=jnp.array(2.0)\n)  # Initialise our kernel lengthscale to 2.0\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nlikelihood = gpx.Gaussian(\nnum_datapoints=D.n, obs_noise=jnp.array(1e-6)\n)  # Our function is noise-free, so we set the observation noise to a very small value\nlikelihood = likelihood.replace_trainable(obs_noise=False)\nno_opt_posterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(no_opt_posterior, train_data=D)\nnegative_mll = jit(negative_mll)\nopt_posterior, history = gpx.fit(\nmodel=no_opt_posterior,\nobjective=negative_mll,\ntrain_data=D,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=2000,\nsafe=True,\nkey=key,\n)\nopt_latent_dist = opt_posterior.predict(test_x, train_data=D)\nopt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)\nopt_predictive_mean = opt_predictive_dist.mean()\nopt_predictive_std = opt_predictive_dist.stddev()\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6))\nax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax1.fill_between(\ntest_x.squeeze(),\nopt_predictive_mean - 2 * opt_predictive_std,\nopt_predictive_mean + 2 * opt_predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax1.plot(\ntest_x,\nopt_predictive_mean - 2 * opt_predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax1.plot(\ntest_x,\nopt_predictive_mean + 2 * opt_predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax1.plot(\ntest_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax1.plot(test_x, opt_predictive_mean, label=\"Predictive mean\", color=cols[1])\nax1.set_title(\"Posterior with Hyperparameter Optimisation\")\nax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nno_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D)\nno_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)\nno_opt_predictive_mean = no_opt_predictive_dist.mean()\nno_opt_predictive_std = no_opt_predictive_dist.stddev()\nax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax2.fill_between(\ntest_x.squeeze(),\nno_opt_predictive_mean - 2 * no_opt_predictive_std,\nno_opt_predictive_mean + 2 * no_opt_predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax2.plot(\ntest_x,\nno_opt_predictive_mean - 2 * no_opt_predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax2.plot(\ntest_x,\nno_opt_predictive_mean + 2 * no_opt_predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax2.plot(\ntest_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax2.plot(test_x, no_opt_predictive_mean, label=\"Predictive mean\", color=cols[1])\nax2.set_title(\"Posterior without Hyperparameter Optimisation\")\nax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nno_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale\nno_opt_variance = no_opt_posterior.prior.kernel.variance\nopt_lengthscale = opt_posterior.prior.kernel.lengthscale\nopt_variance = opt_posterior.prior.kernel.variance\nprint(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\")\nprint(\nf\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\"\n)\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Periodic()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Periodic Kernel\")\nplt.show()\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Linear()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Linear Kernel\")\nplt.show()\nkernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=mean, kernel=sum_kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\")\nplt.show()\nkernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=mean, kernel=sum_kernel)\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\")\nplt.show()\nco2_data = pd.read_csv(\n\"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\"\n)\nco2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12]\ntrain_x = co2_data[\"decimal date\"].values[:, None]\ntrain_y = co2_data[\"average\"].values[:, None]\nfig, ax = plt.subplots()\nax.plot(train_x, train_y)\nax.set_title(\"CO2 Concentration in the Atmosphere\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO2 Concentration (ppm)\")\nplt.show()\ntest_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1)\ny_scaler = StandardScaler().fit(train_y)\nstandardised_train_y = y_scaler.transform(train_y)\nD = gpx.Dataset(X=train_x, y=standardised_train_y)\nmean = gpx.mean_functions.Zero()\nrbf_kernel = gpx.kernels.RBF(lengthscale=100.0)\nperiodic_kernel = gpx.kernels.Periodic()\nlinear_kernel = gpx.kernels.Linear()\nsum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel])\nfinal_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])\nprior = gpx.Prior(mean_function=mean, kernel=final_kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\nnegative_mll = jit(negative_mll)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=D,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=1000,\nsafe=True,\nkey=key,\n)\nlatent_dist = opt_posterior.predict(test_x, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean().reshape(-1, 1)\npredictive_std = predictive_dist.stddev().reshape(-1, 1)\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\ntrain_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5\n)\nax.fill_between(\ntest_x.squeeze(),\npredictive_mean.squeeze() - 2 * predictive_std.squeeze(),\npredictive_mean.squeeze() + 2 * predictive_std.squeeze(),\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\ntest_x,\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\ntest_x,\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\ntrain_x[train_x &gt;= 2010],\nstandardised_train_y[train_x &gt;= 2010],\n\"x\",\nlabel=\"Observations\",\ncolor=cols[0],\nalpha=0.5,\n)\nax.fill_between(\ntest_x[test_x &gt;= 2010].squeeze(),\npredictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\ntest_x[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\ntest_x[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\ntest_x[test_x &gt;= 2010],\npredictive_mean[test_x &gt;= 2010],\nlabel=\"Predictive mean\",\ncolor=cols[1],\n)\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\nprint(\nf\"Periodic Kernel Period: {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\"\n)\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</code></pre>"},{"location":"give_me_the_code/#classification","title":"Classification","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom time import time\nimport blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nimport jax.tree_util as jtu\nfrom jaxtyping import (\nArray,\nFloat,\ninstall_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom tqdm import trange\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\ntfd = tfp.distributions\nidentity_matrix = jnp.eye\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey, subkey = jr.split(key)\nx = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0)\ny = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\nfig, ax = plt.subplots()\nax.scatter(x, y)\nkernel = gpx.RBF()\nmeanf = gpx.Constant()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Bernoulli(num_datapoints=D.n)\nposterior = prior * likelihood\nprint(type(posterior))\nnegative_lpd = jax.jit(gpx.LogPosteriorDensity(negative=True))\noptimiser = ox.adam(learning_rate=0.01)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_lpd,\ntrain_data=D,\noptim=ox.adamw(learning_rate=0.01),\nnum_iters=1000,\nkey=key,\n)\nmap_latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(map_latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - predictive_std,\npredictive_mean + predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"One sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\npredictive_mean + predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\ngram, cross_covariance = (kernel.gram, kernel.cross_covariance)\njitter = 1e-6\n# Compute (latent) function value map estimates at training points:\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nLx = Kxx.to_root()\nf_hat = Lx @ opt_posterior.latent\n# Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f):\nH = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]\nL = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)\n# H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I\nL_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True)\nH_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\nLH = jnp.linalg.cholesky(H_inv)\nlaplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH)\ndef construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:\nmap_latent_dist = opt_posterior.predict(xtest, train_data=D)\nKxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nLx = Kxx.to_root()\n# Lx\u207b\u00b9 Kxt\nLx_inv_Ktx = Lx.solve(Kxt)\n# Kxx\u207b\u00b9 Kxt\nKxx_inv_Ktx = Lx.T.solve(Lx_inv_Ktx)\n# Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt\nlaplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Ktx.T, H_inv), Kxx_inv_Ktx)\nmean = map_latent_dist.mean()\ncovariance = map_latent_dist.covariance() + laplace_cov_term\nL = jnp.linalg.cholesky(covariance)\nreturn tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L)\nlaplace_latent_dist = construct_laplace(xtest)\npredictive_dist = opt_posterior.likelihood(laplace_latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - predictive_std,\npredictive_mean + predictive_std,\nalpha=0.2,\ncolor=cols[1],\nlabel=\"One sigma\",\n)\nax.plot(\nxtest,\npredictive_mean - predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\npredictive_mean + predictive_std,\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\nnum_adapt = 500\nnum_samples = 500\nlpd = jax.jit(gpx.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\nadapt = blackjax.window_adaptation(\nblackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n# Initialise the chain\nstart = time()\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\nprint(f\"Adaption time taken: {time() - start: .1f} seconds\")\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\ndef one_step(state, rng_key):\nstate, info = kernel(rng_key, state)\nreturn state, (state, info)\nkeys = jax.random.split(rng_key, num_samples)\n_, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\nreturn states, infos\n# Sample from the posterior distribution\nstart = time()\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nprint(f\"Sampling time taken: {time() - start: .1f} seconds\")\nacceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\nfig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.prior.kernel.lengthscale)\nax1.plot(states.position.prior.kernel.variance)\nax2.plot(states.position.latent[:, 1, :])\nax0.set_title(\"Kernel Lengthscale\")\nax1.set_title(\"Kernel Variance\")\nax2.set_title(\"Latent Function (index = 1)\")\nthin_factor = 20\nposterior_samples = []\nfor i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):\nsample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)\nsample = sample.constrain()\nlatent_dist = sample.predict(xtest, train_data=D)\npredictive_dist = sample.likelihood(latent_dist)\nposterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\nposterior_samples = jnp.vstack(posterior_samples)\nlower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(posterior_samples, axis=0)\nfig, ax = plt.subplots()\nax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7)\nax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1)\nax.fill_between(\nxtest.flatten(),\nlower_ci.flatten(),\nupper_ci.flatten(),\nalpha=0.2,\ncolor=cols[1],\nlabel=\"95\\\\% CI\",\n)\nax.plot(\nxtest,\nlower_ci.flatten(),\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.plot(\nxtest,\nupper_ci.flatten(),\ncolor=cols[1],\nlinestyle=\"--\",\nlinewidth=1,\n)\nax.legend()\n%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\"\n</code></pre>"},{"location":"give_me_the_code/#regression","title":"Regression","text":"<pre><code># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\nimport gpjax as gpx\nkey = jr.PRNGKey(123)\nplt.style.use(\n\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nn = 100\nnoise = 0.3\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\nkernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nprior_dist = prior.predict(xtest)\nprior_mean = prior_dist.mean()\nprior_std = prior_dist.variance()\nsamples = prior_dist.sample(seed=key, sample_shape=(20,))\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\nxtest.flatten(),\nprior_mean - prior_std,\nprior_mean + prior_std,\nalpha=0.3,\ncolor=cols[1],\nlabel=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\nnegative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\n# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n# optim = ox.chain(\n#     ox.adam(learning_rate=0.01),\n#     ox.masked(ox.set_to_zero(), static_tree)\n#     )\nprint(gpx.cite(negative_mll))\nnegative_mll = jit(negative_mll)\nopt_posterior, history = gpx.fit(\nmodel=posterior,\nobjective=negative_mll,\ntrain_data=D,\noptim=ox.adam(learning_rate=0.01),\nnum_iters=500,\nsafe=True,\nkey=key,\n)\nfig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\")\nlatent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\nfig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\nxtest.squeeze(),\npredictive_mean - 2 * predictive_std,\npredictive_mean + 2 * predictive_std,\nalpha=0.2,\nlabel=\"Two sigma\",\ncolor=cols[1],\n)\nax.plot(\nxtest,\npredictive_mean - 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nxtest,\npredictive_mean + 2 * predictive_std,\nlinestyle=\"--\",\nlinewidth=1,\ncolor=cols[1],\n)\nax.plot(\nxtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-version","title":"Stable version","text":"<p>The latest stable release of <code>GPJax</code> can be installed via <code>pip</code>:</p> <pre><code>pip install gpjax\n</code></pre> <p>Check your installation</p> <p>We recommend you check your installation version: <pre><code>python -c 'import gpjax; print(gpjax.__version__)'\n</code></pre></p>"},{"location":"installation/#gputpu-support","title":"GPU/TPU support","text":"<p>Fancy using GPJax on GPU/TPU? Then you'll need to install JAX with the relevant hardware acceleration support as detailed in the JAX installation guide.</p>"},{"location":"installation/#development-version","title":"Development version","text":"<p>Warning</p> <p>This version is possibly unstable and may contain bugs.</p> <p>The latest development version of <code>GPJax</code> can be installed via running following:</p> <pre><code>git clone https://github.com/thomaspinder/GPJax.git\ncd GPJax\npoetry install\n</code></pre> <p>Tip</p> <p>We advise you create virtual environment before installing:</p> <pre><code>conda create -n gpjax_experimental python=3.10.0\nconda activate gpjax_experimental\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>poetry run pytest tests/\n</code></pre>"},{"location":"sharp_bits/","title":"\ud83d\udd2a Sharp bits","text":""},{"location":"sharp_bits/#the-sharp-bits","title":"\ud83d\udd2a The sharp bits","text":""},{"location":"sharp_bits/#pseudo-randomness","title":"Pseudo-randomness","text":"<p>Libraries like NumPy and Scipy use stateful pseudorandom number generators (PRNGs). However, the PRNG in JAX is stateless. This means that for a given function, the return always returns the same result unless the seed is changed. This is a good thing, but it means that we need to be careful when using JAX's PRNGs.</p> <p>To examine what it means for a PRNG to be stateful, consider the following example:</p> <p><pre><code>import numpy as np\nimport jax.random as jr\nkey = jr.PRNGKey(123)\n# NumPy\nprint('NumPy:')\nprint(np.random.random())\nprint(np.random.random())\nprint('\\nJAX:')\nprint(jr.uniform(key))\nprint(jr.uniform(key))\nprint('\\nSplitting key')\nkey, subkey = jr.split(key)\nprint(jr.uniform(subkey))\n</code></pre> <pre><code>NumPy:\n0.5194454541172852\n0.9815886617924413\nJAX:\n0.95821166\n0.95821166\nSplitting key\n0.23886406\n</code></pre> We can see that, in libraries like NumPy, the PRNG key's state is incremented whenever a pseudorandom call is made. This can make debugging difficult to manage as it is not always clear when a PRNG is being used. In JAX, the PRNG key is not incremented, so the same key will always return the same result. This has further positive benefits for reproducibility.</p> <p>GPJax relies on JAX's PRNGs for all random number generation. Whilst we try wherever possible to handle the PRNG key's state for you, care must be taken when defining your own models and inference schemes to ensure that the PRNG key is handled correctly. The JAX documentation has an excellent section on this.</p>"},{"location":"sharp_bits/#bijectors","title":"Bijectors","text":"<p>Parameters such as the kernel's lengthscale or variance have their support defined on a constrained subset of the real-line. During gradient-based optimisation, as we approach the set's boundary, it becomes possible that we could step outside of the set's support and introduce a numerical and mathematical error into our model. For example, consider the lengthscale parameter \u2113\\ell\u2113, which we know must be strictly positive. If at ttht^{\\text{th}}tth iterate, our current estimate of \u2113\\ell\u2113 was 0.02 and our derivative informed us that \u2113\\ell\u2113 should decrease, then if our learning rate is greater is than 0.03, we would end up with a negative variance term. We visualise this issue below where the red cross denotes the invalid lengthscale value that would be obtained, were we to optimise in the unconstrained parameter space.</p> <p></p> <p>A simple but impractical solution would be to use a tiny learning rate which would reduce the possibility of stepping outside of the parameter's support. However, this would be incredibly costly and does not eradicate the problem. An alternative solution is to apply a functional mapping to the parameter that projects it from a constrained subspace of the real-line onto the entire real-line. Here, gradient updates are applied in the unconstrained parameter space before transforming the value back to the original support of the parameters. Such a transformation is known as a bijection.</p> <p></p> <p>To help understand this, we show the effect of using a log-exp bijector in the above figure. We have six points on the positive real line that range from 0.1 to 3 depicted by a blue cross. We then apply the bijector by log-transforming the constrained value. This gives us the points' unconstrained value which we depict by a red circle. It is this value that we apply gradient updates to. When we wish to recover the constrained value, we apply the inverse of the bijector, which is the exponential function in this case. This gives us back the blue cross.</p> <p>In GPJax, we supply bijective functions using Tensorflow Probability. In our PyTrees doc document, we detail how the user can define their own bijectors and attach them to the parameter(s) of their model.</p>"},{"location":"sharp_bits/#positive-definiteness","title":"Positive-definiteness","text":"<p>\"Symmetric positive definiteness is one of the highest accolades to which a matrix can aspire\" - Nicholas Highman, Accuracy and stability of numerical algorithms (Higham, 2002)1</p>"},{"location":"sharp_bits/#why-is-positive-definiteness-important","title":"Why is positive-definiteness important?","text":"<p>The Gram matrix of a kernel, a concept that we explore more in our kernels notebook and our PyTree notebook, is a symmetric positive definite matrix. As such, we have a range of tools at our disposal to make subsequent operations on the covariance matrix faster. One of these tools is the Cholesky factorisation that uniquely decomposes any symmetric positive-definite matrix \u03a3\\mathbf{\\Sigma}\u03a3 by</p> <p><p>\u03a3=LL\u22a4 , \\begin{align}     \\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\top}\\,, \\end{align} \u03a3=LL\u22a4,\u200b\u200b</p> where L\\mathbf{L}L is a lower triangular matrix.</p> <p>We make use of this result in GPJax when solving linear systems of equations of the form Ax=b\\mathbf{A}\\boldsymbol{x} = \\boldsymbol{b}Ax=b. Whilst seemingly abstract at first, such problems are frequently encountered when constructing Gaussian process models. One such example is frequently encountered in the regression setting for learning Gaussian process kernel hyperparameters. Here we have labels y\u223cN(f(x),\u03c32I)\\boldsymbol{y} \\sim \\mathcal{N}(f(\\boldsymbol{x}), \\sigma^2\\mathbf{I})y\u223cN(f(x),\u03c32I) with f(x)\u223cN(0,Kxx)f(\\boldsymbol{x}) \\sim \\mathcal{N}(\\boldsymbol{0}, \\mathbf{K}_{\\boldsymbol{xx}})f(x)\u223cN(0,Kxx\u200b) arising from zero-mean Gaussian process prior and Gram matrix Kxx\\mathbf{K}_{\\boldsymbol{xx}}Kxx\u200b at the inputs x\\boldsymbol{x}x. Here the marginal log-likelihood comprises the following form</p> <p>log\u2061p(y)=0.5(\u2212y\u22a4(Kxx+\u03c32I)\u22121y\u2212log\u2061\u2223Kxx+\u03c32I\u2223\u2212nlog\u2061(2\u03c0)), \\begin{align}     \\log p(\\boldsymbol{y}) = 0.5\\left(-\\boldsymbol{y}^{\\top}\\left(\\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I} \\right)^{-1}\\boldsymbol{y} -\\log\\lvert \\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I}\\rvert -n\\log(2\\pi)\\right) , \\end{align} logp(y)=0.5(\u2212y\u22a4(Kxx\u200b+\u03c32I)\u22121y\u2212log\u2223Kxx\u200b+\u03c32I\u2223\u2212nlog(2\u03c0)),\u200b\u200b</p> <p>and the goal of inference is to maximise kernel hyperparameters (contained in the Gram matrix Kxx\\mathbf{K}_{\\boldsymbol{xx}}Kxx\u200b) and likelihood hyperparameters (contained in the noise covariance \u03c32I\\sigma^2\\mathbf{I}\u03c32I). Computing the marginal log-likelihood (and its gradients), draws our attention to the term</p> <p>(Kxx+\u03c32I)\u22121\u23dfAy, \\begin{align}     \\underbrace{\\left(\\mathbf{K}_{\\boldsymbol{xx}} + \\sigma^2\\mathbf{I} \\right)^{-1}}_{\\mathbf{A}}\\boldsymbol{y}, \\end{align} A(Kxx\u200b+\u03c32I)\u22121\u200b\u200by,\u200b\u200b</p> <p>then we can see a solution can be obtained by solving the corresponding system of equations. By working with L=chol\u2061A\\mathbf{L} = \\operatorname{chol}{\\mathbf{A}}L=cholA instead of A\\mathbf{A}A, we save a significant amount of floating-point operations (flops) by solving two triangular systems of equations (one for L\\mathbf{L}L and another for L\u22a4\\mathbf{L}^{\\top}L\u22a4) instead of one dense system of equations. Solving two triangular systems of equations has complexity O(n3/6)\\mathcal{O}(n^3/6)O(n3/6); a vast improvement compared to regular solvers that have O(n3)\\mathcal{O}(n^3)O(n3) complexity in the number of datapoints nnn.</p>"},{"location":"sharp_bits/#the-cholesky-drawback","title":"The Cholesky drawback","text":"<p>While the computational acceleration provided by using Cholesky factors instead of dense matrices is hopefully now apparent, an awkward numerical instability gotcha can arise due to floating-point rounding errors. When we evaluate a covariance function on a set of points that are very close to one another, eigenvalues of the corresponding Gram matrix can get very small. While not mathematically less than zero, the smallest eigenvalues can become negative-valued due to finite-precision numerical errors. This becomes a problem when we want to compute a Cholesky factor since this requires that the input matrix is numerically positive-definite. If there are negative eigenvalues, this violates the requirements and results in a \"Cholesky failure\".</p> <p>To resolve this, we apply some numerical jitter to the diagonals of any Gram matrix. Typically this is very small, with 10\u2212610^{-6}10\u22126 being the system default. However, for some problems, this amount may need to be increased.</p>"},{"location":"sharp_bits/#slow-to-evaluate","title":"Slow-to-evaluate","text":"<p>Famously, a regular Gaussian process model (as detailed in our regression notebook) will scale cubically in the number of data points. Consequently, if you try to fit your Gaussian process model to a data set containing more than several thousand data points, then you will likely incur a significant computational overhead. In such cases, we recommend using Sparse Gaussian processes to alleviate this issue.</p> <p>When the data contains less than around 50000 data points, we recommend using the collapsed evidence lower bound objective (Titsias, 2009)2 to optimise the parameters of your sparse Gaussian process model. Such a model will scale linearly in the number of data points and quadratically in the number of inducing points. We demonstrate its use in our sparse regression notebook.</p> <p>For data sets exceeding 50000 data points, even the sparse Gaussian process outlined above will become computationally infeasible. In such cases, we recommend using the uncollapsed evidence lower bound objective (Hensman et al., 2013)3 that allows stochastic mini-batch optimisation of the parameters of your sparse Gaussian process model. Such a model will scale linearly in the batch size and quadratically in the number of inducing points. We demonstrate its use in our sparse stochastic variational inference notebook.</p> <ol> <li> <p>Higham, N. J. (2002) Accuracy and Stability of Numerical Algorithms. Second. Society for Industrial and Applied Mathematics. DOI: 10.1137/1.9780898718027.\u00a0\u21a9</p> </li> <li> <p>Titsias, M. (2009) Variational learning of inducing variables in sparse Gaussian processes. In: Proceedings of the twelth international conference on artificial intelligence and statistics, 2009, pp. 567\u2013574. Proceedings of machine learning research. PMLR.\u00a0\u21a9</p> </li> <li> <p>Hensman, J., Fusi, N. and Lawrence, N. D. (2013) Gaussian processes for big data. Artificial intelligence and statistics.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>GPJax<ul> <li>Base<ul> <li>Module</li> <li>Param</li> </ul> </li> <li>Citation</li> <li>Dataset</li> <li>Fit</li> <li>Gaussian Distribution</li> <li>GPs</li> <li>Integrators</li> <li>Kernels<ul> <li>Approximations<ul> <li>Rff</li> </ul> </li> <li>Base</li> <li>Computations<ul> <li>Base</li> <li>Basis Functions</li> <li>Constant Diagonal</li> <li>Dense</li> <li>Diagonal</li> <li>Eigen</li> </ul> </li> <li>Non Euclidean<ul> <li>Graph</li> <li>Utils</li> </ul> </li> <li>Nonstationary<ul> <li>Arccosine</li> <li>Linear</li> <li>Polynomial</li> </ul> </li> <li>Stationary<ul> <li>Mat\u00e9rn12</li> <li>Mat\u00e9rn32</li> <li>Mat\u00e9rn52</li> <li>Periodic</li> <li>Powered Exponential</li> <li>Rational Quadratic</li> <li>RBF</li> <li>Utils</li> <li>White</li> </ul> </li> </ul> </li> <li>Likelihoods</li> <li>Linops<ul> <li>Constant Diagonal Linear Operator</li> <li>Dense Linear Operator</li> <li>Diagonal Linear Operator</li> <li>Identity Linear Operator</li> <li>Linear Operator</li> <li>Triangular Linear Operator</li> <li>Utils</li> <li>Zero Linear Operator</li> </ul> </li> <li>Mean Functions</li> <li>Objectives</li> <li>Progress Bar</li> <li>Scan</li> <li>Typing</li> <li>Variational Families</li> </ul> </li> </ul>"},{"location":"api/citation/","title":"Citation","text":""},{"location":"api/citation/#gpjax.citation","title":"<code>gpjax.citation</code>","text":""},{"location":"api/citation/#gpjax.citation.MaternKernels","title":"<code>MaternKernels = Union[Matern12, Matern32, Matern52]</code>  <code>module-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.MLLs","title":"<code>MLLs = Union[ConjugateMLL, NonConjugateMLL, LogPosteriorDensity]</code>  <code>module-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.CitationType","title":"<code>CitationType = Union[str, Dict[str, str]]</code>  <code>module-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation","title":"<code>AbstractCitation</code>  <code>dataclass</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.citation_key","title":"<code>citation_key: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.authors","title":"<code>authors: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.title","title":"<code>title: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.year","title":"<code>year: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.as_str","title":"<code>as_str() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.__repr__","title":"<code>__repr__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.AbstractCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.NullCitation","title":"<code>NullCitation</code>","text":"<p>         Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.NullCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.JittedFnCitation","title":"<code>JittedFnCitation</code>","text":"<p>         Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.JittedFnCitation.__str__","title":"<code>__str__() -&gt; str</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation","title":"<code>PhDThesisCitation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.school","title":"<code>school: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.institution","title":"<code>institution: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PhDThesisCitation.citation_type","title":"<code>citation_type: str = 'phdthesis'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation","title":"<code>PaperCitation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.PaperCitation.booktitle","title":"<code>booktitle: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.PaperCitation.citation_type","title":"<code>citation_type: str = 'inproceedings'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation","title":"<code>BookCitation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractCitation</code></p>"},{"location":"api/citation/#gpjax.citation.BookCitation.publisher","title":"<code>publisher: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.volume","title":"<code>volume: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.BookCitation.citation_type","title":"<code>citation_type: str = 'book'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/citation/#gpjax.citation.cite","title":"<code>cite(tree: ELBO) -&gt; PaperCitation</code>","text":""},{"location":"api/dataset/","title":"Dataset","text":""},{"location":"api/dataset/#gpjax.dataset","title":"<code>gpjax.dataset</code>","text":""},{"location":"api/dataset/#gpjax.dataset.__all__","title":"<code>__all__ = ['Dataset']</code>  <code>module-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset","title":"<code>Dataset</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code></p> <p>Base class for datasets.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset--attributes","title":"Attributes","text":"<pre><code>X (Optional[Num[Array, \"N D\"]]): Input data.\ny (Optional[Num[Array, \"N Q\"]]): Output data.\n</code></pre>"},{"location":"api/dataset/#gpjax.dataset.Dataset.X","title":"<code>X: Optional[Num[Array, N D]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.y","title":"<code>y: Optional[Num[Array, N Q]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/dataset/#gpjax.dataset.Dataset.n","title":"<code>n: int</code>  <code>property</code>","text":"<p>Number of observations.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.in_dim","title":"<code>in_dim: int</code>  <code>property</code>","text":"<p>Dimension of the inputs, XXX.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Dimension of the outputs, yyy.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Checks that the shapes of XXX and yyy are compatible.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Returns a string representation of the dataset.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_supervised","title":"<code>is_supervised() -&gt; bool</code>","text":"<p>Returns <code>True</code> if the dataset is supervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.is_unsupervised","title":"<code>is_unsupervised() -&gt; bool</code>","text":"<p>Returns <code>True</code> if the dataset is unsupervised.</p>"},{"location":"api/dataset/#gpjax.dataset.Dataset.__add__","title":"<code>__add__(other: Dataset) -&gt; Dataset</code>","text":"<p>Combine two datasets. Right hand dataset is stacked beneath the left.</p>"},{"location":"api/fit/","title":"Fit","text":""},{"location":"api/fit/#gpjax.fit","title":"<code>gpjax.fit</code>","text":""},{"location":"api/fit/#gpjax.fit.ModuleModel","title":"<code>ModuleModel = TypeVar('ModuleModel', bound=Module)</code>  <code>module-attribute</code>","text":""},{"location":"api/fit/#gpjax.fit.__all__","title":"<code>__all__ = ['fit', 'get_batch']</code>  <code>module-attribute</code>","text":""},{"location":"api/fit/#gpjax.fit.fit","title":"<code>fit(*, model: ModuleModel, objective: Union[AbstractObjective, Callable[[ModuleModel, Dataset], ScalarFloat]], train_data: Dataset, optim: ox.GradientTransformation, key: KeyArray, num_iters: Optional[int] = 100, batch_size: Optional[int] = -1, log_rate: Optional[int] = 10, verbose: Optional[bool] = True, unroll: Optional[int] = 1, safe: Optional[bool] = True) -&gt; Tuple[ModuleModel, Array]</code>","text":"<p>Train a Module model with respect to a supplied Objective function. Optimisers used here should originate from Optax.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; import optax as ox\n&gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (1) Create a dataset:\n&gt;&gt;&gt; X = jnp.linspace(0.0, 10.0, 100)[:, None]\n&gt;&gt;&gt; y = 2.0 * X + 1.0 + 10 * jr.normal(jr.PRNGKey(0), X.shape)\n&gt;&gt;&gt; D = gpx.Dataset(X, y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (2) Define your model:\n&gt;&gt;&gt; class LinearModel(gpx.Module):\nweight: float = gpx.param_field()\nbias: float = gpx.param_field()\ndef __call__(self, x):\nreturn self.weight * x + self.bias\n&gt;&gt;&gt; model = LinearModel(weight=1.0, bias=1.0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (3) Define your loss function:\n&gt;&gt;&gt; class MeanSquareError(gpx.AbstractObjective):\ndef evaluate(self, model: LinearModel, train_data: gpx.Dataset) -&gt; float:\nreturn jnp.mean((train_data.y - model(train_data.X)) ** 2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; loss = MeanSqaureError()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # (4) Train!\n&gt;&gt;&gt; trained_model, history = gpx.fit(\nmodel=model, objective=loss, train_data=D, optim=ox.sgd(0.001), num_iters=1000\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model Module to be optimised.</p> required <code>objective</code> <code>Objective</code> <p>The objective function that we are optimising with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data to be used for the optimisation.</p> required <code>optim</code> <code>GradientTransformation</code> <p>The Optax optimiser that is to be used for learning a parameter set.</p> required <code>num_iters</code> <code>Optional[int]</code> <p>The number of optimisation steps to run. Defaults to 100.</p> <code>100</code> <code>batch_size</code> <code>Optional[int]</code> <p>The size of the mini-batch to use. Defaults to -1 (i.e. full batch).</p> <code>-1</code> <code>key</code> <code>Optional[KeyArray]</code> <p>The random key to use for the optimisation batch selection. Defaults to jr.PRNGKey(42).</p> required <code>log_rate</code> <code>Optional[int]</code> <p>How frequently the objective function's value should be printed. Defaults to 10.</p> <code>10</code> <code>verbose</code> <code>Optional[bool]</code> <p>Whether to print the training loading bar. Defaults to True.</p> <code>True</code> <code>unroll</code> <code>int</code> <p>The number of unrolled steps to use for the optimisation. Defaults to 1.</p> <code>1</code>"},{"location":"api/fit/#gpjax.fit.fit--returns","title":"Returns","text":"<pre><code>Tuple[Module, Array]: A Tuple comprising the optimised model and training\n    history respectively.\n</code></pre>"},{"location":"api/fit/#gpjax.fit.get_batch","title":"<code>get_batch(train_data: Dataset, batch_size: int, key: KeyArray) -&gt; Dataset</code>","text":"<p>Batch the data into mini-batches. Sampling is done with replacement.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>The training dataset.</p> required <code>batch_size</code> <code>int</code> <p>The batch size.</p> required <code>key</code> <code>KeyArray</code> <p>The random key to use for the batch selection.</p> required"},{"location":"api/fit/#gpjax.fit.get_batch--returns","title":"Returns","text":"<pre><code>Dataset: The batched dataset.\n</code></pre>"},{"location":"api/gaussian_distribution/","title":"Gaussian Distribution","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution","title":"<code>gpjax.gaussian_distribution</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.__all__","title":"<code>__all__ = ['GaussianDistribution']</code>  <code>module-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>         Bases: <code>tfd.Distribution</code></p> <p>Multivariate Gaussian distribution with a linear operator scale matrix.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>Optional[Float[Array,  N]]</code> <p>The mean of the distribution. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Optional[LinearOperator]</code> <p>The scale matrix of the distribution. Defaults to None.</p> <code>None</code>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate Gaussian distribution with a linear operator scale matrix.\n</code></pre>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.loc","title":"<code>loc = loc</code>  <code>instance-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.scale","title":"<code>scale = scale</code>  <code>instance-attribute</code>","text":""},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.event_shape","title":"<code>event_shape: Tuple</code>  <code>property</code>","text":"<p>Returns the event shape.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.__init__","title":"<code>__init__(loc: Optional[Float[Array, N]] = None, scale: Optional[LinearOperator] = None) -&gt; None</code>","text":"<p>Initialises the distribution.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.mean","title":"<code>mean() -&gt; Float[Array, N]</code>","text":"<p>Calculates the mean.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.median","title":"<code>median() -&gt; Float[Array, N]</code>","text":"<p>Calculates the median.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.mode","title":"<code>mode() -&gt; Float[Array, N]</code>","text":"<p>Calculates the mode.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.covariance","title":"<code>covariance() -&gt; Float[Array, N N]</code>","text":"<p>Calculates the covariance matrix.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.variance","title":"<code>variance() -&gt; Float[Array, N]</code>","text":"<p>Calculates the variance.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.stddev","title":"<code>stddev() -&gt; Float[Array, N]</code>","text":"<p>Calculates the standard deviation.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.entropy","title":"<code>entropy() -&gt; ScalarFloat</code>","text":"<p>Calculates the entropy of the distribution.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.log_prob","title":"<code>log_prob(y: Float[Array, N]) -&gt; ScalarFloat</code>","text":"<p>Calculates the log pdf of the multivariate Gaussian.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array,  N]</code> <p>The value to calculate the log probability of.</p> required"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.log_prob--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log probability of the value.\n</code></pre>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.sample","title":"<code>sample(seed: KeyArray, sample_shape: Tuple[int, ...])</code>","text":"<p>See <code>Distribution.sample</code>.</p>"},{"location":"api/gaussian_distribution/#gpjax.gaussian_distribution.GaussianDistribution.kl_divergence","title":"<code>kl_divergence(other: GaussianDistribution) -&gt; ScalarFloat</code>","text":""},{"location":"api/gps/","title":"GPs","text":""},{"location":"api/gps/#gpjax.gps","title":"<code>gpjax.gps</code>","text":""},{"location":"api/gps/#gpjax.gps.__all__","title":"<code>__all__ = ['AbstractPrior', 'Prior', 'AbstractPosterior', 'ConjugatePosterior', 'NonConjugatePosterior', 'construct_posterior']</code>  <code>module-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior","title":"<code>AbstractPrior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract Gaussian process prior.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.kernel","title":"<code>kernel: AbstractKernel</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.mean_function","title":"<code>mean_function: AbstractMeanFunction</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process at the given points.</p> <p>The output of this function is a TensorFlow probability distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Evaluate the predictive distribution.</p> <p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to the predict method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to the predict method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPrior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior","title":"<code>Prior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractPrior</code></p> <p>A Gaussian process prior object.</p> <p>The GP is parameterised by a mean and kernel function.</p> <p>A Gaussian process prior parameterised by a mean function m(\u22c5)m(\\cdot)m(\u22c5) and a kernel function k(\u22c5,\u22c5)k(\\cdot, \\cdot)k(\u22c5,\u22c5) is given by p(f(\u22c5))=GP(m(\u22c5),k(\u22c5,\u22c5))p(f(\\cdot)) = \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))p(f(\u22c5))=GP(m(\u22c5),k(\u22c5,\u22c5)).</p> <p>To invoke a <code>Prior</code> distribution, a kernel and mean function must be specified.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; prior = gpx.Prior(mean_function=meanf, kernel = kernel)\n</code></pre></p>"},{"location":"api/gps/#gpjax.gps.Prior.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Combine the prior with a likelihood to form a posterior distribution.</p> <p>The product of a prior and likelihood is proportional to the posterior distribution. By computing the product of a GP prior and a likelihood object, a posterior GP object will be returned. Mathematically, this can be described by: <p>p(f(\u22c5)\u2223y)\u221dp(y\u2223f(\u22c5))p(f(\u22c5)), p(f(\\cdot) \\mid y) \\propto p(y \\mid f(\\cdot))p(f(\\cdot)), p(f(\u22c5)\u2223y)\u221dp(y\u2223f(\u22c5))p(f(\u22c5)),</p> where p(y\u2223f(\u22c5))p(y | f(\\cdot))p(y\u2223f(\u22c5)) is the likelihood and p(f(\u22c5))p(f(\\cdot))p(f(\u22c5)) is the prior.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; prior = gpx.Prior(mean_function=meanf, kernel = kernel)\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; prior * likelihood\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Likelihood</code> <p>The likelihood distribution of the observed dataset.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.__mul__--returns","title":"Returns","text":"<pre><code>Posterior: The relevant GP posterior for the given prior and\n    likelihood. Special cases are accounted for where the model\n    is conjugate.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.__rmul__","title":"<code>__rmul__(other)</code>","text":"<p>Combine the prior with a likelihood to form a posterior distribution.</p> <p>Reimplement the multiplication operator to allow for order-invariant product of a likelihood and a prior i.e., likelihood * prior.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Likelihood</code> <p>The likelihood distribution of the observed dataset.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.__rmul__--returns","title":"Returns","text":"<pre><code>Posterior: The relevant GP posterior for the given prior and\n    likelihood. Special cases are accounted for where the model\n    is conjugate.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.predict","title":"<code>predict(test_inputs: Num[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive prior distribution for a given set of parameters. The output of this function is a function that computes a TFP distribution for a given set of inputs.</p> <p>In the following example, we compute the predictive prior distribution and then evaluate it on the interval :math:<code>[0, 1]</code>:</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; prior = gpx.Prior(mean_function=meanf, kernel = kernel)\n&gt;&gt;&gt;\n&gt;&gt;&gt; prior.predict(jnp.linspace(0, 1, 100))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The inputs at which to evaluate the prior distribution.</p> required"},{"location":"api/gps/#gpjax.gps.Prior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx","title":"<code>sample_approx(num_samples: int, key: KeyArray, num_features: Optional[int] = 100) -&gt; FunctionalSample</code>","text":"<p>Approximate samples from the Gaussian process prior.</p> <p>Build an approximate sample from the Gaussian process prior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>In particular, we approximate the Gaussian processes' prior as the finite feature approximation f^(x)=\u2211i=1m\u03d5i(x)\u03b8i\\hat{f}(x) = \\sum_{i=1}^m\\phi_i(x)\\theta_if^\u200b(x)=\u2211i=1m\u200b\u03d5i\u200b(x)\u03b8i\u200b where \u03d5i\\phi_i\u03d5i\u200b are mmm features sampled from the Fourier feature decomposition of the model's kernel and \u03b8i\\theta_i\u03b8i\u200b are samples from a unit Gaussian.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>In the following example, we build 10 such samples and then evaluate them over the interval [0,1][0, 1][0,1]:</p> <p>For a <code>prior</code> distribution, the following code snippet will build and evaluate an approximate sample.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; key = jr.PRNGKey(123)\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; prior = gpx.Prior(mean_function=meanf, kernel = kernel)\n&gt;&gt;&gt;\n&gt;&gt;&gt; sample_fn = prior.sample_approx(10, key)\n&gt;&gt;&gt; sample_fn(jnp.linspace(0, 1, 100).reshape(-1, 1))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The desired number of samples.</p> required <code>key</code> <code>KeyArray</code> <p>The random seed used for the sample(s).</p> required <code>num_features</code> <code>int</code> <p>The number of features used when approximating the kernel.</p> <code>100</code>"},{"location":"api/gps/#gpjax.gps.Prior.sample_approx--returns","title":"Returns","text":"<pre><code>FunctionalSample: A function representing an approximate sample from the\n    Gaussian process prior.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior","title":"<code>AbstractPosterior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract Gaussian process posterior.</p> <p>The base GP posterior object conditioned on an observed dataset. All posterior objects should inherit from this class.</p>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.prior","title":"<code>prior: AbstractPrior</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.likelihood","title":"<code>likelihood: AbstractLikelihood</code>  <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.jitter","title":"<code>jitter: float = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the Gaussian process posterior at the given points.</p> <p>The output of this function is a TFP distribution from which the the latent function's mean and covariance can be evaluated and the distribution can be sampled.</p> <p>Under the hood, <code>__call__</code> is calling the objects <code>predict</code> method. For this reasons, classes inheriting the <code>AbstractPrior</code> class, should not overwrite the <code>__call__</code> method and should instead define a <code>predict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>The arguments to pass to the GP's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the GP's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Compute the latent function's multivariate normal distribution for a given set of parameters. For any class inheriting the <code>AbstractPrior</code> class, this method must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to the predict method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to the predict method.</p> <code>{}</code>"},{"location":"api/gps/#gpjax.gps.AbstractPosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A multivariate normal random variable representation\n    of the Gaussian process.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior","title":"<code>ConjugatePosterior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractPosterior</code></p> <p>A Conjuate Gaussian process posterior object.</p> <p>A Gaussian process posterior distribution when the constituent likelihood function is a Gaussian distribution. In such cases, the latent function values fff can be analytically integrated out of the posterior distribution. As such, many computational operations can be simplified; something we make use of in this object.</p> <p>For a Gaussian process prior p(f)p(\\mathbf{f})p(f) and a Gaussian likelihood p(y\u2223f)=N(y\u2223f,\u03c32))p(y | \\mathbf{f}) = \\mathcal{N}(y\\mid \\mathbf{f}, \\sigma^2))p(y\u2223f)=N(y\u2223f,\u03c32)) where f=f(x)\\mathbf{f} = f(\\mathbf{x})f=f(x), the predictive posterior distribution at a set of inputs x\\mathbf{x}x is given by <p>p(f\u22c6\u2223y)=\u222bp(f\u22c6,f\u2223y)=N(f\u22c6\u03bc\u2223y,\u03a3\u2223y \\begin{align} p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star}, \\mathbf{f} \\mid \\mathbf{y})\\\\     &amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} \\end{align} p(f\u22c6\u2223y)\u200b=\u222bp(f\u22c6,f\u2223y)=N(f\u22c6\u03bc\u2223y\u200b,\u03a3\u2223y\u200b\u200b\u200b</p> where <p>\u03bc\u2223y=k(x\u22c6,x)(k(x,x\u2032)+\u03c32In)\u22121y\u03a3\u2223y=k(x\u22c6,x\u22c6\u2032)\u2212k(x\u22c6,x)(k(x,x\u2032)+\u03c32In)\u22121k(x,x\u22c6). \\begin{align} \\boldsymbol{\\mu}_{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\\\ \\boldsymbol{\\Sigma}_{\\mid \\mathbf{y}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}). \\end{align} \u03bc\u2223y\u200b\u03a3\u2223y\u200b\u200b=k(x\u22c6,x)(k(x,x\u2032)+\u03c32In\u200b)\u22121y=k(x\u22c6,x\u22c6\u2032)\u2212k(x\u22c6,x)(k(x,x\u2032)+\u03c32In\u200b)\u22121k(x,x\u22c6).\u200b\u200b</p></p> Example <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; prior = gpx.Prior(\nmean_function = gpx.mean_functions.Zero(),\nkernel = gpx.kernels.RBF()\n)\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; posterior = prior * likelihood\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict","title":"<code>predict(test_inputs: Num[Array, N D], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Query the predictive posterior distribution.</p> <p>Conditional on a training data set, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density.</p> <p>The predictive distribution of a conjugate GP is given by $$     p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) &amp; = \\int p(\\mathbf{f}^{\\star} \\mathbf{f} \\mid \\mathbf{y})\\     &amp; =\\mathcal{N}(\\mathbf{f}^{\\star} \\boldsymbol{\\mu}{\\mid \\mathbf{y}}, \\boldsymbol{\\Sigma}} $$ where $$     \\boldsymbol{\\mu}{\\mid \\mathbf{y}} &amp; = k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left(k(\\mathbf{x}, \\mathbf{x}')+\\sigma^2\\mathbf{I}_n\\right)^{-1}\\mathbf{y}  \\     \\boldsymbol{\\Sigma}} &amp; =k(\\mathbf{x}^{\\star}, \\mathbf{x}^{\\star\\prime}) -k(\\mathbf{x}^{\\star}, \\mathbf{x})\\left( k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_n \\right)^{-1}k(\\mathbf{x}, \\mathbf{x}^{\\star}). $$</p> <p>The conditioning set is a GPJax <code>Dataset</code> object, whilst predictions are made on a regular Jax array.</p> Example <p>For a <code>posterior</code> distribution, the following code snippet will evaluate the predictive distribution. <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; xtrain = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt; ytrain = jnp.sin(xtrain)\n&gt;&gt;&gt; D = gpx.Dataset(X=xtrain, y=ytrain)\n&gt;&gt;&gt; xtest = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; prior = gpx.Prior(mean_function = gpx.Zero(), kernel = gpx.RBF())\n&gt;&gt;&gt; posterior = prior * gpx.Gaussian(num_datapoints = D.n)\n&gt;&gt;&gt; predictive_dist = posterior(xtest, D)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Num[Array, N D]</code> <p>A Jax array of test inputs at which the predictive distribution is evaluated.</p> required <code>train_data</code> <code>Dataset</code> <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> required"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A\n    function that accepts an input array and returns the predictive\n        distribution as a `GaussianDistribution`.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx","title":"<code>sample_approx(num_samples: int, train_data: Dataset, key: KeyArray, num_features: Optional[int] = 100) -&gt; FunctionalSample</code>","text":"<p>Draw approximate samples from the Gaussian process posterior.</p> <p>Build an approximate sample from the Gaussian process posterior. This method provides a function that returns the evaluations of a sample across any given inputs.</p> <p>Unlike when building approximate samples from a Gaussian process prior, decompositions based on Fourier features alone rarely give accurate samples. Therefore, we must also include an additional set of features (known as canonical features) to better model the transition from Gaussian process prior to Gaussian process posterior. For more details see Wilson et. al. (2020).</p> <p>In particular, we approximate the Gaussian processes' posterior as the finite feature approximation f^(x)=\u2211i=1m\u03d5i(x)\u03b8i+\u2211j=1Nvjk(.,xj)\\hat{f}(x) = \\sum_{i=1}^m \\phi_i(x)\\theta_i + \\sum{j=1}^N v_jk(.,x_j)f^\u200b(x)=\u2211i=1m\u200b\u03d5i\u200b(x)\u03b8i\u200b+\u2211j=1Nvj\u200bk(.,xj\u200b) where \u03d5i\\phi_i\u03d5i\u200b are m features sampled from the Fourier feature decomposition of the model's kernel and k(.,xj)k(., x_j)k(.,xj\u200b) are N canonical features. The Fourier weights \u03b8i\\theta_i\u03b8i\u200b are samples from a unit Gaussian. See Wilson et. al. (2020) for expressions for the canonical weights vjv_jvj\u200b.</p> <p>A key property of such functional samples is that the same sample draw is evaluated for all queries. Consistency is a property that is prohibitively costly to ensure when sampling exactly from the GP prior, as the cost of exact sampling scales cubically with the size of the sample. In contrast, finite feature representations can be evaluated with constant cost regardless of the required number of queries.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>The desired number of samples.</p> required <code>key</code> <code>KeyArray</code> <p>The random seed used for the sample(s).</p> required <code>num_features</code> <code>int</code> <p>The number of features used when approximating the kernel.</p> <code>100</code>"},{"location":"api/gps/#gpjax.gps.ConjugatePosterior.sample_approx--returns","title":"Returns","text":"<pre><code>FunctionalSample: A function representing an approximate sample from the Gaussian\nprocess prior.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior","title":"<code>NonConjugatePosterior</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractPosterior</code></p> <p>A non-conjugate Gaussian process posterior object.</p> <p>A Gaussian process posterior object for models where the likelihood is non-Gaussian. Unlike the <code>ConjugatePosterior</code> object, the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p>"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.latent","title":"<code>latent: Float[Array, N 1] = param_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.key","title":"<code>key: KeyArray = static_field(PRNGKey(42))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict","title":"<code>predict(test_inputs: Num[Array, N D], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Query the predictive posterior distribution.</p> <p>Conditional on a set of training data, compute the GP's posterior predictive distribution for a given set of parameters. The returned function can be evaluated at a set of test inputs to compute the corresponding predictive density. Note, to gain predictions on the scale of the original data, the returned distribution will need to be transformed through the likelihood function's inverse link function.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>A <code>gpx.Dataset</code> object that contains the input and output data used for training dataset.</p> required"},{"location":"api/gps/#gpjax.gps.NonConjugatePosterior.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts an\n    input array and returns the predictive distribution as\n    a `dx.Distribution`.\n</code></pre>"},{"location":"api/gps/#gpjax.gps.construct_posterior","title":"<code>construct_posterior(prior, likelihood)</code>","text":"<p>Utility function for constructing a posterior object from a prior and likelihood. The function will automatically select the correct posterior object based on the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>prior</code> <code>Prior</code> <p>The Prior distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood that represents our beliefs around the distribution of the data.</p> required"},{"location":"api/gps/#gpjax.gps.construct_posterior--returns","title":"Returns","text":"<pre><code>AbstractPosterior: A posterior distribution. If the likelihood is\n    Gaussian, then a `ConjugatePosterior` will be returned. Otherwise,\n    a `NonConjugatePosterior` will be returned.\n</code></pre>"},{"location":"api/integrators/","title":"Integrators","text":""},{"location":"api/integrators/#gpjax.integrators","title":"<code>gpjax.integrators</code>","text":""},{"location":"api/integrators/#gpjax.integrators.__all__","title":"<code>__all__ = ['AbstractIntegrator', 'GHQuadratureIntegrator', 'AnalyticalGaussianIntegrator']</code>  <code>module-attribute</code>","text":""},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator","title":"<code>AbstractIntegrator</code>  <code>dataclass</code>","text":"<p>Base class for integrators.</p>"},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.integrate","title":"<code>integrate(fun: Callable, y: Float[Array, N D], mean: Float[Array, N D], variance: Float[Array, N D], likelihood: gpjax.likelihoods.AbstractLikelihood = None) -&gt; Float[Array, N]</code>  <code>abstractmethod</code>","text":"<p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function to be integrated.</p> required <code>y</code> <code>Float[Array, N D]</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, N D]</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, N D]</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> <code>None</code>"},{"location":"api/integrators/#gpjax.integrators.AbstractIntegrator.__call__","title":"<code>__call__(fun: Callable, y: Float[Array, N D], mean: Float[Array, N D], variance: Float[Array, N D], likelihood: gpjax.likelihoods.AbstractLikelihood = None) -&gt; Float[Array, N]</code>","text":"<p>Integrate a function with respect to a Gaussian distribution.</p> <p>Typically, the function will be the likelihood function and the mean and variance will be the parameters of the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The function to be integrated.</p> required <code>y</code> <code>Float[Array, N D]</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, N D]</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, N D]</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> <code>None</code>"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator","title":"<code>GHQuadratureIntegrator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractIntegrator</code></p> <p>Compute an integral using Gauss-Hermite quadrature.</p> <p>Gauss-Hermite quadrature is a method for approximating integrals through a weighted sum of function evaluations at specific points <p>\u222bF(t)exp\u2061(\u2212t2)dt\u2248\u2211j=1JwjF(tj) \\int F(t)\\exp(-t^2)\\mathrm{d}t \\approx \\sum_{j=1}^J w_j F(t_j) \u222bF(t)exp(\u2212t2)dt\u2248j=1\u2211J\u200bwj\u200bF(tj\u200b)</p> where tjt_jtj\u200b and wjw_jwj\u200b are the roots and weights of the JJJ-th order Hermite polynomial HJ(t)H_J(t)HJ\u200b(t) that we can look up in table link.</p>"},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.num_points","title":"<code>num_points: int = 20</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/integrators/#gpjax.integrators.GHQuadratureIntegrator.integrate","title":"<code>integrate(fun: Callable, y: Float[Array, N D], mean: Float[Array, N D], variance: Float[Array, N D], likelihood: gpjax.likelihoods.AbstractLikelihood = None) -&gt; Float[Array, N]</code>","text":"<p>Compute a quadrature integral.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The likelihood to be integrated.</p> required <code>y</code> <code>Float[Array, N D]</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, N D]</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, N D]</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>AbstractLikelihood</code> <p>The likelihood function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Array,  N]</code> <p>Float[Array, 'N']: The expected log likelihood.</p>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator","title":"<code>AnalyticalGaussianIntegrator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractIntegrator</code></p> <p>Compute the analytical integral of a Gaussian likelihood.</p> <p>When the likelihood function is Gaussian, the integral can be computed in closed form. For a Gaussian likelihood p(y\u2223f)=N(y\u2223f,\u03c32)p(y|f) = \\mathcal{N}(y|f, \\sigma^2)p(y\u2223f)=N(y\u2223f,\u03c32) and a variational distribution q(f)=N(f\u2223m,s)q(f) = \\mathcal{N}(f|m, s)q(f)=N(f\u2223m,s), the expected log-likelihood is given by ```math \\mathbb{E}{q(f)}[\\log p(y|f)] = -\\frac{1}{2}\\left(\\log(2\\pi\\sigma^2) + \\frac{1}{\\sigma^2}\\mathbb{E}[(y-f)^2 + s]\\right)</p>"},{"location":"api/integrators/#gpjax.integrators.AnalyticalGaussianIntegrator.integrate","title":"<code>integrate(fun: Callable, y: Float[Array, N D], mean: Float[Array, N D], variance: Float[Array, N D], likelihood: gpjax.likelihoods.Gaussian = None) -&gt; Float[Array, N]</code>","text":"<p>Compute a Gaussian integral.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>The Gaussian likelihood to be integrated.</p> required <code>y</code> <code>Float[Array, N D]</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, N D]</code> <p>The mean of the variational distribution.</p> required <code>variance</code> <code>Float[Array, N D]</code> <p>The variance of the variational distribution.</p> required <code>likelihood</code> <code>Gaussian</code> <p>The Gaussian likelihood function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Array,  N]</code> <p>Float[Array, 'N']: The expected log likelihood.</p>"},{"location":"api/likelihoods/","title":"Likelihoods","text":""},{"location":"api/likelihoods/#gpjax.likelihoods","title":"<code>gpjax.likelihoods</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.tfb","title":"<code>tfb = tfp.bijectors</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.NonGaussianLikelihood","title":"<code>NonGaussianLikelihood = Union[Poisson, Bernoulli]</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.__all__","title":"<code>__all__ = ['AbstractLikelihood', 'NonGaussianLikelihood', 'Gaussian', 'Bernoulli', 'Poisson', 'inv_probit']</code>  <code>module-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood","title":"<code>AbstractLikelihood</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract base class for likelihoods.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.num_datapoints","title":"<code>num_datapoints: int = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.integrator","title":"<code>integrator: AbstractIntegrator = static_field(GHQuadratureIntegrator())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.__call__--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; tfd.Distribution</code>  <code>abstractmethod</code>","text":"<p>Evaluate the likelihood function at a given predictive distribution.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to the likelihood's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>  <code>abstractmethod</code>","text":"<p>Return the link function of the likelihood function.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.link_function--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The distribution of observations, y, given values of the\n    Gaussian process, f.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.AbstractLikelihood.expected_log_likelihood","title":"<code>expected_log_likelihood(y: Float[Array, N D], mean: Float[Array, N D], variance: Float[Array, N D]) -&gt; Float[Array, N]</code>","text":"<p>Compute the expected log likelihood.</p> <p>For a variational distribution q(f)\u223cN(m,s)q(f)\\sim\\mathcal{N}(m, s)q(f)\u223cN(m,s) and a likelihood p(y\u2223f)p(y|f)p(y\u2223f), compute the expected log likelihood: <p>Eq(f)[log\u2061p(y\u2223f)] \\mathbb{E}_{q(f)}\\left[\\log p(y|f)\\right] Eq(f)\u200b[logp(y\u2223f)]</p></p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Float[Array, N D]</code> <p>The observed response variable.</p> required <code>mean</code> <code>Float[Array, N D]</code> <p>The variational mean.</p> required <code>variance</code> <code>Float[Array, N D]</code> <p>The variational variance.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>Float[Array,  N]</code> <p>The expected log likelihood.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian","title":"<code>Gaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractLikelihood</code></p> <p>Gaussian likelihood object.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.obs_noise","title":"<code>obs_noise: Union[ScalarFloat, Float[Array, #N]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.integrator","title":"<code>integrator: AbstractIntegrator = static_field(AnalyticalGaussianIntegrator())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Normal</code>","text":"<p>The link function of the Gaussian likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ]</code> <p>Function values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.link_function--returns","title":"Returns","text":"<pre><code>tfd.Normal: The likelihood function.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict","title":"<code>predict(dist: Union[tfd.MultivariateNormalTriL, GaussianDistribution]) -&gt; tfd.MultivariateNormalFullCovariance</code>","text":"<p>Evaluate the Gaussian likelihood.</p> <p>Evaluate the Gaussian likelihood function at a given predictive distribution. Computationally, this is equivalent to summing the observation noise term to the diagonal elements of the predictive distribution's covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>tfd.Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Gaussian.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli","title":"<code>Bernoulli</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractLikelihood</code></p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>","text":"<p>The probit link function of the Bernoulli likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ]</code> <p>Function values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.link_function--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The likelihood function.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict","title":"<code>predict(dist: tfd.Distribution) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the pointwise predictive distribution.</p> <p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>tfd.Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.Bernoulli.predict--returns","title":"Returns","text":"<pre><code>tfd.Distribution: The pointwise predictive distribution.\n</code></pre>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson","title":"<code>Poisson</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractLikelihood</code></p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.link_function","title":"<code>link_function(f: Float[Array, ...]) -&gt; tfd.Distribution</code>","text":"<p>The link function of the Poisson likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Float[Array, ]</code> <p>Function values.</p> required <p>Returns:</p> Type Description <code>tfd.Distribution</code> <p>tfd.Distribution: The likelihood function.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.Poisson.predict","title":"<code>predict(dist: tfd.Distribution) -&gt; tfd.Distribution</code>","text":"<p>Evaluate the pointwise predictive distribution.</p> <p>Evaluate the pointwise predictive distribution, given a Gaussian process posterior and likelihood parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>tfd.Distribution</code> <p>The Gaussian process posterior, evaluated at a finite set of test points.</p> required <p>Returns:</p> Type Description <code>tfd.Distribution</code> <p>tfd.Distribution: The pointwise predictive distribution.</p>"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit","title":"<code>inv_probit(x: Float[Array, *N]) -&gt; Float[Array, *N]</code>","text":"<p>Compute the inverse probit function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, *N]</code> <p>A vector of values.</p> required"},{"location":"api/likelihoods/#gpjax.likelihoods.inv_probit--returns","title":"Returns","text":"<pre><code>Float[Array, \"*N\"]: The inverse probit of the input vector.\n</code></pre>"},{"location":"api/mean_functions/","title":"Mean Functions","text":""},{"location":"api/mean_functions/#gpjax.mean_functions","title":"<code>gpjax.mean_functions</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.SumMeanFunction","title":"<code>SumMeanFunction = partial(CombinationMeanFunction, operator=partial(jnp.sum, axis=0))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.ProductMeanFunction","title":"<code>ProductMeanFunction = partial(CombinationMeanFunction, operator=partial(jnp.sum, axis=0))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Zero","title":"<code>Zero = partial(Constant, constant=jnp.array([0.0]))</code>  <code>module-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction","title":"<code>AbstractMeanFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Mean function that is used to parameterise the Gaussian process.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__","title":"<code>__call__(x: Num[Array, N D]) -&gt; Float[Array, N 1]</code>  <code>abstractmethod</code>","text":"<p>Evaluate the mean function at the given points. This method is required for all subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__","title":"<code>__add__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__add__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__radd__","title":"<code>__radd__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Add two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to add.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__radd__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The sum of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__","title":"<code>__mul__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__mul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__rmul__","title":"<code>__rmul__(other: Union[AbstractMeanFunction, Float[Array, 1]]) -&gt; AbstractMeanFunction</code>","text":"<p>Multiply two mean functions.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractMeanFunction</code> <p>The other mean function to multiply.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.AbstractMeanFunction.__rmul__--returns","title":"Returns","text":"<pre><code>AbstractMeanFunction: The product of the two mean functions.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant","title":"<code>Constant</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractMeanFunction</code></p> <p>Constant mean function.</p> <p>A constant mean function. This function returns a repeated scalar value for all inputs.  The scalar value itself can be treated as a model hyperparameter and learned during training but defaults to 1.0.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.constant","title":"<code>constant: Float[Array, 1] = param_field(jnp.array([0.0]))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__","title":"<code>__call__(x: Num[Array, N D]) -&gt; Float[Array, N 1]</code>","text":"<p>Evaluate the mean function at the given points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.Constant.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \"1\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction","title":"<code>CombinationMeanFunction</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractMeanFunction</code></p> <p>A base class for products or sums of AbstractMeanFunctions.</p>"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.means","title":"<code>means: List[AbstractMeanFunction] = items_list</code>  <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.operator","title":"<code>operator: Callable = operator</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__init__","title":"<code>__init__(means: List[AbstractMeanFunction], operator: Callable, **kwargs: Callable) -&gt; None</code>","text":""},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__","title":"<code>__call__(x: Num[Array, N D]) -&gt; Float[Array, N 1]</code>","text":"<p>Evaluate combination kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The point at which to evaluate the mean function.</p> required"},{"location":"api/mean_functions/#gpjax.mean_functions.CombinationMeanFunction.__call__--returns","title":"Returns","text":"<pre><code>Float[Array, \" Q\"]: The evaluated mean function.\n</code></pre>"},{"location":"api/objectives/","title":"Objectives","text":""},{"location":"api/objectives/#gpjax.objectives","title":"<code>gpjax.objectives</code>","text":""},{"location":"api/objectives/#gpjax.objectives.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.NonConjugateMLL","title":"<code>NonConjugateMLL = LogPosteriorDensity</code>  <code>module-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective","title":"<code>AbstractObjective</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract base class for objectives.</p>"},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.negative","title":"<code>negative: bool = static_field(False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.constant","title":"<code>constant: float = static_field(init=False, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.__call__","title":"<code>__call__(*args, **kwargs) -&gt; ScalarFloat</code>","text":""},{"location":"api/objectives/#gpjax.objectives.AbstractObjective.step","title":"<code>step(*args, **kwargs) -&gt; ScalarFloat</code>  <code>abstractmethod</code>","text":""},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL","title":"<code>ConjugateMLL</code>","text":"<p>         Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.step","title":"<code>step(posterior: gpjax.gps.ConjugatePosterior, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the marginal log-likelihood of the Gaussian process.</p> <p>Compute the marginal log-likelihood function of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here enables exact estimation of the Gaussian process' latent function values.</p> <p>For a training dataset {xn,yn}n=1N\\{x_n, y_n\\}_{n=1}^N{xn\u200b,yn\u200b}n=1N\u200b, set of test inputs x\u22c6\\mathbf{x}^{\\star}x\u22c6 the corresponding latent function evaluations are given by f=f(x)\\mathbf{f}=f(\\mathbf{x})f=f(x) and f\u22c6f(x\u22c6)\\mathbf{f}^{\\star}f(\\mathbf{x}^{\\star})f\u22c6f(x\u22c6), the marginal log-likelihood is given by: <p>log\u2061p(y)=\u222bp(y\u2223f)p(f,f\u22c6df\u22c6=0.5(\u2212y\u22a4(k(x,x\u2032)+\u03c32IN)\u22121y\u2212log\u2061\u2223k(x,x\u2032)+\u03c32IN\u2223\u2212nlog\u20612\u03c0). \\begin{align}     \\log p(\\mathbf{y}) &amp; = \\int p(\\mathbf{y}\\mid\\mathbf{f})p(\\mathbf{f}, \\mathbf{f}^{\\star}\\mathrm{d}\\mathbf{f}^{\\star}\\\\     &amp;=0.5\\left(-\\mathbf{y}^{\\top}\\left(k(\\mathbf{x}, \\mathbf{x}') +\\sigma^2\\mathbf{I}_N  \\right)^{-1}\\mathbf{y}-\\log\\lvert k(\\mathbf{x}, \\mathbf{x}') + \\sigma^2\\mathbf{I}_N\\rvert - n\\log 2\\pi \\right). \\end{align} logp(y)\u200b=\u222bp(y\u2223f)p(f,f\u22c6df\u22c6=0.5(\u2212y\u22a4(k(x,x\u2032)+\u03c32IN\u200b)\u22121y\u2212log\u2223k(x,x\u2032)+\u03c32IN\u200b\u2223\u2212nlog2\u03c0).\u200b\u200b</p></p> <p>For a given <code>ConjugatePosterior</code> object, the following code snippet shows how the marginal log-likelihood can be evaluated.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import gpjax as gpx\n&gt;&gt;&gt;\n&gt;&gt;&gt; xtrain = jnp.linspace(0, 1).reshape(-1, 1)\n&gt;&gt;&gt; ytrain = jnp.sin(xtrain)\n&gt;&gt;&gt; D = gpx.Dataset(X=xtrain, y=ytrain)\n&gt;&gt;&gt;\n&gt;&gt;&gt; meanf = gpx.mean_functions.Constant()\n&gt;&gt;&gt; kernel = gpx.kernels.RBF()\n&gt;&gt;&gt; likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n&gt;&gt;&gt; prior = gpx.Prior(mean_function = meanf, kernel=kernel)\n&gt;&gt;&gt; posterior = prior * likelihood\n&gt;&gt;&gt;\n&gt;&gt;&gt; mll = gpx.ConjugateMLL(negative=True)\n&gt;&gt;&gt; mll(posterior, train_data = D)\n</code></pre></p> <p>Our goal is to maximise the marginal log-likelihood. Therefore, when optimising the model's parameters with respect to the parameters, we use the negative marginal log-likelihood. This can be realised through</p> <pre><code>    mll = gpx.ConjugateMLL(negative=True)\n</code></pre> <p>For optimal performance, the marginal log-likelihood should be <code>jax.jit</code> compiled. <pre><code>    mll = jit(gpx.ConjugateMLL(negative=True))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> required <code>train_data</code> <code>Dataset</code> <p>The training dataset used to compute the marginal log-likelihood.</p> required"},{"location":"api/objectives/#gpjax.objectives.ConjugateMLL.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The marginal log-likelihood of the Gaussian process for the\n    current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity","title":"<code>LogPosteriorDensity</code>","text":"<p>         Bases: <code>AbstractObjective</code></p> <p>The log-posterior density of a non-conjugate Gaussian process. This is sometimes referred to as the marginal log-likelihood.</p>"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.step","title":"<code>step(posterior: gpjax.gps.NonConjugatePosterior, data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Evaluate the log-posterior density of a Gaussian process.</p> <p>Compute the marginal log-likelihood, or log-posterior density of the Gaussian process. The returned function can then be used for gradient based optimisation of the model's parameters or for model comparison. The implementation given here is general and will work for any likelihood support by GPJax.</p> <p>Unlike the marginal_log_likelihood function of the <code>ConjugatePosterior</code> object, the marginal_log_likelihood function of the <code>NonConjugatePosterior</code> object does not provide an exact marginal log-likelihood function. Instead, the <code>NonConjugatePosterior</code> object represents the posterior distributions as a function of the model's hyperparameters and the latent function. Markov chain Monte Carlo, variational inference, or Laplace approximations can then be used to sample from, or optimise an approximation to, the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>NonConjugatePosterior</code> <p>The posterior distribution for which we want to compute the marginal log-likelihood.</p> required <code>data</code> <code>Dataset</code> <p>The training dataset used to compute the marginal log-likelihood.</p> required"},{"location":"api/objectives/#gpjax.objectives.LogPosteriorDensity.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The log-posterior density of the Gaussian process for the\n    current parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.ELBO","title":"<code>ELBO</code>","text":"<p>         Bases: <code>AbstractObjective</code></p>"},{"location":"api/objectives/#gpjax.objectives.ELBO.step","title":"<code>step(variational_family: gpjax.variational_families.AbstractVariationalFamily, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Compute the evidence lower bound of a variational approximation.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data for which we should maximise the ELBO with respect to.</p> required"},{"location":"api/objectives/#gpjax.objectives.ELBO.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation for\n    the current model parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO","title":"<code>CollapsedELBO</code>","text":"<p>         Bases: <code>AbstractObjective</code></p> <p>The collapsed evidence lower bound.</p> <p>Collapsed variational inference for a sparse Gaussian process regression model. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.step","title":"<code>step(variational_family: gpjax.variational_families.AbstractVariationalFamily, train_data: Dataset) -&gt; ScalarFloat</code>","text":"<p>Compute a single step of the collapsed evidence lower bound.</p> <p>Compute the evidence lower bound under this model. In short, this requires evaluating the expectation of the model's log-likelihood under the variational approximation. To this, we sum the KL divergence from the variational posterior to the prior. When batching occurs, the result is scaled by the batch size relative to the full dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational approximation for whose parameters we should maximise the ELBO with respect to.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data for which we should maximise the ELBO with respect to.</p> required"},{"location":"api/objectives/#gpjax.objectives.CollapsedELBO.step--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evidence lower bound of the variational approximation for\n    the current model parameter set.\n</code></pre>"},{"location":"api/objectives/#gpjax.objectives.variational_expectation","title":"<code>variational_expectation(variational_family: gpjax.variational_families.AbstractVariationalFamily, train_data: Dataset) -&gt; Float[Array, N]</code>","text":"<p>Compute the variational expectation.</p> <p>Compute the expectation of our model's log-likelihood under our variational distribution. Batching can be done here to speed up computation.</p> <p>Parameters:</p> Name Type Description Default <code>variational_family</code> <code>AbstractVariationalFamily</code> <p>The variational family that we are using to approximate the posterior.</p> required <code>train_data</code> <code>Dataset</code> <p>The batch for which the expectation should be computed for.</p> required"},{"location":"api/objectives/#gpjax.objectives.variational_expectation--returns","title":"Returns","text":"<pre><code>Array: The expectation of the model's log-likelihood under our variational\n    distribution.\n</code></pre>"},{"location":"api/progress_bar/","title":"Progress Bar","text":""},{"location":"api/progress_bar/#gpjax.progress_bar","title":"<code>gpjax.progress_bar</code>","text":""},{"location":"api/progress_bar/#gpjax.progress_bar.__all__","title":"<code>__all__ = ['progress_bar']</code>  <code>module-attribute</code>","text":""},{"location":"api/progress_bar/#gpjax.progress_bar.progress_bar","title":"<code>progress_bar(num_iters: int, log_rate: int) -&gt; Callable</code>","text":"<p>Progress bar decorator for the body function of a <code>jax.lax.scan</code>.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; carry = jnp.array(0.0)\n&gt;&gt;&gt; iteration_numbers = jnp.arange(100)\n&gt;&gt;&gt; @progress_bar(num_iters=iteration_numbers.shape[0], log_rate=10)\n&gt;&gt;&gt; def body_func(carry, x):\n...    return carry, x\n&gt;&gt;&gt;\n&gt;&gt;&gt; carry, _ = jax.lax.scan(body_func, carry, iteration_numbers)\n</code></pre></p> <p>Adapted from this excellent blog post.</p> <p>Might be nice in future to directly create a general purpose <code>verbose scan</code> inplace of a for a jax.lax.scan, that takes the same arguments as a jax.lax.scan, but prints a progress bar.</p>"},{"location":"api/scan/","title":"Scan","text":""},{"location":"api/scan/#gpjax.scan","title":"<code>gpjax.scan</code>","text":""},{"location":"api/scan/#gpjax.scan.Carry","title":"<code>Carry = TypeVar('Carry')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.X","title":"<code>X = TypeVar('X')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.Y","title":"<code>Y = TypeVar('Y')</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.__all__","title":"<code>__all__ = ['vscan']</code>  <code>module-attribute</code>","text":""},{"location":"api/scan/#gpjax.scan.vscan","title":"<code>vscan(f: Callable[[Carry, X], Tuple[Carry, Y]], init: Carry, xs: X, length: Optional[int] = None, reverse: Optional[bool] = False, unroll: Optional[int] = 1, log_rate: Optional[int] = 10, log_value: Optional[bool] = True) -&gt; Tuple[Carry, Shaped[Array, ...]]</code>","text":"<p>Scan with verbose output.</p> <p>This is based on code from this excellent blog post.</p> <p>Example: <pre><code>    &gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; def f(carry, x):\nreturn carry + x, carry + x\n&gt;&gt;&gt; init = 0\n&gt;&gt;&gt; xs = jnp.arange(10)\n&gt;&gt;&gt; vscan(f, init, xs)\n</code></pre> <pre><code>    (45, DeviceArray([ 0,  1,  3,  6, 10, 15, 21, 28, 36, 45], dtype=int32))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Carry, X], Tuple[Carry, Y]]</code> <p>A function that takes in a carry and an input and returns a tuple of a new carry and an output.</p> required <code>init</code> <code>Carry</code> <p>The initial carry.</p> required <code>xs</code> <code>X</code> <p>The inputs.</p> required <code>length</code> <code>Optional[int]</code> <p>The length of the inputs. If None, then the length of the inputs is inferred.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>Whether to scan in reverse.</p> <code>False</code> <code>unroll</code> <code>int</code> <p>The number of iterations to unroll.</p> <code>1</code> <code>log_rate</code> <code>int</code> <p>The rate at which to log the progress bar.</p> <code>10</code> <code>log_value</code> <code>bool</code> <p>Whether to log the value of the objective function.</p> <code>True</code>"},{"location":"api/scan/#gpjax.scan.vscan--returns","title":"Returns","text":"<pre><code>Tuple[Carry, list[Y]]: A tuple of the final carry and the outputs.\n</code></pre>"},{"location":"api/typing/","title":"Typing","text":""},{"location":"api/typing/#gpjax.typing","title":"<code>gpjax.typing</code>","text":""},{"location":"api/typing/#gpjax.typing.OldKeyArray","title":"<code>OldKeyArray = UInt32[JAXArray, '2']</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.KeyArray","title":"<code>KeyArray = Union[OldKeyArray, JAXKeyArray]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.Array","title":"<code>Array = Union[JAXArray, NumpyArray]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarBool","title":"<code>ScalarBool = Union[bool, Bool[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarInt","title":"<code>ScalarInt = Union[int, Int[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.ScalarFloat","title":"<code>ScalarFloat = Union[float, Float[Array, '']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.VecNOrMatNM","title":"<code>VecNOrMatNM = Union[Float[Array, ' N'], Float[Array, 'N M']]</code>  <code>module-attribute</code>","text":""},{"location":"api/typing/#gpjax.typing.FunctionalSample","title":"<code>FunctionalSample = Callable[[Float[Array, 'N D']], Float[Array, 'N B']]</code>  <code>module-attribute</code>","text":"<p>Type alias for functions representing BBB samples from a model, to be evaluated on any set of NNN inputs (of dimension DDD) and returning the evaluations of each (potentially approximate) sample draw across these inputs.</p>"},{"location":"api/typing/#gpjax.typing.__all__","title":"<code>__all__ = ['KeyArray', 'ScalarBool', 'ScalarInt', 'ScalarFloat', 'FunctionalSample']</code>  <code>module-attribute</code>","text":""},{"location":"api/variational_families/","title":"Variational Families","text":""},{"location":"api/variational_families/#gpjax.variational_families","title":"<code>gpjax.variational_families</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.__all__","title":"<code>__all__ = ['AbstractVariationalFamily', 'AbstractVariationalGaussian', 'VariationalGaussian', 'WhitenedVariationalGaussian', 'NaturalVariationalGaussian', 'ExpectationVariationalGaussian', 'CollapsedVariationalGaussian']</code>  <code>module-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily","title":"<code>AbstractVariationalFamily</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Abstract base class used to represent families of distributions that can be used within variational inference.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.posterior","title":"<code>posterior: AbstractPosterior</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__","title":"<code>__call__(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the variational family's density.</p> <p>For a given set of parameters, compute the latent function's prediction under the variational approximation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.__call__--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's `predict` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict","title":"<code>predict(*args: Any, **kwargs: Any) -&gt; GaussianDistribution</code>  <code>abstractmethod</code>","text":"<p>Predict the GP's output given the input.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments of the variational family's <code>predict</code> method.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments of the variational family's <code>predict</code> method.</p> <code>{}</code>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalFamily.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The output of the variational family's ``predict`` method.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian","title":"<code>AbstractVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalFamily</code></p> <p>The variational Gaussian family of probability distributions.</p>"},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.inducing_inputs","title":"<code>inducing_inputs: Float[Array, N D]</code>  <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.jitter","title":"<code>jitter: ScalarFloat = static_field(1e-06)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.AbstractVariationalGaussian.num_inducing","title":"<code>num_inducing: int</code>  <code>property</code>","text":"<p>The number of inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian","title":"<code>VariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>The variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(\u03bc,S)q(u) = \\mathcal{N}(\\mu, S)q(u)=N(\u03bc,S).  We parameterise this over \u03bc\\mu\u03bc and sqrtsqrtsqrt with S=sqrtsqrt\u22a4S = sqrt sqrt^{\\top}S=sqrtsqrt\u22a4.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.variational_mean","title":"<code>variational_mean: Float[Array, N 1] = param_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.variational_root_covariance","title":"<code>variational_root_covariance: Float[Array, N N] = param_field(None, bijector=tfb.FillTriangular())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the prior KL divergence.</p> <p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(\u03bcz,Kzz)], \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)]\\\\ &amp; = \\operatorname{KL}[ \\mathcal{N}(\\mu, S) \\mid\\mid N(\\mu z, \\mathbf{K}_{zz}) ], \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(\u03bcz,Kzz\u200b)],\u200b\u200b</p> where u=f(z)u = f(z)u=f(z) and zzz are the inducing inputs.</p>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code> ScalarFloat: The KL-divergence between our variational\n    approximation and the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t))=\u222bp(f(t)\u2223u)q(u)duq(f(t)) = \\int p(f(t)\\mid u) q(u) \\mathrm{d}uq(f(t))=\u222bp(f(t)\u2223u)q(u)du, which can be computed in closed form as: <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt).     \\mathcal{N}\\left(f(t); \\mu t + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} (\\mu - \\mu z),  \\mathbf{K}_{tt} - \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} S \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt}\\right). N(f(t);\u03bct+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b).</p></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The test inputs at which we wish to make a prediction.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.VariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the low-rank GP at\n    the test inputs.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian","title":"<code>WhitenedVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>VariationalGaussian</code></p> <p>The whitened variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(Lz\u03bc+mz,LzSLz\u22a4)q(u) = \\mathcal{N}(Lz \\mu + mz, Lz S Lz^{\\top})q(u)=N(Lz\u03bc+mz,LzSLz\u22a4). We parameterise this over \u03bc\\mu\u03bc and sqrtsqrtsqrt with S=sqrtsqrt\u22a4S = sqrt sqrt^{\\top}S=sqrtsqrt\u22a4.</p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the KL-divergence between our variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(0,I)]. \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)]\\\\     &amp; = \\operatorname{KL}[N(\\mu  , S)\\mid\\mid N(0, I)]. \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(0,I)].\u200b\u200b</p></p>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational\n    approximation and the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs t.</p> <p>This is the integral q(f(t)) = \\int p(f(t)\\midu) q(u) du, which can be computed in closed form as <p>N(f(t);\u03bct+KtzLz\u22a4\u03bc,Ktt\u2212KtzKzz\u22121Kzt+KtzLz\u22a4SLz\u22121Kzt).     \\mathcal{N}\\left(f(t); \\mu t  +  \\mathbf{K}_{tz} \\mathbf{L}z^{\\top} \\mu  ,  \\mathbf{K}_{tt}  -  \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt}  +  \\mathbf{K}_{tz} \\mathbf{L}z^{\\top} S \\mathbf{L}z^{-1} \\mathbf{K}_{zt} \\right). N(f(t);\u03bct+Ktz\u200bLz\u22a4\u03bc,Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bLz\u22a4SLz\u22121Kzt\u200b).</p></p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The test inputs at which we wish to make a prediction.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.WhitenedVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the low-rank GP at\n    the test inputs.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian","title":"<code>NaturalVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(\u03bc,S)q(u) = N(\\mu, S)q(u)=N(\u03bc,S). Expressing the variational distribution, in the form of the exponential family, q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8))q(u) = exp(\\theta^{\\top} T(u) - a(\\theta))q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8)), gives rise to the natural parameterisation \u03b8=(\u03b81,\u03b82)=(S\u22121\u03bc,\u2212S\u22121/2)\\theta  = (\\theta_{1}, \\theta_{2}) = (S^{-1}\\mu, -S^{-1}/2)\u03b8=(\u03b81\u200b,\u03b82\u200b)=(S\u22121\u03bc,\u2212S\u22121/2), to perform model inference, where T(u)=[u,uu\u22a4]T(u) = [u, uu^{\\top}]T(u)=[u,uu\u22a4] are the sufficient statistics.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.natural_vector","title":"<code>natural_vector: Float[Array, M 1] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.natural_matrix","title":"<code>natural_matrix: Float[Array, M M] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061[q(f(\u22c5))\u2223\u2223p(\u22c5)]=KL\u2061[q(u)\u2223\u2223p(u)]=KL\u2061[N(\u03bc,S)\u2223\u2223N(mz,Kzz)], \\begin{align} \\operatorname{KL}[q(f(\\cdot))\\mid\\mid p(\\cdot)] &amp; = \\operatorname{KL}[q(u)\\mid\\mid p(u)] \\\\     &amp; = \\operatorname{KL}[N(\\mu, S)\\mid\\mid N(mz, \\mathbf{K}_{zz})], \\end{align} KL[q(f(\u22c5))\u2223\u2223p(\u22c5)]\u200b=KL[q(u)\u2223\u2223p(u)]=KL[N(\u03bc,S)\u2223\u2223N(mz,Kzz\u200b)],\u200b\u200b</p> with $\\mu$ and $S$ computed from the natural parameterisation $\\theta  = (S^{-1}\\mu  , -S^{-1}/2)$.</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational approximation and\n    the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs $t$.</p> <p>This is the integral q(f(t))=\u222bp(f(t)\u2223u)q(u)duq(f(t)) = \\int p(f(t)\\mid u) q(u) \\mathrm{d}uq(f(t))=\u222bp(f(t)\u2223u)q(u)du, which can be computed in closed form as <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt),      \\mathcal{N}\\left(f(t); \\mu  t + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} (\\mu   - \\mu  z),  \\mathbf{K}_{tt} - \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} + \\mathbf{K}_{tz} \\mathbf{K}_{zz}^{-1} S \\mathbf{K}_{zz}^{-1} \\mathbf{K}_{zt} \\right), N(f(t);\u03bct+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b),</p> with \u03bc\\mu\u03bc and SSS computed from the natural parameterisation \u03b8=(S\u22121\u03bc,\u2212S\u22121/2)\\theta = (S^{-1}\\mu  , -S^{-1}/2)\u03b8=(S\u22121\u03bc,\u2212S\u22121/2).</p>"},{"location":"api/variational_families/#gpjax.variational_families.NaturalVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: A function that accepts a set of test points and will\n    return the predictive distribution at those points.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian","title":"<code>ExpectationVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>The natural variational Gaussian family of probability distributions.</p> <p>The variational family is q(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)duq(f(\\cdot)) = \\int p(f(\\cdot)\\mid u) q(u) \\mathrm{d}uq(f(\u22c5))=\u222bp(f(\u22c5)\u2223u)q(u)du, where u=f(z)u = f(z)u=f(z) are the function values at the inducing inputs zzz and the distribution over the inducing inputs is q(u)=N(\u03bc,S)q(u) = \\mathcal{N}(\\mu, S)q(u)=N(\u03bc,S). Expressing the variational distribution, in the form of the exponential family, q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8))q(u) = exp(\\theta^{\\top} T(u) - a(\\theta))q(u)=exp(\u03b8\u22a4T(u)\u2212a(\u03b8)), gives rise to the natural parameterisation \u03b8=(\u03b81,\u03b82)=(S\u22121\u03bc,\u2212S\u22121/2)\\theta  = (\\theta_{1}, \\theta_{2}) = (S^{-1}\\mu  , -S^{-1}/2)\u03b8=(\u03b81\u200b,\u03b82\u200b)=(S\u22121\u03bc,\u2212S\u22121/2) and sufficient statistics T(u)=[u,uu\u22a4]T(u) = [u, uu^{\\top}]T(u)=[u,uu\u22a4]. The expectation parameters are given by \u03bd=\u222bT(u)q(u)du\\nu = \\int T(u) q(u) \\mathrm{d}u\u03bd=\u222bT(u)q(u)du. This gives a parameterisation, \u03bd=(\u03bd1,\u03bd2)=(\u03bc,S+uu\u22a4)\\nu = (\\nu_{1}, \\nu_{2}) = (\\mu  , S + uu^{\\top})\u03bd=(\u03bd1\u200b,\u03bd2\u200b)=(\u03bc,S+uu\u22a4) to perform model inference over.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.expectation_vector","title":"<code>expectation_vector: Float[Array, M 1] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.expectation_matrix","title":"<code>expectation_matrix: Float[Array, M M] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl","title":"<code>prior_kl() -&gt; ScalarFloat</code>","text":"<p>Evaluate the prior KL-divergence.</p> <p>Compute the KL-divergence between our current variational approximation and the Gaussian process prior.</p> <p>For this variational family, we have <p>KL\u2061(q(f(\u22c5))\u2223\u2223p(\u22c5))=KL\u2061(q(u)\u2223\u2223p(u))=KL\u2061(N(\u03bc,S)\u2223\u2223N(mz,Kzz)), \\begin{align} \\operatorname{KL}(q(f(\\cdot))\\mid\\mid p(\\cdot)) &amp; = \\operatorname{KL}(q(u)\\mid\\mid p(u)) \\\\     &amp; =\\operatorname{KL}(\\mathcal{N}(\\mu, S)\\mid\\mid \\mathcal{N}(m_z, K_{zz})), \\end{align} KL(q(f(\u22c5))\u2223\u2223p(\u22c5))\u200b=KL(q(u)\u2223\u2223p(u))=KL(N(\u03bc,S)\u2223\u2223N(mz\u200b,Kzz\u200b)),\u200b\u200b</p> where $\\mu$ and $S$ are the expectation parameters of the variational distribution and $m_z$ and $K_{zz}$ are the mean and covariance of the prior distribution.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.prior_kl--returns","title":"Returns","text":"<pre><code>ScalarFloat: The KL-divergence between our variational approximation and\n    the GP prior.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D]) -&gt; GaussianDistribution</code>","text":"<p>Evaluate the predictive distribution.</p> <p>Compute the predictive distribution of the GP at the test inputs $t$.</p> <p>This is the integral $q(f(t)) = \\int p(f(t)\\mid u)q(u)\\mathrm{d}u$, which can be computed in closed form as  which can be computed in closed form as <p>N(f(t);\u03bct+KtzKzz\u22121(\u03bc\u2212\u03bcz),Ktt\u2212KtzKzz\u22121Kzt+KtzKzz\u22121SKzz\u22121Kzt) \\mathcal{N}(f(t); \\mu_t + \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}(\\mu - \\mu_z), \\mathbf{K}_{tt} - \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}\\mathbf{K}_{zt} + \\mathbf{K}_{tz}\\mathbf{K}_{zz}^{-1}\\mathbf{S} \\mathbf{K}_{zz}^{-1}\\mathbf{K}_{zt}) N(f(t);\u03bct\u200b+Ktz\u200bKzz\u22121\u200b(\u03bc\u2212\u03bcz\u200b),Ktt\u200b\u2212Ktz\u200bKzz\u22121\u200bKzt\u200b+Ktz\u200bKzz\u22121\u200bSKzz\u22121\u200bKzt\u200b)</p></p> <p>with $\\mu$ and $S$ computed from the expectation parameterisation $\\eta = (\\mu, S + uu^\\top)$.</p>"},{"location":"api/variational_families/#gpjax.variational_families.ExpectationVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the GP at the\n    test inputs $t$.\n</code></pre>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian","title":"<code>CollapsedVariationalGaussian</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractVariationalGaussian</code></p> <p>Collapsed variational Gaussian.</p> <p>Collapsed variational Gaussian family of probability distributions. The key reference is Titsias, (2009) - Variational Learning of Inducing Variables in Sparse Gaussian Processes.</p>"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict","title":"<code>predict(test_inputs: Float[Array, N D], train_data: Dataset) -&gt; GaussianDistribution</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, N D]</code> <p>The test inputs $t$ at which to make predictions.</p> required <code>train_data</code> <code>Dataset</code> <p>The training data that was used to fit the GP.</p> required"},{"location":"api/variational_families/#gpjax.variational_families.CollapsedVariationalGaussian.predict--returns","title":"Returns","text":"<pre><code>GaussianDistribution: The predictive distribution of the collapsed\n    variational Gaussian process at the test inputs $t$.\n</code></pre>"},{"location":"api/base/module/","title":"Module","text":""},{"location":"api/base/module/#gpjax.base.module","title":"<code>gpjax.base.module</code>","text":""},{"location":"api/base/module/#gpjax.base.module.__all__","title":"<code>__all__ = ['Module', 'meta_leaves', 'meta_flatten', 'meta_map', 'meta', 'static_field']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Self","title":"<code>Self = TypeVar('Self')</code>  <code>module-attribute</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Module","title":"<code>Module</code>","text":"<p>         Bases: <code>Pytree</code></p>"},{"location":"api/base/module/#gpjax.base.module.Module.__init_subclass__","title":"<code>__init_subclass__(mutable: bool = False)</code>","text":""},{"location":"api/base/module/#gpjax.base.module.Module.replace","title":"<code>replace(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the values of the fields of the object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.replace--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_meta","title":"<code>replace_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Replace the metadata of the fields.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the metadata of the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_meta--returns","title":"Returns","text":"<pre><code>Module: with the metadata of the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.update_meta","title":"<code>update_meta(**kwargs: Any) -&gt; Self</code>","text":"<p>Update the metadata of the fields. The metadata must already exist.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>keyword arguments to replace the fields of the object.</p> <code>{}</code>"},{"location":"api/base/module/#gpjax.base.module.Module.update_meta--returns","title":"Returns","text":"<pre><code>Module: with the fields replaced.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_trainable","title":"<code>replace_trainable(**kwargs: Dict[str, bool]) -&gt; Self</code>","text":"<p>Replace the trainability status of local nodes of the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.replace_bijector","title":"<code>replace_bijector(**kwargs: Dict[str, tfb.Bijector]) -&gt; Self</code>","text":"<p>Replace the bijectors of local nodes of the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.constrain","title":"<code>constrain() -&gt; Self</code>","text":"<p>Transform model parameters to the constrained space according to their defined bijectors.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.constrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the constrained space.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.unconstrain","title":"<code>unconstrain() -&gt; Self</code>","text":"<p>Transform model parameters to the unconstrained space according to their defined bijectors.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.unconstrain--returns","title":"Returns","text":"<pre><code>Module: transformed to the unconstrained space.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.stop_gradient","title":"<code>stop_gradient() -&gt; Self</code>","text":"<p>Stop gradients flowing through the Module.</p>"},{"location":"api/base/module/#gpjax.base.module.Module.stop_gradient--returns","title":"Returns","text":"<pre><code>Module: with gradients stopped.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.Module.trainables","title":"<code>trainables() -&gt; Self</code>","text":""},{"location":"api/base/module/#gpjax.base.module.static_field","title":"<code>static_field(default: Any = dataclasses.MISSING, *, default_factory: Any = dataclasses.MISSING, init: bool = True, repr: bool = True, hash: Optional[bool] = None, compare: bool = True, metadata: Optional[Mapping[str, Any]] = None)</code>","text":""},{"location":"api/base/module/#gpjax.base.module.meta_leaves","title":"<code>meta_leaves(pytree: Module, *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; List[Tuple[Optional[Dict[str, Any]], Any]]</code>","text":"<p>Returns the meta of the leaves of the pytree.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>pytree to get the meta of.</p> required <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_leaves--returns","title":"Returns","text":"<pre><code>List[Tuple[Dict[str, Any], Any]]: meta of the leaves of the pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta_flatten","title":"<code>meta_flatten(pytree: Union[Module, Any], *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Union[Module, Any]</code>","text":"<p>Returns the meta of the Module.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>Module to get the meta of.</p> required <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_flatten--returns","title":"Returns","text":"<pre><code>Module: meta of the Module.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta_map","title":"<code>meta_map(f: Callable[[Any, Dict[str, Any]], Any], pytree: Union[Module, Any], *rest: Any, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Union[Module, Any]</code>","text":"<p>Apply a function to a Module where the first argument are the pytree leaves, and the second argument are the Module metadata leaves.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[[Any, Dict[str, Any]], Any]</code> <p>The function to apply to the pytree.</p> required <code>pytree</code> <code>Module</code> <p>The pytree to apply the function to.</p> required <code>rest</code> <code>Any</code> <p>Additional pytrees to apply the function to. Defaults to None.</p> <code>()</code> <code>is_leaf</code> <code>Callable[[Any], bool]</code> <p>predicate to determine if a node is a leaf. Defaults to None.</p> <code>None</code>"},{"location":"api/base/module/#gpjax.base.module.meta_map--returns","title":"Returns","text":"<pre><code>Module: The transformed pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.meta","title":"<code>meta(pytree: Module, *, is_leaf: Optional[Callable[[Any], bool]] = None) -&gt; Module</code>","text":"<p>Returns the metadata of the Module as a pytree.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Module</code> <p>pytree to get the metadata of.</p> required"},{"location":"api/base/module/#gpjax.base.module.meta--returns","title":"Returns","text":"<pre><code>Module: metadata of the pytree.\n</code></pre>"},{"location":"api/base/module/#gpjax.base.module.save_tree","title":"<code>save_tree(path: str, model: Module, overwrite: bool = False, iterate: int = None) -&gt; None</code>","text":""},{"location":"api/base/module/#gpjax.base.module.load_tree","title":"<code>load_tree(path: str, model: Module) -&gt; Module</code>","text":""},{"location":"api/base/param/","title":"Param","text":""},{"location":"api/base/param/#gpjax.base.param","title":"<code>gpjax.base.param</code>","text":""},{"location":"api/base/param/#gpjax.base.param.__all__","title":"<code>__all__ = ['param_field']</code>  <code>module-attribute</code>","text":""},{"location":"api/base/param/#gpjax.base.param.param_field","title":"<code>param_field(default: Any = dataclasses.MISSING, *, bijector: Optional[tfb.Bijector] = None, trainable: bool = True, default_factory: Any = dataclasses.MISSING, init: bool = True, repr: bool = True, hash: Optional[bool] = None, compare: bool = True, metadata: Optional[Mapping[str, Any]] = None)</code>","text":""},{"location":"api/kernels/base/","title":"Base","text":""},{"location":"api/kernels/base/#gpjax.kernels.base","title":"<code>gpjax.kernels.base</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.SumKernel","title":"<code>SumKernel = partial(CombinationKernel, operator=jnp.sum)</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.ProductKernel","title":"<code>ProductKernel = partial(CombinationKernel, operator=jnp.prod)</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel","title":"<code>AbstractKernel</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Module</code></p> <p>Base kernel class.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(DenseKernelComputation())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.active_dims","title":"<code>active_dims: Optional[List[int]] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.name","title":"<code>name: str = static_field('AbstractKernel')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.ndims","title":"<code>ndims</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.spectral_density","title":"<code>spectral_density: Optional[tfd.Distribution]</code>  <code>property</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.cross_covariance","title":"<code>cross_covariance(x: Num[Array, N D], y: Num[Array, M D])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.gram","title":"<code>gram(x: Num[Array, N D])</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input","title":"<code>slice_input(x: Float[Array, ... D]) -&gt; Float[Array, ... Q]</code>","text":"<p>Slice out the relevant columns of the input matrix.</p> <p>Select the relevant columns of the supplied matrix to be used within the kernel's evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The matrix or vector that is to be sliced.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.slice_input--returns","title":"Returns","text":"<pre><code>Float[Array, \"... Q\"]: A sliced form of the input matrix.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>  <code>abstractmethod</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__","title":"<code>__add__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be added to the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__add__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__radd__","title":"<code>__radd__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Add two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be added to the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__radd__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the sum of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__","title":"<code>__mul__(other: Union[AbstractKernel, ScalarFloat]) -&gt; AbstractKernel</code>","text":"<p>Multiply two kernels together.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>AbstractKernel</code> <p>The kernel to be multiplied with the current kernel.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.AbstractKernel.__mul__--returns","title":"Returns","text":"<pre><code>AbstractKernel: A new kernel that is the product of the two kernels.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant","title":"<code>Constant</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>A constant kernel. This kernel evaluates to a constant for all inputs. The scalar value itself can be treated as a model hyperparameter and learned during training.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.constant","title":"<code>constant: ScalarFloat = param_field(jnp.array(0.0))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.Constant.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel","title":"<code>CombinationKernel</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>A base class for products or sums of MeanFunctions.</p>"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.kernels","title":"<code>kernels: List[AbstractKernel] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.operator","title":"<code>operator: Callable = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/base/#gpjax.kernels.base.CombinationKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/approximations/rff/","title":"Rff","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff","title":"<code>gpjax.kernels.approximations.rff</code>","text":"<p>Compute Random Fourier Feature (RFF) kernel approximations.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF","title":"<code>RFF</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>Computes an approximation of the kernel using Random Fourier Features.</p> <p>All stationary kernels are equivalent to the Fourier transform of a probability distribution. We call the corresponding distribution the spectral density. Using a finite number of basis functions, we can compute the spectral density using a Monte-Carlo approximation. This is done by sampling from the spectral density and computing the Fourier transform of the samples. The kernel is then approximated by the inner product of the Fourier transform of the samples with the Fourier transform of the data.</p> <p>The key reference for this implementation is the following papers: - 'Random Features for Large-Scale Kernel Machines' by Rahimi and Recht (2008). - 'On the Error of Random Fourier Features' by Sutherland and Schneider (2015).</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.base_kernel","title":"<code>base_kernel: AbstractKernel = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.num_basis_fns","title":"<code>num_basis_fns: int = static_field(50)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.frequencies","title":"<code>frequencies: Float[Array, M 1] = param_field(None, bijector=tfb.Identity())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_engine","title":"<code>compute_engine: BasisFunctionComputation = static_field(BasisFunctionComputation(), repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.key","title":"<code>key: KeyArray = static_field(PRNGKey(123))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"<p>Post-initialisation function.</p> <p>This function is called after the initialisation of the kernel. It is used to set the computation engine to be the basis function computation engine.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.__call__","title":"<code>__call__(x: Array, y: Array) -&gt; Array</code>","text":"<p>Superfluous for RFFs.</p>"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features","title":"<code>compute_features(x: Float[Array, N D]) -&gt; Float[Array, N L]</code>","text":"<p>Compute the features for the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N D]</code> <p>A N\u00d7DN \\times DN\u00d7D array of inputs.</p> required"},{"location":"api/kernels/approximations/rff/#gpjax.kernels.approximations.rff.RFF.compute_features--returns","title":"Returns","text":"<pre><code>Float[Array, \"N L\"]: A N\u00d7LN \\times LN\u00d7L array of features where L=2ML = 2ML=2M.\n</code></pre>"},{"location":"api/kernels/computations/base/","title":"Base","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base","title":"<code>gpjax.kernels.computations.base</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation","title":"<code>AbstractKernelComputation</code>  <code>dataclass</code>","text":"<p>Abstract class for kernel computations.</p>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Num[Array, N D]) -&gt; LinearOperator</code>","text":"<p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N N]</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.gram--returns","title":"Returns","text":"<pre><code>LinearOperator: Gram covariance operator of the kernel function.\n</code></pre>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Num[Array, N D], y: Num[Array, M D]) -&gt; Float[Array, N M]</code>  <code>abstractmethod</code>","text":"<p>For a given kernel, compute the NxM gram matrix on an a pair of input matrices with shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>The first input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The second input matrix.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Num[Array, N D]) -&gt; DiagonalLinearOperator</code>","text":"<p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape NxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/base/#gpjax.kernels.computations.base.AbstractKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/","title":"Basis Functions","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions","title":"<code>gpjax.kernels.computations.basis_functions</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation","title":"<code>BasisFunctionComputation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Compute engine class for finite basis function approximations to a kernel.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute an approximate cross-covariance matrix.</p> <p>For a pair of inputs, compute the cross covariance matrix between the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>(Float[Array, \"N D\"]): A N\u00d7DN \\times DN\u00d7D array of inputs.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>(Float[Array, \"M D\"]): A M\u00d7DM \\times DM\u00d7D array of inputs.</p> required <p>Returns:</p> Type Description <code>Float[Array, N M]</code> <p>Float[Array, \"N M\"]: A $N \\times M$ array of cross-covariances.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.gram","title":"<code>gram(kernel: Kernel, inputs: Float[Array, N D]) -&gt; DenseLinearOperator</code>","text":"<p>Compute an approximate Gram matrix.</p> <p>For the Gram matrix, we can save computations by computing only one matrix multiplication between the inputs and the scaled frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, N D]</code> <p>A NxDN x DNxD array of inputs.</p> required <p>Returns:</p> Name Type Description <code>DenseLinearOperator</code> <code>DenseLinearOperator</code> <p>A dense linear operator representing the N\u00d7NN \\times NN\u00d7N Gram matrix.</p>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features","title":"<code>compute_features(kernel: Kernel, x: Float[Array, N D]) -&gt; Float[Array, N L]</code>","text":"<p>Compute the features for the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>A N\u00d7DN \\times DN\u00d7D array of inputs.</p> required"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.compute_features--returns","title":"Returns","text":"<pre><code>Float[Array, \"N L\"]: A N\u00d7LN \\times LN\u00d7L array of features where L=2ML = 2ML=2M.\n</code></pre>"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.scaling","title":"<code>scaling(kernel: Kernel)</code>","text":"<p>Compute the scaling factor for the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required"},{"location":"api/kernels/computations/basis_functions/#gpjax.kernels.computations.basis_functions.BasisFunctionComputation.scaling--returns","title":"Returns","text":"<pre><code>Float[Array, \"\"]: A scalar array.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/","title":"Constant Diagonal","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal","title":"<code>gpjax.kernels.computations.constant_diagonal</code>","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation","title":"<code>ConstantDiagonalKernelComputation</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Float[Array, N D]) -&gt; ConstantDiagonalLinearOperator</code>","text":"<p>Compute the Gram matrix.</p> <p>Compute Gram covariance operator of the kernel function.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N N]</code> <p>The inputs to the kernel function.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal","title":"<code>diagonal(kernel: Kernel, inputs: Float[Array, N D]) -&gt; DiagonalLinearOperator</code>","text":"<p>Compute the diagonal Gram matrix's entries.</p> <p>For a given kernel, compute the elementwise diagonal of the NxN gram matrix on an input matrix of shape N\u00d7DN\\times DN\u00d7D.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>inputs</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.diagonal--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: The computed diagonal variance entries.\n</code></pre>"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the NxM covariance matrix on a pair of input matrices of shape NxD and MxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/constant_diagonal/#gpjax.kernels.computations.constant_diagonal.ConstantDiagonalKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed square Gram matrix.\n</code></pre>"},{"location":"api/kernels/computations/dense/","title":"Dense","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense","title":"<code>gpjax.kernels.computations.dense</code>","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation","title":"<code>DenseKernelComputation</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Dense kernel computation class. Operations with the kernel assume a dense gram matrix structure.</p>"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the NxM covariance matrix on a pair of input matrices of shape NxDNxDNxD and MxDMxDMxD.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/dense/#gpjax.kernels.computations.dense.DenseKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/","title":"Diagonal","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal","title":"<code>gpjax.kernels.computations.diagonal</code>","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation","title":"<code>DiagonalKernelComputation</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Diagonal kernel computation class. Operations with the kernel assume a diagonal Gram matrix.</p>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.gram","title":"<code>gram(kernel: Kernel, x: Float[Array, N D]) -&gt; DiagonalLinearOperator</code>","text":"<p>Compute the Gram matrix.</p> <p>For a kernel with diagonal structure, compute the N\u00d7NN\\times NN\u00d7N Gram matrix on an input matrix of shape N\u00d7DN\\times DN\u00d7D.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.gram--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: The computed square Gram matrix.\n</code></pre>"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Float[Array, N D], y: Float[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For a given kernel, compute the N\u00d7MN\\times MN\u00d7M covariance matrix on a pair of input matrices of shape N\u00d7DN\\times DN\u00d7D and M\u00d7DM\\times DM\u00d7D.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required"},{"location":"api/kernels/computations/diagonal/#gpjax.kernels.computations.diagonal.DiagonalKernelComputation.cross_covariance--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: The computed cross-covariance.\n</code></pre>"},{"location":"api/kernels/computations/eigen/","title":"Eigen","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen","title":"<code>gpjax.kernels.computations.eigen</code>","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.Kernel","title":"<code>Kernel = tp.TypeVar('Kernel', bound='gpjax.kernels.base.AbstractKernel')</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation","title":"<code>EigenKernelComputation</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernelComputation</code></p> <p>Eigen kernel computation class. Kernels who operate on an eigen-decomposed structure should use this computation object.</p>"},{"location":"api/kernels/computations/eigen/#gpjax.kernels.computations.eigen.EigenKernelComputation.cross_covariance","title":"<code>cross_covariance(kernel: Kernel, x: Num[Array, N D], y: Num[Array, M D]) -&gt; Float[Array, N M]</code>","text":"<p>Compute the cross-covariance matrix.</p> <p>For an N\u00d7DN\\times DN\u00d7D and M\u00d7DM\\times DM\u00d7D pair of matrices, evaluate the N\u00d7MN \\times MN\u00d7M cross-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>the kernel function.</p> required <code>x</code> <code>Float[Array, N D]</code> <p>The input matrix.</p> required <code>y</code> <code>Float[Array, M D]</code> <p>The input matrix.</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>Float[Array, N M]</code> <p>description</p>"},{"location":"api/kernels/non_euclidean/graph/","title":"Graph","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph","title":"<code>gpjax.kernels.non_euclidean.graph</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.tfb","title":"<code>tfb = tfp.bijectors</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel","title":"<code>GraphKernel</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn graph kernel defined on the vertex set of a graph.</p> <p>A Mat\u00e9rn graph kernel defined on the vertices of a graph. The key reference for this object is borovitskiy et. al., (2020).</p> <p>Parameters:</p> Name Type Description Default <code>laplacian</code> <code>Float[Array]</code> <p>An N\u00d7NN \\times NN\u00d7N matrix representing the Laplacian matrix of a graph.</p> <code>static_field(None)</code>"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.laplacian","title":"<code>laplacian: Num[Array, N N] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.lengthscale","title":"<code>lengthscale: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.smoothness","title":"<code>smoothness: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.eigenvalues","title":"<code>eigenvalues: Float[Array, N] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.eigenvectors","title":"<code>eigenvectors: Float[Array, N N] = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.num_vertex","title":"<code>num_vertex: ScalarInt = static_field(None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(EigenKernelComputation(), repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.name","title":"<code>name: str = 'Graph Mat\u00e9rn'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__call__","title":"<code>__call__(x: Int[Array, N 1], y: Int[Array, N 1], *, S: Int[Array, N 1], **kwargs: Int[Array, N 1])</code>","text":"<p>Compute the (co)variance between a vertex pair.</p> <p>For a graph G={V,E}\\mathcal{G} = \\{V, E\\}G={V,E} where V={v1,v2,\u2026vn}V = \\{v_1, v_2, \\ldots v_n \\}V={v1\u200b,v2\u200b,\u2026vn\u200b}, evaluate the graph kernel on a pair of vertices (vi,vj)(v_i, v_j)(vi\u200b,vj\u200b) for any i,j&lt;ni,j&lt;ni,j&lt;n.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, N 1]</code> <p>Index of the iiith vertex.</p> required <code>y</code> <code>Float[Array, N 1]</code> <p>Index of the jjjth vertex.</p> required"},{"location":"api/kernels/non_euclidean/graph/#gpjax.kernels.non_euclidean.graph.GraphKernel.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(v_i, v_j)$.\n</code></pre>"},{"location":"api/kernels/non_euclidean/utils/","title":"Utils","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils","title":"<code>gpjax.kernels.non_euclidean.utils</code>","text":""},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd","title":"<code>jax_gather_nd(params: Num[Array, N *rest], indices: Int[Array, M 1]) -&gt; Num[Array, M *rest]</code>","text":"<p>Slice a <code>params</code> array at a set of <code>indices</code>.</p> <p>This is a reimplementation of TensorFlow's <code>gather_nd</code> function: link</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Num[Array]</code> <p>An arbitrary array with leading axes of length $N$ upon which we shall slice.</p> required <code>indices</code> <code>Float[Int]</code> <p>An integer array of length $M$ with values in the range $[0, N)$ whose value at index $i$ will be used to slice <code>params</code> at index $i$.</p> required"},{"location":"api/kernels/non_euclidean/utils/#gpjax.kernels.non_euclidean.utils.jax_gather_nd--returns","title":"Returns","text":"<pre><code>Num[Array: An arbitrary array with leading axes of length $M$.\n</code></pre>"},{"location":"api/kernels/nonstationary/arccosine/","title":"Arccosine","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine","title":"<code>gpjax.kernels.nonstationary.arccosine</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine","title":"<code>ArcCosine</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The ArCosine kernel.</p> <p>This kernel is non-stationary and resembles the behavior of neural networks. See Section 3.1 of Cho and Saul (2011) for additional details.</p>"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.order","title":"<code>order: ScalarInt = static_field(0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.weight_variance","title":"<code>weight_variance: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.bias_variance","title":"<code>bias_variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array, D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/nonstationary/arccosine/#gpjax.kernels.nonstationary.arccosine.ArcCosine.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/nonstationary/linear/","title":"Linear","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear","title":"<code>gpjax.kernels.nonstationary.linear</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear","title":"<code>Linear</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The linear kernel.</p>"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.name","title":"<code>name: str = 'Linear'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the linear kernel between a pair of arrays.</p> <p>For a pair of inputs x,y\u2208RDx, y \\in \\mathbb{R}^{D}x,y\u2208RD, let's evaluate the linear kernel k(x,y)=\u03c32x\u22a4yk(x, y)=\\sigma^2 x^{\\top}yk(x,y)=\u03c32x\u22a4y where \u03c3\u2208R&gt;0\\sigma^\\in \\mathbb{R}_{&gt;0}\u03c3\u2208R&gt;0\u200b is the kernel's variance parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand input of the kernel function.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand input of the kernel function.</p> required"},{"location":"api/kernels/nonstationary/linear/#gpjax.kernels.nonstationary.linear.Linear.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The evaluated kernel function k(x,y)k(x, y)k(x,y) at the supplied inputs.\n</code></pre>"},{"location":"api/kernels/nonstationary/polynomial/","title":"Polynomial","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial","title":"<code>gpjax.kernels.nonstationary.polynomial</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial","title":"<code>Polynomial</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Polynomial kernel with variable degree.</p>"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.degree","title":"<code>degree: ScalarInt = static_field(2)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.shift","title":"<code>shift: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the polynomial kernel of degree ddd between a pair of arrays.</p> <p>For a pair of inputs x,y\u2208RDx, y \\in \\mathbb{R}^{D}x,y\u2208RD, let's evaluate the polynomial kernel k(x,y)=(\u03b1+\u03c32xy)dk(x, y)=\\left( \\alpha + \\sigma^2 x y\\right)^{d}k(x,y)=(\u03b1+\u03c32xy)d where \u03c3\u2208R&gt;0\\sigma^\\in \\mathbb{R}_{&gt;0}\u03c3\u2208R&gt;0\u200b is the kernel's variance parameter, shift parameter \u03b1\\alpha\u03b1 and integer degree ddd.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/nonstationary/polynomial/#gpjax.kernels.nonstationary.polynomial.Polynomial.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/stationary/matern12/","title":"Mat\u00e9rn12","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12","title":"<code>gpjax.kernels.stationary.matern12</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12","title":"<code>Matern12</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 0.5.</p>"},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.name","title":"<code>name: str = 'Mat\u00e9rn12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern12/#gpjax.kernels.stationary.matern12.Matern12.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 1/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32. <p>k(x,y)=\u03c32exp\u2061(\u2212\u2223x\u2212y\u22232\u21132) k(x, y) = \\sigma^2\\exp\\Bigg(-\\frac{\\lvert x-y \\rvert}{2\\ell^2}\\Bigg) k(x,y)=\u03c32exp(\u22122\u21132\u2223x\u2212y\u2223\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of k(x,y)k(x, y)k(x,y)</p>"},{"location":"api/kernels/stationary/matern32/","title":"Mat\u00e9rn32","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32","title":"<code>gpjax.kernels.stationary.matern32</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32","title":"<code>Matern32</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 1.5.</p>"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.name","title":"<code>name: str = 'Mat\u00e9rn32'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 3/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32.</p> <p>k(x,y)=\u03c32exp\u2061(1+3\u2223x\u2212y\u2223\u21132)exp\u2061(\u22123\u2223x\u2212y\u2223\u21132)     k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{3}\\lvert x-y \\rvert}{\\ell^2}  \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{3}\\lvert x-y\\rvert}{\\ell^2} \\Bigg) k(x,y)=\u03c32exp(1+\u211323\u200b\u2223x\u2212y\u2223\u200b)exp(\u2212\u211323\u200b\u2223x\u2212y\u2223\u200b)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/matern32/#gpjax.kernels.stationary.matern32.Matern32.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of $k(x, y)$.\n</code></pre>"},{"location":"api/kernels/stationary/matern52/","title":"Mat\u00e9rn52","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52","title":"<code>gpjax.kernels.stationary.matern52</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52","title":"<code>Matern52</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Mat\u00e9rn kernel with smoothness parameter fixed at 2.5.</p>"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.name","title":"<code>name: str = 'Mat\u00e9rn52'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.spectral_density","title":"<code>spectral_density: tfd.Distribution</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Mat\u00e9rn 5/2 kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32. <p>k(x,y)=\u03c32exp\u2061(1+5\u2223x\u2212y\u2223\u21132+5\u2223x\u2212y\u222323\u21132)exp\u2061(\u22125\u2223x\u2212y\u2223\u21132) k(x, y) = \\sigma^2 \\exp \\Bigg(1+ \\frac{\\sqrt{5}\\lvert x-y \\rvert}{\\ell^2} + \\frac{5\\lvert x - y \\rvert^2}{3\\ell^2} \\Bigg)\\exp\\Bigg(-\\frac{\\sqrt{5}\\lvert x-y\\rvert}{\\ell^2} \\Bigg) k(x,y)=\u03c32exp(1+\u211325\u200b\u2223x\u2212y\u2223\u200b+3\u211325\u2223x\u2212y\u22232\u200b)exp(\u2212\u211325\u200b\u2223x\u2212y\u2223\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/matern52/#gpjax.kernels.stationary.matern52.Matern52.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/stationary/periodic/","title":"Periodic","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic","title":"<code>gpjax.kernels.stationary.periodic</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic","title":"<code>Periodic</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The periodic kernel.</p> <p>Key reference is MacKay 1998 - \"Introduction to Gaussian processes\".</p>"},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.period","title":"<code>period: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.name","title":"<code>name: str = 'Periodic'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/periodic/#gpjax.kernels.stationary.periodic.Periodic.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Periodic kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with length-scale parameter \u2113\\ell\u2113, variance \u03c32\\sigma^2\u03c32 and period ppp. <p>k(x,y)=\u03c32exp\u2061(\u221212\u2211i=1D(sin\u2061(\u03c0(xi\u2212yi)/p)\u2113)2) k(x, y) = \\sigma^2 \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\left(\\frac{\\sin (\\pi (x_i - y_i)/p)}{\\ell}\\right)^2 \\right) k(x,y)=\u03c32exp(\u221221\u200bi=1\u2211D\u200b(\u2113sin(\u03c0(xi\u200b\u2212yi\u200b)/p)\u200b)2)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of k(x,y)k(x, y)k(x,y).</p>"},{"location":"api/kernels/stationary/powered_exponential/","title":"Powered Exponential","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential","title":"<code>gpjax.kernels.stationary.powered_exponential</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential","title":"<code>PoweredExponential</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The powered exponential family of kernels. This also equivalent to the symmetric generalized normal distribution.</p> <p>See Diggle and Ribeiro (2007) - \"Model-based Geostatistics\". and https://en.wikipedia.org/wiki/Generalized_normal_distribution#Symmetric_version</p>"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.power","title":"<code>power: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Sigmoid())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.name","title":"<code>name: str = 'Powered Exponential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Powered Exponential kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with length-scale parameter \u2113\\ell\u2113, \u03c3\\sigma\u03c3 and power \u03ba\\kappa\u03ba. <p>k(x,y)=\u03c32exp\u2061(\u2212(\u2225x\u2212y\u22252\u21132)\u03ba) k(x, y)=\\sigma^2\\exp\\Bigg(-\\Big(\\frac{\\lVert x-y\\rVert^2}{\\ell^2}\\Big)^\\kappa\\Bigg) k(x,y)=\u03c32exp(\u2212(\u21132\u2225x\u2212y\u22252\u200b)\u03ba)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call</p> required"},{"location":"api/kernels/stationary/powered_exponential/#gpjax.kernels.stationary.powered_exponential.PoweredExponential.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/kernels/stationary/rational_quadratic/","title":"Rational Quadratic","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic","title":"<code>gpjax.kernels.stationary.rational_quadratic</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic","title":"<code>RationalQuadratic</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p>"},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.alpha","title":"<code>alpha: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.name","title":"<code>name: str = 'Rational Quadratic'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rational_quadratic/#gpjax.kernels.stationary.rational_quadratic.RationalQuadratic.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the Powered Exponential kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32. <p>k(x,y)=\u03c32exp\u2061(1+\u2225x\u2212y\u2225222\u03b1\u21132) k(x,y)=\\sigma^2\\exp\\Bigg(1+\\frac{\\lVert x-y\\rVert^2_2}{2\\alpha\\ell^2}\\Bigg) k(x,y)=\u03c32exp(1+2\u03b1\u21132\u2225x\u2212y\u222522\u200b\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of k(x,y)k(x, y)k(x,y).</p>"},{"location":"api/kernels/stationary/rbf/","title":"RBF","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf","title":"<code>gpjax.kernels.stationary.rbf</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF","title":"<code>RBF</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p> <p>The Radial Basis Function (RBF) kernel.</p>"},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.lengthscale","title":"<code>lengthscale: Union[ScalarFloat, Float[Array, D]] = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.name","title":"<code>name: str = 'RBF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.spectral_density","title":"<code>spectral_density: tfd.Normal</code>  <code>property</code>","text":""},{"location":"api/kernels/stationary/rbf/#gpjax.kernels.stationary.rbf.RBF.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the RBF kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with lengthscale parameter \u2113\\ell\u2113 and variance \u03c32\\sigma^2\u03c32: <p>k(x,y)=\u03c32exp\u2061(\u2212\u2225x\u2212y\u2225222\u21132) k(x,y)=\\sigma^2\\exp\\Bigg(- \\frac{\\lVert x - y \\rVert^2_2}{2 \\ell^2} \\Bigg) k(x,y)=\u03c32exp(\u22122\u21132\u2225x\u2212y\u222522\u200b\u200b)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required <p>Returns:</p> Name Type Description <code>ScalarFloat</code> <code>ScalarFloat</code> <p>The value of k(x,y)k(x, y)k(x,y).</p>"},{"location":"api/kernels/stationary/utils/","title":"Utils","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils","title":"<code>gpjax.kernels.stationary.utils</code>","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.tfd","title":"<code>tfd = tfp.distributions</code>  <code>module-attribute</code>","text":""},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution","title":"<code>build_student_t_distribution(nu: int) -&gt; tfd.Distribution</code>","text":"<p>Build a Student's t distribution with a fixed smoothness parameter.</p> <p>For a fixed half-integer smoothness parameter, compute the spectral density of a Mat\u00e9rn kernel; a Student's t distribution.</p> <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>int</code> <p>The smoothness parameter of the Mat\u00e9rn kernel.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.build_student_t_distribution--returns","title":"Returns","text":"<pre><code>tfp.Distribution: A Student's t distribution with the same smoothness parameter.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance","title":"<code>squared_distance(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the squared distance between a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>First input.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>Second input.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.squared_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The squared distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance","title":"<code>euclidean_distance(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the euclidean distance between a pair of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>First input.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>Second input.</p> required"},{"location":"api/kernels/stationary/utils/#gpjax.kernels.stationary.utils.euclidean_distance--returns","title":"Returns","text":"<pre><code>ScalarFloat: The euclidean distance between the inputs.\n</code></pre>"},{"location":"api/kernels/stationary/white/","title":"White","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white","title":"<code>gpjax.kernels.stationary.white</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White","title":"<code>White</code>  <code>dataclass</code>","text":"<p>         Bases: <code>AbstractKernel</code></p>"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.variance","title":"<code>variance: ScalarFloat = param_field(jnp.array(1.0), bijector=tfb.Softplus())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.compute_engine","title":"<code>compute_engine: AbstractKernelComputation = static_field(ConstantDiagonalKernelComputation(), repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.name","title":"<code>name: str = 'White'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__call__","title":"<code>__call__(x: Float[Array, D], y: Float[Array, D]) -&gt; ScalarFloat</code>","text":"<p>Compute the White noise kernel between a pair of arrays.</p> <p>Evaluate the kernel on a pair of inputs (x,y)(x, y)(x,y) with variance \u03c32\\sigma^2\u03c32: <p>k(x,y)=\u03c32\u03b4(x\u2212y) k(x, y) = \\sigma^2 \\delta(x-y) k(x,y)=\u03c32\u03b4(x\u2212y)</p></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array,  D]</code> <p>The left hand argument of the kernel function's call.</p> required <code>y</code> <code>Float[Array,  D]</code> <p>The right hand argument of the kernel function's call.</p> required"},{"location":"api/kernels/stationary/white/#gpjax.kernels.stationary.white.White.__call__--returns","title":"Returns","text":"<pre><code>ScalarFloat: The value of k(x,y)k(x, y)k(x,y).\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/","title":"Constant Diagonal Linear Operator","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator","title":"<code>gpjax.linops.constant_diagonal_linear_operator</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.__all__","title":"<code>__all__ = ['ConstantDiagonalLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator","title":"<code>ConstantDiagonalLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>DiagonalLinearOperator</code></p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.value","title":"<code>value: Float[Array, 1] = value</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.size","title":"<code>size: int = size</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.shape","title":"<code>shape = (size, size)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.dtype","title":"<code>dtype = value.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__init__","title":"<code>__init__(value: Float[Array, 1], size: int, dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Initialize the constant diagonal linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Float[Array, 1]</code> <p>Constant value of the diagonal.</p> required <code>size</code> <code>int</code> <p>Size of the diagonal.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__add__","title":"<code>__add__(other: Union[Float[Array, N N], LinearOperator]) -&gt; LinearOperator</code>","text":""},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__mul__","title":"<code>__mul__(other: Union[ScalarFloat, Float[Array, 1]]) -&gt; LinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>LinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.to_root","title":"<code>to_root() -&gt; ConstantDiagonalLinearOperator</code>","text":"<p>Lower triangular.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower triangular matrix.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.inverse","title":"<code>inverse() -&gt; ConstantDiagonalLinearOperator</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.inverse--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Inverse of the covariance operator.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":"<p>Solve linear system.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; ConstantDiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_root","title":"<code>from_root(root: ConstantDiagonalLinearOperator) -&gt; ConstantDiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>ConstantDiagonalLinearOperator</code> <p>Root of the covariance operator.</p> required"},{"location":"api/linops/constant_diagonal_linear_operator/#gpjax.linops.constant_diagonal_linear_operator.ConstantDiagonalLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>ConstantDiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/","title":"Dense Linear Operator","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator","title":"<code>gpjax.linops.dense_linear_operator</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.__all__","title":"<code>__all__ = ['DenseLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator","title":"<code>DenseLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>LinearOperator</code></p> <p>Dense covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.matrix","title":"<code>matrix: Float[Array, N N] = matrix</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.shape","title":"<code>shape = matrix.shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.dtype","title":"<code>dtype = matrix.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__init__","title":"<code>__init__(matrix: Float[Array, N N], dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Initialize the covariance operator.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__add__","title":"<code>__add__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Add diagonal to another linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[LinearOperator, Float[Array, N N]]</code> <p>Other linear operator. Dimension of both operators must match. If the other linear operator is not a DiagonalLinearOperator, dense matrix addition is used.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__add__--returns","title":"Returns","text":"<pre><code>LinearOperator: linear operator plus the diagonal linear operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; LinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>LinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: The diagonal of the covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__matmul__","title":"<code>__matmul__(other: VecNOrMatNM) -&gt; VecNOrMatNM</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Float[Array, N M]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>","text":"<p>Construct dense Covariance matrix from the covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense covariance matrix.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_dense","title":"<code>from_dense(matrix: Float[Array, N N]) -&gt; DenseLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from dense covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>Float[Array, N N]</code> <p>Dense covariance matrix.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>DenseLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_root","title":"<code>from_root(root: LinearOperator) -&gt; DenseLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the root of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Float[Array, N N]</code> <p>Root of the covariance matrix.</p> required"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>DenseLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator","title":"<code>DenseFromRootLinearOperator</code>","text":"<p>         Bases: <code>DenseLinearOperator</code></p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.root","title":"<code>root: LinearOperator = root</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.shape","title":"<code>shape = root.shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.dtype","title":"<code>dtype = root.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.matrix","title":"<code>matrix: Float[Array, N N]</code>  <code>property</code>","text":""},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.__init__","title":"<code>__init__(root: LinearOperator)</code>","text":"<p>Initialize the covariance operator.</p>"},{"location":"api/linops/dense_linear_operator/#gpjax.linops.dense_linear_operator.DenseFromRootLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/diagonal_linear_operator/","title":"Diagonal Linear Operator","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator","title":"<code>gpjax.linops.diagonal_linear_operator</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.__all__","title":"<code>__all__ = ['DiagonalLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator","title":"<code>DiagonalLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>LinearOperator</code></p> <p>Diagonal covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.diag","title":"<code>diag: Float[Array, N] = diag</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.shape","title":"<code>shape = (dim, dim)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.dtype","title":"<code>dtype = diag.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__init__","title":"<code>__init__(diag: Float[Array, N], dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Initialize the covariance operator.</p> <p>Parameters:</p> Name Type Description Default <code>diag</code> <code>Float[Array,  N]</code> <p>Diagonal of the covariance operator.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: Diagonal of the covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__add__","title":"<code>__add__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Add diagonal to another linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[LinearOperator, Float[Array, N N]]</code> <p>Other linear operator. Dimension of both operators must match. If the other linear operator is not a DiagonalLinearOperator, dense matrix addition is used.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__add__--returns","title":"Returns","text":"<pre><code>LinearOperator: linear operator plus the diagonal linear operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; LinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>LinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>","text":"<p>Construct dense Covariance matrix from the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense covariance matrix.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__matmul__","title":"<code>__matmul__(other: VecNOrMatNM) -&gt; VecNOrMatNM</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Float[Array, N M]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_root","title":"<code>to_root() -&gt; DiagonalLinearOperator</code>","text":"<p>Lower triangular.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower triangular matrix.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.inverse","title":"<code>inverse() -&gt; DiagonalLinearOperator</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.inverse--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Inverse of the covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.solve","title":"<code>solve(rhs: VecNOrMatNM) -&gt; VecNOrMatNM</code>","text":"<p>Solve linear system.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_root","title":"<code>from_root(root: DiagonalLinearOperator) -&gt; DiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the lower triangular matrix.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; DiagonalLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from its dense matrix representation.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator","title":"<code>DiagonalFromRootLinearOperator</code>","text":"<p>         Bases: <code>DiagonalLinearOperator</code></p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.root","title":"<code>root: DiagonalLinearOperator = root</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.shape","title":"<code>shape = root.shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.dtype","title":"<code>dtype = root.dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.diag","title":"<code>diag: Float[Array, N]</code>  <code>property</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.__init__","title":"<code>__init__(root: DiagonalLinearOperator)</code>","text":"<p>Initialize the covariance operator.</p>"},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/diagonal_linear_operator/#gpjax.linops.diagonal_linear_operator.DiagonalFromRootLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":""},{"location":"api/linops/identity_linear_operator/","title":"Identity Linear Operator","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator","title":"<code>gpjax.linops.identity_linear_operator</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.__all__","title":"<code>__all__ = ['IdentityLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator","title":"<code>IdentityLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>ConstantDiagonalLinearOperator</code></p> <p>Identity linear operator.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.value","title":"<code>value = jnp.array([1.0], dtype=dtype)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.size","title":"<code>size = size</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.shape","title":"<code>shape = (size, size)</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.dtype","title":"<code>dtype = dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.__init__","title":"<code>__init__(size: int, dtype: jnp.dtype = None) -&gt; None</code>","text":"<p>Identity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the identity matrix.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.__matmul__","title":"<code>__matmul__(other: Float[Array, N M]) -&gt; Float[Array, N M]</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Float[Array, N M]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.to_root","title":"<code>to_root() -&gt; IdentityLinearOperator</code>","text":"<p>Lower triangular.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower triangular matrix.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.inverse","title":"<code>inverse() -&gt; IdentityLinearOperator</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.inverse--returns","title":"Returns","text":"<pre><code>DiagonalLinearOperator: Inverse of the covariance operator.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":"<p>Solve linear system.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.from_root","title":"<code>from_root(root: IdentityLinearOperator) -&gt; IdentityLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct from root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>IdentityLinearOperator</code> <p>Root of the covariance operator.</p> required"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>IdentityLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/identity_linear_operator/#gpjax.linops.identity_linear_operator.IdentityLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; IdentityLinearOperator</code>  <code>classmethod</code>","text":""},{"location":"api/linops/linear_operator/","title":"Linear Operator","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator","title":"<code>gpjax.linops.linear_operator</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.NestedT","title":"<code>NestedT = Union[T, Iterable['NestedT'], Mapping[Any, 'NestedT']]</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.DTypes","title":"<code>DTypes = Union[Type[jnp.float32], Type[jnp.float64], Type[jnp.int32], Type[jnp.int64]]</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.ShapeT","title":"<code>ShapeT = TypeVar('ShapeT', bound=NestedT[Tuple[int, Ellipsis]])</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.DTypeT","title":"<code>DTypeT = TypeVar('DTypeT', bound=NestedT[DTypes])</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.__all__","title":"<code>__all__ = ['LinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator","title":"<code>LinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>Pytree</code>, <code>Generic[ShapeT, DTypeT]</code></p> <p>Linear operator base class.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.shape","title":"<code>shape: ShapeT = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.dtype","title":"<code>dtype: DTypeT = static_field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Linear operator dimension.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.T","title":"<code>T: LinearOperator</code>  <code>property</code>","text":"<p>Transpose linear operator. Currently, we assume all linear operators are square and symmetric.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Linear operator representation.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__sub__","title":"<code>__sub__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Subtract linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__rsub__","title":"<code>__rsub__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Reimplimentation of subtract linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__add__","title":"<code>__add__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Add linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__radd__","title":"<code>__radd__(other: Union[LinearOperator, Float[Array, N N]]) -&gt; LinearOperator</code>","text":"<p>Reimplimentation of add linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; LinearOperator</code>  <code>abstractmethod</code>","text":"<p>Multiply linear operator by scalar.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__rmul__","title":"<code>__rmul__(other: ScalarFloat) -&gt; LinearOperator</code>","text":"<p>Reimplimentation of multiply linear operator by scalar.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__matmul__","title":"<code>__matmul__(other: Union[LinearOperator, Float[Array, N M]]) -&gt; Union[LinearOperator, Float[Array, N M]]</code>  <code>abstractmethod</code>","text":"<p>Matrix multiplication.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.__rmatmul__","title":"<code>__rmatmul__(other: Union[LinearOperator, Float[Array, N M]]) -&gt; Union[LinearOperator, Float[Array, N M]]</code>","text":"<p>Reimplimentation of matrix multiplication.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>  <code>abstractmethod</code>","text":"<p>Diagonal of the linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: Diagonal of the linear operator.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.trace","title":"<code>trace() -&gt; ScalarFloat</code>","text":"<p>Trace of the linear matrix.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.trace--returns","title":"Returns","text":"<pre><code>ScalarFloat: Trace of the linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant of the linear matrix. Default implementation uses dense Cholesky decomposition.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":"<p>Compute the root of the linear operator via the Cholesky decomposition.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_root--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Lower Cholesky decomposition of the linear operator.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.inverse","title":"<code>inverse() -&gt; LinearOperator</code>","text":"<p>Inverse of the linear matrix. Default implementation uses dense Cholesky decomposition.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.inverse--returns","title":"Returns","text":"<pre><code>LinearOperator: Inverse of the linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":"<p>Solve linear system. Default implementation uses dense Cholesky decomposition.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>Float[Array, N M]</code> <p>Right hand side of the linear system.</p> required"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.solve--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Solution of the linear system.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>  <code>abstractmethod</code>","text":"<p>Construct dense matrix from the linear operator.</p>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense linear matrix.\n</code></pre>"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; LinearOperator</code>  <code>classmethod</code>","text":"<p>Construct linear operator from dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/linear_operator/#gpjax.linops.linear_operator.LinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>LinearOperator: Linear operator.\n</code></pre>"},{"location":"api/linops/triangular_linear_operator/","title":"Triangular Linear Operator","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator","title":"<code>gpjax.linops.triangular_linear_operator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.__all__","title":"<code>__all__ = ['LowerTriangularLinearOperator', 'UpperTriangularLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator","title":"<code>LowerTriangularLinearOperator</code>","text":"<p>         Bases: <code>DenseLinearOperator</code></p> <p>Current implementation of the following methods is inefficient. We assume a dense matrix representation of the operator. But take advantage of the solve structure.</p>"},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.T","title":"<code>T: UpperTriangularLinearOperator</code>  <code>property</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.inverse","title":"<code>inverse() -&gt; DenseLinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.from_root","title":"<code>from_root(root: LinearOperator) -&gt; None</code>  <code>classmethod</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.LowerTriangularLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; LowerTriangularLinearOperator</code>  <code>classmethod</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator","title":"<code>UpperTriangularLinearOperator</code>","text":"<p>         Bases: <code>DenseLinearOperator</code></p> <p>Current implementation of the following methods is inefficient. We assume a dense matrix representation of the operator. But take advantage of the solve structure.</p>"},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.T","title":"<code>T: LowerTriangularLinearOperator</code>  <code>property</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.to_root","title":"<code>to_root() -&gt; LinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.inverse","title":"<code>inverse() -&gt; DenseLinearOperator</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; Float[Array, ... M]</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.from_root","title":"<code>from_root(root: LinearOperator) -&gt; None</code>  <code>classmethod</code>","text":""},{"location":"api/linops/triangular_linear_operator/#gpjax.linops.triangular_linear_operator.UpperTriangularLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; UpperTriangularLinearOperator</code>  <code>classmethod</code>","text":""},{"location":"api/linops/utils/","title":"Utils","text":""},{"location":"api/linops/utils/#gpjax.linops.utils","title":"<code>gpjax.linops.utils</code>","text":""},{"location":"api/linops/utils/#gpjax.linops.utils.__all__","title":"<code>__all__ = ['identity', 'to_dense']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/utils/#gpjax.linops.utils.identity","title":"<code>identity(n: int) -&gt; gpjax.linops.identity_linear_operator.IdentityLinearOperator</code>","text":"<p>Identity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Size of the identity matrix.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.identity--returns","title":"Returns","text":"<pre><code>IdentityLinearOperator: Identity matrix of shape [n, n].\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.to_dense","title":"<code>to_dense(obj: Union[Float[Array, ...], LinearOperator])</code>","text":"<p>Ensure an object is a dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Float[Array, ], LinearOperator]</code> <p>Linear operator to convert.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"...\"]: Dense matrix.\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.to_linear_operator","title":"<code>to_linear_operator(obj: Union[Float[Array, ...], LinearOperator])</code>","text":"<p>Ensure an object is a linear operator.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Float[Array, ], LinearOperator]</code> <p>Linear operator to convert.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.to_linear_operator--returns","title":"Returns","text":"<pre><code>LinearOperator: Linear operator.\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.check_shapes_match","title":"<code>check_shapes_match(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -&gt; None</code>","text":"<p>Check shapes of two objects.</p> <p>Parameters:</p> Name Type Description Default <code>shape1</code> <code>Tuple[int, ]</code> <p>Shape of the first object.</p> required <code>shape2</code> <code>Tuple[int, ]</code> <p>Shape of the second object.</p> required"},{"location":"api/linops/utils/#gpjax.linops.utils.check_shapes_match--raises","title":"Raises","text":"<pre><code>ValueError: Shapes of the two objects do not match.\n</code></pre>"},{"location":"api/linops/utils/#gpjax.linops.utils.default_dtype","title":"<code>default_dtype() -&gt; Union[Type[jnp.float64], Type[jnp.float32]]</code>","text":"<p>Get the default dtype for the linear operator.</p>"},{"location":"api/linops/utils/#gpjax.linops.utils.default_dtype--returns","title":"Returns","text":"<pre><code>jnp.dtype: Default dtype for the linear operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/","title":"Zero Linear Operator","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator","title":"<code>gpjax.linops.zero_linear_operator</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.__all__","title":"<code>__all__ = ['ZeroLinearOperator']</code>  <code>module-attribute</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator","title":"<code>ZeroLinearOperator</code>  <code>dataclass</code>","text":"<p>         Bases: <code>LinearOperator</code></p> <p>Zero linear operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.shape","title":"<code>shape = shape</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.dtype","title":"<code>dtype = dtype</code>  <code>instance-attribute</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__init__","title":"<code>__init__(shape: Tuple[int, ...], dtype: jnp.dtype = None) -&gt; None</code>","text":""},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.diagonal","title":"<code>diagonal() -&gt; Float[Array, N]</code>","text":"<p>Diagonal of the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.diagonal--returns","title":"Returns","text":"<pre><code>Float[Array, \" N\"]: The diagonal of the covariance operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__add__","title":"<code>__add__(other: Union[Float[Array, N N], LinearOperator]) -&gt; Union[Float[Array, N N], LinearOperator]</code>","text":"<p>Add covariance operator to another covariance operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[Float[Array, N N], LinearOperator]</code> <p>Covariance operator to add.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__add__--returns","title":"Returns","text":"<pre><code>Union[Float[Array, \"N N\"], LinearOperator]: Sum of the covariance operators.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__mul__","title":"<code>__mul__(other: ScalarFloat) -&gt; ZeroLinearOperator</code>","text":"<p>Multiply covariance operator by scalar.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ConstantDiagonalLinearOperator</code> <p>Scalar.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__mul__--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Covariance operator multiplied by a scalar.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__matmul__","title":"<code>__matmul__(other: Union[LinearOperator, Float[Array, N M]]) -&gt; ZeroLinearOperator</code>","text":"<p>Matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[LinearOperator, Float[Array, N M]]</code> <p>Matrix to multiply with.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.__matmul__--returns","title":"Returns","text":"<pre><code>Float[Array, \"N M\"]: Result of matrix multiplication.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_dense","title":"<code>to_dense() -&gt; Float[Array, N N]</code>","text":"<p>Construct dense Covariance matrix from the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_dense--returns","title":"Returns","text":"<pre><code>Float[Array, \"N N\"]: Dense covariance matrix.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_root","title":"<code>to_root() -&gt; ZeroLinearOperator</code>","text":"<p>Root of the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.to_root--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Root of the covariance operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.log_det","title":"<code>log_det() -&gt; ScalarFloat</code>","text":"<p>Log determinant.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.log_det--returns","title":"Returns","text":"<pre><code>ScalarFloat: Log determinant of the covariance matrix.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.inverse","title":"<code>inverse() -&gt; None</code>","text":"<p>Inverse of the covariance operator.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.inverse--raises","title":"Raises","text":"<pre><code>RuntimeError: ZeroLinearOperator is not invertible.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.solve","title":"<code>solve(rhs: Float[Array, ... M]) -&gt; None</code>","text":"<p>Solve linear system.</p>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.solve--raises","title":"Raises","text":"<pre><code>RuntimeError: ZeroLinearOperator is not invertible.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_root","title":"<code>from_root(root: ZeroLinearOperator) -&gt; ZeroLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>ZeroLinearOperator</code> <p>Root of the covariance operator.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_root--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Covariance operator.\n</code></pre>"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_dense","title":"<code>from_dense(dense: Float[Array, N N]) -&gt; ZeroLinearOperator</code>  <code>classmethod</code>","text":"<p>Construct covariance operator from the dense matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense</code> <code>Float[Array, N N]</code> <p>Dense matrix.</p> required"},{"location":"api/linops/zero_linear_operator/#gpjax.linops.zero_linear_operator.ZeroLinearOperator.from_dense--returns","title":"Returns","text":"<pre><code>ZeroLinearOperator: Covariance operator.\n</code></pre>"},{"location":"examples/","title":"Where to find the docs","text":""},{"location":"examples/#where-to-find-the-docs","title":"Where to find the docs","text":"<p>The GPJax documentation can be found here: https://docs.jaxgaussianprocesses.com/</p>"},{"location":"examples/#how-to-build-the-docs","title":"How to build the docs","text":"<ol> <li>Ensure you have installed the requirements using <code>poetry install</code> in the root    directory.</li> <li>Make sure <code>pandoc</code> is installed</li> <li>Run the command <code>poetry run mkdocs serve</code> in the root directory.</li> </ol> <p>The documentation will then be served at an IP address printed, which can then be opened in a browser of you choice e.g. <code>Serving on http://127.0.0.1:8000/</code>.</p>"},{"location":"examples/#how-to-write-code-documentation","title":"How to write code documentation","text":"<p>Our documentation is generated using MkDocs. This automatically creates online documentation from docstrings, with full support for Markdown. Longer tutorial-style notebooks are also converted to webpages by MkDocs, with these notebooks being stored in the <code>docs/examples</code> directory. If you write a new notebook and wish to add it to the documentation website, add it to the <code>nav</code> section of the <code>mkdocs.yml</code> file found in the root directory.</p> <p>Below we provide some guidelines for writing docstrings.</p>"},{"location":"examples/#how-much-information-to-put-in-a-docstring","title":"How much information to put in a docstring","text":"<p>A docstring should be informative. If in doubt, then it is best to add more information to a docstring than less. Many users will skim documentation, so please ensure the opening sentence or two of a docstring contains the core information. Adding examples and mathematical descriptions to documentation is highly desirable.</p> <p>We are making an active effort within GPJax to improve our documentation. If you spot any areas where there is missing information within the existing documentation, then please either raise an issue or create a pull request.</p>"},{"location":"examples/#an-example-docstring","title":"An example docstring","text":"<p>An example docstring that adheres the principles of GPJax is given below. The docstring contains a simple, snappy introduction with links to auxiliary components. More detail is then provided in the form of a mathematical description and a code example. The docstring is concluded with a description of the objects attributes with corresponding types.</p> <pre><code>from gpjax.gps import AbstractPrior\nfrom gpjax.mean_functions import AbstractMeanFunction\nfrom gpjax.kernels import AbstractKernel\nfrom typing import Optional\nclass Prior(AbstractPrior):\nr\"\"\"A Gaussian process prior object.\n    The GP is parameterised by a\n    [mean](https://docs.jaxgaussianprocesses.com/api/mean_functions/)\n    and [kernel](https://docs.jaxgaussianprocesses.com/api/kernels/base/) function.\n    A Gaussian process prior parameterised by a mean function $`m(\\cdot)`$ and a kernel\n    function $`k(\\cdot, \\cdot)`$ is given by\n    $`p(f(\\cdot)) = \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))`$.\n    To invoke a `Prior` distribution, a kernel and mean function must be specified.\n    Example:\n        &gt;&gt;&gt; import gpjax as gpx\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; meanf = gpx.mean_functions.Zero()\n        &gt;&gt;&gt; kernel = gpx.kernels.RBF()\n        &gt;&gt;&gt; prior = gpx.Prior(mean_function=meanf, kernel = kernel)\n    Attributes:\n        kernel (Kernel): The kernel function used to parameterise the prior.\n        mean_function (MeanFunction): The mean function used to parameterise the prior. Defaults to zero.\n        name (str): The name of the GP prior. Defaults to \"GP prior\".\n    \"\"\"\nkernel: AbstractKernel\nmean_function: AbstractMeanFunction\nname: Optional[str] = \"GP prior\"\n</code></pre>"},{"location":"examples/#documentation-syntax","title":"Documentation syntax","text":"<p>We adopt the following convention when documenting objects:</p> <ul> <li>Class attributes should be specified using the <code>Attributes:</code> tag.</li> <li>Method argument should be specified using the <code>Args:</code> tags.</li> <li>Values returned by a method should be specified using the <code>Returns:</code> tag.</li> <li>All attributes, arguments and returned values should have types.</li> </ul> <p>Note</p> <p>Inline math in docstrings needs to be rendered within both <code>$</code> and <code>symbols to be correctly rendered by MkDocs. For instance, where one would typically write `$k(x,y)$` in standard LaTeX, in docstrings you are required to write</code>k(x,y)k(x,y)k(x,y)`` in order for the math to be correctly rendered by MkDocs.</p>"},{"location":"examples/barycentres/","title":"Gaussian Processes Barycentres","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport typing as tp\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy.linalg as jsl\nfrom jaxtyping import install_import_hook\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax.distributions as tfd\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  import typing as tp  import jax import jax.numpy as jnp import jax.random as jr import jax.scipy.linalg as jsl from jaxtyping import install_import_hook import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax.distributions as tfd  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx   key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>n = 100\nn_test = 200\nn_datasets = 5\n\nx = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1)\nxtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1)\nf = lambda x, a, b: a + jnp.sin(b * x)\n\nys = []\nfor _i in range(n_datasets):\n    key, subkey = jr.split(key)\n    vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)\n    period = jr.uniform(subkey, minval=0.75, maxval=1.25)\n    noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)\n    noise = jr.normal(subkey, shape=x.shape) * noise_amount\n    ys.append(f(x, vertical_shift, period) + noise)\n\ny = jnp.hstack(ys)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"x\")\nplt.show()\n</pre> n = 100 n_test = 200 n_datasets = 5  x = jnp.linspace(-5.0, 5.0, n).reshape(-1, 1) xtest = jnp.linspace(-5.5, 5.5, n_test).reshape(-1, 1) f = lambda x, a, b: a + jnp.sin(b * x)  ys = [] for _i in range(n_datasets):     key, subkey = jr.split(key)     vertical_shift = jr.uniform(subkey, minval=0.0, maxval=2.0)     period = jr.uniform(subkey, minval=0.75, maxval=1.25)     noise_amount = jr.uniform(subkey, minval=0.01, maxval=0.5)     noise = jr.normal(subkey, shape=x.shape) * noise_amount     ys.append(f(x, vertical_shift, period) + noise)  y = jnp.hstack(ys)  fig, ax = plt.subplots() ax.plot(x, y, \"x\") plt.show() In\u00a0[3]: Copied! <pre>def fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    D = gpx.Dataset(X=x, y=y)\n\n    likelihood = gpx.Gaussian(num_datapoints=n)\n    posterior = gpx.Prior(mean_function=gpx.Constant(), kernel=gpx.RBF()) * likelihood\n\n    opt_posterior, _ = gpx.fit(\n        model=posterior,\n        objective=jax.jit(gpx.ConjugateMLL(negative=True)),\n        train_data=D,\n        optim=ox.adamw(learning_rate=0.01),\n        num_iters=500,\n        key=key,\n    )\n    latent_dist = opt_posterior.predict(xtest, train_data=D)\n    return opt_posterior.likelihood(latent_dist)\n\n\nposterior_preds = [fit_gp(x, i) for i in ys]\n</pre> def fit_gp(x: jax.Array, y: jax.Array) -&gt; tfd.MultivariateNormalFullCovariance:     if y.ndim == 1:         y = y.reshape(-1, 1)     D = gpx.Dataset(X=x, y=y)      likelihood = gpx.Gaussian(num_datapoints=n)     posterior = gpx.Prior(mean_function=gpx.Constant(), kernel=gpx.RBF()) * likelihood      opt_posterior, _ = gpx.fit(         model=posterior,         objective=jax.jit(gpx.ConjugateMLL(negative=True)),         train_data=D,         optim=ox.adamw(learning_rate=0.01),         num_iters=500,         key=key,     )     latent_dist = opt_posterior.predict(xtest, train_data=D)     return opt_posterior.likelihood(latent_dist)   posterior_preds = [fit_gp(x, i) for i in ys] <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> In\u00a0[4]: Copied! <pre>def sqrtm(A: jax.Array):\n    return jnp.real(jsl.sqrtm(A))\n\n\ndef wasserstein_barycentres(\n    distributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array\n):\n    covariances = [d.covariance() for d in distributions]\n    cov_stack = jnp.stack(covariances)\n    stack_sqrt = jax.vmap(sqrtm)(cov_stack)\n\n    def step(covariance_candidate: jax.Array, idx: None):\n        inner_term = jax.vmap(sqrtm)(\n            jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)\n        )\n        fixed_point = jnp.tensordot(weights, inner_term, axes=1)\n        return fixed_point, fixed_point\n\n    return step\n</pre> def sqrtm(A: jax.Array):     return jnp.real(jsl.sqrtm(A))   def wasserstein_barycentres(     distributions: tp.List[tfd.MultivariateNormalFullCovariance], weights: jax.Array ):     covariances = [d.covariance() for d in distributions]     cov_stack = jnp.stack(covariances)     stack_sqrt = jax.vmap(sqrtm)(cov_stack)      def step(covariance_candidate: jax.Array, idx: None):         inner_term = jax.vmap(sqrtm)(             jnp.matmul(jnp.matmul(stack_sqrt, covariance_candidate), stack_sqrt)         )         fixed_point = jnp.tensordot(weights, inner_term, axes=1)         return fixed_point, fixed_point      return step <p>With a function defined for learning a barycentre, we'll now compute it using the <code>lax.scan</code> operator that drastically speeds up for loops in Jax (see the Jax documentation). The iterative update will be executed 100 times, with convergence measured by the difference between the previous and current iteration that we can confirm by inspecting the <code>sequence</code> array in the following cell.</p> In\u00a0[5]: Copied! <pre>weights = jnp.ones((n_datasets,)) / n_datasets\n\nmeans = jnp.stack([d.mean() for d in posterior_preds])\nbarycentre_mean = jnp.tensordot(weights, means, axes=1)\n\nstep_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights))\ninitial_covariance = jnp.eye(n_test)\n\nbarycentre_covariance, sequence = jax.lax.scan(\n    step_fn, initial_covariance, jnp.arange(100)\n)\nL = jnp.linalg.cholesky(barycentre_covariance)\n\nbarycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L)\n</pre> weights = jnp.ones((n_datasets,)) / n_datasets  means = jnp.stack([d.mean() for d in posterior_preds]) barycentre_mean = jnp.tensordot(weights, means, axes=1)  step_fn = jax.jit(wasserstein_barycentres(posterior_preds, weights)) initial_covariance = jnp.eye(n_test)  barycentre_covariance, sequence = jax.lax.scan(     step_fn, initial_covariance, jnp.arange(100) ) L = jnp.linalg.cholesky(barycentre_covariance)  barycentre_process = tfd.MultivariateNormalTriL(barycentre_mean, L) In\u00a0[6]: Copied! <pre>def plot(\n    dist: tfd.MultivariateNormalTriL,\n    ax,\n    color: str,\n    label: str = None,\n    ci_alpha: float = 0.2,\n    linewidth: float = 1.0,\n    zorder: int = 0,\n):\n    mu = dist.mean()\n    sigma = dist.stddev()\n    ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)\n    ax.fill_between(\n        xtest.squeeze(),\n        mu - sigma,\n        mu + sigma,\n        alpha=ci_alpha,\n        color=color,\n        zorder=zorder,\n    )\n\n\nfig, ax = plt.subplots()\n[plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds]\nplot(\n    barycentre_process,\n    ax,\n    color=cols[0],\n    label=\"Barycentre\",\n    ci_alpha=0.5,\n    linewidth=2,\n    zorder=1,\n)\nax.legend()\n</pre> def plot(     dist: tfd.MultivariateNormalTriL,     ax,     color: str,     label: str = None,     ci_alpha: float = 0.2,     linewidth: float = 1.0,     zorder: int = 0, ):     mu = dist.mean()     sigma = dist.stddev()     ax.plot(xtest, mu, linewidth=linewidth, color=color, label=label, zorder=zorder)     ax.fill_between(         xtest.squeeze(),         mu - sigma,         mu + sigma,         alpha=ci_alpha,         color=color,         zorder=zorder,     )   fig, ax = plt.subplots() [plot(d, ax, color=cols[1], ci_alpha=0.1) for d in posterior_preds] plot(     barycentre_process,     ax,     color=cols[0],     label=\"Barycentre\",     ci_alpha=0.5,     linewidth=2,     zorder=1, ) ax.legend() Out[6]: <pre>&lt;matplotlib.legend.Legend at 0x7f5138753820&gt;</pre> In\u00a0[7]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\noptax                 : 0.1.5\nmatplotlib            : 3.7.1\ntensorflow_probability: 0.19.0\njax                   : 0.4.9\ngpjax                 : 0.0.0\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/barycentres/#gaussian-processes-barycentres","title":"Gaussian Processes Barycentres\u00b6","text":"<p>In this notebook we'll give an implementation of . In this work, the existence of a Wasserstein barycentre between a collection of Gaussian processes is proven. When faced with trying to average a set of probability distributions, the Wasserstein barycentre is an attractive choice as it enables uncertainty amongst the individual distributions to be incorporated into the averaged distribution. When compared to a naive mean of means and mean of variances approach to computing the average probability distributions, it can be seen that Wasserstein barycentres offer significantly more favourable uncertainty estimation.</p>"},{"location":"examples/barycentres/#background","title":"Background\u00b6","text":""},{"location":"examples/barycentres/#wasserstein-distance","title":"Wasserstein distance\u00b6","text":"<p>The 2-Wasserstein distance metric between two probability measures $\\mu$ and $\\nu$ quantifies the minimal cost required to transport the unit mass from $\\mu$ to $\\nu$, or vice-versa. Typically, computing this metric requires solving a linear program. However, when $\\mu$ and $\\nu$ both belong to the family of multivariate Gaussian distributions, the solution is analytically given by $$W_2^2(\\mu, \\nu) = \\lVert m_1- m_2 \\rVert^2_2 + \\operatorname{Tr}(S_1 + S_2 - 2(S_1^{1/2}S_2S_1^{1/2})^{1/2}),$$ where $\\mu \\sim \\mathcal{N}(m_1, S_1)$ and $\\nu\\sim\\mathcal{N}(m_2, S_2)$.</p>"},{"location":"examples/barycentres/#wasserstein-barycentre","title":"Wasserstein barycentre\u00b6","text":"<p>For a collection of $T$ measures $\\lbrace\\mu_i\\rbrace_{t=1}^T \\in \\mathcal{P}_2(\\theta)$, the Wasserstein barycentre $\\bar{\\mu}$ is the measure that minimises the average Wasserstein distance to all other measures in the set. More formally, the Wasserstein barycentre is the Fr\u00e9chet mean on a Wasserstein space that we can write as $$\\bar{\\mu} = \\operatorname{argmin}_{\\mu\\in\\mathcal{P}_2(\\theta)}\\sum_{t=1}^T \\alpha_t W_2^2(\\mu, \\mu_t),$$ where $\\alpha\\in\\mathbb{R}^T$ is a weight vector that sums to 1.</p> <p>As with the Wasserstein distance, identifying the Wasserstein barycentre $\\bar{\\mu}$ is often an computationally demanding optimisation problem. However, when all the measures admit a multivariate Gaussian density, the barycentre $\\bar{\\mu} = \\mathcal{N}(\\bar{m}, \\bar{S})$ has analytical solutions $$\\bar{m} = \\sum_{t=1}^T \\alpha_t m_t\\,, \\quad \\bar{S}=\\sum_{t=1}^T\\alpha_t (\\bar{S}^{1/2}S_t\\bar{S}^{1/2})^{1/2}\\,. \\qquad (\\star)$$ Identifying $\\bar{S}$ is achieved through a fixed-point iterative update.</p>"},{"location":"examples/barycentres/#barycentre-of-gaussian-processes","title":"Barycentre of Gaussian processes\u00b6","text":"<p>It was shown in  that the barycentre $\\bar{f}$ of a collection of Gaussian processes $\\lbrace f_i\\rbrace_{i=1}^T$ such that $f_i \\sim \\mathcal{GP}(m_i, K_i)$ can be found using the same solutions as in $(\\star)$. For a full theoretical understanding, we recommend reading the original paper. However, the central argument to this result is that one can first show that the barycentre GP $\\bar{f}\\sim\\mathcal{GP}(\\bar{m}, \\bar{S})$ is non-degenerate for any finite set of GPs $\\lbrace f_t\\rbrace_{t=1}^T$ i.e., $T&lt;\\infty$. With this established, one can show that for a $n$-dimensional finite Gaussian distribution $f_{i,n}$, the Wasserstein metric between any two Gaussian distributions $f_{i, n}, f_{j, n}$ converges to the Wasserstein metric between GPs as $n\\to\\infty$.</p> <p>In this notebook, we will demonstrate how this can be achieved in GPJax.</p>"},{"location":"examples/barycentres/#dataset","title":"Dataset\u00b6","text":"<p>We'll simulate five datasets and develop a Gaussian process posterior before identifying the Gaussian process barycentre at a set of test points. Each dataset will be a sine function with a different vertical shift, periodicity, and quantity of noise.</p>"},{"location":"examples/barycentres/#learning-a-posterior-distribution","title":"Learning a posterior distribution\u00b6","text":"<p>We'll now independently learn Gaussian process posterior distributions for each dataset. We won't spend any time here discussing how GP hyperparameters are optimised. For advice on achieving this, see the Regression notebook for advice on optimisation and the Kernels notebook for advice on selecting an appropriate kernel.</p>"},{"location":"examples/barycentres/#computing-the-barycentre","title":"Computing the barycentre\u00b6","text":"<p>In GPJax, the predictive distribution of a GP is given by a TensorFlow Probability distribution, making it straightforward to extract the mean vector and covariance matrix of each GP for learning a barycentre. We implement the fixed point scheme given in (3) in the following cell by utilising Jax's <code>vmap</code> operator to speed up large matrix operations using broadcasting in <code>tensordot</code>.</p>"},{"location":"examples/barycentres/#plotting-the-result","title":"Plotting the result\u00b6","text":"<p>With a barycentre learned, we can visualise the result. We can see that the result looks reasonable as it follows the sinusoidal curve of all the inferred GPs, and the uncertainty bands are sensible.</p>"},{"location":"examples/barycentres/#displacement-interpolation","title":"Displacement interpolation\u00b6","text":"<p>In the above example, we assigned uniform weights to each of the posteriors within the barycentre. In practice, we may have prior knowledge of which posterior is most likely to be the correct one. Regardless of the weights chosen, the barycentre remains a Gaussian process. We can interpolate between a pair of posterior distributions $\\mu_1$ and $\\mu_2$ to visualise the corresponding barycentre $\\bar{\\mu}$.</p> <p></p>"},{"location":"examples/barycentres/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/bayesian_optimisation/","title":"Introduction to Bayesian Optimisation","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float, Int\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom typing import List, Tuple\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\nfrom gpjax.typing import Array, FunctionalSample, ScalarFloat\nfrom jaxopt import ScipyBoundedMinimize\n\nkey = jr.PRNGKey(42)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  import jax from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook, Float, Int import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib import cm import optax as ox import tensorflow_probability.substrates.jax as tfp from typing import List, Tuple  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx from gpjax.typing import Array, FunctionalSample, ScalarFloat from jaxopt import ScipyBoundedMinimize  key = jr.PRNGKey(42) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>def forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:\n    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\n</pre> def forrester(x: Float[Array, \"N 1\"]) -&gt; Float[Array, \"N 1\"]:     return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4) <p>We'll first go through one iteration of the BO loop step-by-step, before wrapping this up in a loop to perform the full optimisation.</p> <p>First we'll specify the domain over which we wish to optimise the function, as well as sampling some initial points for fitting our surrogate model using a space-filling design.</p> In\u00a0[3]: Copied! <pre>lower_bound = jnp.array([0.0])\nupper_bound = jnp.array([1.0])\ninitial_sample_num = 5\n\ninitial_x = tfp.mcmc.sample_halton_sequence(\n    dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\n</pre> lower_bound = jnp.array([0.0]) upper_bound = jnp.array([1.0]) initial_sample_num = 5  initial_x = tfp.mcmc.sample_halton_sequence(     dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64 ).reshape(-1, 1) initial_y = forrester(initial_x) D = gpx.Dataset(X=initial_x, y=initial_y) <pre>/usr/share/miniconda/envs/test/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3613: UserWarning: 'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n  warnings.warn(\"'kind' argument to argsort is ignored; only 'stable' sorts \"\n</pre> <p>Next we'll define our GP model in the usual way, using a Mat\u00e9rn52 kernel, and fit the kernel parameters by minimising the negative log-marginal likelihood. We'll wrap this in a function as we'll be repeating this process at each iteration of the BO loop.</p> In\u00a0[4]: Copied! <pre>def return_optimised_posterior(\n    data: gpx.Dataset, prior: gpx.Module, key: Array\n) -&gt; gpx.Module:\n    likelihood = gpx.Gaussian(\n        num_datapoints=data.n, obs_noise=jnp.array(1e-6)\n    )  # Our function is noise-free, so we set the observation noise to a very small value\n    likelihood = likelihood.replace_trainable(obs_noise=False)\n\n    posterior = prior * likelihood\n\n    negative_mll = gpx.objectives.ConjugateMLL(negative=True)\n    negative_mll(posterior, train_data=data)\n    negative_mll = jit(negative_mll)\n\n    opt_posterior, history = gpx.fit(\n        model=posterior,\n        objective=negative_mll,\n        train_data=data,\n        optim=ox.adam(learning_rate=0.01),\n        num_iters=1000,\n        safe=True,\n        key=key,\n        verbose=False,\n    )\n\n    return opt_posterior\n\n\nmean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\nopt_posterior = return_optimised_posterior(D, prior, key)\n</pre> def return_optimised_posterior(     data: gpx.Dataset, prior: gpx.Module, key: Array ) -&gt; gpx.Module:     likelihood = gpx.Gaussian(         num_datapoints=data.n, obs_noise=jnp.array(1e-6)     )  # Our function is noise-free, so we set the observation noise to a very small value     likelihood = likelihood.replace_trainable(obs_noise=False)      posterior = prior * likelihood      negative_mll = gpx.objectives.ConjugateMLL(negative=True)     negative_mll(posterior, train_data=data)     negative_mll = jit(negative_mll)      opt_posterior, history = gpx.fit(         model=posterior,         objective=negative_mll,         train_data=data,         optim=ox.adam(learning_rate=0.01),         num_iters=1000,         safe=True,         key=key,         verbose=False,     )      return opt_posterior   mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Matern52() prior = gpx.Prior(mean_function=mean, kernel=kernel) opt_posterior = return_optimised_posterior(D, prior, key) <p>We can then sample a function from the posterior distribution of the surrogate model. We will do this using the <code>sample_approx</code> method, which generates an approximate sample from the posterior using decoupled sampling introduced in (Wilson et al., 2020) and discussed in our Pathwise Sampling Notebook. This method is used as it enables us to sample from the posterior in a manner which scales linearly with the number of points sampled, $O(N)$, mitigating the cubic cost associated with drawing exact samples from a GP posterior, $O(N^3)$. It also generates more accurate samples than many other methods for drawing approximate samples from a GP posterior.</p> <p>Note that we also define a <code>utility_fn</code> which calls the approximate sample but returns the value returned as a scalar. This is because the <code>sample_approx</code> function returns an array of shape $[N, B]$, with $N$ being the number of points within each sample and $B$ being the number of samples drawn. We'll only be drawing (and optimising) one sample at a time, and our optimiser requires the function being optimised to return a scalar output (only querying it at $N=1$ points), so we'll remove the axes from the returned value.</p> In\u00a0[5]: Copied! <pre>approx_sample = opt_posterior.sample_approx(\n    num_samples=1, train_data=D, key=key, num_features=500\n)\nutility_fn = lambda x: approx_sample(x)[0][0]\n</pre> approx_sample = opt_posterior.sample_approx(     num_samples=1, train_data=D, key=key, num_features=500 ) utility_fn = lambda x: approx_sample(x)[0][0] <p>In order to minimise the sample, we'll be using the L-BFGS-B (Byrd et al., 1995) optimiser from the <code>jaxopt</code> library. This is a gradient-based optimiser which performs optimisation within a bounded domain. In order to perform optimisation, this optimiser requires a point to start from. Therefore, we will first query our sample from the posterior at a random set of points, and then use the lowest point from this set of points as the starting point for the optimiser. In this example we'll sample 100 points from the posterior, due to the simple nature of the Forrester function. However, in practice it can be beneficial to adopt a more sophisticated approach, and there are several heuristics available in the literature (see for example (Le Riche and Picheny, 2021)). For instance, one may randomly sample the posterior at a number of points proportional to the dimensionality of the input space, and one may run gradient-based optimisation from multiple of these points, to reduce the risk of converging upon local minima.</p> In\u00a0[6]: Copied! <pre>def optimise_sample(\n    sample: FunctionalSample,\n    key: Int[Array, \"\"],\n    lower_bound: Float[Array, \"D\"],\n    upper_bound: Float[Array, \"D\"],\n    num_initial_sample_points: int,\n) -&gt; ScalarFloat:\n    initial_sample_points = jr.uniform(\n        key,\n        shape=(num_initial_sample_points, lower_bound.shape[0]),\n        dtype=jnp.float64,\n        minval=lower_bound,\n        maxval=upper_bound,\n    )\n    initial_sample_y = sample(initial_sample_points)\n    best_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])\n\n    # We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.\n    negative_utility_fn = lambda x: sample(x)[0][0]\n    lbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")\n    bounds = (lower_bound, upper_bound)\n    x_star = lbfgsb.run(best_x, bounds=bounds).params\n    return x_star\n\n\nx_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100)\ny_star = forrester(x_star)\n</pre> def optimise_sample(     sample: FunctionalSample,     key: Int[Array, \"\"],     lower_bound: Float[Array, \"D\"],     upper_bound: Float[Array, \"D\"],     num_initial_sample_points: int, ) -&gt; ScalarFloat:     initial_sample_points = jr.uniform(         key,         shape=(num_initial_sample_points, lower_bound.shape[0]),         dtype=jnp.float64,         minval=lower_bound,         maxval=upper_bound,     )     initial_sample_y = sample(initial_sample_points)     best_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])      # We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.     negative_utility_fn = lambda x: sample(x)[0][0]     lbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")     bounds = (lower_bound, upper_bound)     x_star = lbfgsb.run(best_x, bounds=bounds).params     return x_star   x_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100) y_star = forrester(x_star) <p>Having found the minimum of the sample from the posterior, we can then evaluate the black-box objective function at this point, and append the new observation to our dataset.</p> <p>Below we plot the posterior distribution of the surrogate model, along with the sample drawn from the model, and the minimiser of this sample returned from the optimiser, which we denote with a star.</p> In\u00a0[7]: Copied! <pre>def plot_bayes_opt(\n    posterior: gpx.Module,\n    sample: FunctionalSample,\n    dataset: gpx.Dataset,\n    queried_x: ScalarFloat,\n) -&gt; None:\n    plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\n    forrester_y = forrester(plt_x)\n    sample_y = sample(plt_x)\n\n    latent_dist = posterior.predict(plt_x, train_data=dataset)\n    predictive_dist = posterior.likelihood(latent_dist)\n\n    predictive_mean = predictive_dist.mean()\n    predictive_std = predictive_dist.stddev()\n\n    fig, ax = plt.subplots()\n    ax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])\n    ax.fill_between(\n        plt_x.squeeze(),\n        predictive_mean - 2 * predictive_std,\n        predictive_mean + 2 * predictive_std,\n        alpha=0.2,\n        label=\"Two sigma\",\n        color=cols[1],\n    )\n    ax.plot(\n        plt_x,\n        predictive_mean - 2 * predictive_std,\n        linestyle=\"--\",\n        linewidth=1,\n        color=cols[1],\n    )\n    ax.plot(\n        plt_x,\n        predictive_mean + 2 * predictive_std,\n        linestyle=\"--\",\n        linewidth=1,\n        color=cols[1],\n    )\n    ax.plot(plt_x, sample_y, label=\"Posterior Sample\")\n    ax.plot(\n        plt_x,\n        forrester_y,\n        label=\"Forrester Function\",\n        color=cols[0],\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\n    ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\n    ax.scatter(\n        queried_x,\n        sample(queried_x),\n        label=\"Posterior Sample Optimum\",\n        marker=\"*\",\n        color=cols[3],\n        zorder=3,\n    )\n    ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n    plt.show()\n\n\nplot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n</pre> def plot_bayes_opt(     posterior: gpx.Module,     sample: FunctionalSample,     dataset: gpx.Dataset,     queried_x: ScalarFloat, ) -&gt; None:     plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)     forrester_y = forrester(plt_x)     sample_y = sample(plt_x)      latent_dist = posterior.predict(plt_x, train_data=dataset)     predictive_dist = posterior.likelihood(latent_dist)      predictive_mean = predictive_dist.mean()     predictive_std = predictive_dist.stddev()      fig, ax = plt.subplots()     ax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])     ax.fill_between(         plt_x.squeeze(),         predictive_mean - 2 * predictive_std,         predictive_mean + 2 * predictive_std,         alpha=0.2,         label=\"Two sigma\",         color=cols[1],     )     ax.plot(         plt_x,         predictive_mean - 2 * predictive_std,         linestyle=\"--\",         linewidth=1,         color=cols[1],     )     ax.plot(         plt_x,         predictive_mean + 2 * predictive_std,         linestyle=\"--\",         linewidth=1,         color=cols[1],     )     ax.plot(plt_x, sample_y, label=\"Posterior Sample\")     ax.plot(         plt_x,         forrester_y,         label=\"Forrester Function\",         color=cols[0],         linestyle=\"--\",         linewidth=2,     )     ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")     ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)     ax.scatter(         queried_x,         sample(queried_x),         label=\"Posterior Sample Optimum\",         marker=\"*\",         color=cols[3],         zorder=3,     )     ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))     plt.show()   plot_bayes_opt(opt_posterior, approx_sample, D, x_star) <p>At this point we can update our model with the newly augmented dataset, and repeat the whole process until some stopping criterion is met. Below we repeat this process for 10 iterations, printing out the queried point and the value of the black-box function at each iteration.</p> In\u00a0[8]: Copied! <pre>bo_iters = 5\n\n# Set up initial dataset\ninitial_x = tfp.mcmc.sample_halton_sequence(\n    dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n).reshape(-1, 1)\ninitial_y = forrester(initial_x)\nD = gpx.Dataset(X=initial_x, y=initial_y)\n\nfor i in range(bo_iters):\n    key, subkey = jr.split(key)\n\n    # Generate optimised posterior using previously observed data\n    mean = gpx.mean_functions.Zero()\n    kernel = gpx.kernels.Matern52()\n    prior = gpx.Prior(mean_function=mean, kernel=kernel)\n    opt_posterior = return_optimised_posterior(D, prior, subkey)\n\n    # Draw a sample from the posterior, and find the minimiser of it\n    approx_sample = opt_posterior.sample_approx(\n        num_samples=1, train_data=D, key=subkey, num_features=500\n    )\n    x_star = optimise_sample(\n        approx_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100\n    )\n\n    plot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n\n    # Evaluate the black-box function at the best point observed so far, and add it to the dataset\n    y_star = forrester(x_star)\n    print(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")\n    D = D + gpx.Dataset(X=x_star, y=y_star)\n</pre> bo_iters = 5  # Set up initial dataset initial_x = tfp.mcmc.sample_halton_sequence(     dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64 ).reshape(-1, 1) initial_y = forrester(initial_x) D = gpx.Dataset(X=initial_x, y=initial_y)  for i in range(bo_iters):     key, subkey = jr.split(key)      # Generate optimised posterior using previously observed data     mean = gpx.mean_functions.Zero()     kernel = gpx.kernels.Matern52()     prior = gpx.Prior(mean_function=mean, kernel=kernel)     opt_posterior = return_optimised_posterior(D, prior, subkey)      # Draw a sample from the posterior, and find the minimiser of it     approx_sample = opt_posterior.sample_approx(         num_samples=1, train_data=D, key=subkey, num_features=500     )     x_star = optimise_sample(         approx_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100     )      plot_bayes_opt(opt_posterior, approx_sample, D, x_star)      # Evaluate the black-box function at the best point observed so far, and add it to the dataset     y_star = forrester(x_star)     print(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")     D = D + gpx.Dataset(X=x_star, y=y_star) <pre>Queried Point: [[0.634135]], Black-Box Function Value: [[-1.46947236]]\n</pre> <pre>Queried Point: [[0.73831098]], Black-Box Function Value: [[-5.84027373]]\n</pre> <pre>Queried Point: [[0.75152659]], Black-Box Function Value: [[-6.00354644]]\n</pre> <pre>Queried Point: [[0.75553251]], Black-Box Function Value: [[-6.01917488]]\n</pre> <pre>Queried Point: [[0.75597862]], Black-Box Function Value: [[-6.0198817]]\n</pre> <p>Below we plot the best observed black-box function value against the number of times the black-box function has been evaluated. Note that the first 5 samples are randomly sampled to fit the initial GP model, and we denote the start of using BO to sample with the dotted vertical line.</p> <p>We can see that the BO algorithm quickly converges to the global minimum of the black-box function!</p> In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\ncumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y)\nax.plot(fn_evaluations, cumulative_best_y)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.axhline(y=-6.0207, linestyle=\"--\", label=\"True Minimum\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Best Observed Value\")\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() fn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1) cumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y) ax.plot(fn_evaluations, cumulative_best_y) ax.axvline(x=initial_sample_num, linestyle=\":\") ax.axhline(y=-6.0207, linestyle=\"--\", label=\"True Minimum\") ax.set_xlabel(\"Number of Black-Box Function Evaluations\") ax.set_ylabel(\"Best Observed Value\") ax.legend() plt.show() <p>We'll now apply BO to a more challenging example, the Six-Hump Camel Function. This is a function of two inputs defined as follows:</p> $$f(x_1, x_2) = (4 - 2.1x_1^2 + \\frac{x_1^4}{3})x_1^2 + x_1x_2 + (-4 + 4x_2^2)x_2^2$$<p>We'll be evaluating it over the domain $x_1 \\in [-2, 2]$ and $x_2 \\in [-1, 1]$. The global minima of this function are located at $\\mathbf{x} = (0.0898, -0.7126)$ and $\\mathbf{x} = (-0.0898, 0.7126)$, where the function takes the value $f(\\mathbf{x}) = -1.0316$.</p> In\u00a0[10]: Copied! <pre>def six_hump_camel(x: Float[Array, \"N 2\"]) -&gt; Float[Array, \"N 1\"]:\n    x1 = x[..., :1]\n    x2 = x[..., 1:]\n    term1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2\n    term2 = x1 * x2\n    term3 = (-4 + 4 * x2**2) * x2**2\n    return term1 + term2 + term3\n</pre> def six_hump_camel(x: Float[Array, \"N 2\"]) -&gt; Float[Array, \"N 1\"]:     x1 = x[..., :1]     x2 = x[..., 1:]     term1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2     term2 = x1 * x2     term3 = (-4 + 4 * x2**2) * x2**2     return term1 + term2 + term3 <p>First, we'll visualise the function over the domain of interest:</p> In\u00a0[11]: Copied! <pre>x1 = jnp.linspace(-2, 2, 100)\nx2 = jnp.linspace(-1, 1, 100)\nx1, x2 = jnp.meshgrid(x1, x2)\nx = jnp.stack([x1.flatten(), x2.flatten()], axis=1)\ny = six_hump_camel(x)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nsurf = ax.plot_surface(\n    x1,\n    x2,\n    y.reshape(x1.shape[0], x2.shape[0]),\n    linewidth=0,\n    cmap=cm.coolwarm,\n    antialiased=False,\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nplt.show()\n</pre> x1 = jnp.linspace(-2, 2, 100) x2 = jnp.linspace(-1, 1, 100) x1, x2 = jnp.meshgrid(x1, x2) x = jnp.stack([x1.flatten(), x2.flatten()], axis=1) y = six_hump_camel(x)  fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}) surf = ax.plot_surface(     x1,     x2,     y.reshape(x1.shape[0], x2.shape[0]),     linewidth=0,     cmap=cm.coolwarm,     antialiased=False, ) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") plt.show() <p>For more clarity, we can generate a contour plot of the function which enables us to see the global minima of the function more clearly.</p> In\u00a0[12]: Copied! <pre>x_star_one = jnp.array([[0.0898, -0.7126]])\nx_star_two = jnp.array([[-0.0898, 0.7126]])\nfig, ax = plt.subplots()\ncontour_plot = ax.contourf(\n    x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\n    x_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\"\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2])\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\n</pre> x_star_one = jnp.array([[0.0898, -0.7126]]) x_star_two = jnp.array([[-0.0898, 0.7126]]) fig, ax = plt.subplots() contour_plot = ax.contourf(     x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40 ) ax.scatter(     x_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\" ) ax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2]) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") fig.colorbar(contour_plot) ax.legend() plt.show() <p>Next, we'll run the BO loop using Thompson sampling as before. This time we'll run the experiment 5 times in order to see how the algorithm performs on average, with different starting points for the initial GP model. This is good practice, as the performance obtained is likely to vary between runs depending on the initialisation samples used to fit the initial GP model.</p> In\u00a0[13]: Copied! <pre>lower_bound = jnp.array([-2.0, -1.0])\nupper_bound = jnp.array([2.0, 1.0])\ninitial_sample_num = 5\nbo_iters = 11\nnum_experiments = 5\nbo_experiment_results = []\n\nfor experiment in range(num_experiments):\n    print(f\"Starting Experiment: {experiment + 1}\")\n    # Set up initial dataset\n    initial_x = tfp.mcmc.sample_halton_sequence(\n        dim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n    )\n    initial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)\n    initial_y = six_hump_camel(initial_x)\n    D = gpx.Dataset(X=initial_x, y=initial_y)\n\n    for i in range(bo_iters):\n        key, subkey = jr.split(key)\n\n        # Generate optimised posterior\n        mean = gpx.mean_functions.Zero()\n        kernel = gpx.kernels.Matern52(\n            active_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0\n        )\n        prior = gpx.Prior(mean_function=mean, kernel=kernel)\n        opt_posterior = return_optimised_posterior(D, prior, subkey)\n\n        # Draw a sample from the posterior, and find the minimiser of it\n        approx_sample = opt_posterior.sample_approx(\n            num_samples=1, train_data=D, key=subkey, num_features=500\n        )\n        x_star = optimise_sample(\n            approx_sample,\n            subkey,\n            lower_bound,\n            upper_bound,\n            num_initial_sample_points=1000,\n        )\n\n        # Evaluate the black-box function at the best point observed so far, and add it to the dataset\n        y_star = six_hump_camel(x_star)\n        print(\n            f\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value: {y_star}\"\n        )\n        D = D + gpx.Dataset(X=x_star, y=y_star)\n    bo_experiment_results.append(D)\n</pre> lower_bound = jnp.array([-2.0, -1.0]) upper_bound = jnp.array([2.0, 1.0]) initial_sample_num = 5 bo_iters = 11 num_experiments = 5 bo_experiment_results = []  for experiment in range(num_experiments):     print(f\"Starting Experiment: {experiment + 1}\")     # Set up initial dataset     initial_x = tfp.mcmc.sample_halton_sequence(         dim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64     )     initial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)     initial_y = six_hump_camel(initial_x)     D = gpx.Dataset(X=initial_x, y=initial_y)      for i in range(bo_iters):         key, subkey = jr.split(key)          # Generate optimised posterior         mean = gpx.mean_functions.Zero()         kernel = gpx.kernels.Matern52(             active_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0         )         prior = gpx.Prior(mean_function=mean, kernel=kernel)         opt_posterior = return_optimised_posterior(D, prior, subkey)          # Draw a sample from the posterior, and find the minimiser of it         approx_sample = opt_posterior.sample_approx(             num_samples=1, train_data=D, key=subkey, num_features=500         )         x_star = optimise_sample(             approx_sample,             subkey,             lower_bound,             upper_bound,             num_initial_sample_points=1000,         )          # Evaluate the black-box function at the best point observed so far, and add it to the dataset         y_star = six_hump_camel(x_star)         print(             f\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value: {y_star}\"         )         D = D + gpx.Dataset(X=x_star, y=y_star)     bo_experiment_results.append(D) <pre>Starting Experiment: 1\n</pre> <pre>/usr/share/miniconda/envs/test/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3613: UserWarning: 'kind' argument to argsort is ignored; only 'stable' sorts are supported.\n  warnings.warn(\"'kind' argument to argsort is ignored; only 'stable' sorts \"\n</pre> <pre>BO Iteration: 1, Queried Point: [[-0.30468433 -0.66406999]], Black-Box Function Value: [[-0.43023976]]\nBO Iteration: 2, Queried Point: [[-2.         -0.74276056]], Black-Box Function Value: [[4.22954321]]\nBO Iteration: 3, Queried Point: [[ 0.17081359 -1.        ]], Black-Box Function Value: [[-0.05588394]]\nBO Iteration: 4, Queried Point: [[-0.93121942  0.71619751]], Black-Box Function Value: [[0.44061212]]\nBO Iteration: 5, Queried Point: [[-0.30786216 -0.28561261]], Black-Box Function Value: [[0.14878449]]\nBO Iteration: 6, Queried Point: [[-1.57117442  1.        ]], Black-Box Function Value: [[0.52039597]]\nBO Iteration: 7, Queried Point: [[ 0.31443743 -0.650602  ]], Black-Box Function Value: [[-0.8057543]]\nBO Iteration: 8, Queried Point: [[ 0.30041362 -0.57117872]], Black-Box Function Value: [[-0.70669247]]\nBO Iteration: 9, Queried Point: [[ 0.13699562 -0.67038495]], Black-Box Function Value: [[-1.00727111]]\nBO Iteration: 10, Queried Point: [[ 0.08326753 -0.7480451 ]], Black-Box Function Value: [[-1.02045974]]\nBO Iteration: 11, Queried Point: [[ 0.08432182 -0.71155271]], Black-Box Function Value: [[-1.03150566]]\nStarting Experiment: 2\nBO Iteration: 1, Queried Point: [[-1.67947341  0.99199406]], Black-Box Function Value: [[0.32648321]]\nBO Iteration: 2, Queried Point: [[ 1.06054035 -0.35862161]], Black-Box Function Value: [[1.48805046]]\nBO Iteration: 3, Queried Point: [[0.07220263 0.64703436]], Black-Box Function Value: [[-0.90601753]]\nBO Iteration: 4, Queried Point: [[-0.27470497 -0.91544811]], Black-Box Function Value: [[-0.00138771]]\nBO Iteration: 5, Queried Point: [[-0.06049949  0.88424343]], Black-Box Function Value: [[-0.72104376]]\nBO Iteration: 6, Queried Point: [[-0.99985181  1.        ]], Black-Box Function Value: [[1.23324434]]\nBO Iteration: 7, Queried Point: [[-0.05650721  0.11465498]], Black-Box Function Value: [[-0.04561978]]\nBO Iteration: 8, Queried Point: [[-2.          0.59105817]], Black-Box Function Value: [[1.64199898]]\nBO Iteration: 9, Queried Point: [[-0.17181778  0.71063353]], Black-Box Function Value: [[-1.00573567]]\nBO Iteration: 10, Queried Point: [[-0.11492884  0.67132102]], Black-Box Function Value: [[-1.01495217]]\nBO Iteration: 11, Queried Point: [[-0.11346537  0.70050184]], Black-Box Function Value: [[-1.02798675]]\nStarting Experiment: 3\nBO Iteration: 1, Queried Point: [[-0.00503751  0.23456667]], Black-Box Function Value: [[-0.20905674]]\nBO Iteration: 2, Queried Point: [[2.         0.23791955]], Black-Box Function Value: [[3.99556641]]\nBO Iteration: 3, Queried Point: [[-0.25251944 -0.79889783]], Black-Box Function Value: [[-0.4752122]]\nBO Iteration: 4, Queried Point: [[ 1.29019768 -0.23888747]], Black-Box Function Value: [[1.85354413]]\nBO Iteration: 5, Queried Point: [[ 0.14638221 -0.78463661]], Black-Box Function Value: [[-0.97660281]]\nBO Iteration: 6, Queried Point: [[ 0.09185379 -0.91295842]], Black-Box Function Value: [[-0.60538896]]\nBO Iteration: 7, Queried Point: [[-0.10604165 -0.32859943]], Black-Box Function Value: [[-0.30571422]]\nBO Iteration: 8, Queried Point: [[ 0.26442985 -0.6854472 ]], Black-Box Function Value: [[-0.90807456]]\nBO Iteration: 9, Queried Point: [[ 0.0905522  -0.71329163]], Black-Box Function Value: [[-1.03162363]]\nBO Iteration: 10, Queried Point: [[ 0.09663857 -0.72012996]], Black-Box Function Value: [[-1.03103723]]\nBO Iteration: 11, Queried Point: [[ 0.05648831 -0.70645402]], Black-Box Function Value: [[-1.02716064]]\nStarting Experiment: 4\nBO Iteration: 1, Queried Point: [[-2.          0.16171328]], Black-Box Function Value: [[3.30803759]]\nBO Iteration: 2, Queried Point: [[1.938168   0.74046249]], Black-Box Function Value: [[3.50643117]]\nBO Iteration: 3, Queried Point: [[ 0.55155966 -1.        ]], Black-Box Function Value: [[0.48034545]]\nBO Iteration: 4, Queried Point: [[-0.17236693 -0.15489547]], Black-Box Function Value: [[0.05002751]]\nBO Iteration: 5, Queried Point: [[ 0.15071264 -1.        ]], Black-Box Function Value: [[-0.06093501]]\nBO Iteration: 6, Queried Point: [[0.63509936 0.00367974]], Black-Box Function Value: [[1.29590807]]\nBO Iteration: 7, Queried Point: [[-0.01843887 -0.67128371]], Black-Box Function Value: [[-0.97650974]]\nBO Iteration: 8, Queried Point: [[-0.14016034 -0.64164409]], Black-Box Function Value: [[-0.80111265]]\nBO Iteration: 9, Queried Point: [[ 0.04286039 -0.68210015]], Black-Box Function Value: [[-1.01706681]]\nBO Iteration: 10, Queried Point: [[-0.96728319  1.        ]], Black-Box Function Value: [[1.20991379]]\nBO Iteration: 11, Queried Point: [[ 0.08083567 -0.69434937]], Black-Box Function Value: [[-1.02880152]]\nStarting Experiment: 5\nBO Iteration: 1, Queried Point: [[0.08210901 0.12681342]], Black-Box Function Value: [[-0.02600737]]\nBO Iteration: 2, Queried Point: [[ 1.03099126 -0.470874  ]], Black-Box Function Value: [[1.1036997]]\nBO Iteration: 3, Queried Point: [[ 1.61526546 -0.92097926]], Black-Box Function Value: [[0.05860974]]\nBO Iteration: 4, Queried Point: [[-2.        -0.7866772]], Black-Box Function Value: [[4.3631995]]\nBO Iteration: 5, Queried Point: [[ 0.42006295 -1.        ]], Black-Box Function Value: [[0.2221951]]\nBO Iteration: 6, Queried Point: [[ 0.08122342 -0.59932852]], Black-Box Function Value: [[-0.94307728]]\nBO Iteration: 7, Queried Point: [[ 0.20434512 -0.41180524]], Black-Box Function Value: [[-0.48405995]]\nBO Iteration: 8, Queried Point: [[-0.03214392  1.        ]], Black-Box Function Value: [[-0.02801324]]\nBO Iteration: 9, Queried Point: [[ 0.07408559 -0.72690966]], Black-Box Function Value: [[-1.02873633]]\nBO Iteration: 10, Queried Point: [[0.4740857  0.71855856]], Black-Box Function Value: [[0.13845493]]\nBO Iteration: 11, Queried Point: [[ 0.167513  -0.7090096]], Black-Box Function Value: [[-1.00814303]]\n</pre> <p>We'll also run a random benchmark, whereby we randomly sample from the search space for 20 iterations. This is a useful benchmark to compare the performance of BO against in order to ascertain how much of an advantage BO provides over such a simple approach.</p> In\u00a0[14]: Copied! <pre>random_experiment_results = []\nfor i in range(num_experiments):\n    key, subkey = jr.split(key)\n    initial_x = bo_experiment_results[i].X[:5]\n    initial_y = bo_experiment_results[i].y[:5]\n    final_x = jr.uniform(\n        key,\n        shape=(bo_iters, 2),\n        dtype=jnp.float64,\n        minval=lower_bound,\n        maxval=upper_bound,\n    )\n    final_y = six_hump_camel(final_x)\n    random_x = jnp.concatenate([initial_x, final_x], axis=0)\n    random_y = jnp.concatenate([initial_y, final_y], axis=0)\n    random_experiment_results.append(gpx.Dataset(X=random_x, y=random_y))\n</pre> random_experiment_results = [] for i in range(num_experiments):     key, subkey = jr.split(key)     initial_x = bo_experiment_results[i].X[:5]     initial_y = bo_experiment_results[i].y[:5]     final_x = jr.uniform(         key,         shape=(bo_iters, 2),         dtype=jnp.float64,         minval=lower_bound,         maxval=upper_bound,     )     final_y = six_hump_camel(final_x)     random_x = jnp.concatenate([initial_x, final_x], axis=0)     random_y = jnp.concatenate([initial_y, final_y], axis=0)     random_experiment_results.append(gpx.Dataset(X=random_x, y=random_y)) <p>Finally, we'll process the experiment results to find the log regret at each iteration of the experiments. The regret is defined as the difference between the minimum value of the black-box function observed so far and the true global minimum of the black box function. Mathematically, at time $t$, with observations $\\mathcal{D}_t$, for function $f$ with global minimum $f^*$, the regret is defined as:</p> $$\\text{regret}_t = \\min_{\\mathbf{x} \\in \\mathcal{D_t}}f(\\mathbf{x}) - f^*$$<p>We'll then take the mean and standard deviation of the log of the regret values across the 5 experiments.</p> In\u00a0[15]: Copied! <pre>def obtain_log_regret_statistics(\n    experiment_results: List[gpx.Dataset],\n    global_minimum: ScalarFloat,\n) -&gt; Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:\n    log_regret_results = []\n    for exp_result in experiment_results:\n        observations = exp_result.y\n        cumulative_best_observations = jax.lax.associative_scan(\n            jax.numpy.minimum, observations\n        )\n        regret = cumulative_best_observations - global_minimum\n        log_regret = jnp.log(regret)\n        log_regret_results.append(log_regret)\n\n    log_regret_results = jnp.array(log_regret_results)\n    log_regret_mean = jnp.mean(log_regret_results, axis=0)\n    log_regret_std = jnp.std(log_regret_results, axis=0)\n    return log_regret_mean, log_regret_std\n\n\nbo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(\n    bo_experiment_results, -1.031625\n)\n(\n    random_log_regret_mean,\n    random_log_regret_std,\n) = obtain_log_regret_statistics(random_experiment_results, -1.031625)\n</pre> def obtain_log_regret_statistics(     experiment_results: List[gpx.Dataset],     global_minimum: ScalarFloat, ) -&gt; Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:     log_regret_results = []     for exp_result in experiment_results:         observations = exp_result.y         cumulative_best_observations = jax.lax.associative_scan(             jax.numpy.minimum, observations         )         regret = cumulative_best_observations - global_minimum         log_regret = jnp.log(regret)         log_regret_results.append(log_regret)      log_regret_results = jnp.array(log_regret_results)     log_regret_mean = jnp.mean(log_regret_results, axis=0)     log_regret_std = jnp.std(log_regret_results, axis=0)     return log_regret_mean, log_regret_std   bo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(     bo_experiment_results, -1.031625 ) (     random_log_regret_mean,     random_log_regret_std, ) = obtain_log_regret_statistics(random_experiment_results, -1.031625) <p>Now, when we plot the mean and standard deviation of the log regret at each iteration, we can see that BO outperforms random sampling!</p> In\u00a0[16]: Copied! <pre>fig, ax = plt.subplots()\nfn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\nax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\")\nax.fill_between(\n    fn_evaluations,\n    bo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],\n    bo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],\n    alpha=0.2,\n)\nax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\")\nax.fill_between(\n    fn_evaluations,\n    random_log_regret_mean[:, 0] - random_log_regret_std[:, 0],\n    random_log_regret_mean[:, 0] + random_log_regret_std[:, 0],\n    alpha=0.2,\n)\nax.axvline(x=initial_sample_num, linestyle=\":\")\nax.set_xlabel(\"Number of Black-Box Function Evaluations\")\nax.set_ylabel(\"Log Regret\")\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() fn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1) ax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\") ax.fill_between(     fn_evaluations,     bo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],     bo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],     alpha=0.2, ) ax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\") ax.fill_between(     fn_evaluations,     random_log_regret_mean[:, 0] - random_log_regret_std[:, 0],     random_log_regret_mean[:, 0] + random_log_regret_std[:, 0],     alpha=0.2, ) ax.axvline(x=initial_sample_num, linestyle=\":\") ax.set_xlabel(\"Number of Black-Box Function Evaluations\") ax.set_ylabel(\"Log Regret\") ax.legend() plt.show() <p>It can also be useful to plot the queried points over the course of a single BO run, in order to gain some insight into how the algorithm queries the search space. Below we do this for the first BO experiment, and can see that the algorithm initially performs some exploration of the search space whilst it is uncertain about the black-box function, but it then hones in one one of the global minima of the function, as we would hope!</p> In\u00a0[17]: Copied! <pre>fig, ax = plt.subplots()\ncontour_plot = ax.contourf(\n    x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n)\nax.scatter(\n    x_star_one[0][0],\n    x_star_one[0][1],\n    marker=\"*\",\n    color=cols[2],\n    label=\"Global Minimum\",\n    zorder=2,\n)\nax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2)\nax.scatter(\n    bo_experiment_results[0].X[:, 0],\n    bo_experiment_results[0].X[:, 1],\n    marker=\"x\",\n    color=cols[1],\n    label=\"Bayesian Optimisation Queries\",\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nfig.colorbar(contour_plot)\nax.legend()\nplt.show()\n</pre> fig, ax = plt.subplots() contour_plot = ax.contourf(     x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40 ) ax.scatter(     x_star_one[0][0],     x_star_one[0][1],     marker=\"*\",     color=cols[2],     label=\"Global Minimum\",     zorder=2, ) ax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2) ax.scatter(     bo_experiment_results[0].X[:, 0],     bo_experiment_results[0].X[:, 1],     marker=\"x\",     color=cols[1],     label=\"Bayesian Optimisation Queries\", ) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") fig.colorbar(contour_plot) ax.legend() plt.show() In\u00a0[18]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Christie' <pre>Author: Thomas Christie\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\ngpjax                 : 0.0.0\njax                   : 0.4.9\nmatplotlib            : 3.7.1\noptax                 : 0.1.5\ntensorflow_probability: 0.19.0\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/bayesian_optimisation/#introduction-to-bayesian-optimisation","title":"Introduction to Bayesian Optimisation\u00b6","text":"<p>In this guide we introduce the Bayesian Optimisation (BO) paradigm for optimising black-box functions. We'll assume an understanding of Gaussian processes (GPs), so if you're not familiar with them, check out our GP introduction notebook.</p>"},{"location":"examples/bayesian_optimisation/#some-motivating-examples","title":"Some Motivating Examples\u00b6","text":"<p>Countless problems in the physical world involve optimising functions for which the explicit functional form is unknown, but which can be expensively queried throughout their domain. For example, within the domain of science the task of designing new molecules with optimised properties (Griffiths and Lobato, 2020) is incredibly useful. Here, the domain being optimised over is the space of possible molecules, with the objective function depending on the property being optimised, for instance within drug-design this may be the efficacy of the drug. The function from molecules to efficacy is unknown, but can be queried by synthesising a molecule and running an experiment to measure its efficacy. This is clearly an expensive procedure!</p> <p>Within the domain of machine learning, the task of optimising neural network architectures is another example of such a problem (commonly referred to as Neural Architecture Search (NAS)). Here, the domain is the space of possible neural network architectures, and the objective function is a metric such as the accuracy of the trained model. Again, the function from neural network architectures to accuracy is unknown, but can be queried by training a model with a given architecture and evaluating its accuracy. This is also an expensive procedure, as training models can be incredibly time consuming and computationally demanding.</p> <p>Finally, these problems are ubiquitous within the field of climate science, with (Hellan et al., 2023) providing several excellent examples. One such example is the task of deciding where to place wind turbines in a wind farm in order to maximise the energy generated. Here, the domain is the space of possible locations for the wind turbines, and the objective function is the energy generated by the wind farm. The function from locations to energy generated is unknown, but could be queried by running a simulation of the wind farm with the turbines placed at a given set of locations. Running such simulations can be expensive, particularly if they are high-fidelity.</p> <p>At the heart of all these problems is the task of optimising a function for which we don't have the explicit functional form, but which we can (expensively) query at any point in its domain. Bayesian optimisation provides a principled framework for solving such problems.</p>"},{"location":"examples/bayesian_optimisation/#what-is-bayesian-optimisation","title":"What is Bayesian Optimisation?\u00b6","text":"<p>Bayesian optimisation (BO) (Mo\u010dkus, 1974) provides a principled method for making decisions under uncertainty. The aim of BO is to find the global minimum of a black-box objective function, $\\min_{\\mathbf{x} \\in X} f(\\mathbf{x})$. The function $f$ is said to be a black-box function because its explicit functional form is unknown. However, it is assumed that one is able to ascertain information about the function by evaluating it at points in its domain, $X$. However, these evaluations are assumed to be expensive, as seen in the motivating examples. Therefore, the goal of BO is to minimise $f$ with as few evaluations of the black-box function as possible.</p> <p>As such, BO can be thought of as sequential decision-making problem. At each iteration one must choose which point (or batch of points) in a function's domain to evaluate next, drawing on previously observed values to make optimal decisions. In order to do this effectively, we need a way of representing our uncertainty about the black-box function $f$, which we can update in light of observing more data. Gaussian processes will be an ideal tool for this purpose!</p> <p>Surrogate models lie at the heart of BO, and are used to model the black-box function. GPs are a natural choice for this model, as they not only provide point estimates for the values taken by the function throughout its domain, but crucially provide a full predictive posterior distribution of the range of values the function may take. This rich quantification of uncertainty enables BO to balance exploration and exploitation in order to efficiently converge upon minima.</p> <p>Having chosen a surrogate model, which we can use to express our current beliefs about the black-box function, ideally we would like a method which can use the surrogate model's posterior distribution to automatically decide which point(s) in the black-box function's domain to query next. This is where acquisition functions come in. The acquisition function $\\alpha: X \\to \\mathbb{R}$ is defined over the same domain as the surrogate model, and uses the surrogate model's posterior distribution to quantify the expected utility, $U$, of evaluating the black-box function at a given point. Simply put, for each point in the black-box function's domain, $\\mathbf{x} \\in X$, the acquisition function quantifies how useful it would be to evaluate the black-box function at $\\mathbf{x}$ in order to find the minimum of the black-box function, whilst taking into consideration all the datapoints observed so far. Therefore, in order to decide which point to query next we simply choose the point which maximises the acquisition function, using an optimiser such as L-BFGS (Liu and Nocedal, 1989).</p> <p>The Bayesian optimisation loop can be summarised as follows, with $i$ denoting the current iteration:</p> <ol> <li>Select the next point to query, $\\mathbf{x}_{i}$, by maximising the acquisition function $\\alpha$, defined using the surrogate model $\\mathcal{M}_i$ conditioned on previously observed data $\\mathcal{D}_i$:</li> </ol> $$\\mathbf{x}_{i} = \\arg\\max_{\\mathbf{x}} \\alpha (\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i)$$<ol> <li><p>Evaluate the objective function at $\\mathbf{x}_i$, yielding observation $y_i = f(\\mathbf{x}_i)$.</p> </li> <li><p>Append the most recent observation to the dataset, $\\mathcal{D}_{i+1} = \\mathcal{D}_i \\cup \\{(\\mathbf{x}_i, y_i)\\}$.</p> </li> <li><p>Condition the model on the updated dataset to yield $\\mathcal{M}_{i+1}$.</p> </li> </ol> <p>This process is repeated until some stopping criterion is met, such as a function evaluation budget being exhausted.</p> <p>There are a plethora of acquisition functions to choose from, each with their own advantages and disadvantages, of which (Shahriari et al., 2015) provides an excellent overview.</p> <p>In this guide we will focus on Thompson sampling, a conceptually simple yet effective method for characterising the utility of querying points in a black-box function's domain, which will be useful in demonstrating the key aspects of BO.</p>"},{"location":"examples/bayesian_optimisation/#thompson-sampling","title":"Thompson Sampling\u00b6","text":"<p>Thompson sampling (Thompson, 1933) is a simple method which naturally balances exploration and exploitation. The core idea is to, at each iteration of the BO loop, sample a function, $g$, from the posterior distribution of the surrogate model $\\mathcal{M}_i$, and then evaluate the black-box function at the point(s) which minimise this sample. Given a sample $g$, from the posterior distribution given by the model $\\mathcal{M}_i$ the Thompson sampling utility function is defined as:</p> $$U_{\\text{TS}}(\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i) = - g(\\mathbf{x})$$<p>Note the negative sign; this is included as we want to maximise the utility of evaluating the black-box function $f$ at a given point. We interested in finding the minimum of $f$, so we maximise the negative of the sample from the posterior distribution $g$.</p> <p>As a toy example, we shall be applying BO to the widely used Forrester function:</p> $$f(x) = (6x - 2)^2 \\sin(12x - 4)$$<p>treating $f$ as a black-box function. Moreover, we shall restrict the domain of the function to $\\mathbf{x} \\in [0, 1]$. The global minimum of this function is located at $x = 0.757$, where $f(x) = -6.021$.</p>"},{"location":"examples/bayesian_optimisation/#a-more-challenging-example-the-six-hump-camel-function","title":"A More Challenging Example - The Six-Hump Camel Function\u00b6","text":""},{"location":"examples/bayesian_optimisation/#other-acquisition-functions-and-further-reading","title":"Other Acquisition Functions and Further Reading\u00b6","text":"<p>As mentioned previously, there are many acquisition functions which one may use to characterise the expected utility of querying the black-box function at a given point. We list two of the most popular below:</p> <ul> <li><p>Probability of Improvement (PI) (Kushner, 1964): Given the lowest objective function observation so far, $f(\\mathbf{x}^*)$, PI calculates the probability that the objective function's value at a given point $\\mathbf{x}$ is lower than $f(\\mathbf{x}^*)$. Given a GP surrogate model $\\mathcal{M}_i$, PI is defined mathematically as: $$ \\alpha_{\\text{PI}}(\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i) = \\mathbb{P}[\\mathcal{M}_i (\\mathbf{x}) &lt; f(\\mathbf{x}^*)] = \\Phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right) $$</p> <p>with $\\Phi(\\cdot)$ denoting the standard normal cumulative distribution function.</p> </li> <li><p>Expected Improvement (EI) (Mo\u010dkus, 1974) - EI goes beyond PI by not only considering the probability of improving on the current best observed point, but also taking into account the \\textit{magnitude} of improvement. Mathematically, this is defined as follows: $$ \\begin{aligned} \\alpha_{\\text{EI}}(\\mathbf{x};\\mathcal{D}_i, \\mathcal{M}_i) &amp;= \\mathbb{E}[(f(\\mathbf{x}^*) - \\mathcal{M}_i(\\mathbf{x}))\\mathbb{I}(\\mathcal{M}_i(\\mathbf{x}) &lt; f(\\mathbf{x}^*))] \\\\ &amp;= \\underbrace{(f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x}))\\Phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)}_\\text{exploits areas with low mean} \\\\ &amp;+  \\underbrace{\\sigma_{\\mathcal{M}_i}(\\mathbf{x}) \\phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)}_\\text{explores areas with high variance} \\nonumber \\end{aligned} $$</p> <p>with $\\mathbb{I}(\\cdot)$ denoting the indicator function and $\\phi(\\cdot)$ being the standard normal probability density function.</p> </li> </ul> <p>For those particularly interested in diving deeper into Bayesian optimisation, be sure to check out Shahriari et al.'s \"Taking the Human Out of the Loop: A Review of Bayesian Optimization\", which includes a wide variety of acquisition functions, as well as some examples of more exotic BO problems, such as problems which also feature unknown constraints.</p>"},{"location":"examples/bayesian_optimisation/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/classification/","title":"Classification","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom time import time\nimport blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nimport jax.tree_util as jtu\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\nfrom tqdm import trange\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\ntfd = tfp.distributions\nidentity_matrix = jnp.eye\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from time import time import blackjax import jax import jax.numpy as jnp import jax.random as jr import jax.scipy as jsp import jax.tree_util as jtu from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax as tfp from tqdm import trange  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  tfd = tfp.distributions identity_matrix = jnp.eye key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>key, subkey = jr.split(key)\nx = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0)\ny = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\n</pre> key, subkey = jr.split(key) x = jr.uniform(key, shape=(100, 1), minval=-1.0, maxval=1.0) y = 0.5 * jnp.sign(jnp.cos(3 * x + jr.normal(subkey, shape=x.shape) * 0.05)) + 0.5  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-1.0, 1.0, 500).reshape(-1, 1)  fig, ax = plt.subplots() ax.scatter(x, y) Out[2]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f8ec01d97f0&gt;</pre> In\u00a0[3]: Copied! <pre>kernel = gpx.RBF()\nmeanf = gpx.Constant()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Bernoulli(num_datapoints=D.n)\n</pre> kernel = gpx.RBF() meanf = gpx.Constant() prior = gpx.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.Bernoulli(num_datapoints=D.n) <p>We construct the posterior through the product of our prior and likelihood.</p> In\u00a0[4]: Copied! <pre>posterior = prior * likelihood\nprint(type(posterior))\n</pre> posterior = prior * likelihood print(type(posterior)) <pre>&lt;class 'gpjax.gps.NonConjugatePosterior'&gt;\n</pre> <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. We begin with maximum a posteriori (MAP) estimation, a fast inference procedure to obtain point estimates for the latent function and the kernel's hyperparameters by maximising the marginal log-likelihood.</p> <p>We can obtain a MAP estimate by optimising the log-posterior density with Optax's optimisers.</p> In\u00a0[5]: Copied! <pre>negative_lpd = jax.jit(gpx.LogPosteriorDensity(negative=True))\n\noptimiser = ox.adam(learning_rate=0.01)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_lpd,\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.01),\n    num_iters=1000,\n    key=key,\n)\n</pre> negative_lpd = jax.jit(gpx.LogPosteriorDensity(negative=True))  optimiser = ox.adam(learning_rate=0.01)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_lpd,     train_data=D,     optim=ox.adamw(learning_rate=0.01),     num_iters=1000,     key=key, ) <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <p>From which we can make predictions at novel inputs, as illustrated below.</p> In\u00a0[6]: Copied! <pre>map_latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(map_latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\n\nax.legend()\n</pre> map_latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(map_latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.2,     color=cols[1],     label=\"One sigma\", ) ax.plot(     xtest,     predictive_mean - predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, )  ax.legend() Out[6]: <pre>&lt;matplotlib.legend.Legend at 0x7f8eb8bb8580&gt;</pre> <p>Here we projected the map estimates $\\hat{\\boldsymbol{f}}$ for the function values $\\boldsymbol{f}$ at the data points $\\boldsymbol{x}$ to get predictions over the whole domain,</p> \\begin{align} p(f(\\cdot)| \\mathcal{D})  \\approx q_{map}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) \\delta(\\boldsymbol{f} - \\hat{\\boldsymbol{f}}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}  <p>However, as a point estimate, MAP estimation is severely limited for uncertainty quantification, providing only a single piece of information about the posterior.</p> In\u00a0[7]: Copied! <pre>gram, cross_covariance = (kernel.gram, kernel.cross_covariance)\njitter = 1e-6\n\n# Compute (latent) function value map estimates at training points:\nKxx = opt_posterior.prior.kernel.gram(x)\nKxx += identity_matrix(D.n) * jitter\nLx = Kxx.to_root()\nf_hat = Lx @ opt_posterior.latent\n\n# Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f):\nH = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]\n\nL = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)\n\n# H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I\nL_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True)\nH_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False)\nLH = jnp.linalg.cholesky(H_inv)\nlaplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH)\n</pre> gram, cross_covariance = (kernel.gram, kernel.cross_covariance) jitter = 1e-6  # Compute (latent) function value map estimates at training points: Kxx = opt_posterior.prior.kernel.gram(x) Kxx += identity_matrix(D.n) * jitter Lx = Kxx.to_root() f_hat = Lx @ opt_posterior.latent  # Negative Hessian,  H = -\u2207\u00b2p_tilde(y|f): H = jax.jacfwd(jax.jacrev(negative_lpd))(opt_posterior, D).latent.latent[:, 0, :, 0]  L = jnp.linalg.cholesky(H + identity_matrix(D.n) * jitter)  # H\u207b\u00b9 = H\u207b\u00b9 I = (LL\u1d40)\u207b\u00b9 I = L\u207b\u1d40L\u207b\u00b9 I L_inv = jsp.linalg.solve_triangular(L, identity_matrix(D.n), lower=True) H_inv = jsp.linalg.solve_triangular(L.T, L_inv, lower=False) LH = jnp.linalg.cholesky(H_inv) laplace_approximation = tfd.MultivariateNormalTriL(f_hat.squeeze(), LH) <p>For novel inputs, we must project the above approximating distribution through the Gaussian conditional distribution $p(f(\\cdot)| \\boldsymbol{f})$,</p> \\begin{align} p(f(\\cdot)| \\mathcal{D}) \\approx q_{Laplace}(f(\\cdot)) := \\int p(f(\\cdot)| \\boldsymbol{f}) q(\\boldsymbol{f}) d \\boldsymbol{f} = \\mathcal{N}(\\mathbf{K}_{\\boldsymbol{(\\cdot)x}}  \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\hat{\\boldsymbol{f}},  \\mathbf{K}_{\\boldsymbol{(\\cdot, \\cdot)}} - \\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} (\\mathbf{K}_{\\boldsymbol{xx}} - [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1}) \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}). \\end{align}<p>This is the same approximate distribution $q_{map}(f(\\cdot))$, but we have perturbed the covariance by a curvature term of $\\mathbf{K}_{\\boldsymbol{(\\cdot)\\boldsymbol{x}}} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} \\mathbf{K}_{\\boldsymbol{xx}}^{-1} \\mathbf{K}_{\\boldsymbol{\\boldsymbol{x}(\\cdot)}}$. We take the latent distribution computed in the previous section and add this term to the covariance to construct $q_{Laplace}(f(\\cdot))$.</p> In\u00a0[8]: Copied! <pre>def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:\n    map_latent_dist = opt_posterior.predict(xtest, train_data=D)\n\n    Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)\n    Kxx = opt_posterior.prior.kernel.gram(x)\n    Kxx += identity_matrix(D.n) * jitter\n    Lx = Kxx.to_root()\n\n    # Lx\u207b\u00b9 Kxt\n    Lx_inv_Ktx = Lx.solve(Kxt)\n\n    # Kxx\u207b\u00b9 Kxt\n    Kxx_inv_Ktx = Lx.T.solve(Lx_inv_Ktx)\n\n    # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt\n    laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Ktx.T, H_inv), Kxx_inv_Ktx)\n\n    mean = map_latent_dist.mean()\n    covariance = map_latent_dist.covariance() + laplace_cov_term\n    L = jnp.linalg.cholesky(covariance)\n    return tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L)\n</pre> def construct_laplace(test_inputs: Float[Array, \"N D\"]) -&gt; tfd.MultivariateNormalTriL:     map_latent_dist = opt_posterior.predict(xtest, train_data=D)      Kxt = opt_posterior.prior.kernel.cross_covariance(x, test_inputs)     Kxx = opt_posterior.prior.kernel.gram(x)     Kxx += identity_matrix(D.n) * jitter     Lx = Kxx.to_root()      # Lx\u207b\u00b9 Kxt     Lx_inv_Ktx = Lx.solve(Kxt)      # Kxx\u207b\u00b9 Kxt     Kxx_inv_Ktx = Lx.T.solve(Lx_inv_Ktx)      # Ktx Kxx\u207b\u00b9[ H\u207b\u00b9 ] Kxx\u207b\u00b9 Kxt     laplace_cov_term = jnp.matmul(jnp.matmul(Kxx_inv_Ktx.T, H_inv), Kxx_inv_Ktx)      mean = map_latent_dist.mean()     covariance = map_latent_dist.covariance() + laplace_cov_term     L = jnp.linalg.cholesky(covariance)     return tfd.MultivariateNormalTriL(jnp.atleast_1d(mean.squeeze()), L) <p>From this we can construct the predictive distribution at the test points.</p> In\u00a0[9]: Copied! <pre>laplace_latent_dist = construct_laplace(xtest)\npredictive_dist = opt_posterior.likelihood(laplace_latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - predictive_std,\n    predictive_mean + predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"One sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> laplace_latent_dist = construct_laplace(xtest) predictive_dist = opt_posterior.likelihood(laplace_latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - predictive_std,     predictive_mean + predictive_std,     alpha=0.2,     color=cols[1],     label=\"One sigma\", ) ax.plot(     xtest,     predictive_mean - predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x7f8eb89d13a0&gt;</pre> <p>However, the Laplace approximation is still limited by considering information about the posterior at a single location. On the other hand, through approximate sampling, MCMC methods allow us to learn all information about the posterior distribution.</p> In\u00a0[10]: Copied! <pre>num_adapt = 500\nnum_samples = 500\n\nlpd = jax.jit(gpx.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n\n# Initialise the chain\nstart = time()\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\nprint(f\"Adaption time taken: {time() - start: .1f} seconds\")\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Sample from the posterior distribution\nstart = time()\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\nprint(f\"Sampling time taken: {time() - start: .1f} seconds\")\n</pre> num_adapt = 500 num_samples = 500  lpd = jax.jit(gpx.LogPosteriorDensity(negative=False)) unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))  adapt = blackjax.window_adaptation(     blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65 )  # Initialise the chain start = time() last_state, kernel, _ = adapt.run(key, posterior.unconstrain()) print(f\"Adaption time taken: {time() - start: .1f} seconds\")   def inference_loop(rng_key, kernel, initial_state, num_samples):     def one_step(state, rng_key):         state, info = kernel(rng_key, state)         return state, (state, info)      keys = jax.random.split(rng_key, num_samples)     _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)      return states, infos   # Sample from the posterior distribution start = time() states, infos = inference_loop(key, kernel, last_state, num_samples) print(f\"Sampling time taken: {time() - start: .1f} seconds\") <pre>Adaption time taken:  53.6 seconds\nSampling time taken:  58.4 seconds\n</pre> In\u00a0[11]: Copied! <pre>acceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</pre> acceptance_rate = jnp.mean(infos.acceptance_probability) print(f\"Acceptance rate: {acceptance_rate:.2f}\") <pre>Acceptance rate: 0.60\n</pre> <p>Our acceptance rate is slightly too large, prompting an examination of the chain's trace plots. A well-mixing chain will have very few (if any) flat spots in its trace plot whilst also not having too many steps in the same direction. In addition to the model's hyperparameters, there will be 500 samples for each of the 100 latent function values in the <code>states.position</code> dictionary. We depict the chains that correspond to the model hyperparameters and the first value of the latent function for brevity.</p> In\u00a0[12]: Copied! <pre>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.prior.kernel.lengthscale)\nax1.plot(states.position.prior.kernel.variance)\nax2.plot(states.position.latent[:, 1, :])\nax0.set_title(\"Kernel Lengthscale\")\nax1.set_title(\"Kernel Variance\")\nax2.set_title(\"Latent Function (index = 1)\")\n</pre> fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3)) ax0.plot(states.position.prior.kernel.lengthscale) ax1.plot(states.position.prior.kernel.variance) ax2.plot(states.position.latent[:, 1, :]) ax0.set_title(\"Kernel Lengthscale\") ax1.set_title(\"Kernel Variance\") ax2.set_title(\"Latent Function (index = 1)\") Out[12]: <pre>Text(0.5, 1.0, 'Latent Function (index = 1)')</pre> In\u00a0[13]: Copied! <pre>thin_factor = 20\nposterior_samples = []\n\nfor i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):\n    sample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)\n    sample = sample.constrain()\n    latent_dist = sample.predict(xtest, train_data=D)\n    predictive_dist = sample.likelihood(latent_dist)\n    posterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n\nposterior_samples = jnp.vstack(posterior_samples)\nlower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(posterior_samples, axis=0)\n</pre> thin_factor = 20 posterior_samples = []  for i in trange(0, num_samples, thin_factor, desc=\"Drawing posterior samples\"):     sample = jtu.tree_map(lambda samples, i=i: samples[i], states.position)     sample = sample.constrain()     latent_dist = sample.predict(xtest, train_data=D)     predictive_dist = sample.likelihood(latent_dist)     posterior_samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))  posterior_samples = jnp.vstack(posterior_samples) lower_ci, upper_ci = jnp.percentile(posterior_samples, jnp.array([2.5, 97.5]), axis=0) expected_val = jnp.mean(posterior_samples, axis=0) <pre>Drawing posterior samples: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:01&lt;00:00, 14.88it/s]\n</pre> <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots()\nax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7)\nax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=cols[1],\n    label=\"95\\\\% CI\",\n)\nax.plot(\n    xtest,\n    lower_ci.flatten(),\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    upper_ci.flatten(),\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> fig, ax = plt.subplots() ax.scatter(x, y, color=cols[0], label=\"Observations\", zorder=2, alpha=0.7) ax.plot(xtest, expected_val, color=cols[1], label=\"Predicted mean\", zorder=1) ax.fill_between(     xtest.flatten(),     lower_ci.flatten(),     upper_ci.flatten(),     alpha=0.2,     color=cols[1],     label=\"95\\\\% CI\", ) ax.plot(     xtest,     lower_ci.flatten(),     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     upper_ci.flatten(),     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() Out[14]: <pre>&lt;matplotlib.legend.Legend at 0x7f8eb8066df0&gt;</pre> In\u00a0[15]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\"\n</pre> %load_ext watermark %watermark -n -u -v -iv -w -a \"Thomas Pinder &amp; Daniel Dodd\" <pre>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\nmatplotlib            : 3.7.1\njax                   : 0.4.9\noptax                 : 0.1.5\nblackjax              : 0.9.6\ngpjax                 : 0.0.0\ntensorflow_probability: 0.19.0\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/classification/#classification","title":"Classification\u00b6","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via maximum a posteriori (MAP) and Markov chain Monte Carlo (MCMC). We focus on a classification task here and use BlackJax for sampling.</p>"},{"location":"examples/classification/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-1., 1)$ and corresponding binary outputs</p> $$\\boldsymbol{y} = 0.5 * \\text{sign}(\\cos(2 *  + \\boldsymbol{\\epsilon})) + 0.5, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N} \\left(\\textbf{0}, \\textbf{I} * (0.05)^{2} \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs for later.</p>"},{"location":"examples/classification/#map-inference","title":"MAP inference\u00b6","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. Since our observations are binary, we choose a Bernoulli likelihood with a probit link function.</p>"},{"location":"examples/classification/#laplace-approximation","title":"Laplace approximation\u00b6","text":"<p>The Laplace approximation improves uncertainty quantification by incorporating curvature induced by the marginal log-likelihood's Hessian to construct an approximate Gaussian distribution centered on the MAP estimate. Writing $\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})$ as the unormalised posterior for function values $\\boldsymbol{f}$ at the datapoints $\\boldsymbol{x}$, we can expand the log of this about the posterior mode $\\hat{\\boldsymbol{f}}$ via a Taylor expansion. This gives:</p> \\begin{align} \\log\\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) = \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) + \\left[\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})|_{\\hat{\\boldsymbol{f}}}\\right]^{T} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) + \\mathcal{O}(\\lVert \\boldsymbol{f} - \\hat{\\boldsymbol{f}} \\rVert^3). \\end{align}<p>Since $\\nabla \\log\\tilde{p}({\\boldsymbol{f}}|\\mathcal{D})$ is zero at the mode, this suggests the following approximation \\begin{align} \\tilde{p}(\\boldsymbol{f}|\\mathcal{D}) \\approx \\log\\tilde{p}(\\hat{\\boldsymbol{f}}|\\mathcal{D}) \\exp\\left\\{ \\frac{1}{2} (\\boldsymbol{f}-\\hat{\\boldsymbol{f}})^{T} \\left[-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} \\right] (\\boldsymbol{f}-\\hat{\\boldsymbol{f}}) \\right\\} \\end{align},</p> <p>that we identify as a Gaussian distribution, $p(\\boldsymbol{f}| \\mathcal{D}) \\approx q(\\boldsymbol{f}) := \\mathcal{N}(\\hat{\\boldsymbol{f}}, [-\\nabla^2 \\tilde{p}(\\boldsymbol{y}|\\boldsymbol{f})|_{\\hat{\\boldsymbol{f}}} ]^{-1} )$. Since the negative Hessian is positive definite, we can use the Cholesky decomposition to obtain the covariance matrix of the Laplace approximation at the datapoints below.</p>"},{"location":"examples/classification/#mcmc-inference","title":"MCMC inference\u00b6","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"examples/classification/#mcmc-through-blackjax","title":"MCMC through BlackJax\u00b6","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We'll use the No U-Turn Sampler (NUTS) implementation given in BlackJax for sampling. For the interested reader, NUTS is a Hamiltonian Monte Carlo sampling scheme where the number of leapfrog integration steps is computed at each step of the change according to the NUTS algorithm. In general, samplers constructed under this framework are very efficient.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 500 values from our Markov chain. In practice, drawing more samples will be necessary.</p>"},{"location":"examples/classification/#sampler-efficiency","title":"Sampler efficiency\u00b6","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain). For NUTS and Hamiltonian Monte Carlo sampling, we typically seek an acceptance rate of 60-70% to strike the right balance between having a chain which is stuck and rarely moves versus a chain that is too jumpy with frequent small steps.</p>"},{"location":"examples/classification/#prediction","title":"Prediction\u00b6","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every $n$th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>"},{"location":"examples/classification/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/collapsed_vi/","title":"Sparse Gaussian Process Regression","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from docs.examples.utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>n = 2500\nnoise = 0.5\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 2500 noise = 0.5  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(2 * x) + x * jnp.cos(5 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.1, 3.1, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise. We also plot an initial set of inducing points over the space.</p> In\u00a0[3]: Copied! <pre>n_inducing = 50\nz = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1])\nax.vlines(\n    x=z,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=0.5,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend(loc=\"best\")\nplt.show()\n</pre> n_inducing = 50 z = jnp.linspace(-3.0, 3.0, n_inducing).reshape(-1, 1)  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.25, label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", linewidth=2, color=cols[1]) ax.vlines(     x=z,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=0.5,     label=\"Inducing point\",     color=cols[2], ) ax.legend(loc=\"best\") plt.show() <p>Next we define the true posterior model for the data - note that whilst we can define this, it is intractable to evaluate.</p> In\u00a0[4]: Copied! <pre>meanf = gpx.Constant()\nkernel = gpx.RBF()\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nposterior = prior * likelihood\n</pre> meanf = gpx.Constant() kernel = gpx.RBF() likelihood = gpx.Gaussian(num_datapoints=D.n) prior = gpx.Prior(mean_function=meanf, kernel=kernel) posterior = prior * likelihood <p>We now define the SGPR model through <code>CollapsedVariationalGaussian</code>. Through a set of inducing points $\\boldsymbol{z}$ this object builds an approximation to the true posterior distribution. Consequently, we pass the true posterior and initial inducing points into the constructor as arguments.</p> In\u00a0[5]: Copied! <pre>q = gpx.CollapsedVariationalGaussian(posterior=posterior, inducing_inputs=z)\n</pre> q = gpx.CollapsedVariationalGaussian(posterior=posterior, inducing_inputs=z) <p>We define our variational inference algorithm through <code>CollapsedVI</code>. This defines the collapsed variational free energy bound considered in Titsias (2009).</p> In\u00a0[6]: Copied! <pre>elbo = gpx.CollapsedELBO(negative=True)\n</pre> elbo = gpx.CollapsedELBO(negative=True) <p>For researchers, GPJax has the capacity to print the bibtex citation for objects such as the ELBO through the <code>cite()</code> function.</p> In\u00a0[7]: Copied! <pre>print(gpx.cite(elbo))\n</pre> print(gpx.cite(elbo)) <pre>@inproceedings{titsias2009variational,\nauthors = {Titsias, Michalis},\ntitle = {Variational learning of inducing variables in sparse Gaussian processes},\nyear = {2009},\nbooktitle = {International Conference on Artificial Intelligence and Statistics},\n}\n</pre> <p>JIT-compiling expensive-to-compute functions such as the ELBO is advisable. This can be achieved by wrapping the function in <code>jax.jit()</code>.</p> In\u00a0[8]: Copied! <pre>elbo = jit(elbo)\n</pre>  elbo = jit(elbo) <p>We now train our model akin to a Gaussian process regression model via the <code>fit</code> abstraction. Unlike the regression example given in the conjugate regression notebook, the inducing locations that induce our variational posterior distribution are now part of the model's parameters. Using a gradient-based optimiser, we can then optimise their location such that the evidence lower bound is maximised.</p> In\u00a0[9]: Copied! <pre>opt_posterior, history = gpx.fit(\n    model=q,\n    objective=elbo,\n    train_data=D,\n    optim=ox.adamw(learning_rate=1e-2),\n    num_iters=500,\n    key=key,\n)\n</pre> opt_posterior, history = gpx.fit(     model=q,     objective=elbo,     train_data=D,     optim=ox.adamw(learning_rate=1e-2),     num_iters=500,     key=key, ) <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\")\n</pre> fig, ax = plt.subplots() ax.plot(history, color=cols[1]) ax.set(xlabel=\"Training iterate\", ylabel=\"ELBO\") Out[10]: <pre>[Text(0.5, 0, 'Training iterate'), Text(0, 0.5, 'ELBO')]</pre> <p>We show predictions of our model with the learned inducing points overlaid in grey.</p> In\u00a0[11]: Copied! <pre>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\ninducing_points = opt_posterior.inducing_inputs\n\nsamples = latent_dist.sample(seed=key, sample_shape=(20,))\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\n\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1)\nax.plot(\n    xtest,\n    ytest,\n    label=\"Latent function\",\n    color=cols[1],\n    linestyle=\"-\",\n    linewidth=1,\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=0.5,\n)\n\n\nax.vlines(\n    x=inducing_points,\n    ymin=ytest.min(),\n    ymax=ytest.max(),\n    alpha=0.3,\n    linewidth=0.5,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\nplt.show()\n</pre> latent_dist = opt_posterior(xtest, train_data=D) predictive_dist = opt_posterior.posterior.likelihood(latent_dist)  inducing_points = opt_posterior.inducing_inputs  samples = latent_dist.sample(seed=key, sample_shape=(20,))  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots()  ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.1) ax.plot(     xtest,     ytest,     label=\"Latent function\",     color=cols[1],     linestyle=\"-\",     linewidth=1, ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])  ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     color=cols[1],     label=\"Two sigma\", ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=0.5, ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=0.5, )   ax.vlines(     x=inducing_points,     ymin=ytest.min(),     ymax=ytest.max(),     alpha=0.3,     linewidth=0.5,     label=\"Inducing point\",     color=cols[2], ) ax.legend() ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\") plt.show() In\u00a0[12]: Copied! <pre>full_rank_model = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.RBF()) * gpx.Gaussian(\n    num_datapoints=D.n\n)\nnegative_mll = jit(gpx.ConjugateMLL(negative=True))\n%timeit negative_mll(full_rank_model, D).block_until_ready()\n</pre> full_rank_model = gpx.Prior(mean_function=gpx.Zero(), kernel=gpx.RBF()) * gpx.Gaussian(     num_datapoints=D.n ) negative_mll = jit(gpx.ConjugateMLL(negative=True)) %timeit negative_mll(full_rank_model, D).block_until_ready() <pre>202 ms \u00b1 35.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[13]: Copied! <pre>negative_elbo = jit(gpx.CollapsedELBO(negative=True))\n%timeit negative_elbo(q, D).block_until_ready()\n</pre> negative_elbo = jit(gpx.CollapsedELBO(negative=True)) %timeit negative_elbo(q, D).block_until_ready() <pre>2.73 ms \u00b1 134 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> <p>As we can see, the sparse approximation given here is around 50 times faster when compared against a full-rank model.</p> In\u00a0[14]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Daniel Dodd' <pre>Author: Daniel Dodd\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\noptax     : 0.1.5\ngpjax     : 0.0.0\nmatplotlib: 3.7.1\njax       : 0.4.9\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/collapsed_vi/#sparse-gaussian-process-regression","title":"Sparse Gaussian Process Regression\u00b6","text":"<p>In this notebook we consider sparse Gaussian process regression (SGPR) Titsias (2009). This is a solution for medium to large-scale conjugate regression problems. In order to arrive at a computationally tractable method, the approximate posterior is parameterized via a set of $m$ pseudo-points $\\boldsymbol{z}$. Critically, the approach leads to $\\mathcal{O}(nm^2)$ complexity for approximate maximum likelihood learning and $O(m^2)$ per test point for prediction.</p>"},{"location":"examples/collapsed_vi/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{500}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(7\\boldsymbol{x}) + x \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.5^2 \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/collapsed_vi/#runtime-comparison","title":"Runtime comparison\u00b6","text":"<p>Given the size of the data being considered here, inference in a GP with a full-rank covariance matrix is possible, albeit quite slow. We can therefore compare the speedup that we get from using the above sparse approximation with corresponding bound on the marginal log-likelihood against the marginal log-likelihood in the full model.</p>"},{"location":"examples/collapsed_vi/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/constructing_new_kernels/","title":"Kernel Guide","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax as ox\nfrom simple_pytree import static_field\nimport tensorflow_probability.substrates.jax as tfp\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base.param import param_field\n\nkey = jr.PRNGKey(123)\ntfb = tfp.bijectors\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from dataclasses import dataclass from typing import Dict  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib.pyplot as plt import numpy as np import optax as ox from simple_pytree import static_field import tensorflow_probability.substrates.jax as tfp  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base.param import param_field  key = jr.PRNGKey(123) tfb = tfp.bijectors plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>kernels = [\n    gpx.kernels.Matern12(),\n    gpx.kernels.Matern32(),\n    gpx.kernels.Matern52(),\n    gpx.kernels.RBF(),\n    gpx.kernels.Polynomial(),\n    gpx.kernels.Polynomial(degree=2),\n]\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n\nmeanf = gpx.mean_functions.Zero()\n\nfor k, ax in zip(kernels, axes.ravel()):\n    prior = gpx.Prior(mean_function=meanf, kernel=k)\n    rv = prior(x)\n    y = rv.sample(seed=key, sample_shape=(10,))\n    ax.plot(x, y.T, alpha=0.7)\n    ax.set_title(k.name)\n</pre> kernels = [     gpx.kernels.Matern12(),     gpx.kernels.Matern32(),     gpx.kernels.Matern52(),     gpx.kernels.RBF(),     gpx.kernels.Polynomial(),     gpx.kernels.Polynomial(degree=2), ] fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(10, 6), tight_layout=True)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)  meanf = gpx.mean_functions.Zero()  for k, ax in zip(kernels, axes.ravel()):     prior = gpx.Prior(mean_function=meanf, kernel=k)     rv = prior(x)     y = rv.sample(seed=key, sample_shape=(10,))     ax.plot(x, y.T, alpha=0.7)     ax.set_title(k.name) In\u00a0[3]: Copied! <pre>slice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3])\n</pre> slice_kernel = gpx.kernels.RBF(active_dims=[0, 1, 3]) <p>The resulting kernel has one length-scale parameter per input dimension --- an ARD kernel.</p> In\u00a0[4]: Copied! <pre>print(f\"Lengthscales: {slice_kernel.lengthscale}\")\n</pre> print(f\"Lengthscales: {slice_kernel.lengthscale}\") <pre>Lengthscales: 1.0\n</pre> <p>We'll now simulate some data and evaluate the kernel on the previously selected input dimensions.</p> In\u00a0[5]: Copied! <pre># Inputs\nx_matrix = jr.normal(key, shape=(50, 5))\n\n# Compute the Gram matrix\nK = slice_kernel.gram(x_matrix)\nprint(K.shape)\n</pre> # Inputs x_matrix = jr.normal(key, shape=(50, 5))  # Compute the Gram matrix K = slice_kernel.gram(x_matrix) print(K.shape) <pre>(50, 50)\n</pre> In\u00a0[6]: Copied! <pre>k1 = gpx.kernels.RBF()\nk2 = gpx.kernels.Polynomial()\nsum_k = gpx.kernels.SumKernel(kernels=[k1, k2])\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(sum_k.gram(x).to_dense())\n\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\n</pre> k1 = gpx.kernels.RBF() k2 = gpx.kernels.Polynomial() sum_k = gpx.kernels.SumKernel(kernels=[k1, k2])  fig, ax = plt.subplots(ncols=3, figsize=(9, 3)) im0 = ax[0].matshow(k1.gram(x).to_dense()) im1 = ax[1].matshow(k2.gram(x).to_dense()) im2 = ax[2].matshow(sum_k.gram(x).to_dense())  fig.colorbar(im0, ax=ax[0], fraction=0.05) fig.colorbar(im1, ax=ax[1], fraction=0.05) fig.colorbar(im2, ax=ax[2], fraction=0.05) Out[6]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f2ed4148700&gt;</pre> <p>Similarly, products of kernels can be created through the <code>ProductKernel</code> class.</p> In\u00a0[7]: Copied! <pre>k3 = gpx.kernels.Matern32()\n\nprod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])\n\nfig, ax = plt.subplots(ncols=4, figsize=(12, 3))\nim0 = ax[0].matshow(k1.gram(x).to_dense())\nim1 = ax[1].matshow(k2.gram(x).to_dense())\nim2 = ax[2].matshow(k3.gram(x).to_dense())\nim3 = ax[3].matshow(prod_k.gram(x).to_dense())\n\nfig.colorbar(im0, ax=ax[0], fraction=0.05)\nfig.colorbar(im1, ax=ax[1], fraction=0.05)\nfig.colorbar(im2, ax=ax[2], fraction=0.05)\nfig.colorbar(im3, ax=ax[3], fraction=0.05)\n</pre> k3 = gpx.kernels.Matern32()  prod_k = gpx.kernels.ProductKernel(kernels=[k1, k2, k3])  fig, ax = plt.subplots(ncols=4, figsize=(12, 3)) im0 = ax[0].matshow(k1.gram(x).to_dense()) im1 = ax[1].matshow(k2.gram(x).to_dense()) im2 = ax[2].matshow(k3.gram(x).to_dense()) im3 = ax[3].matshow(prod_k.gram(x).to_dense())  fig.colorbar(im0, ax=ax[0], fraction=0.05) fig.colorbar(im1, ax=ax[1], fraction=0.05) fig.colorbar(im2, ax=ax[2], fraction=0.05) fig.colorbar(im3, ax=ax[3], fraction=0.05) Out[7]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f2ec49f2400&gt;</pre> In\u00a0[8]: Copied! <pre>def angular_distance(x, y, c):\n    return jnp.abs((x - y + c) % (c * 2) - c)\n\n\nbij = tfb.Chain([tfb.Softplus(), tfb.Shift(np.array(4.0).astype(np.float64))])\n\n\n@dataclass\nclass Polar(gpx.kernels.AbstractKernel):\n    period: float = static_field(2 * jnp.pi)\n    tau: float = param_field(jnp.array([4.0]), bijector=bij)\n\n    def __call__(\n        self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        c = self.period / 2.0\n        t = angular_distance(x, y, c)\n        K = (1 + self.tau * t / c) * jnp.clip(1 - t / c, 0, jnp.inf) ** self.tau\n        return K.squeeze()\n</pre> def angular_distance(x, y, c):     return jnp.abs((x - y + c) % (c * 2) - c)   bij = tfb.Chain([tfb.Softplus(), tfb.Shift(np.array(4.0).astype(np.float64))])   @dataclass class Polar(gpx.kernels.AbstractKernel):     period: float = static_field(2 * jnp.pi)     tau: float = param_field(jnp.array([4.0]), bijector=bij)      def __call__(         self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"]     ) -&gt; Float[Array, \"1\"]:         c = self.period / 2.0         t = angular_distance(x, y, c)         K = (1 + self.tau * t / c) * jnp.clip(1 - t / c, 0, jnp.inf) ** self.tau         return K.squeeze() <p>We unpack this now to make better sense of it. In the kernel's initialiser we specify the length of a single period. As the underlying domain is a circle, this is $2\\pi$. We then define the kernel's <code>__call__</code> function which is a direct implementation of Equation (1) where we define <code>c</code> as half the value of <code>period</code>.</p> <p>To constrain $\\tau$ to be greater than 4, we use a <code>Softplus</code> bijector with a clipped lower bound of 4.0. This is done by specifying the <code>bijector</code> argument when we define the parameter field.</p> In\u00a0[9]: Copied! <pre># Simulate data\nangles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1)\nn = 20\nnoise = 0.2\n\nX = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0)\ny = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise\n\nD = gpx.Dataset(X=X, y=y)\n\n# Define polar Gaussian process\nPKern = Polar()\nmeanf = gpx.mean_functions.Zero()\nlikelihood = gpx.Gaussian(num_datapoints=n)\ncircular_posterior = gpx.Prior(mean_function=meanf, kernel=PKern) * likelihood\n\n# Optimise GP's marginal log-likelihood using Adam\nopt_posterior, history = gpx.fit(\n    model=circular_posterior,\n    objective=jit(gpx.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.05),\n    num_iters=500,\n    key=key,\n)\n</pre> # Simulate data angles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1) n = 20 noise = 0.2  X = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0) y = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise  D = gpx.Dataset(X=X, y=y)  # Define polar Gaussian process PKern = Polar() meanf = gpx.mean_functions.Zero() likelihood = gpx.Gaussian(num_datapoints=n) circular_posterior = gpx.Prior(mean_function=meanf, kernel=PKern) * likelihood  # Optimise GP's marginal log-likelihood using Adam opt_posterior, history = gpx.fit(     model=circular_posterior,     objective=jit(gpx.ConjugateMLL(negative=True)),     train_data=D,     optim=ox.adamw(learning_rate=0.05),     num_iters=500,     key=key, ) <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre>posterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D))\nmu = posterior_rv.mean()\none_sigma = posterior_rv.stddev()\n</pre> posterior_rv = opt_posterior.likelihood(opt_posterior.predict(angles, train_data=D)) mu = posterior_rv.mean() one_sigma = posterior_rv.stddev() In\u00a0[11]: Copied! <pre>fig = plt.figure(figsize=(7, 3.5))\ngridspec = fig.add_gridspec(1, 1)\nax = plt.subplot(gridspec[0], polar=True)\n\nax.fill_between(\n    angles.squeeze(),\n    mu - one_sigma,\n    mu + one_sigma,\n    alpha=0.3,\n    label=r\"1 Posterior s.d.\",\n    color=cols[1],\n    lw=0,\n)\nax.fill_between(\n    angles.squeeze(),\n    mu - 3 * one_sigma,\n    mu + 3 * one_sigma,\n    alpha=0.15,\n    label=r\"3 Posterior s.d.\",\n    color=cols[1],\n    lw=0,\n)\nax.plot(angles, mu, label=\"Posterior mean\")\nax.scatter(D.X, D.y, alpha=1, label=\"Observations\")\nax.legend()\n</pre> fig = plt.figure(figsize=(7, 3.5)) gridspec = fig.add_gridspec(1, 1) ax = plt.subplot(gridspec[0], polar=True)  ax.fill_between(     angles.squeeze(),     mu - one_sigma,     mu + one_sigma,     alpha=0.3,     label=r\"1 Posterior s.d.\",     color=cols[1],     lw=0, ) ax.fill_between(     angles.squeeze(),     mu - 3 * one_sigma,     mu + 3 * one_sigma,     alpha=0.15,     label=r\"3 Posterior s.d.\",     color=cols[1],     lw=0, ) ax.plot(angles, mu, label=\"Posterior mean\") ax.scatter(D.X, D.y, alpha=1, label=\"Observations\") ax.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x7f2ec42a94f0&gt;</pre> In\u00a0[12]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\ntensorflow_probability: 0.19.0\ngpjax                 : 0.0.0\njax                   : 0.4.9\nmatplotlib            : 3.7.1\noptax                 : 0.1.5\nnumpy                 : 1.24.3\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/constructing_new_kernels/#kernel-guide","title":"Kernel Guide\u00b6","text":"<p>In this guide, we introduce the kernels available in GPJax and demonstrate how to create custom kernels.</p>"},{"location":"examples/constructing_new_kernels/#supported-kernels","title":"Supported Kernels\u00b6","text":"<p>The following kernels are natively supported in GPJax.</p> <ul> <li>Mat\u00e9rn 1/2, 3/2 and 5/2.</li> <li>RBF (or squared exponential).</li> <li>Rational quadratic.</li> <li>Powered exponential.</li> <li>Polynomial.</li> <li>White noise</li> <li>Linear.</li> <li>Polynomial.</li> <li>Graph kernels.</li> </ul> <p>While the syntax is consistent, each kernel's type influences the characteristics of the sample paths drawn. We visualise this below with 10 function draws per kernel.</p>"},{"location":"examples/constructing_new_kernels/#active-dimensions","title":"Active dimensions\u00b6","text":"<p>By default, kernels operate over every dimension of the supplied inputs. In some use cases, it is desirable to restrict kernels to specific dimensions of the input data. We can achieve this by the <code>active dims</code> argument, which determines which input index values the kernel evaluates.</p> <p>To see this, consider the following 5-dimensional dataset for which we would like our RBF kernel to act on the first, second and fourth dimensions.</p>"},{"location":"examples/constructing_new_kernels/#kernel-combinations","title":"Kernel combinations\u00b6","text":"<p>The product or sum of two positive definite matrices yields a positive definite matrix. Consequently, summing or multiplying sets of kernels is a valid operation that can give rich kernel functions. In GPJax, functionality for a sum kernel is provided by the <code>SumKernel</code> class.</p>"},{"location":"examples/constructing_new_kernels/#custom-kernel","title":"Custom kernel\u00b6","text":"<p>GPJax makes the process of implementing kernels of your choice straightforward with two key steps:</p> <ol> <li>Listing the kernel's parameters.</li> <li>Defining the kernel's pairwise operation.</li> </ol> <p>We'll demonstrate this process now for a circular kernel --- an adaption of the excellent guide given in the PYMC3 documentation. We encourage curious readers to visit their notebook here.</p>"},{"location":"examples/constructing_new_kernels/#circular-kernel","title":"Circular kernel\u00b6","text":"<p>When the underlying space is polar, typical Euclidean kernels such as Mat\u00e9rn kernels are insufficient at the boundary where discontinuities will present themselves. This is due to the fact that for a polar space $\\lvert 0, 2\\pi\\rvert=0$ i.e., the space wraps. Euclidean kernels have no mechanism in them to represent this logic and will instead treat $0$ and $2\\pi$ and elements far apart. Circular kernels do not exhibit this behaviour and instead wrap around the boundary points to create a smooth function. Such a kernel was given in Padonou &amp; Roustant (2015) where any two angles $\\theta$ and $\\theta'$ are written as $$W_c(\\theta, \\theta') = \\left\\lvert \\left(1 + \\tau \\frac{d(\\theta, \\theta')}{c} \\right) \\left(1 - \\frac{d(\\theta, \\theta')}{c} \\right)^{\\tau} \\right\\rvert \\quad \\tau \\geq 4 \\tag{1}.$$</p> <p>Here the hyperparameter $\\tau$ is analogous to a lengthscale for Euclidean stationary kernels, controlling the correlation between pairs of observations. While $d$ is an angular distance metric</p> $$d(\\theta, \\theta') = \\lvert (\\theta-\\theta'+c) \\operatorname{mod} 2c - c \\rvert.$$<p>To implement this, one must write the following class.</p>"},{"location":"examples/constructing_new_kernels/#using-our-polar-kernel","title":"Using our polar kernel\u00b6","text":"<p>We proceed to fit a GP with our custom circular kernel to a random sequence of points on a circle (see the Regression notebook for further details on this process).</p>"},{"location":"examples/constructing_new_kernels/#prediction","title":"Prediction\u00b6","text":"<p>We'll now query the GP's predictive posterior at linearly spaced novel inputs and illustrate the results.</p>"},{"location":"examples/constructing_new_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/deep_kernels/","title":"Deep Kernel Learning","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import (\n    dataclass,\n    field,\n)\nfrom typing import Any\n\nimport flax\nfrom flax import linen as nn\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom scipy.signal import sawtooth\nfrom gpjax.base import static_field\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base import param_field\n    import gpjax.kernels as jk\n    from gpjax.kernels import DenseKernelComputation\n    from gpjax.kernels.base import AbstractKernel\n    from gpjax.kernels.computations import AbstractKernelComputation\n\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from dataclasses import (     dataclass,     field, ) from typing import Any  import flax from flax import linen as nn import jax import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from scipy.signal import sawtooth from gpjax.base import static_field  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base import param_field     import gpjax.kernels as jk     from gpjax.kernels import DenseKernelComputation     from gpjax.kernels.base import AbstractKernel     from gpjax.kernels.computations import AbstractKernelComputation  key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>n = 500\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x))\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\nytest = f(xtest)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5)\nax.plot(xtest, ytest, label=\"True function\")\nax.legend(loc=\"best\")\n</pre> n = 500 noise = 0.2  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-2.0, maxval=2.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.asarray(sawtooth(2 * jnp.pi * x)) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1) ytest = f(xtest)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Training data\", alpha=0.5) ax.plot(xtest, ytest, label=\"True function\") ax.legend(loc=\"best\") Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x7fa624adbfd0&gt;</pre> In\u00a0[3]: Copied! <pre>@dataclass\nclass DeepKernelFunction(AbstractKernel):\n    base_kernel: AbstractKernel = None\n    network: nn.Module = static_field(None)\n    dummy_x: jax.Array = static_field(None)\n    key: jr.PRNGKeyArray = static_field(jr.PRNGKey(123))\n    nn_params: Any = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if self.base_kernel is None:\n            raise ValueError(\"base_kernel must be specified\")\n        if self.network is None:\n            raise ValueError(\"network must be specified\")\n        self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))\n\n    def __call__(\n        self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]\n    ) -&gt; Float[Array, \"1\"]:\n        state = self.network.init(self.key, x)\n        xt = self.network.apply(state, x)\n        yt = self.network.apply(state, y)\n        return self.base_kernel(xt, yt)\n</pre> @dataclass class DeepKernelFunction(AbstractKernel):     base_kernel: AbstractKernel = None     network: nn.Module = static_field(None)     dummy_x: jax.Array = static_field(None)     key: jr.PRNGKeyArray = static_field(jr.PRNGKey(123))     nn_params: Any = field(init=False, repr=False)      def __post_init__(self):         if self.base_kernel is None:             raise ValueError(\"base_kernel must be specified\")         if self.network is None:             raise ValueError(\"network must be specified\")         self.nn_params = flax.core.unfreeze(self.network.init(key, self.dummy_x))      def __call__(         self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]     ) -&gt; Float[Array, \"1\"]:         state = self.network.init(self.key, x)         xt = self.network.apply(state, x)         yt = self.network.apply(state, y)         return self.base_kernel(xt, yt) In\u00a0[4]: Copied! <pre>feature_space_dim = 3\n\n\nclass Network(nn.Module):\n\"\"\"A simple MLP.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=32)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=64)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=feature_space_dim)(x)\n        return x\n\n\nforward_linear = Network()\n</pre> feature_space_dim = 3   class Network(nn.Module):     \"\"\"A simple MLP.\"\"\"      @nn.compact     def __call__(self, x):         x = nn.Dense(features=32)(x)         x = nn.relu(x)         x = nn.Dense(features=64)(x)         x = nn.relu(x)         x = nn.Dense(features=feature_space_dim)(x)         return x   forward_linear = Network() In\u00a0[5]: Copied! <pre>base_kernel = gpx.Matern52(active_dims=list(range(feature_space_dim)))\nkernel = DeepKernelFunction(\n    network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x\n)\nmeanf = gpx.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n</pre> base_kernel = gpx.Matern52(active_dims=list(range(feature_space_dim))) kernel = DeepKernelFunction(     network=forward_linear, base_kernel=base_kernel, key=key, dummy_x=x ) meanf = gpx.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.Gaussian(num_datapoints=D.n) posterior = prior * likelihood In\u00a0[6]: Copied! <pre>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.01,\n    warmup_steps=75,\n    decay_steps=700,\n    end_value=0.0,\n)\n\noptimiser = ox.chain(\n    ox.clip(1.0),\n    ox.adamw(learning_rate=schedule),\n)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=jax.jit(gpx.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=optimiser,\n    num_iters=800,\n    key=key,\n)\n</pre> schedule = ox.warmup_cosine_decay_schedule(     init_value=0.0,     peak_value=0.01,     warmup_steps=75,     decay_steps=700,     end_value=0.0, )  optimiser = ox.chain(     ox.clip(1.0),     ox.adamw(learning_rate=schedule), )  opt_posterior, history = gpx.fit(     model=posterior,     objective=jax.jit(gpx.ConjugateMLL(negative=True)),     train_data=D,     optim=optimiser,     num_iters=800,     key=key, ) <pre>  0%|          | 0/800 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre>latent_dist = opt_posterior(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    color=cols[1],\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.legend()\n</pre> latent_dist = opt_posterior(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev()  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     color=cols[1],     label=\"Two sigma\", ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     color=cols[1],     linestyle=\"--\",     linewidth=1, ) ax.legend() Out[7]: <pre>&lt;matplotlib.legend.Legend at 0x7fa624adb6d0&gt;</pre> In\u00a0[8]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\njax       : 0.4.9\ngpjax     : 0.0.0\nmatplotlib: 3.7.1\noptax     : 0.1.5\nflax      : 0.6.10\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/deep_kernels/#deep-kernel-learning","title":"Deep Kernel Learning\u00b6","text":"<p>In this notebook we demonstrate how GPJax can be used in conjunction with Flax to build deep kernel Gaussian processes. Modelling data with discontinuities is a challenging task for regular Gaussian process models. However, as shown in , transforming the inputs to our Gaussian process model's kernel through a neural network can offer a solution to this.</p>"},{"location":"examples/deep_kernels/#dataset","title":"Dataset\u00b6","text":"<p>As previously mentioned, deep kernels are particularly useful when the data has discontinuities. To highlight this, we will use a sawtooth function as our data.</p>"},{"location":"examples/deep_kernels/#deep-kernels","title":"Deep kernels\u00b6","text":""},{"location":"examples/deep_kernels/#details","title":"Details\u00b6","text":"<p>Instead of applying a kernel $k(\\cdot, \\cdot')$ directly on some data, we seek to apply a feature map $\\phi(\\cdot)$ that projects the data to learn more meaningful representations beforehand. In deep kernel learning, $\\phi$ is a neural network whose parameters are learned jointly with the GP model's hyperparameters. The corresponding kernel is then computed by $k(\\phi(\\cdot), \\phi(\\cdot'))$. Here $k(\\cdot,\\cdot')$ is referred to as the base kernel.</p>"},{"location":"examples/deep_kernels/#implementation","title":"Implementation\u00b6","text":"<p>Although deep kernels are not currently supported natively in GPJax, defining one is straightforward as we now demonstrate. Inheriting from the base <code>AbstractKernel</code> in GPJax, we create the <code>DeepKernelFunction</code> object that allows the user to supply the neural network and base kernel of their choice. Kernel matrices are then computed using the regular <code>gram</code> and <code>cross_covariance</code> functions.</p>"},{"location":"examples/deep_kernels/#defining-a-network","title":"Defining a network\u00b6","text":"<p>With a deep kernel object created, we proceed to define a neural network. Here we consider a small multi-layer perceptron with two linear hidden layers and ReLU activation functions between the layers. The first hidden layer contains 64 units, while the second layer contains 32 units. Finally, we'll make the output of our network a three units wide. The corresponding kernel that we define will then be of ARD form to allow for different lengthscales in each dimension of the feature space. Users may wish to design more intricate network structures for more complex tasks, which functionality is supported well in Haiku.</p>"},{"location":"examples/deep_kernels/#defining-a-model","title":"Defining a model\u00b6","text":"<p>Having characterised the feature extraction network, we move to define a Gaussian process parameterised by this deep kernel. We consider a third-order Mat\u00e9rn base kernel and assume a Gaussian likelihood.</p>"},{"location":"examples/deep_kernels/#optimisation","title":"Optimisation\u00b6","text":"<p>We train our model via maximum likelihood estimation of the marginal log-likelihood. The parameters of our neural network are learned jointly with the model's hyperparameter set.</p> <p>With the inclusion of a neural network, we take this opportunity to highlight the additional benefits gleaned from using Optax for optimisation. In particular, we showcase the ability to use a learning rate scheduler that decays the optimiser's learning rate throughout the inference. We decrease the learning rate according to a half-cosine curve over 700 iterations, providing us with large step sizes early in the optimisation procedure before approaching more conservative values, ensuring we do not step too far. We also consider a linear warmup, where the learning rate is increased from 0 to 1 over 50 steps to get a reasonable initial learning rate value.</p>"},{"location":"examples/deep_kernels/#prediction","title":"Prediction\u00b6","text":"<p>With a set of learned parameters, the only remaining task is to predict the output of the model. We can do this by simply applying the model to a test data set.</p>"},{"location":"examples/deep_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/graph_kernels/","title":"Graph Kernels","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport random\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport optax as ox\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  import random  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import networkx as nx import optax as ox  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>vertex_per_side = 20\nn_edges_to_remove = 30\np = 0.8\n\nG = nx.barbell_graph(vertex_per_side, 0)\n\nrandom.seed(123)\n[G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]\n\npos = nx.spring_layout(G, seed=123)  # positions for all nodes\n\nnx.draw(\n    G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False\n)\n</pre> vertex_per_side = 20 n_edges_to_remove = 30 p = 0.8  G = nx.barbell_graph(vertex_per_side, 0)  random.seed(123) [G.remove_edge(*i) for i in random.sample(list(G.edges), n_edges_to_remove)]  pos = nx.spring_layout(G, seed=123)  # positions for all nodes  nx.draw(     G, pos, node_size=100, node_color=cols[1], edge_color=\"black\", with_labels=False ) <pre>/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/events.py:89: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[3]: Copied! <pre>L = nx.laplacian_matrix(G).toarray()\n</pre> L = nx.laplacian_matrix(G).toarray() In\u00a0[4]: Copied! <pre>x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)\n\ntrue_kernel = gpx.GraphKernel(\n    laplacian=L,\n    lengthscale=2.3,\n    variance=3.2,\n    smoothness=6.1,\n)\nprior = gpx.Prior(mean_function=gpx.Zero(), kernel=true_kernel)\n\nfx = prior(x)\ny = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)\n\nD = gpx.Dataset(X=x, y=y)\n</pre> x = jnp.arange(G.number_of_nodes()).reshape(-1, 1)  true_kernel = gpx.GraphKernel(     laplacian=L,     lengthscale=2.3,     variance=3.2,     smoothness=6.1, ) prior = gpx.Prior(mean_function=gpx.Zero(), kernel=true_kernel)  fx = prior(x) y = fx.sample(seed=key, sample_shape=(1,)).reshape(-1, 1)  D = gpx.Dataset(X=x, y=y) <p>We can visualise this signal in the following cell.</p> In\u00a0[5]: Copied! <pre>nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)\n\nvmin, vmax = y.min(), y.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\ncbar = plt.colorbar(sm)\n</pre> nx.draw(G, pos, node_color=y, with_labels=False, alpha=0.5)  vmin, vmax = y.min(), y.max() sm = plt.cm.ScalarMappable(     cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax) ) sm.set_array([]) cbar = plt.colorbar(sm) <pre>/tmp/ipykernel_11762/3436043563.py:8: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n  cbar = plt.colorbar(sm)\n/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[6]: Copied! <pre>likelihood = gpx.Gaussian(num_datapoints=D.n)\nkernel = gpx.GraphKernel(laplacian=L)\nprior = gpx.Prior(mean_function=gpx.Zero(), kernel=kernel)\nposterior = prior * likelihood\n</pre> likelihood = gpx.Gaussian(num_datapoints=D.n) kernel = gpx.GraphKernel(laplacian=L) prior = gpx.Prior(mean_function=gpx.Zero(), kernel=kernel) posterior = prior * likelihood <p>For researchers and the curious reader, GPJax provides the ability to print the bibtex citation for objects such as the graph kernel through the <code>cite()</code> function.</p> In\u00a0[7]: Copied! <pre>print(gpx.cite(kernel))\n</pre> print(gpx.cite(kernel)) <pre>@inproceedings{borovitskiy2021matern,\nauthors = {Borovitskiy, Viacheslav and Azangulov, Iskander and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc and Durrande, Nicolas},\ntitle = {Mat\u00e9rn Gaussian Processes on Graphs},\nyear = {2021},\nbooktitle = {International Conference on Artificial Intelligence and Statistics},\n}\n</pre> <p>With a posterior defined, we can now optimise the model's hyperparameters.</p> In\u00a0[8]: Copied! <pre>opt_posterior, training_history = gpx.fit(\n    model=posterior,\n    objective=jit(gpx.ConjugateMLL(negative=True)),\n    train_data=D,\n    optim=ox.adamw(learning_rate=0.01),\n    num_iters=1000,\n    key=key,\n)\n</pre> opt_posterior, training_history = gpx.fit(     model=posterior,     objective=jit(gpx.ConjugateMLL(negative=True)),     train_data=D,     optim=ox.adamw(learning_rate=0.01),     num_iters=1000,     key=key, ) <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> In\u00a0[9]: Copied! <pre>initial_dist = likelihood(posterior(x, D))\npredictive_dist = opt_posterior.likelihood(opt_posterior(x, D))\n\ninitial_mean = initial_dist.mean()\nlearned_mean = predictive_dist.mean()\n\nrmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))\n\ninitial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean)))\nlearned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean)))\nprint(\n    f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"\n    f\" {learned_rmse: .2f}\"\n)\n</pre> initial_dist = likelihood(posterior(x, D)) predictive_dist = opt_posterior.likelihood(opt_posterior(x, D))  initial_mean = initial_dist.mean() learned_mean = predictive_dist.mean()  rmse = lambda ytrue, ypred: jnp.sum(jnp.sqrt(jnp.square(ytrue - ypred)))  initial_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - initial_mean))) learned_rmse = jnp.sum(jnp.sqrt(jnp.square(y.squeeze() - learned_mean))) print(     f\"RMSE with initial parameters: {initial_rmse: .2f}\\nRMSE with learned parameters:\"     f\" {learned_rmse: .2f}\" ) <pre>RMSE with initial parameters:  6.80\nRMSE with learned parameters:  0.35\n</pre> <p>We can also plot the source of error in our model's predictions on the graph by the following.</p> In\u00a0[10]: Copied! <pre>error = jnp.abs(learned_mean - y.squeeze())\n\nnx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)\n\nvmin, vmax = error.min(), error.max()\nsm = plt.cm.ScalarMappable(\n    cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax)\n)\nsm.set_array([])\ncbar = plt.colorbar(sm)\n</pre> error = jnp.abs(learned_mean - y.squeeze())  nx.draw(G, pos, node_color=error, with_labels=False, alpha=0.5)  vmin, vmax = error.min(), error.max() sm = plt.cm.ScalarMappable(     cmap=plt.cm.inferno, norm=plt.Normalize(vmin=vmin, vmax=vmax) ) sm.set_array([]) cbar = plt.colorbar(sm) <pre>/tmp/ipykernel_11762/2868571321.py:10: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n  cbar = plt.colorbar(sm)\n/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/events.py:89: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <p>Reassuringly, our model seems to provide equally good predictions in each cluster.</p> In\u00a0[11]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\njax       : 0.4.9\ngpjax     : 0.0.0\noptax     : 0.1.5\nnetworkx  : 3.1\nmatplotlib: 3.7.1\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/graph_kernels/#graph-kernels","title":"Graph Kernels\u00b6","text":"<p>This notebook demonstrates how regression models can be constructed on the vertices of a graph using a Gaussian process with a Mat\u00e9rn kernel presented in . For a general discussion of the kernels supported within GPJax, see the kernels notebook.</p>"},{"location":"examples/graph_kernels/#graph-construction","title":"Graph construction\u00b6","text":"<p>Our graph $\\mathcal{G}=\\lbrace V, E \\rbrace$ comprises a set of vertices $V = \\lbrace v_1, v_2, \\ldots, v_n\\rbrace$ and edges $E=\\lbrace (v_i, v_j)\\in V \\ : \\ i \\neq j\\rbrace$. In particular, we will consider a barbell graph that is an undirected graph containing two clusters of vertices with a single shared edge between the two clusters.</p> <p>Contrary to the typical barbell graph, we'll randomly remove a subset of 30 edges within each of the two clusters. Given the 40 vertices within the graph, this results in 351 edges as shown below.</p>"},{"location":"examples/graph_kernels/#computing-the-graph-laplacian","title":"Computing the graph Laplacian\u00b6","text":"<p>Graph kernels use the Laplacian matrix $L$ to quantify the smoothness of a signal (or function) on a graph $$L=D-A,$$ where $D$ is the diagonal degree matrix containing each vertices' degree and $A$ is the adjacency matrix that has an $(i,j)^{\\text{th}}$ entry of 1 if $v_i, v_j$ are connected and 0 otherwise. Networkx gives us an easy way to compute this.</p>"},{"location":"examples/graph_kernels/#simulating-a-signal-on-the-graph","title":"Simulating a signal on the graph\u00b6","text":"<p>Our task is to construct a Gaussian process $f(\\cdot)$ that maps from the graph's vertex set $V$ onto the real line. To that end, we begin by simulating a signal on the graph's vertices that we will go on to try and predict. We use a single draw from a Gaussian process prior to draw our response values $\\boldsymbol{y}$ where we hardcode parameter values. The corresponding input value set for this model, denoted $\\boldsymbol{x}$, is the index set of the graph's vertices.</p>"},{"location":"examples/graph_kernels/#constructing-a-graph-gaussian-process","title":"Constructing a graph Gaussian process\u00b6","text":"<p>With our dataset created, we proceed to define our posterior Gaussian process and optimise the model's hyperparameters. Whilst our underlying space is the graph's vertex set and is therefore non-Euclidean, our likelihood is still Gaussian and the model is still conjugate. For this reason, we simply perform gradient descent on the GP's marginal log-likelihood term as in the regression notebook. We do this using the Adam optimiser provided in <code>optax</code>.</p>"},{"location":"examples/graph_kernels/#making-predictions","title":"Making predictions\u00b6","text":"<p>Having optimised our hyperparameters, we can now make predictions on the graph. Though we haven't defined a training and testing dataset here, we'll simply query the predictive posterior for the full graph to compare the root-mean-squared error (RMSE) of the model for the initialised parameters vs the optimised set.</p>"},{"location":"examples/graph_kernels/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/intro_to_gps/","title":"New to Gaussian Processes?","text":"In\u00a0[1]: Copied! <pre>import warnings\n\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow_probability.substrates.jax as tfp\nfrom docs.examples.utils import confidence_ellipse\n\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ntfd = tfp.distributions\n\nud1 = tfd.Normal(0.0, 1.0)\nud2 = tfd.Normal(-1.0, 0.5)\nud3 = tfd.Normal(0.25, 1.5)\n\nxs = jnp.linspace(-5.0, 5.0, 500)\n\nfig, ax = plt.subplots()\nfor d in [ud1, ud2, ud3]:\n    ax.plot(\n        xs,\n        jnp.exp(d.log_prob(xs)),\n        label=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",\n    )\n    ax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2)\nax.legend(loc=\"best\")\n</pre> import warnings  import jax.numpy as jnp import jax.random as jr import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import tensorflow_probability.substrates.jax as tfp from docs.examples.utils import confidence_ellipse  plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] tfd = tfp.distributions  ud1 = tfd.Normal(0.0, 1.0) ud2 = tfd.Normal(-1.0, 0.5) ud3 = tfd.Normal(0.25, 1.5)  xs = jnp.linspace(-5.0, 5.0, 500)  fig, ax = plt.subplots() for d in [ud1, ud2, ud3]:     ax.plot(         xs,         jnp.exp(d.log_prob(xs)),         label=f\"$\\\\mathcal{{N}}({{{float(d.mean())}}},\\\\  {{{float(d.stddev())}}}^2)$\",     )     ax.fill_between(xs, jnp.zeros_like(xs), jnp.exp(d.log_prob(xs)), alpha=0.2) ax.legend(loc=\"best\") <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> Out[1]: <pre>&lt;matplotlib.legend.Legend at 0x7f318c7fc040&gt;</pre> <p>A Gaussian random variable is uniquely defined in distribution by its mean $\\mu$ and variance $\\sigma^2$ and we therefore write $y\\sim\\mathcal{N}(\\mu, \\sigma^2)$ when describing a Gaussian random variable. We can compute these two quantities by $$ \\begin{align}     \\mathbb{E}[y] = \\mu\\,, \\quad \\quad \\mathbb{E}\\left[(y-\\mu)^2\\right] =\\sigma^2\\,. \\end{align} $$ Extending this concept to vector-valued random variables reveals the multivariate Gaussian random variables which brings us closer to the full definition of a GP.</p> <p>Let $\\mathbf{y}$ be a $D$-dimensional random variable, $\\boldsymbol{\\mu}$ be a $D$-dimensional mean vector and $\\boldsymbol{\\Sigma}$ be a $D\\times D$ covariance matrix. If $\\mathbf{y}$ is a Gaussian random variable, then the density of $\\mathbf{y}$ is $$ \\begin{align}     \\mathcal{N}(\\mathbf{y}\\,|\\, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2\\pi}^{D/2} \\lvert\\boldsymbol{\\Sigma}\\rvert^{1/2}} \\exp\\left(-\\frac{1}{2} \\left(\\mathbf{y} - \\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1} \\left(\\mathbf{y}-\\boldsymbol{\\mu}\\right) \\right) \\,. \\end{align} $$ Three example parameterisations of this can be visualised below where $\\rho$ determines the correlation of the multivariate Gaussian.</p> In\u00a0[2]: Copied! <pre>key = jr.PRNGKey(123)\n\nd1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2))\nd2 = tfd.MultivariateNormalTriL(\n    jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]]))\n)\nd3 = tfd.MultivariateNormalTriL(\n    jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]]))\n)\n\ndists = [d1, d2, d3]\n\nxvals = jnp.linspace(-5.0, 5.0, 500)\nyvals = jnp.linspace(-5.0, 5.0, 500)\n\nxx, yy = jnp.meshgrid(xvals, yvals)\n\npos = jnp.empty(xx.shape + (2,))\npos.at[:, :, 0].set(xx)\npos.at[:, :, 1].set(yy)\n\nfig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True)\ntitles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]\n\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)\n\nfor a, t, d in zip([ax0, ax1, ax2], titles, dists):\n    d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(\n        xx.shape\n    )\n    cntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)\n    for c in cntf.collections:\n        c.set_edgecolor(\"face\")\n    a.set_xlim(-2.75, 2.75)\n    a.set_ylim(-2.75, 2.75)\n    samples = d.sample(seed=key, sample_shape=(5000,))\n    xsample, ysample = samples[:, 0], samples[:, 1]\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8\n    )\n    confidence_ellipse(\n        xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"\n    )\n    a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)\n    a.set(xlabel=\"x\", ylabel=\"y\", title=t)\n</pre> key = jr.PRNGKey(123)  d1 = tfd.MultivariateNormalDiag(loc=jnp.zeros(2), scale_diag=jnp.ones(2)) d2 = tfd.MultivariateNormalTriL(     jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, 0.9], [0.9, 1.0]])) ) d3 = tfd.MultivariateNormalTriL(     jnp.zeros(2), jnp.linalg.cholesky(jnp.array([[1.0, -0.5], [-0.5, 1.0]])) )  dists = [d1, d2, d3]  xvals = jnp.linspace(-5.0, 5.0, 500) yvals = jnp.linspace(-5.0, 5.0, 500)  xx, yy = jnp.meshgrid(xvals, yvals)  pos = jnp.empty(xx.shape + (2,)) pos.at[:, :, 0].set(xx) pos.at[:, :, 1].set(yy)  fig, (ax0, ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=3, tight_layout=True) titles = [r\"$\\rho = 0$\", r\"$\\rho = 0.9$\", r\"$\\rho = -0.5$\"]  cmap = mpl.colors.LinearSegmentedColormap.from_list(\"custom\", [\"white\", cols[1]], N=256)  for a, t, d in zip([ax0, ax1, ax2], titles, dists):     d_prob = d.prob(jnp.hstack([xx.reshape(-1, 1), yy.reshape(-1, 1)])).reshape(         xx.shape     )     cntf = a.contourf(xx, yy, jnp.exp(d_prob), levels=20, antialiased=True, cmap=cmap)     for c in cntf.collections:         c.set_edgecolor(\"face\")     a.set_xlim(-2.75, 2.75)     a.set_ylim(-2.75, 2.75)     samples = d.sample(seed=key, sample_shape=(5000,))     xsample, ysample = samples[:, 0], samples[:, 1]     confidence_ellipse(         xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=1.0, linestyle=\"--\", alpha=0.8     )     confidence_ellipse(         xsample, ysample, a, edgecolor=\"#3f3f3f\", n_std=2.0, linestyle=\"--\"     )     a.plot(0, 0, \"x\", color=cols[0], markersize=8, mew=2)     a.set(xlabel=\"x\", ylabel=\"y\", title=t) <p>Extending the intuition given for the moments of a univariate Gaussian random variables, we can obtain the mean and covariance by $$ \\begin{align}    \\mathbb{E}[\\mathbf{y}] = \\mathbf{\\mu}, \\quad \\operatorname{Cov}(\\mathbf{y}) &amp; = \\mathbf{E}\\left[(\\mathbf{y} - \\mathbf{\\mu)}(\\mathbf{y} - \\mathbf{\\mu)}^{\\top} \\right]\\\\       &amp; =\\mathbb{E}[\\mathbf{y}\\mathbf{y}^{\\top}] - \\mathbb{E}[\\mathbf{y}]\\mathbb{E}[\\mathbf{y}]^{\\top} \\\\       &amp; =\\mathbf{\\Sigma}\\,. \\end{align} $$ The covariance matrix is a symmetric positive definite matrix that generalises the notion of variance to multiple dimensions. The matrix's diagonal entries contain the variance of each element, whilst the off-diagonal entries quantify the degree to which the respective pair of random variables are linearly related; this quantity is called the covariance.</p> <p>Assuming a Gaussian likelihood function in a Bayesian model is attractive as the mean and variance parameters are highly interpretable. This makes prior elicitation straightforward as the parameters' value can be intuitively contextualised within the scope of the problem at hand. Further, in models where the posterior distribution is Gaussian, we again use the distribution's mean and variance to describe our prediction and corresponding uncertainty around a given event occurring.</p> <p>Not only are Gaussian random variables highly interpretable, but linear operations involving them lead to analytical solutions. An example of this that will be useful in the sequel is the marginalisation and conditioning property of sets of Gaussian random variables. We will present these two results now for a pair of Gaussian random variables, but it should be stressed that these results hold for any finite set of Gaussian random variables.</p> <p>For a pair of random variables $\\mathbf{x}$ and $\\mathbf{y}$ defined on the same support, the distribution over them both is known as the joint distribution. The joint distribution $p(\\mathbf{x}, \\mathbf{y})$ quantifies the probability of two events, one from $p(\\mathbf{x})$ and another from $p(\\mathbf{y})$, occurring at the same time. We visualise this idea below.</p> In\u00a0[3]: Copied! <pre>n = 1000\nx = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,))\nkey, subkey = jr.split(key)\ny = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,))\nkey, subkey = jr.split(subkey)\nxfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\nyfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,))\nkey, subkey = jr.split(subkey)\ndf = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    g = sns.jointplot(\n        data=df,\n        x=\"x\",\n        y=\"y\",\n        hue=\"idx\",\n        marker=\".\",\n        space=0.0,\n        xlim=(-4.0, 4.0),\n        ylim=(-4.0, 4.0),\n        height=4,\n        marginal_ticks=False,\n        legend=False,\n        palette=\"inferno\",\n        marginal_kws={\n            \"fill\": True,\n            \"linewidth\": 1,\n            \"color\": cols[1],\n            \"alpha\": 0.3,\n            \"bw_adjust\": 2,\n            \"cmap\": cmap,\n        },\n        joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},\n    )\n    g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))\n    g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))\n    g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=1.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=2.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n    confidence_ellipse(\n        xfull,\n        yfull,\n        g.ax_joint,\n        edgecolor=\"#3f3f3f\",\n        n_std=3.0,\n        linestyle=\"--\",\n        linewidth=0.5,\n    )\n</pre> n = 1000 x = tfd.Normal(loc=0.0, scale=1.0).sample(seed=key, sample_shape=(n,)) key, subkey = jr.split(key) y = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n,)) key, subkey = jr.split(subkey) xfull = tfd.Normal(loc=0.0, scale=1.0).sample(seed=subkey, sample_shape=(n * 10,)) key, subkey = jr.split(subkey) yfull = tfd.Normal(loc=0.25, scale=0.5).sample(seed=subkey, sample_shape=(n * 10,)) key, subkey = jr.split(subkey) df = pd.DataFrame({\"x\": x, \"y\": y, \"idx\": jnp.ones(n)})  with warnings.catch_warnings():     warnings.simplefilter(\"ignore\")     g = sns.jointplot(         data=df,         x=\"x\",         y=\"y\",         hue=\"idx\",         marker=\".\",         space=0.0,         xlim=(-4.0, 4.0),         ylim=(-4.0, 4.0),         height=4,         marginal_ticks=False,         legend=False,         palette=\"inferno\",         marginal_kws={             \"fill\": True,             \"linewidth\": 1,             \"color\": cols[1],             \"alpha\": 0.3,             \"bw_adjust\": 2,             \"cmap\": cmap,         },         joint_kws={\"color\": cols[1], \"size\": 3.5, \"alpha\": 0.4, \"cmap\": cmap},     )     g.ax_joint.annotate(text=r\"$p(\\mathbf{x}, \\mathbf{y})$\", xy=(-3, -1.75))     g.ax_marg_x.annotate(text=r\"$p(\\mathbf{x})$\", xy=(-2.0, 0.225))     g.ax_marg_y.annotate(text=r\"$p(\\mathbf{y})$\", xy=(0.4, -0.78))     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=1.0,         linestyle=\"--\",         linewidth=0.5,     )     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=2.0,         linestyle=\"--\",         linewidth=0.5,     )     confidence_ellipse(         xfull,         yfull,         g.ax_joint,         edgecolor=\"#3f3f3f\",         n_std=3.0,         linestyle=\"--\",         linewidth=0.5,     ) <pre>/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/events.py:89: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  func(*args, **kwargs)\n/usr/share/miniconda/envs/test/lib/python3.8/site-packages/IPython/core/pylabtools.py:152: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> <p>Formally, we can define this by letting $p(\\mathbf{x}, \\mathbf{y})$ be the joint probability distribution defined over $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}}, \\boldsymbol{\\Sigma}_{\\mathbf{xx}})$ and $\\mathbf{y}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}})$. We define the joint distribution as $$ \\begin{align}     p\\left(\\begin{bmatrix}         \\mathbf{x} \\\\ \\mathbf{y}     \\end{bmatrix}\\right) = \\mathcal{N}\\left(\\begin{bmatrix}         \\boldsymbol{\\mu}_{\\mathbf{x}} \\\\ \\boldsymbol{\\mu}_{\\mathbf{y}}     \\end{bmatrix}, \\begin{bmatrix}         \\boldsymbol{\\Sigma}_{\\mathbf{yx}}, \\boldsymbol{\\Sigma}_{\\mathbf{yy}}     \\end{bmatrix} \\right)\\,, \\end{align} $$ where $\\boldsymbol{\\Sigma}_{\\mathbf{x}\\mathbf{y}}$ is the cross-covariance matrix of $\\mathbf{x}$ and $\\mathbf{y}$.</p> <p>When presented with a joint distribution, two tasks that we may wish to perform are marginalisation and conditioning. For a joint distribution $p(\\mathbf{x}, \\mathbf{y})$ where we are interested only in $p(\\mathbf{x})$, we must integrate over all possible values of $\\mathbf{y}$ to obtain $p(\\mathbf{x})$. This process is marginalisation. Conditioning allows us to evaluate the probability of one random variable, given that the other random variable is fixed. For a joint Gaussian distribution, marginalisation and conditioning have analytical expressions where the resulting distribution is also a Gaussian random variable.</p> <p>For a joint Gaussian random variable, the marginalisation of $\\mathbf{x}$ or $\\mathbf{y}$ is given by $$ \\begin{alignat}{3}     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{y} &amp;&amp; = p(\\mathbf{x})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{x}},     &amp; \\int p(\\mathbf{x}, \\mathbf{y})\\mathrm{d}\\mathbf{x} &amp;&amp; = p(\\mathbf{y})     &amp;&amp; = \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathbf{y}},     \\boldsymbol{\\Sigma}_{\\mathbf{yy}})\\,. \\end{alignat} $$ The conditional distributions are given by $$ \\begin{align}     p(\\mathbf{y}\\,|\\, \\mathbf{x}) &amp; = \\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\mathbf{y}} + \\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{\\mathbf{x}}), \\boldsymbol{\\Sigma}_{\\mathbf{yy}}-\\boldsymbol{\\Sigma}_{\\mathbf{yx}}\\boldsymbol{\\Sigma}_{\\mathbf{xx}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{xy}}\\right)\\,. \\end{align} $$</p> <p>Within this section, we have introduced the idea of multivariate Gaussian random variables and presented some key results concerning their properties. In the following section, we will lift our presentation of Gaussian random variables to GPs.</p> In\u00a0[4]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\njax                   : 0.4.9\ntensorflow_probability: 0.19.0\nseaborn               : 0.12.2\nmatplotlib            : 3.7.1\npandas                : 1.5.3\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/intro_to_gps/#new-to-gaussian-processes","title":"New to Gaussian Processes?\u00b6","text":"<p>Fantastic that you're here! This notebook is designed to be a gentle introduction to the mathematics of Gaussian processes (GPs). No prior knowledge of Bayesian inference or GPs is assumed, and this notebook is self-contained. At a high level, we begin by introducing Bayes' theorem and its implications within probabilistic modelling. We then proceed to introduce the Gaussian random variable along with its multivariate form. We conclude by showing how this notion can be extended to GPs.</p>"},{"location":"examples/intro_to_gps/#bayes-theorem","title":"Bayes' Theorem\u00b6","text":"<p>A probabilistic modelling task is comprised of an observed dataset $\\mathbf{y}$ for which we construct a model. The parameters $\\theta$ of our model are unknown, and our goal is to conduct inference to determine their range of likely values. To achieve this, we apply Bayes' theorem $$ \\begin{align}     \\label{eq:BayesTheorem}     p(\\theta\\,|\\, \\mathbf{y}) = \\frac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{p(\\mathbf{y})} = \\frac{p(\\theta)p(\\mathbf{y}\\,|\\,\\theta)}{\\int_{\\theta}p(\\mathbf{y}, \\theta)\\mathrm{d}\\theta}\\,, \\end{align} $$ where $p(\\mathbf{y}\\,|\\,\\theta)$ denotes the likelihood, or model, and quantifies how likely the observed dataset $\\mathbf{y}$ is, given the parameter estimate $\\theta$. The prior distribution $p(\\theta)$ reflects our initial beliefs about the value of $\\theta$ before observing data, whilst the posterior $p(\\theta\\,|\\, \\mathbf{y})$ gives an updated estimate of the parameters' value, after observing $\\mathbf{y}$. The marginal likelihood, or Bayesian model evidence, $p(\\mathbf{y})$ is the probability of the observed data under all possible hypotheses that our prior model can generate. Within Bayesian model selection, this property makes the marginal log-likelihood an indispensable tool. Selecting models under this criterion places a higher emphasis on models that can generalise better to new data points.</p> <p>When the posterior distribution belongs to the same family of probability distributions as the prior, we describe the prior and the likelihood as conjugate to each other. Such a scenario is convenient in Bayesian inference as it allows us to derive closed-form expressions for the posterior distribution. When the likelihood function is a member of the exponential family, then there exists a conjugate prior. However, the conjugate prior may not have a form that precisely reflects the practitioner's belief surrounding the parameter. For this reason, conjugate models seldom appear; one exception to this is GP regression that we present fully in our Regression notebook.</p> <p>For models that do not contain a conjugate prior, the marginal log-likelihood must be calculated to normalise the posterior distribution and ensure it integrates to 1. For models with a single, 1-dimensional parameter, it may be possible to compute this integral analytically or through a quadrature scheme, such as Gauss-Hermite. However, in machine learning, the dimensionality of $\\theta$ is often large and the corresponding integral required to compute $p(\\mathbf{y})$ quickly becomes intractable as the dimension grows. Techniques such as Markov Chain Monte Carlo and variational inference allow us to approximate integrals such as the one seen in $p(\\mathbf{y})$.</p> <p>Once a posterior distribution has been obtained, we can make predictions at new points $\\mathbf{y}^{\\star}$ through the posterior predictive distribution. This is achieved by integrating out the parameter set $\\theta$ from our posterior distribution through $$ \\begin{align}     p(\\mathbf{y}^{\\star}\\mid \\mathbf{y}) = \\int p(\\mathbf{y}^{\\star} \\,|\\, \\theta, \\mathbf{y} ) p(\\theta\\,|\\, \\mathbf{y})\\mathrm{d}\\theta\\,. \\end{align} $$ As with the marginal log-likelihood, evaluating this quantity requires computing an integral which may not be tractable, particularly when $\\theta$ is high-dimensional.</p> <p>It is difficult to communicate statistics directly through a posterior distribution, so we often compute and report moments of the posterior distribution. Most commonly, we report the first moment and the centred second moment $$ \\begin{alignat}{2}     \\mu  = \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]  &amp; = \\int \\theta p(\\theta\\mid\\mathbf{y})\\mathrm{d}\\theta\\\\     \\sigma^2  = \\mathbb{V}[\\theta\\,|\\,\\mathbf{y}] &amp; = \\int \\left(\\theta -     \\mathbb{E}[\\theta\\,|\\,\\mathbf{y}]\\right)^2p(\\theta\\,|\\,\\mathbf{y})\\mathrm{d}\\theta&amp;\\,. \\end{alignat} $$ Through this pair of statistics, we can communicate our beliefs about the most likely value of $\\theta$ i.e., $\\mu$, and the uncertainty $\\sigma$ around the expected value. However, as with the marginal log-likelihood and predictive posterior distribution, computing these statistics again requires a potentially intractable integral.</p>"},{"location":"examples/intro_to_gps/#gaussian-random-variables","title":"Gaussian random variables\u00b6","text":"<p>We begin our review with the simplest case; a univariate Gaussian random variable. For a random variable $y$, let $\\mu\\in\\mathbb{R}$ be a mean scalar and $\\sigma^2\\in\\mathbb{R}_{&gt;0}$ a variance scalar. If $y$ is a Gaussian random variable, then the density of $y$ is $$ \\begin{align}     \\mathcal{N}(y\\,|\\, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^{2}}\\right)\\,. \\end{align} $$ We can plot three different parameterisations of this density.</p>"},{"location":"examples/intro_to_gps/#gaussian-processes","title":"Gaussian processes\u00b6","text":"<p>When transitioning from Gaussian random variables to GP there is a shift in thought required to parse the forthcoming material. Firstly, to be consistent with the general literature, we hereon use $\\mathbf{X}$ to denote an observed vector of data points, not a random variable as has been true up until now. To distinguish between matrices and vectors, we use bold upper case characters e.g., $\\mathbf{X}$ for matrices, and bold lower case characters for vectors e.g., $\\mathbf{x}$.</p> <p>We are interested in modelling supervised learning problems, where we have $n$ observations $\\mathbf{y}=\\{y_1, y_2,\\ldots ,y_n\\}\\subset\\mathcal{Y}$ at corresponding inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n\\}\\subset\\mathcal{X}$. We aim to capture the relationship between $\\mathbf{X}$ and $\\mathbf{y}$ using a model $f$ with which we may make predictions at an unseen set of test points $\\mathbf{X}^{\\star}\\subset\\mathcal{X}$. We formalise this by $$ \\begin{align}     y = f(\\mathbf{X}) + \\varepsilon\\,, \\end{align} $$ where $\\varepsilon$ is an observational noise term. We collectively refer to $(\\mathbf{X}, \\mathbf{y})$ as the training data and $\\mathbf{X}^{\\star}$ as the set of test points. This process is visualised below</p> <p></p> <p>As we shall go on to see, GPs offer an appealing workflow for scenarios such as this, all under a Bayesian framework.</p> <p>We write a GP $f(\\cdot) \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot))$ with mean function $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ and $\\boldsymbol{\\theta}$-parameterised kernel $k: \\mathcal{X} \\times \\mathcal{X}\\rightarrow \\mathbb{R}$. When evaluating the GP on a finite set of points $\\mathbf{X}\\subset\\mathcal{X}$, $k$ gives rise to the Gram matrix $\\mathbf{K}_{ff}$ such that the $(i, j)^{\\text{th}}$ entry of the matrix is given by $[\\mathbf{K}_{ff}]_{i, j} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. As is conventional within the literature, we centre our training data and assume $\\mu(\\mathbf{X}):= 0$ for all $\\mathbf{X}\\in\\mathbf{X}$. We further drop dependency on $\\boldsymbol{\\theta}$ and $\\mathbf{X}$ for notational convenience in the remainder of this article.</p> <p>We define a joint GP prior over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}) = \\mathcal{N}\\left(\\mathbf{0}, \\begin{bmatrix}         \\mathbf{K}_{xf} &amp; \\mathbf{K}_{xx}     \\end{bmatrix}\\right)\\,, \\end{align} $$ where $\\mathbf{f}^{\\star} = f(\\mathbf{X}^{\\star})$. Conditional on the GP's latent function $f$, we assume a factorising likelihood generates our observations $$ \\begin{align}     p(\\mathbf{y}\\,|\\,\\mathbf{f}) = \\prod_{i=1}^n p(y_i\\,|\\, f_i)\\,. \\end{align} $$ Strictly speaking, the likelihood function is $p(\\mathbf{y}\\,|\\,\\phi(\\mathbf{f}))$ where $\\phi$ is the likelihood function's associated link function. Example link functions include the probit or logistic functions for a Bernoulli likelihood and the identity function for a Gaussian likelihood. We eschew this notation for now as this section primarily considers Gaussian likelihood functions where the role of $\\phi$ is superfluous. However, this intuition will be helpful for models with a non-Gaussian likelihood, such as those encountered in classification.</p> <p>Applying Bayes' theorem \\eqref{eq:BayesTheorem} yields the joint posterior distribution over the latent function $$ \\begin{align}     p(\\mathbf{f}, \\mathbf{f}^{\\star}\\,|\\,\\mathbf{y}) = \\frac{p(\\mathbf{y}\\,|\\,\\mathbf{f})p(\\mathbf{f},\\mathbf{f}^{\\star})}{p(\\mathbf{y})}\\,. \\end{align} $$</p> <p>The choice of kernel function that we use to parameterise our GP is an important modelling decision as the choice of kernel dictates properties such as differentiability, variance and characteristic lengthscale of the functions that are admissible under the GP prior. A kernel is a positive-definite function with parameters $\\boldsymbol{\\theta}$ that maps pairs of inputs $\\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}$ onto the real line. We dedicate the entirety of the Introduction to Kernels notebook to exploring the different GPs each kernel can yield.</p>"},{"location":"examples/intro_to_gps/#gaussian-process-regression","title":"Gaussian process regression\u00b6","text":"<p>When the likelihood function is a Gaussian distribution $p(y_i\\,|\\, f_i) = \\mathcal{N}(y_i\\,|\\, f_i, \\sigma_n^2)$, marginalising $\\mathbf{f}$ from the joint posterior to obtain the posterior predictive distribution is exact $$ \\begin{align}     p(\\mathbf{f}^{\\star}\\mid \\mathbf{y}) = \\mathcal{N}(\\mathbf{f}^{\\star}\\,|\\,\\boldsymbol{\\mu}_{\\,|\\,\\mathbf{y}}, \\Sigma_{\\,|\\,\\mathbf{y}})\\,, \\end{align} $$ where $$ \\begin{align}     \\mathbf{\\mu}_{\\mid \\mathbf{y}} &amp; = \\mathbf{K}_{\\star f}\\left( \\mathbf{K}_{ff}+\\sigma^2_n\\mathbf{I}_n\\right)^{-1}\\mathbf{y} \\\\     \\Sigma_{\\,|\\,\\mathbf{y}} &amp; = \\mathbf{K}_{\\star\\star} - \\mathbf{K}_{xf}\\left(\\mathbf{K}_{ff} + \\sigma_n^2\\mathbf{I}_n\\right)^{-1}\\mathbf{K}_{fx} \\,. \\end{align} $$ Further, the log of the  marginal likelihood of the GP can be analytically expressed as $$ \\begin{align}         &amp; = 0.5\\left(-\\underbrace{\\mathbf{y}^{\\top}\\left(\\mathbf{K}_{ff} - \\sigma_n^2\\mathbf{I}_n \\right)^{-1}\\mathbf{y}}_{\\text{Data fit}} -\\underbrace{\\log\\lvert \\mathbf{K}_{ff} + \\sigma^2_n\\rvert}_{\\text{Complexity}} -\\underbrace{n\\log 2\\pi}_{\\text{Constant}} \\right)\\,. \\end{align} $$</p> <p>Model selection can be performed for a GP through gradient-based optimisation of $\\log p(\\mathbf{y})$ with respect to the kernel's parameters $\\boldsymbol{\\theta}$ and the observational noise $\\sigma^2_n$. Collectively, we call these terms the model hyperparameters $\\boldsymbol{\\xi} = \\{\\boldsymbol{\\theta},\\sigma_n^2\\}$ from which the maximum likelihood estimate is given by $$ \\begin{align*}     \\boldsymbol{\\xi}^{\\star} = \\operatorname{argmax}_{\\boldsymbol{\\xi} \\in \\Xi} \\log p(\\mathbf{y})\\,. \\end{align*} $$</p> <p>Observing the individual terms in the marginal log-likelihood can help understand exactly why optimising the marginal log-likelihood gives reasonable solutions. The data fit term is the only component of the marginal log-likelihood that includes the observed response $\\mathbf{y}$ and will therefore encourage solutions that model the data well. Conversely, the complexity term contains a determinant operator and therefore measures the volume of the function space covered by the GP. Whilst a more complex function has a better chance of modelling the observed data well, this is only true to a point and functions that are overly complex will overfit the data. Optimising with respect to the marginal log-likelihood balances these two objectives when identifying the optimal solution, as visualised below.</p> <p></p>"},{"location":"examples/intro_to_gps/#conclusions","title":"Conclusions\u00b6","text":"<p>Within this notebook we have built up the concept of a GP, starting from Bayes' theorem and the definition of a Gaussian random variable. Using the ideas presented in this notebook, the user should be in a position to dive into our Regression notebook and start getting their hands on some code. For those looking to learn more about the underling theory of GPs, an excellent starting point is the Gaussian Processes for Machine Learning textbook. Alternatively, the thesis of Alexander Terenin provides a rigorous exposition of GPs that served as the inspiration for this notebook.</p>"},{"location":"examples/intro_to_gps/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/intro_to_kernels/","title":"Introduction to Kernels","text":"<p>In this guide we provide an introduction to kernels, and the role they play in Gaussian process models.</p> In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook, Float\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nfrom docs.examples.utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\nfrom gpjax.typing import Array\nfrom sklearn.preprocessing import StandardScaler\n\nkey = jr.PRNGKey(42)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook, Float import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox import pandas as pd from docs.examples.utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx from gpjax.typing import Array from sklearn.preprocessing import StandardScaler  key = jr.PRNGKey(42) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <p>Using Gaussian Processes (GPs) to model functions can offer several advantages over alternative methods, such as deep neural networks. One key advantage is their rich quantification of uncertainty; not only do they provide point estimates for the values taken by a function throughout its domain, but they provide a full predictive posterior distribution over the range of values the function may take. This rich quantification of uncertainty is useful in many applications, such as Bayesian optimisation, which relies on being able to make uncertainty-aware decisions.</p> <p>However, another advantage of GPs is the ability for one to place priors on the functions being modelled. For instance, one may know that the underlying function being modelled observes certain characteristics, such as being periodic or having a certain level of smoothness. The kernel, or covariance function, is the primary means through which one is able to encode such prior knowledge about the function being modelled. This enables one to equip the GP with inductive biases which enable it to learn from data more efficiently, whilst generalising to unseen data more effectively.</p> <p>In this notebook we'll develop some intuition for what kinds of priors are encoded through the use of different kernels, and how this can be useful when modelling different types of functions.</p> <p>One of the most widely used families of kernels is the Mat\u00e9rn family (Mat\u00e9rn, 1960). These kernels take on the following form:</p> $$k_{\\nu}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2\\nu} \\frac{|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right)^{\\nu} K_{\\nu} \\left(\\sqrt{2\\nu} \\frac{|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right)$$<p>where $K_{\\nu}$ is a modified Bessel function, $\\nu$, $\\kappa$ and $\\sigma^2$ are hyperparameters specifying the mean-square differentiability, lengthscale and variability respectively, and $|\\cdot|$ is used to denote the Euclidean norm. Note that for those of you less interested in the mathematical underpinnings of kernels, it isn't necessary to understand the exact functional form of the Mat\u00e9rn kernels to gain an understanding of how they behave. The key takeaway is that they are parameterised by several hyperparameters, and that these hyperparameters dictate the behaviour of functions sampled from the corresponding GP. The plots below will provide some more intuition for how these hyperparameters affect the behaviour of functions sampled from the corresponding GP.</p> <p>Some commonly used Mat\u00e9rn kernels use half-integer values of $\\nu$, such as $\\nu = 1/2$ or $\\nu = 3/2$. The fraction is sometimes omitted when naming the kernel, so that $\\nu = 1/2$ is referred to as the Mat\u00e9rn12 kernel, and $\\nu = 3/2$ is referred to as the Mat\u00e9rn32 kernel. When $\\nu$ takes in a half-integer value, $\\nu = k + 1/2$, the kernel can be expressed as the product of a polynomial of order $k$ and an exponential:</p> $$k_{k + 1/2}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\exp\\left(-\\frac{\\sqrt{2\\nu}|\\mathbf{x} - \\mathbf{x'}|}{\\kappa}\\right) \\frac{\\Gamma(k+1)}{\\Gamma(2k+1)} \\times \\sum_{i= 0}^k \\frac{(k+i)!}{i!(k-i)!} \\left(\\frac{(\\sqrt{8\\nu}|\\mathbf{x} - \\mathbf{x'}|)}{\\kappa}\\right)^{k-i}$$<p>In the limit of $\\nu \\to \\infty$ this yields the squared-exponential, or radial basis function (RBF), kernel, which is infinitely mean-square differentiable:</p> $$k_{\\infty}(\\mathbf{x}, \\mathbf{x'}) = \\sigma^2 \\exp\\left(-\\frac{|\\mathbf{x} - \\mathbf{x'}|^2}{2\\kappa^2}\\right)$$<p>But what kind of functions does this kernel encode prior knowledge about? Let's take a look at some samples from GP priors defined used Mat\u00e9rn kernels with different values of $\\nu$:</p> In\u00a0[2]: Copied! <pre>kernels = [\n    gpx.kernels.Matern12(),\n    gpx.kernels.Matern32(),\n    gpx.kernels.Matern52(),\n    gpx.kernels.RBF(),\n]\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n\nmeanf = gpx.mean_functions.Zero()\n\nfor k, ax in zip(kernels, axes.ravel()):\n    prior = gpx.Prior(mean_function=meanf, kernel=k)\n    rv = prior(x)\n    y = rv.sample(seed=key, sample_shape=(10,))\n    ax.plot(x, y.T, alpha=0.7)\n    ax.set_title(k.name)\n</pre> kernels = [     gpx.kernels.Matern12(),     gpx.kernels.Matern32(),     gpx.kernels.Matern52(),     gpx.kernels.RBF(), ] fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(7, 6), tight_layout=True)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)  meanf = gpx.mean_functions.Zero()  for k, ax in zip(kernels, axes.ravel()):     prior = gpx.Prior(mean_function=meanf, kernel=k)     rv = prior(x)     y = rv.sample(seed=key, sample_shape=(10,))     ax.plot(x, y.T, alpha=0.7)     ax.set_title(k.name) <p>The plots above clearly show that the choice of $\\nu$ has a large impact on the smoothness of the functions being modelled by the GP, with functions drawn from GPs defined with the Mat\u00e9rn kernel becoming increasingly smooth as $\\nu \\to \\infty$. More formally, this notion of smoothness is captured through the mean-square differentiability of the function being modelled. Functions sampled from GPs using a Mat\u00e9rn kernel are $k$-times mean-square differentiable, if and only if $\\nu &gt; k$. For instance, functions sampled from a GP using a Mat\u00e9rn12 kernel are zero times mean-square differentiable, and functions sampled from a GP using the RBF kernel are infinitely mean-square differentiable.</p> <p>As an important aside, a general property of the Mat\u00e9rn family of kernels is that they are examples of stationary kernels. This means that they only depend on the displacement of the two points being compared, $\\mathbf{x} - \\mathbf{x}'$, and not on their absolute values. This is a useful property to have, as it means that the kernel is invariant to translations in the input space. They also go beyond this, as they only depend on the Euclidean distance between the two points being compared, $|\\mathbf{x} - \\mathbf{x}'|$. Kernels which satisfy this property are known as isotropic kernels. This makes the function invariant to all rigid motions in the input space, such as rotations.</p> <p>Most kernels have several hyperparameters, which we denote $\\mathbf{\\theta}$, which encode different assumptions about the underlying function being modelled. For the Mat\u00e9rn family described above, $\\mathbf{\\theta} = \\{\\nu, \\kappa, \\sigma\\}$. A fully Bayesian approach to dealing with hyperparameters would be to place a prior over them, and marginalise over the posterior derived from the data in order to perform predictions. However, this is often computationally very expensive, and so a common approach is to instead optimise the hyperparameters by maximising the log marginal likelihood of the data. Given training data $\\mathbf{D} = (\\mathbf{X}, \\mathbf{y})$, assumed to contain some additive Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, the log marginal likelihood of the dataset is defined as:</p> $$ \\begin{aligned} \\log(p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\theta})) &amp;= \\log\\left(\\int p(\\mathbf{y} | \\mathbf{f}, \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{f} | \\mathbf{X}, \\boldsymbol{\\theta}) d\\mathbf{f}\\right) \\nonumber \\\\ &amp;= - \\frac{1}{2} \\mathbf{y} ^ \\top \\left(K(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I} \\right)^{-1} \\mathbf{y} - \\frac{1}{2} \\log |K(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}| - \\frac{n}{2} \\log 2 \\pi \\end{aligned}$$  <p>This expression can then be maximised with respect to the hyperparameters using a gradient-based approach such as Adam or L-BFGS. Note that we may choose to fix some hyperparameters, and in GPJax the parameter $\\nu$ is set by the user, and not inferred though optimisation. For more details on using the log marginal likelihood to optimise kernel hyperparameters, see our GP introduction notebook.</p> <p>We'll demonstrate the advantages of being able to infer kernel parameters from the training data by fitting a GP to the widely used Forrester function:</p> $$f(x) = (6x - 2)^2 \\sin(12x - 4)$$  In\u00a0[3]: Copied! <pre># Forrester function\ndef forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:\n    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)\n\n\nn = 5\n\ntraining_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1)\ntraining_y = forrester(training_x)\nD = gpx.Dataset(X=training_x, y=training_y)\n\ntest_x = jnp.linspace(0, 1, 100).reshape(-1, 1)\ntest_y = forrester(test_x)\n</pre> # Forrester function def forrester(x: Float[Array, \"N\"]) -&gt; Float[Array, \"N\"]:     return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)   n = 5  training_x = jr.uniform(key=key, minval=0, maxval=1, shape=(n,)).reshape(-1, 1) training_y = forrester(training_x) D = gpx.Dataset(X=training_x, y=training_y)  test_x = jnp.linspace(0, 1, 100).reshape(-1, 1) test_y = forrester(test_x) <p>First we define our model, using the Mat\u00e9rn52 kernel, and construct our posterior without optimising the kernel hyperparameters:</p> In\u00a0[4]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Matern52(\n    lengthscale=jnp.array(2.0)\n)  # Initialise our kernel lengthscale to 2.0\n\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\n\nlikelihood = gpx.Gaussian(\n    num_datapoints=D.n, obs_noise=jnp.array(1e-6)\n)  # Our function is noise-free, so we set the observation noise to a very small value\nlikelihood = likelihood.replace_trainable(obs_noise=False)\n\nno_opt_posterior = prior * likelihood\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Matern52(     lengthscale=jnp.array(2.0) )  # Initialise our kernel lengthscale to 2.0  prior = gpx.Prior(mean_function=mean, kernel=kernel)  likelihood = gpx.Gaussian(     num_datapoints=D.n, obs_noise=jnp.array(1e-6) )  # Our function is noise-free, so we set the observation noise to a very small value likelihood = likelihood.replace_trainable(obs_noise=False)  no_opt_posterior = prior * likelihood <p>We can then optimise the hyperparameters by minimising the negative log marginal likelihood of the data:</p> In\u00a0[5]: Copied! <pre>negative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(no_opt_posterior, train_data=D)\nnegative_mll = jit(negative_mll)\n\nopt_posterior, history = gpx.fit(\n    model=no_opt_posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=2000,\n    safe=True,\n    key=key,\n)\n</pre> negative_mll = gpx.objectives.ConjugateMLL(negative=True) negative_mll(no_opt_posterior, train_data=D) negative_mll = jit(negative_mll)  opt_posterior, history = gpx.fit(     model=no_opt_posterior,     objective=negative_mll,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=2000,     safe=True,     key=key, ) <pre>  0%|          | 0/2000 [00:00&lt;?, ?it/s]</pre> <p>Having optimised the hyperparameters, we can now make predictions using the posterior with the optimised hyperparameters, and compare them to the predictions made using the posterior with the default hyperparameters:</p> In\u00a0[6]: Copied! <pre>opt_latent_dist = opt_posterior.predict(test_x, train_data=D)\nopt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)\n\nopt_predictive_mean = opt_predictive_dist.mean()\nopt_predictive_std = opt_predictive_dist.stddev()\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6))\nax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax1.fill_between(\n    test_x.squeeze(),\n    opt_predictive_mean - 2 * opt_predictive_std,\n    opt_predictive_mean + 2 * opt_predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax1.plot(\n    test_x,\n    opt_predictive_mean - 2 * opt_predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax1.plot(\n    test_x,\n    opt_predictive_mean + 2 * opt_predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax1.plot(\n    test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax1.plot(test_x, opt_predictive_mean, label=\"Predictive mean\", color=cols[1])\nax1.set_title(\"Posterior with Hyperparameter Optimisation\")\nax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n\nno_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D)\nno_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)\n\nno_opt_predictive_mean = no_opt_predictive_dist.mean()\nno_opt_predictive_std = no_opt_predictive_dist.stddev()\n\nax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax2.fill_between(\n    test_x.squeeze(),\n    no_opt_predictive_mean - 2 * no_opt_predictive_std,\n    no_opt_predictive_mean + 2 * no_opt_predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax2.plot(\n    test_x,\n    no_opt_predictive_mean - 2 * no_opt_predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax2.plot(\n    test_x,\n    no_opt_predictive_mean + 2 * no_opt_predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax2.plot(\n    test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax2.plot(test_x, no_opt_predictive_mean, label=\"Predictive mean\", color=cols[1])\nax2.set_title(\"Posterior without Hyperparameter Optimisation\")\nax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> opt_latent_dist = opt_posterior.predict(test_x, train_data=D) opt_predictive_dist = opt_posterior.likelihood(opt_latent_dist)  opt_predictive_mean = opt_predictive_dist.mean() opt_predictive_std = opt_predictive_dist.stddev()  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 6)) ax1.plot(training_x, training_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5) ax1.fill_between(     test_x.squeeze(),     opt_predictive_mean - 2 * opt_predictive_std,     opt_predictive_mean + 2 * opt_predictive_std,     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax1.plot(     test_x,     opt_predictive_mean - 2 * opt_predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax1.plot(     test_x,     opt_predictive_mean + 2 * opt_predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax1.plot(     test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax1.plot(test_x, opt_predictive_mean, label=\"Predictive mean\", color=cols[1]) ax1.set_title(\"Posterior with Hyperparameter Optimisation\") ax1.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))  no_opt_latent_dist = no_opt_posterior.predict(test_x, train_data=D) no_opt_predictive_dist = no_opt_posterior.likelihood(no_opt_latent_dist)  no_opt_predictive_mean = no_opt_predictive_dist.mean() no_opt_predictive_std = no_opt_predictive_dist.stddev()  ax2.plot(training_x, training_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5) ax2.fill_between(     test_x.squeeze(),     no_opt_predictive_mean - 2 * no_opt_predictive_std,     no_opt_predictive_mean + 2 * no_opt_predictive_std,     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax2.plot(     test_x,     no_opt_predictive_mean - 2 * no_opt_predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax2.plot(     test_x,     no_opt_predictive_mean + 2 * no_opt_predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax2.plot(     test_x, test_y, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax2.plot(test_x, no_opt_predictive_mean, label=\"Predictive mean\", color=cols[1]) ax2.set_title(\"Posterior without Hyperparameter Optimisation\") ax2.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[6]: <pre>&lt;matplotlib.legend.Legend at 0x7f1f70f80370&gt;</pre> <p>We can see that optimising the hyperparameters by minimising the negative log marginal likelihood of the data results in a more faithful fit of the GP to the data. In particular, we can observe that the GP using optimised hyperparameters is more accurately able to reflect uncertainty in its predictions, as opposed to the GP using the default parameters, which is overconfident in its predictions.</p> <p>The lengthscale, $\\kappa$, and variance, $\\sigma^2$, are shown below, both before and after optimisation:</p> In\u00a0[7]: Copied! <pre>no_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale\nno_opt_variance = no_opt_posterior.prior.kernel.variance\nopt_lengthscale = opt_posterior.prior.kernel.lengthscale\nopt_variance = opt_posterior.prior.kernel.variance\n\nprint(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\")\nprint(\n    f\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\"\n)\n</pre> no_opt_lengthscale = no_opt_posterior.prior.kernel.lengthscale no_opt_variance = no_opt_posterior.prior.kernel.variance opt_lengthscale = opt_posterior.prior.kernel.lengthscale opt_variance = opt_posterior.prior.kernel.variance  print(f\"Optimised Lengthscale: {opt_lengthscale} and Variance: {opt_variance}\") print(     f\"Non-Optimised Lengthscale: {no_opt_lengthscale} and Variance: {no_opt_variance}\" ) <pre>Optimised Lengthscale: 0.4381594930767977 and Variance: 2.3646441184373423\nNon-Optimised Lengthscale: 2.0 and Variance: 1.0\n</pre> <p>Whilst the Mat\u00e9rn kernels are often used as a first choice of kernel, and they often perform well due to their smoothing properties often being well-aligned with the properties of the underlying function being modelled, sometimes more prior knowledge is known about the function being modelled. For instance, it may be known that the function being modelled is periodic. In this case, a suitable kernel choice would be the periodic kernel:</p> $$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\left(\\frac{\\sin (\\pi (\\mathbf{x}_i - \\mathbf{x}_i')/p)}{\\ell}\\right)^2 \\right)$$<p>with $D$ being the dimensionality of the inputs.</p> <p>Below we show $10$ samples drawn from a GP prior using the periodic kernel:</p> In\u00a0[8]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Periodic()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\n\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Periodic Kernel\")\nplt.show()\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Periodic() prior = gpx.Prior(mean_function=mean, kernel=kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,))  fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from the Periodic Kernel\") plt.show() <p>In other scenarios, it may be known that the underlying function is linear, in which case the linear kernel would be a suitable choice:</p> $$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\mathbf{x}^\\top \\mathbf{x}'$$<p>Unlike the kernels shown above, the linear kernel is not stationary, and so it is not invariant to translations in the input space.</p> <p>Below we show $10$ samples drawn from a GP prior using the linear kernel:</p> In\u00a0[9]: Copied! <pre>mean = gpx.mean_functions.Zero()\nkernel = gpx.kernels.Linear()\nprior = gpx.Prior(mean_function=mean, kernel=kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\n\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from the Linear Kernel\")\nplt.show()\n</pre> mean = gpx.mean_functions.Zero() kernel = gpx.kernels.Linear() prior = gpx.Prior(mean_function=mean, kernel=kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,))  fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from the Linear Kernel\") plt.show() <p>It is also mathematically valid to compose kernels through operations such as addition and multiplication in order to produce more expressive kernels. For the mathematically interested amongst you, this is valid as the resulting kernel functions still satisfy the necessary conditions introduced at the start of this notebook. Adding or multiplying kernel functions is equivalent to performing elementwise addition or multiplication of the corresponding covariance matrices, and fortunately symmetric, positive semi-definite kernels are closed under these operations. This means that kernels produced by adding or multiplying other kernels will also be symmetric and positive semi-definite, and so will also be valid kernels. GPJax provides the functionality required to easily compose kernels via addition and multiplication, which we'll demonstrate below.</p> <p>First, we'll take a look at some samples drawn from a GP prior using a kernel which is composed of the sum of a linear kernel and a periodic kernel:</p> In\u00a0[10]: Copied! <pre>kernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=mean, kernel=sum_kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\")\nplt.show()\n</pre> kernel_one = gpx.kernels.Linear() kernel_two = gpx.kernels.Periodic() sum_kernel = gpx.kernels.SumKernel(kernels=[kernel_one, kernel_two]) mean = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=mean, kernel=sum_kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,)) fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from a GP Prior with Kernel = Linear + Periodic\") plt.show() <p>We can see that the samples drawn behave as one would naturally expect through adding the two kernels together. In particular, the samples are still periodic, as with the periodic kernel, but their mean also linearly increases/decreases as they move away from the origin, as seen with the linear kernel.</p> <p>Below we take a look at some samples drawn from a GP prior using a kernel which is composed of the same two kernels, but this time multiplied together:</p> In\u00a0[11]: Copied! <pre>kernel_one = gpx.kernels.Linear()\nkernel_two = gpx.kernels.Periodic()\nsum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two])\nmean = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=mean, kernel=sum_kernel)\n\nx = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\nrv = prior(x)\ny = rv.sample(seed=key, sample_shape=(10,))\nfig, ax = plt.subplots()\nax.plot(x, y.T, alpha=0.7)\nax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\")\nplt.show()\n</pre> kernel_one = gpx.kernels.Linear() kernel_two = gpx.kernels.Periodic() sum_kernel = gpx.kernels.ProductKernel(kernels=[kernel_one, kernel_two]) mean = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=mean, kernel=sum_kernel)  x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1) rv = prior(x) y = rv.sample(seed=key, sample_shape=(10,)) fig, ax = plt.subplots() ax.plot(x, y.T, alpha=0.7) ax.set_title(\"Samples from a GP with Kernel = Linear x Periodic\") plt.show() <p>Once again, the samples drawn behave as one would naturally expect through multiplying the two kernels together. In particular, the samples are still periodic but their mean linearly increases/decreases as they move away from the origin, and the amplitude of the oscillations also linearly increases with increasing distance from the origin.</p> <p>We'll put together some of the ideas we've discussed in this notebook by fitting a GP to the Mauna Loa CO2 dataset. This dataset measures atmospheric CO2 concentration at the Mauna Loa Observatory in Hawaii, and is widely used in the GP literature. It contains monthly CO2 readings starting in March 1958. Interestingly, there was an eruption at the Mauna Loa volcano in November 2022, so readings from December 2022 have changed to a site roughly 21 miles North of the Mauna Loa Observatory. We'll use the data from March 1958 to November 2022, and see how our GP extrapolates to 8 years before and after the data in the training set.</p> <p>First we'll load the data and plot it:</p> In\u00a0[12]: Copied! <pre>co2_data = pd.read_csv(\n    \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\"\n)\nco2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12]\ntrain_x = co2_data[\"decimal date\"].values[:, None]\ntrain_y = co2_data[\"average\"].values[:, None]\n\nfig, ax = plt.subplots()\nax.plot(train_x, train_y)\nax.set_title(\"CO2 Concentration in the Atmosphere\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO2 Concentration (ppm)\")\nplt.show()\n</pre> co2_data = pd.read_csv(     \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", comment=\"#\" ) co2_data = co2_data.loc[co2_data[\"decimal date\"] &lt; 2022 + 11 / 12] train_x = co2_data[\"decimal date\"].values[:, None] train_y = co2_data[\"average\"].values[:, None]  fig, ax = plt.subplots() ax.plot(train_x, train_y) ax.set_title(\"CO2 Concentration in the Atmosphere\") ax.set_xlabel(\"Year\") ax.set_ylabel(\"CO2 Concentration (ppm)\") plt.show() <p>Looking at the data, we can see that there is clearly a periodic trend, with a period of roughly 1 year. We can also see that the data is increasing over time, which is also expected. This looks roughly linear, although it may have a non-linear component. This information will be useful when we come to choose our kernel.</p> <p>First, we'll construct our GPJax dataset, and will standardise the outputs, to match our assumption that the data has zero mean.</p> In\u00a0[13]: Copied! <pre>test_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1)\ny_scaler = StandardScaler().fit(train_y)\nstandardised_train_y = y_scaler.transform(train_y)\n\nD = gpx.Dataset(X=train_x, y=standardised_train_y)\n</pre> test_x = jnp.linspace(1950, 2030, 5000, dtype=jnp.float64).reshape(-1, 1) y_scaler = StandardScaler().fit(train_y) standardised_train_y = y_scaler.transform(train_y)  D = gpx.Dataset(X=train_x, y=standardised_train_y) <p>Having constructed our dataset, we'll now define our kernel. We'll use a kernel which is composed of the sum of a linear kernel and a periodic kernel, as we saw in the previous section that this kernel is able to capture both the periodic and linear trends in the data. We'll also add an RBF kernel to the sum, which will allow us to capture any non-linear trends in the data:</p> $$\\text{Kernel = Linear + Periodic + RBF}$$  In\u00a0[14]: Copied! <pre>mean = gpx.mean_functions.Zero()\nrbf_kernel = gpx.kernels.RBF(lengthscale=100.0)\nperiodic_kernel = gpx.kernels.Periodic()\nlinear_kernel = gpx.kernels.Linear()\nsum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel])\nfinal_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])\n\nprior = gpx.Prior(mean_function=mean, kernel=final_kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\n\nposterior = prior * likelihood\n</pre> mean = gpx.mean_functions.Zero() rbf_kernel = gpx.kernels.RBF(lengthscale=100.0) periodic_kernel = gpx.kernels.Periodic() linear_kernel = gpx.kernels.Linear() sum_kernel = gpx.kernels.SumKernel(kernels=[linear_kernel, periodic_kernel]) final_kernel = gpx.kernels.SumKernel(kernels=[rbf_kernel, sum_kernel])  prior = gpx.Prior(mean_function=mean, kernel=final_kernel) likelihood = gpx.Gaussian(num_datapoints=D.n)  posterior = prior * likelihood <p>With our model constructed, let's now fit it to the data, by minimising the negative log marginal likelihood of the data:</p> In\u00a0[15]: Copied! <pre>negative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\nnegative_mll = jit(negative_mll)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=1000,\n    safe=True,\n    key=key,\n)\n</pre> negative_mll = gpx.objectives.ConjugateMLL(negative=True) negative_mll(posterior, train_data=D) negative_mll = jit(negative_mll)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=1000,     safe=True,     key=key, ) <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <p>Now we can obtain the model's prediction over a period of time which includes the training data, as well as 8 years before and after the training data:</p> In\u00a0[16]: Copied! <pre>latent_dist = opt_posterior.predict(test_x, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean().reshape(-1, 1)\npredictive_std = predictive_dist.stddev().reshape(-1, 1)\n</pre> latent_dist = opt_posterior.predict(test_x, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean().reshape(-1, 1) predictive_std = predictive_dist.stddev().reshape(-1, 1) <p>Let's plot the model's predictions over this period of time:</p> In\u00a0[17]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\n    train_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5\n)\nax.fill_between(\n    test_x.squeeze(),\n    predictive_mean.squeeze() - 2 * predictive_std.squeeze(),\n    predictive_mean.squeeze() + 2 * predictive_std.squeeze(),\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    test_x,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(10, 5)) ax.plot(     train_x, standardised_train_y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5 ) ax.fill_between(     test_x.squeeze(),     predictive_mean.squeeze() - 2 * predictive_std.squeeze(),     predictive_mean.squeeze() + 2 * predictive_std.squeeze(),     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     test_x,     predictive_mean - 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     test_x,     predictive_mean + 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(test_x, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.set_xlabel(\"Year\") ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[17]: <pre>&lt;matplotlib.legend.Legend at 0x7f1f706049a0&gt;</pre> <p>We can see that the model seems to have captured the periodic trend in the data, as well as the (roughly) linear trend. This enables our model to make reasonable seeming predictions over the 8 years before and after the training data. Let's zoom in on the period from 2010 onwards:</p> In\u00a0[18]: Copied! <pre>fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(\n    train_x[train_x &gt;= 2010],\n    standardised_train_y[train_x &gt;= 2010],\n    \"x\",\n    label=\"Observations\",\n    color=cols[0],\n    alpha=0.5,\n)\nax.fill_between(\n    test_x[test_x &gt;= 2010].squeeze(),\n    predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    test_x[test_x &gt;= 2010],\n    predictive_mean[test_x &gt;= 2010],\n    label=\"Predictive mean\",\n    color=cols[1],\n)\nax.set_xlabel(\"Year\")\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(10, 5)) ax.plot(     train_x[train_x &gt;= 2010],     standardised_train_y[train_x &gt;= 2010],     \"x\",     label=\"Observations\",     color=cols[0],     alpha=0.5, ) ax.fill_between(     test_x[test_x &gt;= 2010].squeeze(),     predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     test_x[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010] - 2 * predictive_std[test_x &gt;= 2010],     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     test_x[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010] + 2 * predictive_std[test_x &gt;= 2010],     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     test_x[test_x &gt;= 2010],     predictive_mean[test_x &gt;= 2010],     label=\"Predictive mean\",     color=cols[1], ) ax.set_xlabel(\"Year\") ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[18]: <pre>&lt;matplotlib.legend.Legend at 0x7f1f70626f70&gt;</pre> <p>This certainly looks like a reasonable fit to the data, with sensible extrapolation beyond the training data, which finishes in November 2022. Moreover, the learned parameters of the kernel are interpretable. Let's take a look at the learned period of the periodic kernel:</p> In\u00a0[19]: Copied! <pre>print(\n    f\"Periodic Kernel Period: {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\"\n)\n</pre> print(     f\"Periodic Kernel Period: {[i for i in opt_posterior.prior.kernel.kernels if isinstance(i, gpx.kernels.Periodic)][0].period}\" ) <pre>Periodic Kernel Period: 0.9990420306760126\n</pre> <p>This tells us that the periodic trend learned has a period of $\\approx 1$. This makes intuitive sense, as the unit of the input data is years, and we can see that the periodic trend tends to repeat itself roughly every year!</p> In\u00a0[20]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Christie'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Christie' <pre>Author: Thomas Christie\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\noptax     : 0.1.5\npandas    : 1.5.3\njax       : 0.4.9\ngpjax     : 0.0.0\nmatplotlib: 3.7.1\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/intro_to_kernels/#introduction-to-kernels","title":"Introduction to Kernels\u00b6","text":""},{"location":"examples/intro_to_kernels/#what-is-a-kernel","title":"What is a Kernel?\u00b6","text":"<p>Intuitively, for a function $f$, the kernel defines the notion of similarity between the value of the function at two points, $f(\\mathbf{x})$ and $f(\\mathbf{x}')$, and will be denoted as $k(\\mathbf{x}, \\mathbf{x}')$:</p> $$\\begin{aligned} k(\\mathbf{x}, \\mathbf{x}') &amp;= \\text{Cov}[f(\\mathbf{x}), f(\\mathbf{x}')] \\\\ &amp;= \\mathbb{E}[(f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{x})])(f(\\mathbf{x}') - \\mathbb{E}[f(\\mathbf{x}')])] \\end{aligned}$$<p>One would expect that, given a previously unobserved test point $\\mathbf{x}^*$, the  training points which are closest to this unobserved point will be most similar to  it. As such, the kernel is used to define this notion of similarity within the GP  framework. It is up to the user to select a kernel function which is appropriate for  the function being modelled. In this notebook we are going to give some examples of  commonly used kernels, and try to develop an understanding of when one may wish to use  one kernel over another. However, before we do this, it is worth discussing the  necessary conditions for a function to be a valid kernel/covariance function. This  requires a little bit of maths, so for those of you who just wish to obtain an  intuitive understanding, feel free to skip to the section introducing the Mat\u00e9rn  family of kernels.</p>"},{"location":"examples/intro_to_kernels/#what-are-the-necessary-conditions-for-a-function-to-be-a-valid-kernel","title":"What are the necessary conditions for a function to be a valid kernel?\u00b6","text":"<p>Whilst intuitively the kernel function is used to define the notion of similarity within the GP framework, it is important to note that there are two necessary conditions that a kernel function must satisfy in order to be a valid covariance function. For clarity, we will refer to any function mapping two inputs to a scalar output as a kernel function, and we will refer to a valid kernel function satisfying the two necessary conditions as a covariance function. However, it is worth noting that the GP community often uses the terms kernel function and covariance function interchangeably.</p> <p>The first necessary condition is that the covariance function must be symmetric, i.e. $k(\\mathbf{x}, \\mathbf{x}') = k(\\mathbf{x}', \\mathbf{x})$. This is because the covariance between two random variables $X$ and $X'$ is symmetric; if one looks at the definition of covariance given above, it is clear that it is invariant to swapping the order of the inputs $\\mathbf{x}$ and $\\mathbf{x}'$.</p> <p>The second necessary condition is that the covariance function must be positive semi-definite (PSD). In order to understand this condition, it is useful to first introduce the concept of a Gram matrix. We'll use the same notation as the GP introduction notebook, and denote $n$ input points as $\\mathbf{X} = \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}$. Given these input points and a kernel function $k$ the Gram matrix stores the pairwise kernel evaluations between all input points. Mathematically, this leads to the Gram matrix being defined as:</p> $$K(\\mathbf{X}, \\mathbf{X}) = \\begin{bmatrix} k(\\mathbf{x}_1, \\mathbf{x}_1) &amp; \\cdots &amp; k(\\mathbf{x}_1, \\mathbf{x}_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ k(\\mathbf{x}_n, \\mathbf{x}_1) &amp; \\cdots &amp; k(\\mathbf{x}_n, \\mathbf{x}_n) \\end{bmatrix}$$<p>such that $K(\\mathbf{X}, \\mathbf{X})_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$.</p> <p>In order for $k$ to be a valid covariance function, the corresponding Gram matrix must be positive semi-definite. In this case the Gram matrix is referred to as a covariance matrix. A real $n \\times n$ matrix $K$ is positive semi-definite if and only if for all vectors $\\mathbf{z} \\in \\mathbb{R}^n$:</p> $$\\mathbf{z}^\\top K \\mathbf{z} \\geq 0$$<p>Alternatively, a real $n \\times n$ matrix $K$ is positive semi-definite if and only if all of its eigenvalues are non-negative.</p> <p>Therefore, the two necessary conditions for a function to be a valid covariance function are that it must be symmetric and positive semi-definite. In this section we have referred to any function from two inputs to a scalar output as a kernel function, with its corresponding matrix of pairwise evaluations referred to as the Gram matrix, and a function satisfying the two necessary conditions as a covariance function, with its corresponding matrix of pairwise evaluations referred to as the covariance matrix. This enabled us to easily define the necessary conditions for a function to be a valid covariance function. However, as noted previously, the GP community often uses these terms interchangeably, and so we will for the remainder of this notebook.</p>"},{"location":"examples/intro_to_kernels/#introducing-a-common-family-of-kernels-the-matern-family","title":"Introducing a Common Family of Kernels - The Mat\u00e9rn Family\u00b6","text":""},{"location":"examples/intro_to_kernels/#inferring-kernel-hyperparameters","title":"Inferring Kernel Hyperparameters\u00b6","text":""},{"location":"examples/intro_to_kernels/#expressing-other-priors-with-different-kernels","title":"Expressing Other Priors with Different Kernels\u00b6","text":""},{"location":"examples/intro_to_kernels/#composing-kernels","title":"Composing Kernels\u00b6","text":""},{"location":"examples/intro_to_kernels/#putting-it-all-together-on-a-real-world-dataset","title":"Putting it All Together on a Real-World Dataset\u00b6","text":""},{"location":"examples/intro_to_kernels/#mauna-loa-co2-dataset","title":"Mauna Loa CO2 Dataset\u00b6","text":""},{"location":"examples/intro_to_kernels/#defining-kernels-on-non-euclidean-spaces","title":"Defining Kernels on Non-Euclidean Spaces\u00b6","text":"<p>In this notebook, we have focused solely on kernels whose domain resides in Euclidean space. However, what if one wished to work with data whose domain is non-Euclidean? For instance, one may wish to work with graph-structured data, or data which lies on a manifold, or even strings. Fortunately, kernels exist for a wide variety of domains. Whilst this is beyond the scope of this notebook, feel free to checkout out our notebook on graph kernels for an introduction on how to define the Mat\u00e9rn kernel on graph-structured data, and there are a wide variety of resources online for learning about defining kernels in other domains. In terms of open-source libraries, the Geometric Kernels library could be a good place to start if you're interested in looking at how these kernels may be implemented, with the additional benefit that it is compatible with GPJax.</p>"},{"location":"examples/intro_to_kernels/#further-reading","title":"Further Reading\u00b6","text":"<p>Congratulations on making it this far! We hope that this guide has given you a good introduction to kernels and how they can be used in GPJax. If you're interested in learning more about kernels, we recommend the following resources, which have also been used as inspiration for this guide:</p> <ul> <li>Gaussian Processes for Machine Learning - Chapter 4 provides a comprehensive overview of kernels, diving deep into some of the technical details and also providing some kernels defined on non-Euclidean spaces such as strings.</li> <li>David Duvenaud's Kernel Cookbook is a great resource for learning about kernels, and also provides some information about some of the pitfalls people commonly encounter when using the Mat\u00e9rn family of kernels. His PhD thesis, Automatic Model Construction with Gaussian Processes, also provides some in-depth recipes for how one may incorporate their prior knowledge when constructing kernels.</li> <li>Finally, please check out our more advanced kernel guide, which details some more kernels available in GPJax as well as how one may combine kernels together to form more complex kernels.</li> </ul>"},{"location":"examples/intro_to_kernels/#system-configuration","title":"System Configuration\u00b6","text":""},{"location":"examples/likelihoods_guide/","title":"Likelihood guide","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nimport gpjax as gpx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\n\ntfd = tfp.distributions\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\nkey = jr.PRNGKey(123)\n\n\nn = 50\nx = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0)\nxtest = jnp.linspace(-3, 3, 100)[:, None]\nf = lambda x: jnp.sin(x)\ny = f(x) + 0.1 * jr.normal(key, shape=x.shape)\nD = gpx.Dataset(x, y)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\")\nax.plot(x, f(x), label=\"Latent function\")\nax.legend()\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  import gpjax as gpx import jax import jax.numpy as jnp import jax.random as jr import matplotlib.pyplot as plt import tensorflow_probability.substrates.jax as tfp  tfd = tfp.distributions plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] key = jr.PRNGKey(123)   n = 50 x = jnp.sort(jr.uniform(key=key, shape=(n, 1), minval=-3.0, maxval=3.0), axis=0) xtest = jnp.linspace(-3, 3, 100)[:, None] f = lambda x: jnp.sin(x) y = f(x) + 0.1 * jr.normal(key, shape=x.shape) D = gpx.Dataset(x, y)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\") ax.plot(x, f(x), label=\"Latent function\") ax.legend() <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> Out[1]: <pre>&lt;matplotlib.legend.Legend at 0x7fc10c1a9f10&gt;</pre> <p>In this example, our observations have support $[-3, 3]$ and are generated from a sinusoidal function with Gaussian noise. As such, our response values $\\mathbf{y}$ range between $-1$ and $1$, subject to Gaussian noise. Due to this, a Gaussian likelihood is appropriate for this dataset as it allows for negative values.</p> <p>As we see in \\eqref{eq:likelihood_fn}, the likelihood function factorises over the $n$ observations. As such, we must provide this information to GPJax when instantiating a likelihood object. We do this by specifying the <code>num_datapoints</code> argument.</p> In\u00a0[2]: Copied! <pre>gpx.likelihoods.Gaussian(num_datapoints=D.n)\n</pre> gpx.likelihoods.Gaussian(num_datapoints=D.n) Out[2]: <pre>Gaussian(num_datapoints=50, integrator=AnalyticalGaussianIntegrator(), obs_noise=Array(1., dtype=float64, weak_type=True))</pre> In\u00a0[3]: Copied! <pre>gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_noise=0.5)\n</pre> gpx.likelihoods.Gaussian(num_datapoints=D.n, obs_noise=0.5) Out[3]: <pre>Gaussian(num_datapoints=50, integrator=AnalyticalGaussianIntegrator(), obs_noise=0.5)</pre> <p>To control other properties of the observation noise such as trainability and value constraints, see our PyTree guide.</p> In\u00a0[4]: Copied! <pre>kernel = gpx.Matern32()\nmeanf = gpx.Zero()\nprior = gpx.Prior(kernel=kernel, mean_function=meanf)\n\nlikelihood = gpx.Gaussian(num_datapoints=D.n, obs_noise=0.1)\n\nposterior = prior * likelihood\n\nlatent_dist = posterior.predict(xtest, D)\n\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\n\nfor ax in axes.ravel():\n    subkey, _ = jr.split(subkey)\n    ax.plot(\n        latent_dist.sample(sample_shape=(1,), seed=subkey).T,\n        lw=1,\n        color=cols[0],\n        label=\"Latent samples\",\n    )\n    ax.plot(\n        likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n        \"o\",\n        markersize=5,\n        alpha=0.3,\n        color=cols[1],\n        label=\"Predictive samples\",\n    )\n</pre> kernel = gpx.Matern32() meanf = gpx.Zero() prior = gpx.Prior(kernel=kernel, mean_function=meanf)  likelihood = gpx.Gaussian(num_datapoints=D.n, obs_noise=0.1)  posterior = prior * likelihood  latent_dist = posterior.predict(xtest, D)  fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2)) key, subkey = jr.split(key)  for ax in axes.ravel():     subkey, _ = jr.split(subkey)     ax.plot(         latent_dist.sample(sample_shape=(1,), seed=subkey).T,         lw=1,         color=cols[0],         label=\"Latent samples\",     )     ax.plot(         likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,         \"o\",         markersize=5,         alpha=0.3,         color=cols[1],         label=\"Predictive samples\",     ) <p>Similarly, for a Bernoulli likelihood function, the samples of $y$ would be binary.</p> In\u00a0[5]: Copied! <pre>likelihood = gpx.Bernoulli(num_datapoints=D.n)\n\n\nfig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2))\nkey, subkey = jr.split(key)\n\nfor ax in axes.ravel():\n    subkey, _ = jr.split(subkey)\n    ax.plot(\n        latent_dist.sample(sample_shape=(1,), seed=subkey).T,\n        lw=1,\n        color=cols[0],\n        label=\"Latent samples\",\n    )\n    ax.plot(\n        likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,\n        \"o\",\n        markersize=3,\n        alpha=0.5,\n        color=cols[1],\n        label=\"Predictive samples\",\n    )\n</pre> likelihood = gpx.Bernoulli(num_datapoints=D.n)   fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(9, 2)) key, subkey = jr.split(key)  for ax in axes.ravel():     subkey, _ = jr.split(subkey)     ax.plot(         latent_dist.sample(sample_shape=(1,), seed=subkey).T,         lw=1,         color=cols[0],         label=\"Latent samples\",     )     ax.plot(         likelihood.predict(latent_dist).sample(sample_shape=(1,), seed=subkey).T,         \"o\",         markersize=3,         alpha=0.5,         color=cols[1],         label=\"Predictive samples\",     ) In\u00a0[6]: Copied! <pre>z = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1)\nq = gpx.VariationalGaussian(posterior=posterior, inducing_inputs=z)\n\n\ndef q_moments(x):\n    qx = q(x)\n    return qx.mean(), qx.variance()\n\n\nmean, variance = jax.vmap(q_moments)(x[:, None])\n</pre> z = jnp.linspace(-3.0, 3.0, 10).reshape(-1, 1) q = gpx.VariationalGaussian(posterior=posterior, inducing_inputs=z)   def q_moments(x):     qx = q(x)     return qx.mean(), qx.variance()   mean, variance = jax.vmap(q_moments)(x[:, None]) <p>Now that we have the variational mean and variational (co)variance, we can compute the expected log-likelihood using the <code>expected_log_likelihood</code> method of the likelihood object.</p> In\u00a0[7]: Copied! <pre>jnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance))\n</pre> jnp.sum(likelihood.expected_log_likelihood(y=y, mean=mean, variance=variance)) Out[7]: <pre>Array(-47.73779086, dtype=float64)</pre> <p>However, had we wanted to do this using quadrature, then we would have done the following:</p> In\u00a0[8]: Copied! <pre>lquad = gpx.Gaussian(\n    num_datapoints=D.n,\n    obs_noise=jnp.array([0.1]),\n    integrator=gpx.integrators.GHQuadratureIntegrator(num_points=20),\n)\n</pre> lquad = gpx.Gaussian(     num_datapoints=D.n,     obs_noise=jnp.array([0.1]),     integrator=gpx.integrators.GHQuadratureIntegrator(num_points=20), ) <p>However, this is not recommended for the Gaussian likelihood given that the expectation can be computed analytically.</p> In\u00a0[9]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\ntensorflow_probability: 0.19.0\ngpjax                 : 0.0.0\nmatplotlib            : 3.7.1\njax                   : 0.4.9\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/likelihoods_guide/#likelihood-guide","title":"Likelihood guide\u00b6","text":"<p>In this notebook, we will walk users through the process of creating a new likelihood in GPJax.</p>"},{"location":"examples/likelihoods_guide/#background","title":"Background\u00b6","text":"<p>In this section we'll provide a short introduction to likelihoods and why they are important. For users who are already familiar with likelihoods, feel free to skip to the next section, and for users who would like more information than is provided here, please see our introduction to Gaussian processes notebook.</p>"},{"location":"examples/likelihoods_guide/#what-is-a-likelihood","title":"What is a likelihood?\u00b6","text":"<p>We adopt the notation of our introduction to Gaussian processes notebook where we have a Gaussian process (GP) $f(\\cdot)\\sim\\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))$ and a dataset $\\mathbf{y} = \\{y_n\\}_{n=1}^N$ observed at corresponding inputs $\\mathbf{x} = \\{x_n\\}_{n=1}^N$. The evaluation of $f$ at $\\mathbf{x}$ is denoted by $\\mathbf{f} = \\{f(x_n)\\}_{n=1}^N$. The likelihood function of the GP is then given by $$ \\begin{align}     \\label{eq:likelihood_fn}     p(\\mathbf{y}\\mid \\mathbf{f}) = \\prod_{n=1}^N p(y_n\\mid f(x_n))\\,. \\end{align} $$ Conceptually, this conditional distribution describes the probability of the observed data, conditional on the latent function values.</p>"},{"location":"examples/likelihoods_guide/#why-is-the-likelihood-important","title":"Why is the likelihood important?\u00b6","text":"<p>Choosing the correct likelihood function when building a GP, or any Bayesian model for that matter, is crucial. The likelihood function encodes our assumptions about the data and the noise that we expect to observe. For example, if we are modelling air pollution, then we would not expect to observe negative values of pollution. In this case, we would choose a likelihood function that is only defined for positive values. Similarly, if our data is the proportion of people who voted for a particular political party, then we would expect to observe values between 0 and 1. In this case, we would choose a likelihood function that is only defined for values between 0 and 1.</p>"},{"location":"examples/likelihoods_guide/#likelihoods-in-gpjax","title":"Likelihoods in GPJax\u00b6","text":"<p>In GPJax, all likelihoods are a subclass of the <code>AbstractLikelihood</code> class. This base abstract class contains the three core methods that all likelihoods must implement: <code>predict</code>, <code>link_function</code>, and <code>expected_log_likelihood</code>. We will discuss each of these methods in the forthcoming sections, but first, we will show how to instantiate a likelihood object. To do this, we'll need a dataset.</p>"},{"location":"examples/likelihoods_guide/#likelihood-parameters","title":"Likelihood parameters\u00b6","text":"<p>Some likelihoods, such as the Gaussian likelihood, contain parameters that we seek to infer. In the case of the Gaussian likelihood, we have a single parameter $\\sigma^2$ that determines the observation noise. In GPJax, we can specify the value of this parameter when instantiating the likelihood object. If we do not specify a value, then the likelihood will be initialised with a default value. In the case of the Gaussian likelihood, the default value is $1.0$. If we instead wanted to initialise the likelihood with a value of $0.5$, then we would do this as follows:</p>"},{"location":"examples/likelihoods_guide/#prediction","title":"Prediction\u00b6","text":"<p>The <code>predict</code> method of a likelihood object transforms the latent distribution of the Gaussian process. In the case of a Gaussian likelihood, this simply applies the observational noise value to the diagonal values of the covariance matrix. For other likelihoods, this may be a more complex transformation. For example, the Bernoulli likelihood transforms the latent distribution of the Gaussian process into a distribution over binary values.</p> <p>We visualise this below for the Gaussian likelihood function. In blue we can see samples of $\\mathbf{f}^{\\star}$, whilst in red we see samples of $\\mathbf{y}^{\\star}$.</p>"},{"location":"examples/likelihoods_guide/#link-functions","title":"Link functions\u00b6","text":"<p>In the above figure, we can see the latent samples being constrained to be either 0 or 1 when a Bernoulli likelihood is specified. This is achieved by the <code>inverse link_function</code> $\\eta(\\cdot)$ of the likelihood. The link function is a deterministic function that maps the latent distribution of the Gaussian process to the support of the likelihood function. For example, the link function of the Bernoulli likelihood that is used in GPJax is the inverse probit function $$ \\eta(x) = 0.5\\left(1 + \\Phi\\left(\\frac{x}{\\sqrt{2}}\\right) * (1-2)\\right)\\,, $$ where $\\Phi$ is the cumulative distribution function of the standard normal distribution.</p> <p>A table of commonly used link functions and their corresponding likelihood can be found here.</p>"},{"location":"examples/likelihoods_guide/#expected-log-likelihood","title":"Expected log likelihood\u00b6","text":"<p>The final method that is associated with a likelihood function in GPJax is the expected log-likelihood. This term is evaluated in the stochastic variational Gaussian process in the ELBO term. For a variational approximation $q(f)= \\mathcal{N}(f\\mid m, S)$, the ELBO can be written as $$ \\begin{align}     \\label{eq:elbo}     \\mathcal{L}(q) = \\mathbb{E}_{f\\sim q(f)}\\left[ p(\\mathbf{y}\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,. \\end{align} $$ As both $q(f)$ and $p(f)$ are Gaussian distributions, the Kullback-Leibler term can be analytically computed. However, the expectation term is not always so easy to compute. Fortunately, the bound in \\eqref{eq:elbo} can be decomposed as a sum of the datapoints $$ \\begin{align}     \\label{eq:elbo_decomp}     \\mathcal{L}(q) = \\sum_{n=1}^N \\mathbb{E}_{f\\sim q(f)}\\left[ p(y_n\\mid f)\\right] - \\mathrm{KL}\\left(q(f)\\mid\\mid p(f)\\right)\\,. \\end{align} $$ This simplifies computation of the expectation as it is now a series of $N$ 1-dimensional integrals. As such, GPJax by default uses quadrature to compute these integrals. However, for some likelihoods, such as the Gaussian likelihood, the expectation can be computed analytically. In these cases, we can supply an object that inherits from <code>AbstractIntegrator</code> to the likelihood upon instantiation. To see this, let us consider a Gaussian likelihood where we'll first define a variational approximation to the posterior.</p>"},{"location":"examples/likelihoods_guide/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/poisson/","title":"Count data regression","text":"In\u00a0[1]: Copied! <pre>import blackjax\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport tensorflow_probability.substrates.jax as tfp\nfrom jax.config import config\nfrom jaxtyping import install_import_hook\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nconfig.update(\"jax_enable_x64\", True)\ntfd = tfp.distributions\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> import blackjax import jax import jax.numpy as jnp import jax.random as jr import jax.tree_util as jtu import matplotlib as mpl import matplotlib.pyplot as plt import tensorflow_probability.substrates.jax as tfp from jax.config import config from jaxtyping import install_import_hook  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. config.update(\"jax_enable_x64\", True) tfd = tfp.distributions key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>key, subkey = jr.split(key)\nn = 50\nx = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0)\nf = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function\ny = jr.poisson(key, jnp.exp(f(x)))\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[1])\nax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\")\nax.legend()\n</pre> key, subkey = jr.split(key) n = 50 x = jr.uniform(key, shape=(n, 1), minval=-2.0, maxval=2.0) f = lambda x: 2.0 * jnp.sin(3 * x) + 0.5 * x  # latent function y = jr.poisson(key, jnp.exp(f(x)))  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-2.0, 2.0, 500).reshape(-1, 1)  fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[1]) ax.plot(xtest, jnp.exp(f(xtest)), label=r\"Rate $\\lambda$\") ax.legend() Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x7fe76ca18d30&gt;</pre> In\u00a0[3]: Copied! <pre>kernel = gpx.RBF()\nmeanf = gpx.Constant()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\nlikelihood = gpx.Poisson(num_datapoints=D.n)\n</pre> kernel = gpx.RBF() meanf = gpx.Constant() prior = gpx.Prior(mean_function=meanf, kernel=kernel) likelihood = gpx.Poisson(num_datapoints=D.n) <p>We construct the posterior through the product of our prior and likelihood.</p> In\u00a0[4]: Copied! <pre>posterior = prior * likelihood\nprint(type(posterior))\n</pre> posterior = prior * likelihood print(type(posterior)) <pre>&lt;class 'gpjax.gps.NonConjugatePosterior'&gt;\n</pre> <p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function's inverse link function. This step prevents us from being able to analytically integrate the latent function's values out of our posterior, and we must instead adopt alternative inference techniques. Here, we show how to use MCMC methods.</p> In\u00a0[5]: Copied! <pre># Adapted from BlackJax's introduction notebook.\nnum_adapt = 100\nnum_samples = 200\n\nlpd = jax.jit(gpx.LogPosteriorDensity(negative=False))\nunconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))\n\nadapt = blackjax.window_adaptation(\n    blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65\n)\n\n# Initialise the chain\nlast_state, kernel, _ = adapt.run(key, posterior.unconstrain())\n\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, info = kernel(rng_key, state)\n        return state, (state, info)\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n\n    return states, infos\n\n\n# Sample from the posterior distribution\nstates, infos = inference_loop(key, kernel, last_state, num_samples)\n</pre> # Adapted from BlackJax's introduction notebook. num_adapt = 100 num_samples = 200  lpd = jax.jit(gpx.LogPosteriorDensity(negative=False)) unconstrained_lpd = jax.jit(lambda tree: lpd(tree.constrain(), D))  adapt = blackjax.window_adaptation(     blackjax.nuts, unconstrained_lpd, num_adapt, target_acceptance_rate=0.65 )  # Initialise the chain last_state, kernel, _ = adapt.run(key, posterior.unconstrain())   def inference_loop(rng_key, kernel, initial_state, num_samples):     def one_step(state, rng_key):         state, info = kernel(rng_key, state)         return state, (state, info)      keys = jax.random.split(rng_key, num_samples)     _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)      return states, infos   # Sample from the posterior distribution states, infos = inference_loop(key, kernel, last_state, num_samples) In\u00a0[6]: Copied! <pre>acceptance_rate = jnp.mean(infos.acceptance_probability)\nprint(f\"Acceptance rate: {acceptance_rate:.2f}\")\n</pre> acceptance_rate = jnp.mean(infos.acceptance_probability) print(f\"Acceptance rate: {acceptance_rate:.2f}\") <pre>Acceptance rate: 0.42\n</pre> In\u00a0[7]: Copied! <pre>fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3))\nax0.plot(states.position.constrain().prior.kernel.variance)\nax1.plot(states.position.constrain().prior.kernel.lengthscale)\nax2.plot(states.position.constrain().prior.mean_function.constant)\nax0.set_title(\"Kernel variance\")\nax1.set_title(\"Kernel lengthscale\")\nax2.set_title(\"Mean function constant\")\n</pre> fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(10, 3)) ax0.plot(states.position.constrain().prior.kernel.variance) ax1.plot(states.position.constrain().prior.kernel.lengthscale) ax2.plot(states.position.constrain().prior.mean_function.constant) ax0.set_title(\"Kernel variance\") ax1.set_title(\"Kernel lengthscale\") ax2.set_title(\"Mean function constant\") Out[7]: <pre>Text(0.5, 1.0, 'Mean function constant')</pre> In\u00a0[8]: Copied! <pre>thin_factor = 10\nsamples = []\n\nfor i in range(num_adapt, num_samples + num_adapt, thin_factor):\n    sample = jtu.tree_map(lambda samples: samples[i], states.position)\n    sample = sample.constrain()\n    latent_dist = sample.predict(xtest, train_data=D)\n    predictive_dist = sample.likelihood(latent_dist)\n    samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))\n\nsamples = jnp.vstack(samples)\n\nlower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0)\nexpected_val = jnp.mean(samples, axis=0)\n</pre> thin_factor = 10 samples = []  for i in range(num_adapt, num_samples + num_adapt, thin_factor):     sample = jtu.tree_map(lambda samples: samples[i], states.position)     sample = sample.constrain()     latent_dist = sample.predict(xtest, train_data=D)     predictive_dist = sample.likelihood(latent_dist)     samples.append(predictive_dist.sample(seed=key, sample_shape=(10,)))  samples = jnp.vstack(samples)  lower_ci, upper_ci = jnp.percentile(samples, jnp.array([2.5, 97.5]), axis=0) expected_val = jnp.mean(samples, axis=0) <p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p> In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(\n    x, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7\n)\nax.plot(\n    xtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1\n)\nax.fill_between(\n    xtest.flatten(),\n    lower_ci.flatten(),\n    upper_ci.flatten(),\n    alpha=0.2,\n    color=cols[0],\n    label=\"95% CI\",\n)\n</pre> fig, ax = plt.subplots() ax.plot(     x, y, \"o\", markersize=5, color=cols[1], label=\"Observations\", zorder=2, alpha=0.7 ) ax.plot(     xtest, expected_val, linewidth=2, color=cols[0], label=\"Predicted mean\", zorder=1 ) ax.fill_between(     xtest.flatten(),     lower_ci.flatten(),     upper_ci.flatten(),     alpha=0.2,     color=cols[0],     label=\"95% CI\", ) Out[9]: <pre>&lt;matplotlib.collections.PolyCollection at 0x7fe74231a100&gt;</pre> In\u00a0[10]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w -a \"Francesco Zanetta\"\n</pre> %load_ext watermark %watermark -n -u -v -iv -w -a \"Francesco Zanetta\" <pre>Author: Francesco Zanetta\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\njax                   : 0.4.9\nmatplotlib            : 3.7.1\nblackjax              : 0.9.6\ngpjax                 : 0.0.0\ntensorflow_probability: 0.19.0\n\nWatermark: 2.3.1\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/poisson/#count-data-regression","title":"Count data regression\u00b6","text":"<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via Markov chain Monte Carlo (MCMC). We focus on a count data regression task here and use BlackJax for sampling.</p>"},{"location":"examples/poisson/#dataset","title":"Dataset\u00b6","text":"<p>For count data regression, the Poisson distribution is a natural choice for the likelihood function. The probability mass function of the Poisson distribution is given by</p> $$ p(y \\,|\\, \\lambda) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!},$$<p>where $y$ is the count and the parameter $\\lambda \\in \\mathbb{R}_{&gt;0}$ is the rate of the Poisson distribution.</p> <p>We than set $\\lambda = \\exp(f)$ where $f$ is the latent Gaussian process. The exponential function is the link function for the Poisson distribution: it maps the output of a GP to the positive real line, which is suitable for modeling count data.</p> <p>We simulate a dataset $\\mathcal{D} = \\{(\\mathbf{X}, \\mathbf{y})\\}$ with inputs $\\mathbf{X} \\in \\mathbb{R}^d$ and corresponding count outputs $\\mathbf{y}$. We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code>.</p>"},{"location":"examples/poisson/#gaussian-process-definition","title":"Gaussian Process definition\u00b6","text":"<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. We adopt the Poisson likelihood available in GPJax.</p>"},{"location":"examples/poisson/#mcmc-inference","title":"MCMC inference\u00b6","text":"<p>An MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the proposal. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an acceptance probability determined via the sampler's transition kernel which depends on the current position and the unnormalised target posterior distribution. If the new sample is more likely, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of A Handbook of Markov Chain Monte Carlo.</p>"},{"location":"examples/poisson/#mcmc-through-blackjax","title":"MCMC through BlackJax\u00b6","text":"<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on BlackJax in this notebook, which we recommend adopting for general applications.</p> <p>We begin by generating sensible initial positions for our sampler before defining an inference loop and sampling 200 values from our Markov chain. In practice, drawing more samples will be necessary.</p>"},{"location":"examples/poisson/#sampler-efficiency","title":"Sampler efficiency\u00b6","text":"<p>BlackJax gives us easy access to our sampler's efficiency through metrics such as the sampler's acceptance probability (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain).</p>"},{"location":"examples/poisson/#prediction","title":"Prediction\u00b6","text":"<p>Having obtained samples from the posterior, we draw ten instances from our model's predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p> <p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain's sample set. A commonly used technique to try and reduce this correlation is thinning whereby we select every $n$-th sample where $n$ is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain's autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>"},{"location":"examples/poisson/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/pytrees/","title":"\ud83c\udf33 GPJax PyTrees","text":""},{"location":"examples/pytrees/#gpjax-module","title":"\ud83c\udf33 GPJax Module","text":"<p><code>GPJax</code> represents all objects as JAX PyTrees, giving</p> <ul> <li>A simple API with a TensorFlow / PyTorch feel ...</li> <li>... whilst fully compatible with JAX's functional paradigm ...</li> <li>... And works out of the box (no filtering) with JAX's transformations   such as <code>grad</code>.</li> </ul> <p>We achieve this through providing a base <code>Module</code> abstraction to cleanly handle parameter trainability and optimising transformations of JAX models.</p>"},{"location":"examples/pytrees/#gaussian-process-objects-as-data","title":"Gaussian process objects as data","text":"<p>Our abstraction is inspired by the Equinox library and aims to offer a Bayesian/Gaussian process extension to their neural network abstractions. Our approach enables users to easily create Python classes and define parameter domains and training statuses for optimisation within a single model object. This object can be used with JAX autogradients without any filtering.</p> <p>The fundamental concept is to describe every model object as an immutable tree structure, where every method is a function of the state (represented by the tree's leaves).</p> <p>To help you understand how to create custom objects in GPJax, we will look at a simple example in the following section.</p>"},{"location":"examples/pytrees/#the-rbf-kernel","title":"The RBF kernel","text":"<p>The kernel in a Gaussian process model is a mathematical function that defines the covariance structure between data points, allowing us to model complex relationships and make predictions based on the observed data. The radial basis function (RBF, or squared exponential) kernel is a popular choice. For any pair of vectors x,y\u2208Rdx, y \\in \\mathbb{R}^dx,y\u2208Rd, its form is given by</p> <p>k(x,y)=\u03c32exp\u2061(\u2225x\u2212y\u2225222\u21132) k(x, y) = \\sigma^2\\exp\\left(\\frac{\\lVert x-y\\rVert_{2}^2}{2\\ell^2} \\right) k(x,y)=\u03c32exp(2\u21132\u2225x\u2212y\u222522\u200b\u200b)</p> <p>where \u03c32\u2208R&gt;0\\sigma^2\\in\\mathbb{R}_{&gt;0}\u03c32\u2208R&gt;0\u200b is a variance parameter and \u21132\u2208R&gt;0\\ell^2\\in\\mathbb{R}_{&gt;0}\u21132\u2208R&gt;0\u200b a lengthscale parameter. Terming the evaluation of k(x,y)k(x, y)k(x,y) the covariance, we can represent this object as a Python <code>dataclass</code> as follows:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom dataclasses import dataclass, field\n@dataclass\nclass RBF:\nlengthscale: float = field(default=1.0)\nvariance: float = field(default=1.0)\ndef covariance(self, x: float, y: float) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>Here, the Python <code>dataclass</code> is a class that simplifies the process of creating classes that primarily store data. It reduces boilerplate code and provides convenient methods for initialising and representing the data. An equivalent class could be written as:</p> <pre><code>class RBF:\ndef __init__(self, lengthscale: float = 1.0, variance: float = 1.0) -&gt; None:\nself.lengthscale = lengthscale\nself.variance = variance\ndef covariance(self, x: float, y: float) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x-y) / self.lengthscale)**2)\n</code></pre> <p>To establish some terminology, within the above RBF <code>dataclass</code>, we refer to the lengthscale and variance as fields. Further, the <code>RBF.covariance</code> is a method. So far so good. However, if we wanted to take the gradient of the kernel with respect to its parameters $<code>\\nabla_{\\ell, \\sigma^2} k(1.0, 2.0; \\ell, \\sigma^2)</code>$ at inputs x=1.0x=1.0x=1.0 and y=2.0y=2.0y=2.0, then we encounter a problem:</p> <p><pre><code>kernel = RBF()\ntry:\njax.grad(lambda kern: kern.covariance(1.0, 2.0))(kernel)\nexcept TypeError as e:\nprint(e)\n</code></pre> <pre><code>Argument 'RBF(lengthscale=1.0, variance=1.0)' of type &lt;class '__main__.RBF'&gt; is not a valid JAX type.\n</code></pre></p> <p>This issues arises as the object we have defined is not yet compatible with JAX. To achieve this we must consider JAX's PyTree abstraction.</p>"},{"location":"examples/pytrees/#pytrees","title":"PyTrees","text":"<p>JAX PyTrees are a powerful tool in the JAX library that enable users to work with complex data structures in a way that is efficient, flexible, and easy to use. A PyTree is a data structure that is composed of other data structures, and it can be thought of as a tree where each 'node' is either a leaf (a simple data structure) or another PyTree. By default, the set of 'node' types that are regarded a PyTree are Python lists, tuples, and dicts.</p> <p>For instance:</p> <p><pre><code>tree = [3.14, {\"Monte\": object(), \"Carlo\": False}]\nprint(tree)\n</code></pre> <pre><code>[3.14, {'Monte': &lt;object object at 0x1188ba6b0&gt;, 'Carlo': False}]\n</code></pre> is a PyTree with structure</p> <p><pre><code>import jax.tree_util as jtu\nprint(jtu.tree_structure(tree))\n</code></pre> <pre><code>PyTreeDef([*, {'Carlo': *, 'Monte': *}])\n</code></pre> with the following leaves</p> <p><pre><code>print(jtu.tree_leaves(tree))\n</code></pre> <pre><code>[3.14, False, &lt;object object at 0x1188ba6b0&gt;]\n</code></pre></p> <p>Consider a second example, a PyTree of JAX arrays</p> <pre><code>tree = (\njnp.array([1.0, 2.0, 3.0]),\njnp.array([4.0, 5.0, 6.0]),\njnp.array([7.0, 8.0, 9.0]),\n)\n</code></pre> <p>You can use this template to perform various operations on the data, such as applying a function to each leaf of the PyTree.</p> <p>For example, suppose you want to square each element of the arrays. You can then apply this using the <code>tree_map</code> function from the <code>jax.tree_util</code> module:</p> <p><pre><code>print(jtu.tree_map(lambda x: x**2, tree))\n</code></pre> <pre><code>(Array([1., 4., 9.], dtype=float32), Array([16., 25., 36.], dtype=float32), Array([49., 64., 81.], dtype=float32))\n</code></pre></p> <p>In this example, the PyTree makes it easy to apply a function to each leaf of a complex data structure, without having to manually traverse the data structure and handle each leaf individually. JAX PyTrees, therefore, are a powerful tool that can simplify many tasks in machine learning and scientific computing. As such, most JAX functions operate over PyTrees of JAX arrays. For instance, <code>jax.lax.scan</code>, accepts as input and produces as output a PyTree of JAX arrays.</p> <p>Another key advantages of using JAX PyTrees is that they are designed to work efficiently with JAX's automatic differentiation and compilation features. For example, suppose you have a function that takes a PyTree as input and returns a scalar value:</p> <p><pre><code>def sum_squares(x):\nreturn jnp.sum(x[0] ** 2 + x[1] ** 2 + x[2] ** 2)\nsum_squares(tree)\n</code></pre> <pre><code>Array(285., dtype=float32)\n</code></pre></p> <p>You can use JAX's <code>grad</code> function to automatically compute the gradient of this function with respect to the input PyTree:</p> <p><pre><code>gradient = jax.grad(sum_squares)(tree)\nprint(gradient)\n</code></pre> <pre><code>(Array([2., 4., 6.], dtype=float32), Array([ 8., 10., 12.], dtype=float32), Array([14., 16., 18.], dtype=float32))\n</code></pre></p> <p>This computes the gradient of the <code>sum_squares</code> function with respect to the input PyTree, and returns a new PyTree with the same shape and structure.</p> <p>JAX PyTrees are also designed to be highly extensible, where custom types can be readily registered through a global registry with the values of such traversed recursively (i.e., as a tree!). This means we can define our own custom data structures and use them as PyTrees. This is the functionality that we exploit, whereby we construct all Gaussian process models via a tree-structure through our <code>Module</code> object.</p>"},{"location":"examples/pytrees/#module","title":"Module","text":"<p>Our design, first and foremost, minimises additional abstractions on top of standard JAX: everything is just PyTrees and transformations on PyTrees, and secondly, provides full compatibility with the main JAX library itself, enhancing integrability with the broader ecosystem of third-party JAX libraries. To achieve this, our core idea is represent all model objects via an immutable PyTree. Here the leaves of the PyTree represent the parameters that are to be trained, and we describe their domain and trainable status as <code>dataclass</code> metadata.</p> <p>For our RBF kernel we have two parameters; the lengthscale and the variance. Both of these have positive domains, and by default we want to train both of these parameters. To encode this we use a <code>param_field</code>, where we can define the domain of both parameters via a <code>Softplus</code> bijector (that restricts them to the positive domain), and set their trainable status to <code>True</code>.</p> <pre><code>import tensorflow_probability.substrates.jax.bijectors as tfb\nfrom gpjax.base import Module, param_field\n@dataclass\nclass RBF(Module):\nlengthscale: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\nvariance: float = param_field(1.0, bijector=tfb.Softplus(), trainable=True)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>Here <code>param_field</code> is just a special type of <code>dataclasses.field</code>. As such the following:</p> <pre><code>param_field(1.0, bijector= tfb.Identity(), trainable=False)\n</code></pre> <p>is equivalent to the following <code>dataclasses.field</code></p> <pre><code>field(default=1.0, metadata={\"trainable\": False, \"bijector\": tfb.Identity()})\n</code></pre> <p>By default unmarked leaf attributes default to an <code>Identity</code> bijector and trainablility set to <code>True</code>.</p>"},{"location":"examples/pytrees/#replacing-values","title":"Replacing values","text":"<p>For consistency with JAX\u2019s functional programming principles, <code>Module</code> instances are immutable. PyTree nodes can be changed out-of-place via the <code>replace</code> method.</p> <p><pre><code>kernel = RBF()\nkernel = kernel.replace(lengthscale=3.14)  # Update e.g., the lengthscale.\nprint(kernel)\n</code></pre> <pre><code>RBF(lengthscale=3.14, variance=1.0)\n</code></pre></p>"},{"location":"examples/pytrees/#transformations","title":"Transformations \ud83e\udd16","text":"<p>Use <code>constrain</code> / <code>unconstrain</code> to return a <code>Module</code> with each parameter's bijector <code>forward</code> / <code>inverse</code> operation applied!</p> <p><pre><code># Transform kernel to unconstrained space\nunconstrained_kernel = kernel.unconstrain()\nprint(unconstrained_kernel)\n# Transform kernel back to constrained space\nkernel = unconstrained_kernel.constrain()\nprint(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(3.0957527, dtype=float32), variance=Array(0.54132485, dtype=float32))\nRBF(lengthscale=Array(3.14, dtype=float32), variance=Array(1., dtype=float32))\n</code></pre></p> <p>Default transformations can be replaced on an instance via the <code>replace_bijector</code> method.</p> <p><pre><code>new_kernel = kernel.replace_bijector(lengthscale=tfb.Identity())\n# Transform kernel to unconstrained space\nunconstrained_kernel = new_kernel.unconstrain()\nprint(unconstrained_kernel)\n# Transform kernel back to constrained space\nnew_kernel = unconstrained_kernel.constrain()\nprint(new_kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(3.14, dtype=float32), variance=Array(0.54132485, dtype=float32))\nRBF(lengthscale=Array(3.14, dtype=float32), variance=Array(1., dtype=float32))\n</code></pre></p>"},{"location":"examples/pytrees/#trainability","title":"Trainability \ud83d\ude82","text":"<p>Recall the example earlier, where we wanted to take the gradient of the kernel with respect to its parameters \u2207\u2113,\u03c32k(1.0,2.0;\u2113,\u03c32)\\nabla_{\\ell, \\sigma^2} k(1.0, 2.0; \\ell,\\sigma^2)\u2207\u2113,\u03c32\u200bk(1.0,2.0;\u2113,\u03c32) at inputs x=1.0x=1.0x=1.0 and y=2.0y=2.0y=2.0. We can now confirm we can do this with the new <code>Module</code>.</p> <p><pre><code>kernel = RBF()\njax.grad(lambda kern: kern.covariance(1.0, 2.0))(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0.60653067, dtype=float32, weak_type=True), variance=Array(0.60653067, dtype=float32, weak_type=True))\n</code></pre></p> <p>During gradient learning of models, it can sometimes be useful to fix certain parameters during the optimisation routine. For this, JAX provides a <code>stop_gradient</code> operand to prevent the flow of gradients during forward or reverse-mode automatic differentiation, as illustrated below for a function f(x)=x2f(x) = x^2f(x)=x2.</p> <p><pre><code>from jax import lax\ndef f(x):\nx = lax.stop_gradient(x)\nreturn x**2\njax.grad(f)(1.0)\n</code></pre> <pre><code>Array(0., dtype=float32, weak_type=True)\n</code></pre></p> <p>We see that gradient return is <code>0.0</code> instead of <code>2.0</code> due to the stopping of the gradient. Analogous to this, we provide this functionality to gradient flows on our <code>Module</code> class, via a <code>stop_gradient</code> method.</p> <p>Setting a (leaf) parameter's trainability to false can be achieved via the <code>replace_trainable</code> method.</p> <p><pre><code>kernel = RBF()\nkernel = kernel.replace_trainable(lengthscale=False)\njax.grad(lambda kern: kern.stop_gradient().covariance(1.0, 2.0))(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0., dtype=float32, weak_type=True), variance=Array(0.60653067, dtype=float32, weak_type=True))\n</code></pre></p> <p>As expected, the gradient is zero for the lengthscale parameter.</p>"},{"location":"examples/pytrees/#static-fields","title":"Static fields","text":"<p>In machine learning, initialising model parameters from random points is a common practice because it helps to break the symmetry in the model and allows the optimization algorithm to explore different regions of the parameter space.</p> <p>We could cleanly do this within the RBF class via a <code>post_init</code> method as follows:</p> <p><pre><code>import jax.random as jr\nimport tensorflow_probability.substrates.jax.distributions as tfd\nfrom dataclasses import field\n@dataclass\nclass RBF(Module):\nlengthscale: float = param_field(\ninit=False, bijector=tfb.Softplus(), trainable=True\n)\nvariance: float = param_field(init=False, bijector=tfb.Softplus(), trainable=True)\nkey: jr.KeyArray = field(default_factory = lambda: jr.PRNGKey(42))\n# Note, for Python &lt;3.11 you may use the following:\n# key: jr.KeyArray = jr.PRNGKey(42)\ndef __post_init__(self):\n# Split key into two keys\nkey1, key2 = jr.split(self.key)\n# Sample from Gamma distribution to initialise lengthscale and variance\nself.lengthscale = tfd.Gamma(1.0, 0.1).sample(seed=key1)\nself.variance = tfd.Gamma(1.0, 0.1).sample(seed=key2)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\nkernel = RBF()\nprint(kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0.54950446, dtype=float32), variance=Array(2.8077831, dtype=float32), key=Array([ 0, 42], dtype=uint32))\n</code></pre></p> <p>So far so good. But however, if we now took our gradient again</p> <p><pre><code>try:\njax.grad(lambda kern: kern.stop_gradient().covariance(1.0, 2.0))(kernel)\nexcept TypeError as e:\nprint(e)\n</code></pre> <pre><code>grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got uint32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.\n</code></pre></p> <p>We observe that we get a TypeError because the key is not differentiable. We can fix this by using a <code>static_field</code> for defining our key attribute.</p> <p><pre><code>from gpjax.base import static_field\n@dataclass\nclass RBF(Module):\nlengthscale: float = param_field(\ninit=False, bijector=tfb.Softplus(), trainable=True\n)\nvariance: float = param_field(init=False, bijector=tfb.Softplus(), trainable=True)\nkey: jr.KeyArray = static_field(default_factory=lambda: jr.PRNGKey(42))\ndef __post_init__(self):\n# Split key into two keys\nkey1, key2 = jr.split(self.key)\n# Sample from Gamma distribution to initialise lengthscale and variance\nself.lengthscale = tfd.Gamma(1.0, 0.1).sample(seed=key1)\nself.variance = tfd.Gamma(1.0, 0.1).sample(seed=key2)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\nfixed_kernel = RBF()\nprint(fixed_kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(0.54950446, dtype=float32), variance=Array(2.8077831, dtype=float32), key=Array([ 0, 42], dtype=uint32))\n</code></pre></p> <p>So we get the same class as before. But this time</p> <p><pre><code>jax.grad(lambda kern: kern.stop_gradient().covariance(1.0, 2.0))(fixed_kernel)\n</code></pre> <pre><code>RBF(lengthscale=Array(3.230818, dtype=float32), variance=Array(0.19092491, dtype=float32), key=Array([ 0, 42], dtype=uint32))\n</code></pre></p> <p>What happened to get the result we wanted? The difference lies in the treatment of the key attribute as a PyTree leaf in the first example, which caused the gradient computation to fail. Examining the flattened PyTree's of both cases:</p> <p><pre><code>print(jax.tree_util.tree_flatten(fixed_kernel))\nprint(jax.tree_util.tree_flatten(kernel))\n</code></pre> <pre><code>([Array(0.54950446, dtype=float32), Array(2.8077831, dtype=float32)], PyTreeDef(CustomNode(RBF[(['lengthscale', 'variance'], [('key', Array([ 0, 42], dtype=uint32))])], [*, *])))\n([Array([ 0, 42], dtype=uint32), Array(0.54950446, dtype=float32), Array(2.8077831, dtype=float32)], PyTreeDef(CustomNode(RBF[(['key', 'lengthscale', 'variance'], [])], [*, *, *])))\n</code></pre></p> <p>We see that assigning <code>static_field</code> tells JAX not to regard the attribute as leaf of the PyTree.</p>"},{"location":"examples/pytrees/#metadata","title":"Metadata","text":"<p>To determine the parameter domain and trainable statuses of each parameter, the <code>Module</code> stores metadata for each leaf of the PyTree. This metadata is defined through a <code>dataclasses.field</code>. Thus, under the hood, we can define our <code>RBF</code> kernel object (equivalent to before) manually as follows:</p> <pre><code>from dataclasses import field\n@dataclass\nclass RBF(Module):\nlengthscale: float = field(\ndefault=1.0, metadata={\"bijector\": tfb.Softplus(), \"trainable\": True}\n)\nvariance: float = field(\ndefault=1.0, metadata={\"bijector\": tfb.Softplus(), \"trainable\": True}\n)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>Here the <code>metadata</code> in the <code>dataclasses.field</code>, defines the metadata we associate with each PyTree leaf. This metadata can be a dictionary of any attributes we wish to store about each leaf. For example, we could extend this further by introducing a <code>name</code> attribute:</p> <pre><code>from dataclasses import field\n@dataclass\nclass RBF(Module):\nlengthscale: float = field(\ndefault=1.0,\nmetadata={\"bijector\": tfb.Softplus(), \"trainable\": True, \"name\": \"lengthscale\"},\n)\nvariance: float = field(\ndefault=1.0,\nmetadata={\"bijector\": tfb.Softplus(), \"trainable\": True, \"name\": \"variance\"},\n)\ndef covariance(self, x: jax.Array, y: jax.Array) -&gt; jax.Array:\nreturn self.variance * jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2)\n</code></pre> <p>We can trace the metadata defined on the class via <code>meta_leaves</code>.</p> <p><pre><code>from gpjax.base import meta_leaves\nrbf = RBF()\nmeta_leaves(rbf)\n</code></pre> <pre><code>[({'bijector': &lt;tfp.bijectors.Softplus 'softplus' batch_shape=[] forward_min_event_ndims=0 inverse_min_event_ndims=0 dtype_x=? dtype_y=?&gt;,\n   'trainable': True,\n   'name': 'lengthscale'},\n  1.0),\n ({'bijector': &lt;tfp.bijectors.Softplus 'softplus' batch_shape=[] forward_min_event_ndims=0 inverse_min_event_ndims=0 dtype_x=? dtype_y=?&gt;,\n   'trainable': True,\n   'name': 'variance'},\n  1.0)]\n</code></pre></p> <p>Similar to <code>jax.tree_utils.tree_leaves</code>, this function returns a flattened PyTree. However, instead of just the values, it returns a list of tuples that contain both the metadata and value of each PyTree leaf. This traced metadata can be utilised for applying maps (how <code>constrain</code>, <code>unconstrain</code>, <code>stop_gradient</code> work), as described in the next section.</p>"},{"location":"examples/pytrees/#metamap","title":"Metamap","text":"<p>The <code>constrain</code>, <code>unconstrain</code>, and <code>stop_gradient</code> methods on the <code>Module</code> use a <code>meta_map</code> function under the hood. This function enables us to apply metadata functions to the PyTree leaves, making it a powerful tool.</p> <p>To achieve this, the function involves the same tracing as <code>meta_leaves</code> to create a flattened list of tuples consisting of (metadata, leaf value). However, it also allows us to apply a function to this list and return a new transformed PyTree, as demonstrated in the examples that follow.</p>"},{"location":"examples/pytrees/#filter-example","title":"Filter example:","text":"<p>A <code>meta_map</code> works similarly to <code>jax.tree_utils.tree_map</code>. However, it differs in that it allows us to define a function that operates on the tuple (metadata, leaf value). For example, we could use a function to filter based on a <code>name</code> attribute.</p> <p><pre><code>from gpjax.base import meta_map\ndef filter_lengthscale(meta_leaf):\nmeta, leaf = meta_leaf\nif meta.get(\"name\", None) == \"lengthscale\":\nreturn 3.14\nelse:\nreturn leaf\nprint(meta_map(filter_lengthscale, rbf))\n</code></pre> <pre><code>RBF(lengthscale=3.14, variance=1.0)\n</code></pre></p>"},{"location":"examples/pytrees/#how-constrain-works","title":"How <code>constrain</code> works:","text":"<p>To apply a constrain, we filter on the attribute \"bijector\", and apply a forward transformation to the PyTree leaf:</p> <p><pre><code># This is how constrain works! \u26cf\ndef _apply_constrain(meta_leaf):\nmeta, leaf = meta_leaf\nif meta is None:\nreturn leaf\nreturn meta.get(\"bijector\", tfb.Identity()).forward(leaf)\nmeta_map(_apply_constrain, rbf)\n</code></pre> <pre><code>RBF(lengthscale=Array(1.3132617, dtype=float32), variance=Array(1.3132617, dtype=float32))\n</code></pre></p> <p>As expected, we find the same result as calling <code>rbf.constrain()</code>.</p>"},{"location":"examples/regression/","title":"Regression","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nfrom docs.examples.utils import clean_legend\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox from docs.examples.utils import clean_legend  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>n = 100\nnoise = 0.3\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\n\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\nytest = f(xtest)\n</pre> n = 100 noise = 0.3  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise  D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1) ytest = f(xtest) <p>To better understand what we have simulated, we plot both the underlying latent function and the observed data that is subject to Gaussian noise.</p> In\u00a0[3]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\nax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\nax.legend(loc=\"best\")\n</pre> fig, ax = plt.subplots() ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0]) ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1]) ax.legend(loc=\"best\") Out[3]: <pre>&lt;matplotlib.legend.Legend at 0x7ff2c4e99d30&gt;</pre> <p>Our aim in this tutorial will be to reconstruct the latent function from our noisy observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a Gaussian process prior in the next section.</p> In\u00a0[4]: Copied! <pre>kernel = gpx.kernels.RBF()\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\n</pre> kernel = gpx.kernels.RBF() meanf = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel) <p>The above construction forms the foundation for GPJax's models. Moreover, the GP prior we have just defined can be represented by a TensorFlow Probability multivariate Gaussian distribution. Such functionality enables trivial sampling, and the evaluation of the GP's mean and covariance .</p> In\u00a0[5]: Copied! <pre>prior_dist = prior.predict(xtest)\n\nprior_mean = prior_dist.mean()\nprior_std = prior_dist.variance()\nsamples = prior_dist.sample(seed=key, sample_shape=(20,))\n\n\nfig, ax = plt.subplots()\nax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\nax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\nax.fill_between(\n    xtest.flatten(),\n    prior_mean - prior_std,\n    prior_mean + prior_std,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Prior variance\",\n)\nax.legend(loc=\"best\")\nax = clean_legend(ax)\n</pre> prior_dist = prior.predict(xtest)  prior_mean = prior_dist.mean() prior_std = prior_dist.variance() samples = prior_dist.sample(seed=key, sample_shape=(20,))   fig, ax = plt.subplots() ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\") ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\") ax.fill_between(     xtest.flatten(),     prior_mean - prior_std,     prior_mean + prior_std,     alpha=0.3,     color=cols[1],     label=\"Prior variance\", ) ax.legend(loc=\"best\") ax = clean_legend(ax) In\u00a0[6]: Copied! <pre>likelihood = gpx.Gaussian(num_datapoints=D.n)\n</pre> likelihood = gpx.Gaussian(num_datapoints=D.n) <p>The posterior is proportional to the prior multiplied by the likelihood, written as</p> <p>$$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$</p> <p>Mimicking this construct, the posterior is established in GPJax through the <code>*</code> operator.</p> In\u00a0[7]: Copied! <pre>posterior = prior * likelihood\n</pre> posterior = prior * likelihood In\u00a0[8]: Copied! <pre>negative_mll = gpx.objectives.ConjugateMLL(negative=True)\nnegative_mll(posterior, train_data=D)\n\n\n# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n# optim = ox.chain(\n#     ox.adam(learning_rate=0.01),\n#     ox.masked(ox.set_to_zero(), static_tree)\n#     )\n</pre> negative_mll = gpx.objectives.ConjugateMLL(negative=True) negative_mll(posterior, train_data=D)   # static_tree = jax.tree_map(lambda x: not(x), posterior.trainables) # optim = ox.chain( #     ox.adam(learning_rate=0.01), #     ox.masked(ox.set_to_zero(), static_tree) #     ) Out[8]: <pre>Array(124.80517341, dtype=float64)</pre> <p>For researchers, GPJax has the capacity to print the bibtex citation for objects such as the marginal log-likelihood through the <code>cite()</code> function.</p> In\u00a0[9]: Copied! <pre>print(gpx.cite(negative_mll))\n</pre> print(gpx.cite(negative_mll)) <pre>@book{rasmussen2006gaussian,\nauthors = {Rasmussen, Carl Edward and Williams, Christopher K},\ntitle = {Gaussian Processes for Machine Learning},\nyear = {2006},\npublisher = {MIT press Cambridge, MA},\nvolume = {2},\n}\n</pre> <p>JIT-compiling expensive-to-compute functions such as the marginal log-likelihood is advisable. This can be achieved by wrapping the function in <code>jax.jit()</code>.</p> In\u00a0[10]: Copied! <pre>negative_mll = jit(negative_mll)\n</pre> negative_mll = jit(negative_mll) <p>Since most optimisers (including here) minimise a given function, we have realised the negative marginal log-likelihood and just-in-time (JIT) compiled this to accelerate training.</p> <p>We can now define an optimiser with <code>optax</code>. For this example we'll use the <code>adam</code> optimiser.</p> In\u00a0[11]: Copied! <pre>opt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=500,\n    safe=True,\n    key=key,\n)\n</pre> opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=500,     safe=True,     key=key, ) <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> <p>The calling of <code>fit</code> returns two objects: the optimised posterior and a history of training losses. We can plot the training loss to see how the optimisation has progressed.</p> In\u00a0[12]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(history, color=cols[1])\nax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\")\n</pre> fig, ax = plt.subplots() ax.plot(history, color=cols[1]) ax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\") Out[12]: <pre>[Text(0.5, 0, 'Training iteration'),\n Text(0, 0.5, 'Negative marginal log likelihood')]</pre> In\u00a0[13]: Copied! <pre>latent_dist = opt_posterior.predict(xtest, train_data=D)\npredictive_dist = opt_posterior.likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior.predict(xtest, train_data=D) predictive_dist = opt_posterior.likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_std = predictive_dist.stddev() <p>With the predictions and their uncertainty acquired, we illustrate the GP's performance at explaining the data $\\mathcal{D}$ and recovering the underlying latent function of interest.</p> In\u00a0[14]: Copied! <pre>fig, ax = plt.subplots(figsize=(7.5, 2.5))\nax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    label=\"Two sigma\",\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean - 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest,\n    predictive_mean + 2 * predictive_std,\n    linestyle=\"--\",\n    linewidth=1,\n    color=cols[1],\n)\nax.plot(\n    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n)\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\nax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n</pre> fig, ax = plt.subplots(figsize=(7.5, 2.5)) ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5) ax.fill_between(     xtest.squeeze(),     predictive_mean - 2 * predictive_std,     predictive_mean + 2 * predictive_std,     alpha=0.2,     label=\"Two sigma\",     color=cols[1], ) ax.plot(     xtest,     predictive_mean - 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest,     predictive_mean + 2 * predictive_std,     linestyle=\"--\",     linewidth=1,     color=cols[1], ) ax.plot(     xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2 ) ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1]) ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5)) Out[14]: <pre>&lt;matplotlib.legend.Legend at 0x7ff2b0eddd00&gt;</pre> In\u00a0[15]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder &amp; Daniel Dodd' <pre>Author: Thomas Pinder &amp; Daniel Dodd\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\ngpjax     : 0.0.0\njax       : 0.4.9\nmatplotlib: 3.7.1\noptax     : 0.1.5\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/regression/#regression","title":"Regression\u00b6","text":"<p>In this notebook we demonstate how to fit a Gaussian process regression model.</p>"},{"location":"examples/regression/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs and labels for later.</p>"},{"location":"examples/regression/#defining-the-prior","title":"Defining the prior\u00b6","text":"<p>A zero-mean Gaussian process (GP) places a prior distribution over real-valued functions $f(\\cdot)$ where $f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$ for any finite collection of inputs $\\boldsymbol{x}$.</p> <p>Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$ with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$. The choice of kernel function is critical as, among other things, it governs the smoothness of the outputs that our GP can generate.</p> <p>For simplicity, we consider a radial basis function (RBF) kernel: $$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$</p> <p>On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$, we can reciprocate this process in GPJax via defining a <code>Prior</code> with our chosen <code>RBF</code> kernel.</p>"},{"location":"examples/regression/#constructing-the-posterior","title":"Constructing the posterior\u00b6","text":"<p>Having defined our GP, we proceed to define a description of our data $\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of likelihood is a critical in Bayesian modelling, for simplicity we consider a Gaussian with noise parameter $\\alpha$ $$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$ This is defined in GPJax through calling a <code>Gaussian</code> instance.</p>"},{"location":"examples/regression/#parameter-state","title":"Parameter state\u00b6","text":"<p>As outlined in the PyTrees documentation, parameters are contained within the model and for the leaves of the PyTree. Consequently, in this particular model, we have three parameters: the kernel lengthscale, kernel variance and the observation noise variance. Whilst we have initialised each of these to 1, we can learn Type 2 MLEs for each of these parameters by optimising the marginal log-likelihood (MLL).</p>"},{"location":"examples/regression/#prediction","title":"Prediction\u00b6","text":"<p>Equipped with the posterior and a set of optimised hyperparameter values, we are now in a position to query our GP's predictive distribution at novel test inputs. To do this, we use our defined <code>posterior</code> and <code>likelihood</code> at our test inputs to obtain the predictive distribution as a <code>Distrax</code> multivariate Gaussian upon which <code>mean</code> and <code>stddev</code> can be used to extract the predictive mean and standard deviatation.</p>"},{"location":"examples/regression/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/spatial/","title":"Pathwise Sampling for Spatial Modelling","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom dataclasses import dataclass\n\nimport fsspec\nimport geopandas as gpd\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import (\n    Array,\n    Float,\n    install_import_hook,\n)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport pandas as pd\nimport planetary_computer\nimport pystac_client\nimport rioxarray as rio\nfrom rioxarray.merge import merge_arrays\nimport xarray as xr\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    from gpjax.base import param_field\n    from gpjax.dataset import Dataset\n\n\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\n# Observed temperature data\ntry:\n    temperature = pd.read_csv(\"data/max_tempeature_switzerland.csv\")\nexcept FileNotFoundError:\n    temperature = pd.read_csv(\"docs/examples/data/max_tempeature_switzerland.csv\")\n\ntemperature = gpd.GeoDataFrame(\n    temperature,\n    geometry=gpd.points_from_xy(temperature.longitude, temperature.latitude),\n).dropna(how=\"any\")\n\n# Country borders shapefile\npath = \"simplecache::https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\"\nwith fsspec.open(path) as file:\n    ch_shp = gpd.read_file(file).query(\"ADMIN == 'Switzerland'\")\n\n\n# Read DEM data and clip it to switzerland\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\nsearch = catalog.search(collections=[\"cop-dem-glo-90\"], bbox=[5.5, 45.5, 10.0, 48.5])\nitems = list(search.get_all_items())\ntiles = [rio.open_rasterio(i.assets[\"data\"].href).squeeze().drop(\"band\") for i in items]\ndem = merge_arrays(tiles).coarsen(x=10, y=10).mean().rio.clip(ch_shp[\"geometry\"])\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from dataclasses import dataclass  import fsspec import geopandas as gpd import jax import jax.numpy as jnp import jax.random as jr from jaxtyping import (     Array,     Float,     install_import_hook, ) import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox import pandas as pd import planetary_computer import pystac_client import rioxarray as rio from rioxarray.merge import merge_arrays import xarray as xr  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     from gpjax.base import param_field     from gpjax.dataset import Dataset   key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]  # Observed temperature data try:     temperature = pd.read_csv(\"data/max_tempeature_switzerland.csv\") except FileNotFoundError:     temperature = pd.read_csv(\"docs/examples/data/max_tempeature_switzerland.csv\")  temperature = gpd.GeoDataFrame(     temperature,     geometry=gpd.points_from_xy(temperature.longitude, temperature.latitude), ).dropna(how=\"any\")  # Country borders shapefile path = \"simplecache::https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\" with fsspec.open(path) as file:     ch_shp = gpd.read_file(file).query(\"ADMIN == 'Switzerland'\")   # Read DEM data and clip it to switzerland catalog = pystac_client.Client.open(     \"https://planetarycomputer.microsoft.com/api/stac/v1\",     modifier=planetary_computer.sign_inplace, ) search = catalog.search(collections=[\"cop-dem-glo-90\"], bbox=[5.5, 45.5, 10.0, 48.5]) items = list(search.get_all_items()) tiles = [rio.open_rasterio(i.assets[\"data\"].href).squeeze().drop(\"band\") for i in items] dem = merge_arrays(tiles).coarsen(x=10, y=10).mean().rio.clip(ch_shp[\"geometry\"]) <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <p>Let us take a look at the data. The topography of Switzerland is quite complex, and there are sometimes very large height differences over short distances. This measuring network is fairly dense, and you may already notice that there's a dependency between maximum daily temperature and elevation.</p> In\u00a0[2]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 5), layout=\"constrained\")\ndem.plot(\n    cmap=\"terrain\", cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Elevation [m]\"}\n)\ntemperature.plot(\"t_max\", ax=ax, cmap=\"RdBu_r\", vmin=-15, vmax=15, edgecolor=\"k\", s=50)\nax.set(title=\"Switzerland's topography and SwissMetNet stations\", aspect=\"auto\")\ncb = fig.colorbar(ax.collections[-1], aspect=50, pad=0.02)\ncb.set_label(\"Max. daily temperature [\u00b0C]\", labelpad=-2)\n</pre> fig, ax = plt.subplots(figsize=(8, 5), layout=\"constrained\") dem.plot(     cmap=\"terrain\", cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Elevation [m]\"} ) temperature.plot(\"t_max\", ax=ax, cmap=\"RdBu_r\", vmin=-15, vmax=15, edgecolor=\"k\", s=50) ax.set(title=\"Switzerland's topography and SwissMetNet stations\", aspect=\"auto\") cb = fig.colorbar(ax.collections[-1], aspect=50, pad=0.02) cb.set_label(\"Max. daily temperature [\u00b0C]\", labelpad=-2) <p>As always, we store our training data in a <code>Dataset</code> object.</p> In\u00a0[3]: Copied! <pre>x = temperature[[\"latitude\", \"longitude\", \"elevation\"]].values\ny = temperature[[\"t_max\"]].values\nD = Dataset(\n    X=jnp.array(x),\n    y=jnp.array(y),\n)\n</pre> x = temperature[[\"latitude\", \"longitude\", \"elevation\"]].values y = temperature[[\"t_max\"]].values D = Dataset(     X=jnp.array(x),     y=jnp.array(y), ) In\u00a0[4]: Copied! <pre>kernel = gpx.kernels.RBF(\n    active_dims=[0, 1, 2],\n    lengthscale=jnp.array([0.1, 0.1, 100.0]),\n)\n</pre> kernel = gpx.kernels.RBF(     active_dims=[0, 1, 2],     lengthscale=jnp.array([0.1, 0.1, 100.0]), ) In\u00a0[5]: Copied! <pre>@dataclass\nclass MeanFunction(gpx.gps.AbstractMeanFunction):\n    w: Float[Array, \"1\"] = param_field(jnp.array([0.0]))\n    b: Float[Array, \"1\"] = param_field(jnp.array([0.0]))\n\n    def __call__(self, x: Float[Array, \"N D\"]) -&gt; Float[Array, \"N 1\"]:\n        elevation = x[:, 2:3]\n        out = elevation * self.w + self.b\n        return out\n</pre> @dataclass class MeanFunction(gpx.gps.AbstractMeanFunction):     w: Float[Array, \"1\"] = param_field(jnp.array([0.0]))     b: Float[Array, \"1\"] = param_field(jnp.array([0.0]))      def __call__(self, x: Float[Array, \"N D\"]) -&gt; Float[Array, \"N 1\"]:         elevation = x[:, 2:3]         out = elevation * self.w + self.b         return out <p>Now we can define our prior. We'll also choose a Gaussian likelihood.</p> In\u00a0[6]: Copied! <pre>mean_function = MeanFunction()\nprior = gpx.Prior(kernel=kernel, mean_function=mean_function)\nlikelihood = gpx.Gaussian(D.n)\n</pre> mean_function = MeanFunction() prior = gpx.Prior(kernel=kernel, mean_function=mean_function) likelihood = gpx.Gaussian(D.n) <p>Finally, we construct the posterior.</p> In\u00a0[7]: Copied! <pre>posterior = prior * likelihood\n</pre> posterior = prior * likelihood In\u00a0[8]: Copied! <pre>negative_mll = jax.jit(gpx.objectives.ConjugateMLL(negative=True))\nnegative_mll(posterior, train_data=D)\n</pre> negative_mll = jax.jit(gpx.objectives.ConjugateMLL(negative=True)) negative_mll(posterior, train_data=D) Out[8]: <pre>Array(2096.78016314, dtype=float64)</pre> In\u00a0[9]: Copied! <pre>optim = ox.chain(ox.adam(learning_rate=0.1), ox.clip(1.0))\nposterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=D,\n    optim=optim,\n    num_iters=3000,\n    safe=True,\n    key=key,\n)\nposterior: gpx.gps.ConjugatePosterior\n</pre> optim = ox.chain(ox.adam(learning_rate=0.1), ox.clip(1.0)) posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=D,     optim=optim,     num_iters=3000,     safe=True,     key=key, ) posterior: gpx.gps.ConjugatePosterior <pre>  0%|          | 0/3000 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre># select the target pixels and exclude nans\nxtest = dem.drop(\"spatial_ref\").stack(p=[\"y\", \"x\"]).to_dataframe(name=\"dem\")\nmask = jnp.any(jnp.isnan(xtest.values), axis=-1)\n\n# generate 50 samples\nytest = posterior.sample_approx(50, D, key, num_features=200)(\n    jnp.array(xtest.values[~mask])\n)\n</pre> # select the target pixels and exclude nans xtest = dem.drop(\"spatial_ref\").stack(p=[\"y\", \"x\"]).to_dataframe(name=\"dem\") mask = jnp.any(jnp.isnan(xtest.values), axis=-1)  # generate 50 samples ytest = posterior.sample_approx(50, D, key, num_features=200)(     jnp.array(xtest.values[~mask]) ) <p>Let's take a look at the results. We start with the mean and standard deviation.</p> In\u00a0[11]: Copied! <pre>predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan\npredtest[~mask] = ytest.mean(axis=-1)\npredtest = predtest.unstack()\n\npredtest.plot(\n    vmin=-15.0,\n    vmax=15.0,\n    cmap=\"RdBu_r\",\n    cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"},\n)\nplt.gca().set_title(\"Interpolated maximum daily temperature\")\n</pre> predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan predtest[~mask] = ytest.mean(axis=-1) predtest = predtest.unstack()  predtest.plot(     vmin=-15.0,     vmax=15.0,     cmap=\"RdBu_r\",     cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"}, ) plt.gca().set_title(\"Interpolated maximum daily temperature\") Out[11]: <pre>Text(0.5, 1.0, 'Interpolated maximum daily temperature')</pre> In\u00a0[12]: Copied! <pre>predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan\npredtest[~mask] = ytest.std(axis=-1)\npredtest = predtest.unstack()\n\n# plot\npredtest.plot(\n    cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Standard deviation [\u00b0C]\"},\n)\nplt.gca().set_title(\"Standard deviation\")\n</pre> predtest = xr.zeros_like(dem.stack(p=[\"y\", \"x\"])) * jnp.nan predtest[~mask] = ytest.std(axis=-1) predtest = predtest.unstack()  # plot predtest.plot(     cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Standard deviation [\u00b0C]\"}, ) plt.gca().set_title(\"Standard deviation\") Out[12]: <pre>Text(0.5, 1.0, 'Standard deviation')</pre> <p>And now some individual realizations of our GP posterior.</p> In\u00a0[13]: Copied! <pre>predtest = (\n    xr.zeros_like(dem.stack(p=[\"y\", \"x\"]))\n    .expand_dims(realization=range(9))\n    .transpose(\"p\", \"realization\")\n    .copy()\n)\npredtest[~mask] = ytest[:, :9]\npredtest = predtest.unstack()\npredtest.plot(\n    col=\"realization\",\n    col_wrap=3,\n    cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"},\n)\n</pre> predtest = (     xr.zeros_like(dem.stack(p=[\"y\", \"x\"]))     .expand_dims(realization=range(9))     .transpose(\"p\", \"realization\")     .copy() ) predtest[~mask] = ytest[:, :9] predtest = predtest.unstack() predtest.plot(     col=\"realization\",     col_wrap=3,     cbar_kwargs={\"aspect\": 50, \"pad\": 0.02, \"label\": \"Max. daily temperature [\u00b0C]\"}, ) <pre>/usr/share/miniconda/envs/test/lib/python3.8/site-packages/xarray/plot/facetgrid.py:668: UserWarning: The figure layout has changed to tight\n  self.fig.tight_layout()\n</pre> Out[13]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x7f39e4277ac0&gt;</pre> <p>Remember when we said that on average the temperature decreases with height at a rate of approximately -6.5\u00b0C/km? That's -0.0065\u00b0C/m. The <code>w</code> parameter of our mean function is very close: we have learned the environmental lapse rate!</p> In\u00a0[14]: Copied! <pre>print(posterior.prior.mean_function)\n</pre> print(posterior.prior.mean_function) <pre>MeanFunction(w=Array([-0.00623273], dtype=float64), b=Array([11.92324253], dtype=float64))\n</pre> <p>That's it! We've successfully interpolated an observed meteorological parameter on a grid. We have used several components of GPJax and adapted them to our needs: a custom mean function that modelled the average temperature lapse rate; an ARD kernel that learned to give more relevance to elevation rather than horizontal distance; an efficient sampling technique to produce probabilistic realizations of our posterior on a large number of test points, which is important for many spatiotemporal modelling applications. If you're interested in a more elaborate work on temperature interpolation for the same domain used here, refer to Frei 2014.</p> In\u00a0[15]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Francesco Zanetta'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Francesco Zanetta' <pre>Author: Francesco Zanetta\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\npystac_client     : 0.6.1\npandas            : 1.5.3\nrioxarray         : 0.13.4\nfsspec            : 2023.5.0\njax               : 0.4.9\ngeopandas         : 0.12.2\noptax             : 0.1.5\ngpjax             : 0.0.0\nxarray            : 2023.1.0\nplanetary_computer: 0.5.1\nmatplotlib        : 3.7.1\n\nWatermark: 2.3.1\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/spatial/#pathwise-sampling-for-spatial-modelling","title":"Pathwise Sampling for Spatial Modelling\u00b6","text":"<p>In this notebook, we demonstrate an application of Gaussian Processes to a spatial interpolation problem. We will show how to efficiently sample from a GP posterior as shown in .</p>"},{"location":"examples/spatial/#data-loading","title":"Data loading\u00b6","text":"<p>We'll use open-source data from SwissMetNet, the surface weather monitoring network of the Swiss national weather service, and digital elevation model (DEM) data from Copernicus, accessible here via the Planetary Computer data catalog. We will coarsen this data by a factor of 10 (going from 90m to 900m resolution), but feel free to change this.</p> <p>Our variable of interest is the maximum daily temperature, observed on the 4th of April 2023 at 150 weather stations, and we'll try to interpolate it on a spatial grid using geographical coordinates (latitude and longitude) and elevation as input variables.</p>"},{"location":"examples/spatial/#ard-kernel","title":"ARD Kernel\u00b6","text":"<p>As temperature decreases with height (at a rate of approximately -6.5 \u00b0C/km in average conditions), we can expect that using the geographical distance alone isn't enough to to a decent job at interpolating this data. Therefore, we can also use elevation and optimize the parameters of our kernel such that more relevance should be given to elevation. This is possible by using a kernel that has one length-scale parameter per input dimension: an automatic relevance determination (ARD) kernel. See our kernel notebook for more an introduction to kernels in GPJax.</p>"},{"location":"examples/spatial/#mean-function","title":"Mean function\u00b6","text":"<p>As stated before, we already know that temperature strongly depends on elevation. So why not use it for our mean function? GPJax lets you define custom mean functions; simply subclass <code>AbstractMeanFunction</code>.</p>"},{"location":"examples/spatial/#model-fitting","title":"Model fitting\u00b6","text":"<p>We proceed to train our model. Because we used a Gaussian likelihood, the resulting posterior is a <code>ConjugatePosterior</code>, which allows us to optimize the analytically expressed marginal loglikelihood.</p> <p>As always, we can jit-compile the objective function to speed things up.</p>"},{"location":"examples/spatial/#sampling-on-a-grid","title":"Sampling on a grid\u00b6","text":"<p>Now comes the cool part. In a standard GP implementation, for n test points, we have a $\\mathcal{O}(n^2)$ computational complexity and $\\mathcal{O}(n^2)$ memory requirement. We want to make predictions on a total of roughly 70'000 pixels, and that would require us to compute a covariance matrix of <code>70000 ** 2 = 4900000000</code> elements. If these are <code>float64</code>s, as it is often the case in GPJax, it would be equivalent to more than 36 Gigabytes of memory. And that's for a fairly coarse and tiny grid. If we were to make predictions on a 1000x1000 grid, the total memory required would be 8 Terabytes of memory, which is intractable. Fortunately, the pathwise conditioning method allows us to sample from our posterior in linear complexity, $\\mathcal{O}(n)$, with the number of pixels.</p> <p>GPJax provides the <code>sample_approx</code> method to generate random conditioned samples from our posterior.</p>"},{"location":"examples/spatial/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/uncollapsed_vi/","title":"Sparse Stochastic Variational Inference","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.numpy as jnp\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport optax as ox\nimport tensorflow_probability.substrates.jax as tfp\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n    import gpjax.kernels as jk\n\nkey = jr.PRNGKey(123)\ntfb = tfp.bijectors\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.numpy as jnp import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import optax as ox import tensorflow_probability.substrates.jax as tfp  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx     import gpjax.kernels as jk  key = jr.PRNGKey(123) tfb = tfp.bijectors plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>n = 50000\nnoise = 0.2\n\nkey, subkey = jr.split(key)\nx = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1)\nf = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\nsignal = f(x)\ny = signal + jr.normal(subkey, shape=signal.shape) * noise\nD = gpx.Dataset(X=x, y=y)\n\nxtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1)\n</pre> n = 50000 noise = 0.2  key, subkey = jr.split(key) x = jr.uniform(key=key, minval=-5.0, maxval=5.0, shape=(n,)).reshape(-1, 1) f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x) signal = f(x) y = signal + jr.normal(subkey, shape=signal.shape) * noise D = gpx.Dataset(X=x, y=y)  xtest = jnp.linspace(-5.5, 5.5, 500).reshape(-1, 1) In\u00a0[3]: Copied! <pre>z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.vlines(\n    z,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\")\nax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\")\nax.legend()\nax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\n</pre> z = jnp.linspace(-5.0, 5.0, 50).reshape(-1, 1)  fig, ax = plt.subplots() ax.vlines(     z,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=1,     label=\"Inducing point\",     color=cols[2], ) ax.scatter(x, y, alpha=0.2, color=cols[0], label=\"Observations\") ax.plot(xtest, f(xtest), color=cols[1], label=\"Latent function\") ax.legend() ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\") Out[3]: <pre>[Text(0.5, 0, '$x$'), Text(0, 0.5, '$f(x)$')]</pre> <p>The inducing inputs will summarise our dataset, and since they are treated as variational parameters, their locations will be optimised. The next step to SVGP is to define a variational family.</p> In\u00a0[4]: Copied! <pre>meanf = gpx.mean_functions.Zero()\nlikelihood = gpx.Gaussian(num_datapoints=n)\nprior = gpx.Prior(mean_function=meanf, kernel=jk.RBF())\np = prior * likelihood\nq = gpx.VariationalGaussian(posterior=p, inducing_inputs=z)\n</pre> meanf = gpx.mean_functions.Zero() likelihood = gpx.Gaussian(num_datapoints=n) prior = gpx.Prior(mean_function=meanf, kernel=jk.RBF()) p = prior * likelihood q = gpx.VariationalGaussian(posterior=p, inducing_inputs=z) <p>Here, the variational process $q(\\cdot)$ depends on the prior through $p(f(\\cdot)|f(\\boldsymbol{z}))$ in $(\\times)$.</p> In\u00a0[5]: Copied! <pre>negative_elbo = gpx.ELBO(negative=True)\n</pre> negative_elbo = gpx.ELBO(negative=True) <p>For researchers, GPJax has the capacity to print the bibtex citation for objects such as the ELBO through the <code>cite()</code> function.</p> In\u00a0[6]: Copied! <pre>print(gpx.cite(negative_elbo))\n</pre> print(gpx.cite(negative_elbo)) <pre>@article{hensman2013gaussian,\nauthors = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},\ntitle = {Gaussian Processes for Big Data},\nyear = {2013},\nbooktitle = {Uncertainty in Artificial Intelligence},\n}\n</pre> <p>JIT-compiling expensive-to-compute functions such as the ELBO is advisable. This can be achieved by wrapping the function in <code>jax.jit()</code>.</p> In\u00a0[7]: Copied! <pre>negative_elbo = jit(negative_elbo)\n</pre> negative_elbo = jit(negative_elbo) In\u00a0[8]: Copied! <pre>schedule = ox.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=0.01,\n    warmup_steps=75,\n    decay_steps=1500,\n    end_value=0.001,\n)\n\nopt_posterior, history = gpx.fit(\n    model=q,\n    objective=negative_elbo,\n    train_data=D,\n    optim=ox.adam(learning_rate=schedule),\n    num_iters=3000,\n    key=jr.PRNGKey(42),\n    batch_size=128,\n)\n</pre> schedule = ox.warmup_cosine_decay_schedule(     init_value=0.0,     peak_value=0.01,     warmup_steps=75,     decay_steps=1500,     end_value=0.001, )  opt_posterior, history = gpx.fit(     model=q,     objective=negative_elbo,     train_data=D,     optim=ox.adam(learning_rate=schedule),     num_iters=3000,     key=jr.PRNGKey(42),     batch_size=128, ) <pre>  0%|          | 0/3000 [00:00&lt;?, ?it/s]</pre> In\u00a0[9]: Copied! <pre>latent_dist = opt_posterior(xtest)\npredictive_dist = opt_posterior.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.vlines(\n    opt_posterior.inducing_inputs,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\n</pre> latent_dist = opt_posterior(xtest) predictive_dist = opt_posterior.posterior.likelihood(latent_dist)  meanf = predictive_dist.mean() sigma = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0]) ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1]) ax.fill_between(     xtest.flatten(),     meanf - 2 * sigma,     meanf + 2 * sigma,     alpha=0.3,     color=cols[1],     label=\"Two sigma\", ) ax.vlines(     opt_posterior.inducing_inputs,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=1,     label=\"Inducing point\",     color=cols[2], ) ax.legend() Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x7f7b44857790&gt;</pre> In\u00a0[10]: Copied! <pre>triangular_transform = tfb.FillScaleTriL(\n    diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter)\n)\nreparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform)\n</pre> triangular_transform = tfb.FillScaleTriL(     diag_bijector=tfb.Square(), diag_shift=jnp.array(q.jitter) ) reparameterised_q = q.replace_bijector(variational_root_covariance=triangular_transform) In\u00a0[11]: Copied! <pre>opt_rep, history = gpx.fit(\n    model=reparameterised_q,\n    objective=negative_elbo,\n    train_data=D,\n    optim=ox.adam(learning_rate=0.01),\n    num_iters=3000,\n    key=jr.PRNGKey(42),\n    batch_size=128,\n)\n</pre> opt_rep, history = gpx.fit(     model=reparameterised_q,     objective=negative_elbo,     train_data=D,     optim=ox.adam(learning_rate=0.01),     num_iters=3000,     key=jr.PRNGKey(42),     batch_size=128, ) <pre>  0%|          | 0/3000 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre>latent_dist = opt_rep(xtest)\npredictive_dist = opt_rep.posterior.likelihood(latent_dist)\n\nmeanf = predictive_dist.mean()\nsigma = predictive_dist.stddev()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0])\nax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1])\nax.fill_between(\n    xtest.flatten(),\n    meanf - 2 * sigma,\n    meanf + 2 * sigma,\n    alpha=0.3,\n    color=cols[1],\n    label=\"Two sigma\",\n)\nax.vlines(\n    opt_rep.inducing_inputs,\n    ymin=y.min(),\n    ymax=y.max(),\n    alpha=0.3,\n    linewidth=1,\n    label=\"Inducing point\",\n    color=cols[2],\n)\nax.legend()\n</pre> latent_dist = opt_rep(xtest) predictive_dist = opt_rep.posterior.likelihood(latent_dist)  meanf = predictive_dist.mean() sigma = predictive_dist.stddev()  fig, ax = plt.subplots() ax.scatter(x, y, alpha=0.15, label=\"Training Data\", color=cols[0]) ax.plot(xtest, meanf, label=\"Posterior mean\", color=cols[1]) ax.fill_between(     xtest.flatten(),     meanf - 2 * sigma,     meanf + 2 * sigma,     alpha=0.3,     color=cols[1],     label=\"Two sigma\", ) ax.vlines(     opt_rep.inducing_inputs,     ymin=y.min(),     ymax=y.max(),     alpha=0.3,     linewidth=1,     label=\"Inducing point\",     color=cols[2], ) ax.legend() Out[12]: <pre>&lt;matplotlib.legend.Legend at 0x7f7b44933670&gt;</pre> <p>We can see that <code>Square</code> transformation is able to get relatively better fit compared to <code>Softplus</code> with the same number of iterations, but <code>Softplus</code> is recommended over <code>Square</code> for stability of optimisation.</p> In\u00a0[13]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder, Daniel Dodd &amp; Zeel B Patel' <pre>Author: Thomas Pinder, Daniel Dodd &amp; Zeel B Patel\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\ntensorflow_probability: 0.19.0\njax                   : 0.4.9\nmatplotlib            : 3.7.1\ngpjax                 : 0.0.0\noptax                 : 0.1.5\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/uncollapsed_vi/#sparse-stochastic-variational-inference","title":"Sparse Stochastic Variational Inference\u00b6","text":"<p>In this notebook we demonstrate how to implement sparse variational Gaussian processes (SVGPs) of Hensman et al. (2015). In particular, this approximation framework provides a tractable option for working with non-conjugate Gaussian processes with more than ~5000 data points. However, for conjugate models of less than 5000 data points, we recommend using the marginal log-likelihood approach presented in the regression notebook. Though we illustrate SVGPs here with a conjugate regression example, the same GPJax code works for general likelihoods, such as a Bernoulli for classification.</p>"},{"location":"examples/uncollapsed_vi/#dataset","title":"Dataset\u00b6","text":"<p>With the necessary modules imported, we simulate a dataset $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{5000}$ with inputs $\\boldsymbol{x}$ sampled uniformly on $(-5, 5)$ and corresponding binary outputs</p> $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4 * \\boldsymbol{x}) + \\sin(2 * \\boldsymbol{x}), \\textbf{I} * (0.2)^{2} \\right).$$<p>We store our data $\\mathcal{D}$ as a GPJax <code>Dataset</code> and create test inputs for later.</p>"},{"location":"examples/uncollapsed_vi/#sparse-gps-via-inducing-inputs","title":"Sparse GPs via inducing inputs\u00b6","text":"<p>Despite their endowment with elegant theoretical properties, GPs are burdened with prohibitive $\\mathcal{O}(n^3)$ inference and $\\mathcal{O}(n^2)$ memory costs in the number of data points $n$ due to the necessity of computing inverses and determinants of the kernel Gram matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ during inference and hyperparameter learning. Sparse GPs seek to resolve tractability through low-rank approximations.</p> <p>Their name originates with the idea of using subsets of the data to approximate the kernel matrix, with sparseness occurring through the selection of the data points. Given inputs $\\boldsymbol{x}$ and outputs $\\boldsymbol{y}$ the task was to select an $m&lt;n$ lower-dimensional dataset $(\\boldsymbol{z},\\boldsymbol{\\tilde{y}}) \\subset (\\boldsymbol{x}, \\boldsymbol{y})$ to train a Gaussian process on instead. By generalising the set of selected points $\\boldsymbol{z}$, known as inducing inputs, to remove the restriction of being part of the dataset, we can arrive at a flexible low-rank approximation framework of the model using functions of $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$ to replace the true covariance matrix $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ at significantly lower costs. For example,  review many popular approximation schemes in this vein. However, because the model and the approximation are intertwined, assigning performance and faults to one or the other becomes tricky.</p> <p>On the other hand, sparse variational Gaussian processes (SVGPs) approximate the posterior, not the model. These provide a low-rank approximation scheme via variational inference. Here we posit a family of densities parameterised by \u201cvariational parameters\u201d. We then seek to find the closest family member to the posterior by minimising the Kullback-Leibler divergence over the variational parameters. The fitted variational density then serves as a proxy for the exact posterior. This procedure makes variational methods efficiently solvable via off-the-shelf optimisation techniques whilst retaining the true-underlying model. Furthermore, SVGPs offer further cost reductions with mini-batch stochastic gradient descent   and address non-conjugacy . We show a cost comparison between the approaches below, where $b$ is the mini-batch size.</p> GPs sparse GPs SVGP Inference cost $\\mathcal{O}(n^3)$ $\\mathcal{O}(n m^2)$ $\\mathcal{O}(b m^2 + m^3)$ Memory cost $\\mathcal{O}(n^2)$ $\\mathcal{O}(n m)$ $\\mathcal{O}(b m + m^2)$ <p>To apply SVGP inference to our dataset, we begin by initialising $m = 50$ equally spaced inducing inputs $\\boldsymbol{z}$ across our observed data's support. These are depicted below via horizontal black lines.</p>"},{"location":"examples/uncollapsed_vi/#defining-the-variational-process","title":"Defining the variational process\u00b6","text":"<p>We begin by considering the form of the posterior distribution for all function values $f(\\cdot)$</p> \\begin{align} p(f(\\cdot) | \\mathcal{D}) = \\int p(f(\\cdot)|f(\\boldsymbol{x})) p(f(\\boldsymbol{x})|\\mathcal{D}) \\text{d}f(\\boldsymbol{x}). \\qquad (\\dagger) \\end{align}<p>To arrive at an approximation framework, we assume some redundancy in the data. Instead of predicting $f(\\cdot)$ with function values at the datapoints $f(\\boldsymbol{x})$, we assume this can be achieved with only function values at $m$ inducing inputs $\\boldsymbol{z}$</p> $$ p(f(\\cdot) | \\mathcal{D}) \\approx \\int p(f(\\cdot)|f(\\boldsymbol{z})) p(f(\\boldsymbol{z})|\\mathcal{D}) \\text{d}f(\\boldsymbol{z}). \\qquad (\\star) $$<p>This lower dimensional integral results in computational savings in the model's predictive component from $p(f(\\cdot)|f(\\boldsymbol{x}))$ to $p(f(\\cdot)|f(\\boldsymbol{z}))$ where inverting $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is replaced with inverting $\\mathbf{K}_{\\boldsymbol{z}\\boldsymbol{z}}$. However, since we did not observe our data $\\mathcal{D}$ at $\\boldsymbol{z}$ we ask, what exactly is the posterior $p(f(\\boldsymbol{z})|\\mathcal{D})$?</p> <p>Notice this is simply obtained by substituting $\\boldsymbol{z}$ into $(\\dagger)$, but we arrive back at square one with computing the expensive integral. To side-step this, we consider replacing $p(f(\\boldsymbol{z})|\\mathcal{D})$ in $(\\star)$ with a cheap-to-compute approximate distribution $q(f(\\boldsymbol{z}))$</p> <p>$$ q(f(\\cdot)) = \\int p(f(\\cdot)|f(\\boldsymbol{z})) q(f(\\boldsymbol{z})) \\text{d}f(\\boldsymbol{z}). \\qquad (\\times) $$</p> <p>To measure the quality of the approximation, we consider the Kullback-Leibler divergence $\\operatorname{KL}(\\cdot || \\cdot)$ from our approximate process $q(f(\\cdot))$ to the true process $p(f(\\cdot)|\\mathcal{D})$. By parametrising $q(f(\\boldsymbol{z}))$ over a variational family of distributions, we can optimise Kullback-Leibler divergence with respect to the variational parameters. Moreover, since inducing input locations $\\boldsymbol{z}$ augment the model, they themselves can be treated as variational parameters without altering the true underlying model $p(f(\\boldsymbol{z})|\\mathcal{D})$. This is exactly what gives SVGPs great flexibility whilst retaining robustness to overfitting.</p> <p>It is popular to elect a Gaussian variational distribution $q(f(\\boldsymbol{z})) = \\mathcal{N}(f(\\boldsymbol{z}); \\mathbf{m}, \\mathbf{S})$ with parameters $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S}\\}$, since conjugacy is provided between $q(f(\\boldsymbol{z}))$ and $p(f(\\cdot)|f(\\boldsymbol{z}))$ so that the resulting variational process $q(f(\\cdot))$ is a GP. We can implement this in GPJax by the following.</p>"},{"location":"examples/uncollapsed_vi/#inference","title":"Inference\u00b6","text":""},{"location":"examples/uncollapsed_vi/#evidence-lower-bound","title":"Evidence lower bound\u00b6","text":"<p>With our model defined, we seek to infer the optimal inducing inputs $\\boldsymbol{z}$, variational mean $\\mathbf{m}$ and covariance $\\mathbf{S}$ that define our approximate posterior. To achieve this, we maximise the evidence lower bound (ELBO) with respect to $\\{\\boldsymbol{z}, \\mathbf{m}, \\mathbf{S} \\}$, a proxy for minimising the Kullback-Leibler divergence. Moreover, as hinted by its name, the ELBO is a lower bound to the marginal log-likelihood, providing a tractable objective to optimise the model's hyperparameters akin to the conjugate setting. For further details on this, see Sections 3.1 and 4.1 of the excellent review paper .</p> <p>Since Optax's optimisers work to minimise functions, to maximise the ELBO we return its negative.</p>"},{"location":"examples/uncollapsed_vi/#mini-batching","title":"Mini-batching\u00b6","text":"<p>Despite introducing inducing inputs into our model, inference can still be intractable with large datasets. To circumvent this, optimisation can be done using stochastic mini-batches.</p>"},{"location":"examples/uncollapsed_vi/#predictions","title":"Predictions\u00b6","text":"<p>With optimisation complete, we can use our inferred parameter set to make predictions at novel inputs akin to all other models within GPJax on our variational process object $q(\\cdot)$ (for example, see the regression notebook).</p>"},{"location":"examples/uncollapsed_vi/#custom-transformations","title":"Custom transformations\u00b6","text":"<p>To train a covariance matrix, GPJax uses <code>tfb.FillScaleTriL</code> transformation by default. <code>tfb.FillScaleTriL</code> fills a 1d vector into a lower triangular matrix and then applies <code>Softplus</code> transformation on the diagonal to satisfy the necessary conditions for a valid Cholesky matrix. Users can change this default transformation with another valid transformation of their choice. For example, <code>Square</code> transformation on the diagonal can also serve the purpose.</p>"},{"location":"examples/uncollapsed_vi/#system-configuration","title":"System configuration\u00b6","text":""},{"location":"examples/yacht/","title":"UCI Data Benchmarking","text":"In\u00a0[1]: Copied! <pre># Enable Float64 for more stable matrix inversions.\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n\nfrom jax import jit\nimport jax.random as jr\nfrom jaxtyping import install_import_hook\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax as ox\nimport pandas as pd\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nwith install_import_hook(\"gpjax\", \"beartype.beartype\"):\n    import gpjax as gpx\n\n# Enable Float64 for more stable matrix inversions.\nkey = jr.PRNGKey(123)\nplt.style.use(\n    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n)\ncols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n</pre> # Enable Float64 for more stable matrix inversions. from jax.config import config  config.update(\"jax_enable_x64\", True)  from jax import jit import jax.random as jr from jaxtyping import install_import_hook import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import optax as ox import pandas as pd from sklearn.metrics import (     mean_squared_error,     r2_score, ) from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  with install_import_hook(\"gpjax\", \"beartype.beartype\"):     import gpjax as gpx  # Enable Float64 for more stable matrix inversions. key = jr.PRNGKey(123) plt.style.use(     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\" ) cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> In\u00a0[2]: Copied! <pre>try:\n    yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :]\nexcept FileNotFoundError:\n    yacht = pd.read_fwf(\n        \"docs/examples/data/yacht_hydrodynamics.data\", header=None\n    ).values[:-1, :]\n\nX = yacht[:, :-1]\ny = yacht[:, -1].reshape(-1, 1)\n</pre> try:     yacht = pd.read_fwf(\"data/yacht_hydrodynamics.data\", header=None).values[:-1, :] except FileNotFoundError:     yacht = pd.read_fwf(         \"docs/examples/data/yacht_hydrodynamics.data\", header=None     ).values[:-1, :]  X = yacht[:, :-1] y = yacht[:, -1].reshape(-1, 1) In\u00a0[3]: Copied! <pre>Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\n</pre> Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) In\u00a0[4]: Copied! <pre>log_ytr = np.log(ytr)\nlog_yte = np.log(yte)\n\ny_scaler = StandardScaler().fit(log_ytr)\nscaled_ytr = y_scaler.transform(log_ytr)\nscaled_yte = y_scaler.transform(log_yte)\n</pre> log_ytr = np.log(ytr) log_yte = np.log(yte)  y_scaler = StandardScaler().fit(log_ytr) scaled_ytr = y_scaler.transform(log_ytr) scaled_yte = y_scaler.transform(log_yte) <p>We can see the effect of these transformations in the below three panels.</p> In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\nax[0].hist(ytr, bins=30, color=cols[1])\nax[0].set_title(\"y\")\nax[1].hist(log_ytr, bins=30, color=cols[1])\nax[1].set_title(\"log(y)\")\nax[2].hist(scaled_ytr, bins=30, color=cols[1])\nax[2].set_title(\"scaled log(y)\")\n</pre> fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5)) ax[0].hist(ytr, bins=30, color=cols[1]) ax[0].set_title(\"y\") ax[1].hist(log_ytr, bins=30, color=cols[1]) ax[1].set_title(\"log(y)\") ax[2].hist(scaled_ytr, bins=30, color=cols[1]) ax[2].set_title(\"scaled log(y)\") Out[5]: <pre>Text(0.5, 1.0, 'scaled log(y)')</pre> In\u00a0[6]: Copied! <pre>x_scaler = StandardScaler().fit(Xtr)\nscaled_Xtr = x_scaler.transform(Xtr)\nscaled_Xte = x_scaler.transform(Xte)\n</pre> x_scaler = StandardScaler().fit(Xtr) scaled_Xtr = x_scaler.transform(Xtr) scaled_Xte = x_scaler.transform(Xte) In\u00a0[7]: Copied! <pre>n_train, n_covariates = scaled_Xtr.shape\nkernel = gpx.RBF(active_dims=list(range(n_covariates)))\nmeanf = gpx.mean_functions.Zero()\nprior = gpx.Prior(mean_function=meanf, kernel=kernel)\n\nlikelihood = gpx.Gaussian(num_datapoints=n_train)\n\nposterior = prior * likelihood\n</pre> n_train, n_covariates = scaled_Xtr.shape kernel = gpx.RBF(active_dims=list(range(n_covariates))) meanf = gpx.mean_functions.Zero() prior = gpx.Prior(mean_function=meanf, kernel=kernel)  likelihood = gpx.Gaussian(num_datapoints=n_train)  posterior = prior * likelihood In\u00a0[8]: Copied! <pre>training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)\n\nnegative_mll = jit(gpx.ConjugateMLL(negative=True))\noptimiser = ox.adamw(0.05)\n\nopt_posterior, history = gpx.fit(\n    model=posterior,\n    objective=negative_mll,\n    train_data=training_data,\n    optim=ox.adamw(learning_rate=0.05),\n    num_iters=500,\n    key=key,\n)\n</pre> training_data = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)  negative_mll = jit(gpx.ConjugateMLL(negative=True)) optimiser = ox.adamw(0.05)  opt_posterior, history = gpx.fit(     model=posterior,     objective=negative_mll,     train_data=training_data,     optim=ox.adamw(learning_rate=0.05),     num_iters=500,     key=key, ) <pre>  0%|          | 0/500 [00:00&lt;?, ?it/s]</pre> In\u00a0[9]: Copied! <pre>latent_dist = opt_posterior(scaled_Xte, training_data)\npredictive_dist = likelihood(latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_stddev = predictive_dist.stddev()\n</pre> latent_dist = opt_posterior(scaled_Xte, training_data) predictive_dist = likelihood(latent_dist)  predictive_mean = predictive_dist.mean() predictive_stddev = predictive_dist.stddev() In\u00a0[10]: Copied! <pre>rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nr2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean)\nprint(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\")\n</pre> rmse = mean_squared_error(y_true=scaled_yte.squeeze(), y_pred=predictive_mean) r2 = r2_score(y_true=scaled_yte.squeeze(), y_pred=predictive_mean) print(f\"Results:\\n\\tRMSE: {rmse: .4f}\\n\\tR2: {r2: .2f}\") <pre>Results:\n\tRMSE:  0.0058\n\tR2:  0.99\n</pre> <p>Both of these metrics seem very promising, so, based off these, we can be quite happy that our first attempt at modelling the Yacht data is promising.</p> In\u00a0[11]: Copied! <pre>residuals = scaled_yte.squeeze() - predictive_mean\n\nfig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)\n\nax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1])\nax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes)\nax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")\n\nax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1])\nax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes)\nax[1].set_ylim([-1.0, 1.0])\nax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n\nax[2].hist(np.asarray(residuals), bins=30, color=cols[1])\nax[2].set_title(\"Residuals\")\n</pre> residuals = scaled_yte.squeeze() - predictive_mean  fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5), tight_layout=True)  ax[0].scatter(predictive_mean, scaled_yte.squeeze(), color=cols[1]) ax[0].plot([0, 1], [0, 1], color=cols[0], transform=ax[0].transAxes) ax[0].set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Predicted vs Actual\")  ax[1].scatter(predictive_mean.squeeze(), residuals, color=cols[1]) ax[1].plot([0, 1], [0.5, 0.5], color=cols[0], transform=ax[1].transAxes) ax[1].set_ylim([-1.0, 1.0]) ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")  ax[2].hist(np.asarray(residuals), bins=30, color=cols[1]) ax[2].set_title(\"Residuals\") Out[11]: <pre>Text(0.5, 1.0, 'Residuals')</pre> <p>From this, we can see that our model is struggling to predict the smallest values of the Yacht's hydrodynamic and performs increasingly well as the Yacht's hydrodynamic performance increases. This is likely due to the original data's heavy right-skew, and successive modelling attempts may wish to introduce a heteroscedastic likelihood function that would enable more flexible modelling of the smaller response values.</p> In\u00a0[12]: Copied! <pre>%reload_ext watermark\n%watermark -n -u -v -iv -w -a 'Thomas Pinder'\n</pre> %reload_ext watermark %watermark -n -u -v -iv -w -a 'Thomas Pinder' <pre>Author: Thomas Pinder\n\nLast updated: Mon Jul 31 2023\n\nPython implementation: CPython\nPython version       : 3.8.17\nIPython version      : 8.12.2\n\npandas    : 1.5.3\njax       : 0.4.9\nnumpy     : 1.24.3\ngpjax     : 0.0.0\nmatplotlib: 3.7.1\noptax     : 0.1.5\n\nWatermark: 2.3.1\n\n</pre>"},{"location":"examples/yacht/#uci-data-benchmarking","title":"UCI Data Benchmarking\u00b6","text":"<p>In this notebook, we will show how to apply GPJax on a benchmark UCI regression problem. These kind of tasks are often used in the research community to benchmark and assess new techniques against those already in the literature. Much of the code contained in this notebook can be adapted to applied problems concerning datasets other than the one presented here.</p>"},{"location":"examples/yacht/#data-loading","title":"Data Loading\u00b6","text":"<p>We'll be using the Yacht dataset from the UCI machine learning data repository. Each observation describes the hydrodynamic performance of a yacht through its resistance. The dataset contains 6 covariates and a single positive, real valued response variable. There are 308 observations in the dataset, so we can comfortably use a conjugate regression Gaussian process here (for more more details, checkout the Regression notebook).</p>"},{"location":"examples/yacht/#preprocessing","title":"Preprocessing\u00b6","text":"<p>With a dataset loaded, we'll now preprocess it such that it is more amenable to modelling with a Gaussian process.</p>"},{"location":"examples/yacht/#data-partitioning","title":"Data Partitioning\u00b6","text":"<p>We'll first partition our data into a training and testing split. We'll fit our Gaussian process to the training data and evaluate its performance on the test data. This allows us to investigate how effectively our Gaussian process generalises to out-of-sample datapoints and ensure that we are not overfitting. We'll hold 30% of our data back for testing purposes.</p>"},{"location":"examples/yacht/#response-variable","title":"Response Variable\u00b6","text":"<p>We'll now process our response variable $\\mathbf{y}$. As the below plots show, the data has a very long tail and is certainly not Gaussian. However, we would like to model a Gaussian response variable so that we can adopt a Gaussian likelihood function and leverage the model's conjugacy. To achieve this, we'll first log-scale the data, to bring the long right tail in closer to the data's mean. We'll then standardise the data such that is distributed according to a unit normal distribution. Both of these transformations are invertible through the log-normal expectation and variance formulae and the the inverse standardisation identity, should we ever need our model's predictions to be back on the scale of the original dataset.</p> <p>For transforming both the input and response variable, all transformations will be done with respect to the training data where relevant.</p>"},{"location":"examples/yacht/#input-variable","title":"Input Variable\u00b6","text":"<p>We'll now transform our input variable $\\mathbf{X}$ to be distributed according to a unit Gaussian.</p>"},{"location":"examples/yacht/#model-fitting","title":"Model fitting\u00b6","text":"<p>With data now loaded and preprocessed, we'll proceed to defining a Gaussian process model and optimising its parameters. This notebook purposefully does not go into great detail on this process, so please see notebooks such as the Regression notebook and Classification notebook for further information.</p>"},{"location":"examples/yacht/#model-specification","title":"Model specification\u00b6","text":"<p>We'll use a radial basis function kernel to parameterise the Gaussian process in this notebook. As we have 5 covariates, we'll assign each covariate its own lengthscale parameter. This form of kernel is commonly known as an automatic relevance determination (ARD) kernel.</p> <p>In practice, the exact form of kernel used should be selected such that it represents your understanding of the data. For example, if you were to model temperature; a process that we know to be periodic, then you would likely wish to select a periodic kernel. Having Gaussian-ised our data somewhat, we'll also adopt a Gaussian likelihood function.</p>"},{"location":"examples/yacht/#model-optimisation","title":"Model Optimisation\u00b6","text":"<p>With a model now defined, we can proceed to optimise the hyperparameters of our model using Optax.</p>"},{"location":"examples/yacht/#prediction","title":"Prediction\u00b6","text":"<p>With an optimal set of parameters learned, we can make predictions on the set of data that we held back right at the start. We'll do this in the usual way by first computing the latent function's distribution before computing the predictive posterior distribution.</p>"},{"location":"examples/yacht/#evaluation","title":"Evaluation\u00b6","text":"<p>We'll now show how the performance of our Gaussian process can be evaluated by numerically and visually.</p>"},{"location":"examples/yacht/#metrics","title":"Metrics\u00b6","text":"<p>To numerically assess the performance of our model, two commonly used metrics are root mean squared error (RMSE) and the R2 coefficient. RMSE is simply the square root of the squared difference between predictions and actuals. A value of 0 for this metric implies that our model has 0 generalisation error on the test set. R2 measures the amount of variation within the data that is explained by the model. This can be useful when designing variance reduction methods such as control variates as it allows you to understand what proportion of the data's variance will be soaked up. A perfect model here would score 1 for R2 score, whereas predicting the data's mean would score 0 and models doing worse than simple mean predictions can score less than 0.</p>"},{"location":"examples/yacht/#diagnostic-plots","title":"Diagnostic plots\u00b6","text":"<p>To accompany the above metrics, we can also produce residual plots to explore exactly where our model's shortcomings lie. If we define a residual as the true value minus the prediction, then we can produce three plots:</p> <ol> <li>Predictions vs. actuals.</li> <li>Predictions vs. residuals.</li> <li>Residual density.</li> </ol> <p>The first plot allows us to explore if our model struggles to predict well for larger or smaller values by observing where the model deviates more from the line $y=x$. In the second plot we can inspect whether or not there were outliers or structure within the errors of our model. A well-performing model would have predictions close to and symmetrically distributed either side of $y=0$. Such a plot can be useful for diagnosing heteroscedasticity. Finally, by plotting a histogram of our residuals we can observe whether or not there is any skew to our residuals.</p>"},{"location":"examples/yacht/#system-configuration","title":"System configuration\u00b6","text":""}]}