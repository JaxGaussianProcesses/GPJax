{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d000127",
   "metadata": {},
   "source": [
    "# Kernel Guide\n",
    "\n",
    "In this guide, we introduce the kernels available in GPJax and demonstrate how to create custom ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpjax as gpx\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "from jax import jit\n",
    "import jax\n",
    "from optax import adam\n",
    "import distrax as dx\n",
    "from jaxtyping import Float, Array\n",
    "\n",
    "key = jr.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4973d7",
   "metadata": {},
   "source": [
    "## Supported Kernels\n",
    "\n",
    "The following kernels are natively supported in GPJax.\n",
    "\n",
    "* Matérn 1/2, 3/2 and 5/2.\n",
    "* RBF (or squared exponential).\n",
    "* Polynomial.\n",
    "* [Graph kernels](https://gpjax.readthedocs.io/en/latest/nbs/graph_kernels.html).\n",
    "\n",
    "While the syntax is consistent, each kernel’s type influences the characteristics of the sample paths drawn. We visualise this below with 10 function draws per kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\n",
    "    gpx.Matern12(),\n",
    "    gpx.Matern32(),\n",
    "    gpx.Matern52(),\n",
    "    gpx.RBF(),\n",
    "    gpx.Polynomial(degree=1),\n",
    "    gpx.Polynomial(degree=2),\n",
    "]\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(20, 10))\n",
    "\n",
    "x = jnp.linspace(-3.0, 3.0, num=200).reshape(-1, 1)\n",
    "\n",
    "\n",
    "for k, ax in zip(kernels, axes.ravel()):\n",
    "    prior = gpx.Prior(kernel=k)\n",
    "    params, _, _, _ = gpx.initialise(prior, key).unpack()\n",
    "    rv = prior(params)(x)\n",
    "    y = rv.sample(sample_shape=10, seed=key)\n",
    "\n",
    "    ax.plot(x, y.T, alpha=0.7)\n",
    "    ax.set_title(k.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73379a45",
   "metadata": {},
   "source": [
    "### Active dimensions\n",
    "\n",
    "By default, kernels operate over every dimension of the supplied inputs. In some use cases, it is desirable to restrict kernels to specific dimensions of the input data. We can achieve this by the `active dims` argument, which determines which input index values the kernel evaluates.\n",
    "\n",
    "To see this, consider the following 5-dimensional dataset for which we would like our RBF kernel to act on the first, second and fourth dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_kernel = gpx.RBF(active_dims=[0, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a676f0",
   "metadata": {},
   "source": [
    "\n",
    "The resulting kernel has one length-scale parameter per input dimension --- an ARD kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ARD: {slice_kernel.ard}\")\n",
    "print(f\"Lengthscales: {slice_kernel._initialise_params(key)['lengthscale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d54c86",
   "metadata": {},
   "source": [
    "We'll now simulate some data and evaluate the kernel on the previously selected input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea47655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = jr.normal(key, shape=(50, 5))\n",
    "K = gpx.kernels.gram(slice_kernel, x_matrix, slice_kernel._initialise_params(key))\n",
    "print(K.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a6cdf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Kernel combinations\n",
    "\n",
    "The product or sum of two positive definite matrices yields a positive definite matrix. Consequently, summing or multiplying sets of kernels is a valid operation that can give rich kernel functions. In GPJax, sums of kernels can be created by applying the `+` operator as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = gpx.RBF()\n",
    "k2 = gpx.Polynomial()\n",
    "sum_k = k1 + k2\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "im0 = ax[0].matshow(gpx.kernels.gram(k1, x, k1._initialise_params(key)))\n",
    "im1 = ax[1].matshow(gpx.kernels.gram(k2, x, k2._initialise_params(key)))\n",
    "im2 = ax[2].matshow(gpx.kernels.gram(sum_k, x, sum_k._initialise_params(key)))\n",
    "\n",
    "fig.colorbar(im0, ax=ax[0])\n",
    "fig.colorbar(im1, ax=ax[1])\n",
    "fig.colorbar(im2, ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e610f",
   "metadata": {},
   "source": [
    "Similarily, products of kernels can be created through the `*` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ae9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k3 = gpx.Matern32()\n",
    "\n",
    "prod_k = k1 * k2 * k3\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 5))\n",
    "im0 = ax[0].matshow(gpx.kernels.gram(k1, x, k1._initialise_params(key)))\n",
    "im1 = ax[1].matshow(gpx.kernels.gram(k2, x, k2._initialise_params(key)))\n",
    "im2 = ax[2].matshow(gpx.kernels.gram(k3, x, k3._initialise_params(key)))\n",
    "im3 = ax[3].matshow(gpx.kernels.gram(prod_k, x, prod_k._initialise_params(key)))\n",
    "\n",
    "fig.colorbar(im0, ax=ax[0])\n",
    "fig.colorbar(im1, ax=ax[1])\n",
    "fig.colorbar(im2, ax=ax[2])\n",
    "fig.colorbar(im3, ax=ax[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4674e9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Alternatively kernel sums and multiplications can be created by passing a list of kernels into the `SumKernel` `ProductKernel` objects respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_k = gpx.SumKernel(kernel_set=[k1, k2])\n",
    "prod_k = gpx.ProductKernel(kernel_set=[k1, k2, k3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33f634",
   "metadata": {},
   "source": [
    "## Custom kernel\n",
    "\n",
    "GPJax makes the process of implementing kernels of your choice straightforward with two key steps:\n",
    "\n",
    "1. Listing the kernel's parameters.\n",
    "2. Defining the kernel's pairwise operation.\n",
    "\n",
    "We'll demonstrate this process now for a circular kernel --- an adaption of the excellent guide given in the PYMC3 documentation. We encourage curious readers to visit their notebook [here](https://docs.pymc.io/pymc-examples/examples/gaussian_processes/GP-Circular.html).\n",
    "\n",
    "### Circular kernel\n",
    "\n",
    "When the underlying space is polar, typical Euclidean kernels such as Matérn kernels are insufficient at the boundary as discontinuities will be present. This is due to the fact that for a polar space $\\lvert 0, 2\\pi\\rvert=0$ i.e., the space wraps. Euclidean kernels have no mechanism in them to represent this logic and will instead treat $0$ and $2\\pi$ and elements far apart. Circular kernels do not exhibit this behaviour and instead _wrap_ around the boundary points to create a smooth function. Such a kernel was given in [Padonou & Roustant (2015)](https://hal.inria.fr/hal-01119942v1) where any two angles $\\theta$ and $\\theta'$ are written as\n",
    "$$W_c(\\theta, \\theta') = \\left\\lvert \\left(1 + \\tau \\frac{d(\\theta, \\theta')}{c} \\right) \\left(1 - \\frac{d(\\theta, \\theta')}{c} \\right)^{\\tau} \\right\\rvert \\quad \\tau \\geq 4 \\tag{1}.$$\n",
    "\n",
    "Here the hyperparameter $\\tau$ is analogous to a lengthscale for Euclidean stationary kernels, controlling the correlation between pairs of observations. While $d$ is an angular distance metric\n",
    "\n",
    "$$d(\\theta, \\theta') = \\lvert (\\theta-\\theta'+c) \\operatorname{mod} 2c - c \\rvert.$$\n",
    "\n",
    "To implement this, one must write the following class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffffca6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from chex import dataclass\n",
    "\n",
    "\n",
    "def angular_distance(x, y, c):\n",
    "    return jnp.abs((x - y + c) % (c * 2) - c)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Polar(gpx.kernels.Kernel):\n",
    "    period: float = 2 * jnp.pi\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.c = self.period / 2.0  # in [0, \\pi]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"1 D\"], y: Float[Array, \"1 D\"], params: dict) -> Float[Array, \"1\"]:\n",
    "        tau = params[\"tau\"]\n",
    "        t = angular_distance(x, y, self.c)\n",
    "        K = (1 + tau * t / self.c) * jnp.clip(1 - t / self.c, 0, jnp.inf) ** tau\n",
    "        return K.squeeze()\n",
    "    \n",
    "    def _initialise_params(self, key: jr.PRNGKey) -> dict:\n",
    "        return {\"tau\": jnp.array([4.0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62c48c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We unpack this now to make better sense of it. In the kernel's `__init__` function we simply specify the length of a single period. As the underlying domain is a circle, this is $2\\pi$. Next we define the kernel's `__call__` function which is a direct implementation of Equation (1). Finally, we define the Kernel's parameter property which contains just one value $\\tau$ that we initialise to 4 in the kernel's `__post_init__`.\n",
    "\n",
    "#### Aside on dataclasses\n",
    "\n",
    "One can see in the above definition of a `Polar` kernel that we decorated the class with a `@dataclass` command. Dataclasses are simply regular classs objects in Python, however, much of the boilerplate code has been removed. For example, without a `@dataclass` decorator, the instantiation of the above `Polar` kernel would be done through\n",
    "```python\n",
    "class Polar(gpx.kernels.Kernel):\n",
    "    def __init__(self, period: float = 2*jnp.pi):\n",
    "        super().__init__()\n",
    "        self.period = period\n",
    "```\n",
    "As objects become increasingly large and complex, the conciseness of a dataclass becomes increasingly attractive. To ensure full compatability with Jax, it is crucial that the dataclass decorator is imported from Chex, not base Python's `dataclass` module. Functionally, the two objects are identical. However, unlike regular Python dataclasses, it is possilbe to apply operations such as`jit`, `vmap` and `grad` to the dataclasses given by Chex as they are registrered PyTrees. \n",
    "\n",
    "\n",
    "### Custom Parameter Bijection\n",
    "\n",
    "The constraint on $\\tau$ makes optimisation challenging with gradient descent. It would be much easier if we could instead parameterise $\\tau$ to be on the real line. Fortunately, this can be taken care of with GPJax's `add parameter` function, only requiring us to define the parameter's name and matching bijection (either a Distrax of TensorFlow probability bijector). Under the hood, calling this function updates a configuration object to register this parameter and its corresponding transform.\n",
    "\n",
    "To define a bijector here we'll make use of the `Lambda` operator given in Distrax. This lets us convert any regular Jax function into a bijection. Given that we require $\\tau$ to be strictly greater than $4.$, we'll apply a [softplus transformation](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softplus.html) where the lower bound is shifted by $4.$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpjax.config import add_parameter\n",
    "\n",
    "bij_fn = lambda x: jax.nn.softplus(x+jnp.array(4.))\n",
    "bij = dx.Lambda(bij_fn)\n",
    "\n",
    "add_parameter(\"tau\", bij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bbdb6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Using our polar kernel\n",
    "\n",
    "We proceed to fit a GP with our custom circular kernel to a random sequence of points on a circle (see the [Regression notebook](https://gpjax.readthedocs.io/en/latest/nbs/regression.html) for further details on this process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "angles = jnp.linspace(0, 2 * jnp.pi, num=200).reshape(-1, 1)\n",
    "n = 20\n",
    "noise = 0.2\n",
    "\n",
    "X = jnp.sort(jr.uniform(key, minval=0.0, maxval=jnp.pi * 2, shape=(n, 1)), axis=0)\n",
    "y = 4 + jnp.cos(2 * X) + jr.normal(key, shape=X.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=X, y=y)\n",
    "\n",
    "# Define polar Gaussian process\n",
    "PKern = Polar()\n",
    "likelihood = gpx.Gaussian(num_datapoints=n)\n",
    "circlular_posterior = gpx.Prior(kernel=PKern) * likelihood\n",
    "\n",
    "# Initialise parameters and corresponding transformations\n",
    "params, trainable, constrainer, unconstrainer = gpx.initialise(circlular_posterior, key).unpack()\n",
    "\n",
    "# Optimise GP's marginal log-likelihood using Adam\n",
    "mll = jit(circlular_posterior.marginal_log_likelihood(D, constrainer, negative=True))\n",
    "learned_params, training_history = gpx.fit(\n",
    "    mll,\n",
    "    params,\n",
    "    trainable,\n",
    "    adam(learning_rate=0.05),\n",
    "    n_iters=1000,\n",
    ").unpack()\n",
    "\n",
    "# Untransform learned parameters\n",
    "final_params = gpx.transform(learned_params, constrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53104dad",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We'll now query the GP's predictive posterior at linearly spaced novel inputs and illustrate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ea35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_rv = likelihood(circlular_posterior(D, final_params)(angles), final_params)\n",
    "mu = posterior_rv.mean()\n",
    "one_sigma = posterior_rv.stddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d216a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "gridspec = fig.add_gridspec(1, 1)\n",
    "ax = plt.subplot(gridspec[0], polar=True)\n",
    "\n",
    "ax.fill_between(\n",
    "    angles.squeeze(),\n",
    "    mu - one_sigma,\n",
    "    mu + one_sigma,\n",
    "    alpha=0.3,\n",
    "    label=r\"1 Posterior s.d.\",\n",
    "    color=\"#B5121B\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    angles.squeeze(),\n",
    "    mu - 3 * one_sigma,\n",
    "    mu + 3 * one_sigma,\n",
    "    alpha=0.15,\n",
    "    label=r\"3 Posterior s.d.\",\n",
    "    color=\"#B5121B\",\n",
    ")\n",
    "ax.plot(angles, mu, label=\"Posterior mean\")\n",
    "ax.scatter(D.X, D.y, alpha=1, label=\"Observations\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6c731",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder (edited by Daniel Dodd)'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('gpjax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "920091140e6b97de16b405af485d142952a229f5dad61a888f46227f5acb94cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
