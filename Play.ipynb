{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax\n",
    "import json\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "import cola\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import (\n",
    "    Array,\n",
    "    Float,\n",
    "    install_import_hook,\n",
    "    Num,\n",
    ")\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "#with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "import gpjax as gpx\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarInt,\n",
    "    ScalarFloat,\n",
    ")\n",
    "from gpjax.distributions import GaussianDistribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "# plt.style.use(\n",
    "#     \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    "# )\n",
    "# colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "\n",
    "\n",
    "import gpjax as gpx\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "import scipy.optimize as spo\n",
    "from dataclasses import dataclass\n",
    "from gpjax.base import param_field, static_field\n",
    "from gpjax.objectives import AbstractObjective\n",
    "import math\n",
    "from gpjax.kernels.base import AbstractKernel\n",
    "from gpjax.kernels import RBF\n",
    "from gpjax.objectives import AbstractObjective\n",
    "import optax as ox\n",
    "from gpjax.kernels.stationary.utils import squared_distance\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarFloat,\n",
    ")\n",
    "from jaxtyping import (\n",
    "    Float,\n",
    "    Num,\n",
    ")\n",
    "from simple_pytree import Pytree\n",
    "from optax import GradientTransformation\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optim_builder(optim_pytree):\n",
    "\n",
    "    def _init_leaf(o, p):\n",
    "        if isinstance(o, GradientTransformation):\n",
    "            return o.init(p)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _update_leaf(o, u, s, p):\n",
    "        if isinstance(o, GradientTransformation):\n",
    "            return tuple(o.update(u, s, p))\n",
    "        else:\n",
    "            return jtu.tree_map(jnp.zeros_like, p)\n",
    "\n",
    "    def _get_updates(o, u, p):\n",
    "        if isinstance(o, GradientTransformation):\n",
    "            return u[0]\n",
    "        else:\n",
    "            return u\n",
    "    \n",
    "    def _get_state(o, u):\n",
    "        if isinstance(o, GradientTransformation):\n",
    "            return u[1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def init_fn(params):\n",
    "        return jtu.tree_map(_init_leaf, optim_pytree, params, is_leaf=lambda x: isinstance(x, GradientTransformation))\n",
    "\n",
    "    def update_fn(updates, state, params):\n",
    "        updates_state = jtu.tree_map(_update_leaf, optim_pytree, updates, state, params, is_leaf=lambda x: isinstance(x, GradientTransformation))\n",
    "        updates = jtu.tree_map(_get_updates, optim_pytree, updates_state, params, is_leaf=lambda x: isinstance(x, GradientTransformation))\n",
    "        state = jtu.tree_map(_get_state, optim_pytree, updates_state, is_leaf=lambda x: isinstance(x, GradientTransformation))\n",
    "\n",
    "        return updates, state\n",
    "\n",
    "    return GradientTransformation(init_fn, update_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100_000 entries sampled across time/lat/lon over first day of data\n",
    "\n",
    "## X2D has:\n",
    "\n",
    "0\"Sea surface temperature (K)\"\n",
    "\n",
    "1\"Sensible heat flux (W/m^2)\"\n",
    "\n",
    "2\"Latent heat flux (W/m^2)\"\n",
    "\n",
    "3\"Vertically-integrated moisture convergence (kg/m^2)\"\n",
    "\n",
    "4\"Column relative humidity (%)\"\n",
    "\n",
    "\n",
    "## X3D has:\n",
    "\n",
    "0\"Absolute temperature (K)\"\n",
    "\n",
    "1\"Relative humidity (%)\"\n",
    "\n",
    "2\"Specific humidity (kg/kg)\"\n",
    "\n",
    "3\"Geopotential height (m^2 s^-2)\"\n",
    "\n",
    "4\"Zonal wind (m/s)\"\n",
    "\n",
    "5\"Meridional wind (m/s)\"\n",
    "\n",
    "6\"Potential temperature (K)\"\n",
    "\n",
    "7\"Equivalent potential temperature (K)\"\n",
    "\n",
    "8\"Equivalent potential temperature saturation deficit (K)\"\n",
    "\n",
    "9\"Saturated equivalent potential temperature (K)\"\n",
    "\n",
    "10\"MSE-conserving plume buoyancy (m/s^2)\"\n",
    "\n",
    "\n",
    "## static has:\n",
    "\n",
    "0\"Land-sea mask\"\n",
    "\n",
    "1\"Angle of sub-gridscale orography (rad)Anisotropy of sub-gridscale orography\"\n",
    "\n",
    "2\"Standard deviation of sub-gridscale orography\"\n",
    "\n",
    "3\"Slope of sub-gridscale orography\"\n",
    "\n",
    "## Y has:\n",
    "\n",
    "0\"ERA5 Precipitation (mm/hr)\"\n",
    "\n",
    "1\"TRMM Precipitation (mm/hr)\"\n",
    "\n",
    "2\"TRMM Relative Error (%)\"\n",
    "\n",
    "# plev are\n",
    "1000.,   2000.,   3000.,   5000.,   7000.,  10000., 15000.,\n",
    "20000.,  25000.,  30000.,  40000.,  50000.,  60000.,  70000.,80000.,  85000.,  90000.,  92500.,  95000.,  97500., 100000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 24779 entries with nan\n",
      "Removed all pressure levels below [50000.] hPa\n",
      "Applied log transform to Y\n",
      "then standardized Y\n",
      "Using 75121 training and 100 testing points!\n",
      "using 1 static variables\n",
      "using 5 2d variables\n",
      "using 4 3d variables\n",
      "using variables with names ['RH', 'tehta_e', 'theta_e+', 'theta_e*', 'K_surface', 'flux_s', 'flux_l', 'moisture', 'RH_column', 'O_sd']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X2d = jnp.array(jnp.load(\"..//data/100_000_one_day/X2D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "X3d = jnp.array(jnp.load(\"../data/100_000_one_day/X3D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Xstatic = jnp.array(jnp.load(\"../data/100_000_one_day/XSTATIC_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Y = jnp.array(jnp.load(\"../data/100_000_one_day/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "pressure = jnp.array([[1000,2000,3000,5000,7000,10000,15000,20000,25000,30000,40000,50000,60000,70000,80000, 85000,90000,92500,95000,97500,100000]], dtype=jnp.float64)\n",
    "\n",
    "\n",
    "# random shuffle\n",
    "X2d = jr.permutation(key, X2d)\n",
    "X3d = jr.permutation(key, X3d)\n",
    "Xstatic = jr.permutation(key, Xstatic)\n",
    "Y = jr.permutation(key, Y)\n",
    "\n",
    "# look at ERA5 rain\n",
    "Y = Y[:,0:1]  \n",
    "\n",
    "# remove any entries with nan\n",
    "X3d_nan_idx = jnp.isnan(X3d).any(axis=1).any(axis=1)\n",
    "X2d_nan_idx = jnp.isnan(X2d).any(axis=1)\n",
    "Xstatic_nan_idx = jnp.isnan(Xstatic).any(axis=1)\n",
    "Y_nan_idx = jnp.isnan(Y).any(axis=1)\n",
    "any_nan = X3d_nan_idx | X2d_nan_idx | Y_nan_idx | Xstatic_nan_idx\n",
    "no_nan = ~ any_nan\n",
    "print(f\"Removed {any_nan.sum()} entries with nan\")\n",
    "X2d = X2d[no_nan,:]\n",
    "X3d = X3d[no_nan,:,:]\n",
    "Xstatic = Xstatic[no_nan,:]\n",
    "Y = Y[no_nan,:]\n",
    "\n",
    "\n",
    "# just keep the \"prognostic\" 3d inputs that joe considered (for now)\n",
    "# RH, tehta_e^+, theta_e, theta_e^*\n",
    "names_3d =  [\"K,\",\"RH\", \"q\", \"gh\", \"wind_z\",\"wind_m\",\"theta\",\"tehta_e\", \"theta_e+\", \"theta_e*\", \"plume\"]\n",
    "# idx_3d = [i for i in range(len(names_3d))]\n",
    "idx_3d = [1, 7, 8, 9]\n",
    "names_3d = [names_3d[i] for i in idx_3d]\n",
    "X3d = X3d[:,idx_3d,:]\n",
    "\n",
    "\n",
    "# # also use his \"normalisatopm\" for sigma_o\n",
    "sigma_o = jnp.where(Xstatic[:,0]<0.5, 0.0, 1.0+jnp.log(1+Xstatic[:,2]))\n",
    "Xstatic = Xstatic.at[:,2].set(sigma_o)\n",
    "names_static = [\"LSM\",\"O_angle\",\"O_sd\",\"O_slope\"]\n",
    "# idx_static = [i for i in range(len(names_static))]\n",
    "idx_static = [2]\n",
    "names_static = [names_static[i] for i in idx_static]\n",
    "Xstatic = Xstatic[:,idx_static]\n",
    "\n",
    "\n",
    "names_2d = [\"K_surface\",\"flux_s\",\"flux_l\",\"moisture\",\"RH_column\"]\n",
    "idx_2d =[i for i in range(len(names_2d))]\n",
    "names_2d = [names_2d[i] for i in idx_2d]\n",
    "X2d = X2d[:,idx_2d]\n",
    "\n",
    "\n",
    "#remove all pressure levels below 500 hPA\n",
    "lowest_idx = 11 # 7\n",
    "print(f\"Removed all pressure levels below {pressure[:,lowest_idx]} hPa\")\n",
    "X3d = X3d[:, :, lowest_idx:]\n",
    "pressure_levels = pressure[:,lowest_idx:]\n",
    "\n",
    "# # remove no rain days\n",
    "# print(f\"Removed {(Y[:,0]==0).sum()} entries with zero rain\")\n",
    "# X3d = X3d[Y[:,0]>0,:]\n",
    "# X2d = X2d[Y[:,0]>0,:]\n",
    "# Xstatic = Xstatic[Y[:,0]>0,:]\n",
    "# Y = Y[Y[:,0]>0,:]\n",
    "\n",
    "\n",
    "# also log Y\n",
    "print(f\"Applied log transform to Y\")\n",
    "Y = jnp.log(Y+1e-12)\n",
    "print(f\"then standardized Y\")\n",
    "Y_mean = jnp.mean(Y)\n",
    "Y_std = jnp.std(Y)\n",
    "Y = (Y - Y_mean) / Y_std\n",
    "\n",
    "# standardize inputs\n",
    "X3d_std = jnp.std(X3d, axis=0)\n",
    "X3d_mean = jnp.mean(X3d,axis=0)\n",
    "X3d = (X3d - X3d_mean) / X3d_std\n",
    "X2d_std = jnp.std(X2d, axis=0)\n",
    "X2d_mean = jnp.mean(X2d,axis=0)\n",
    "X2d = (X2d - X2d_mean) / X2d_std\n",
    "Xstatic_std = jnp.std(Xstatic, axis=0)\n",
    "Xstatic_mean = jnp.mean(Xstatic,axis=0)\n",
    "Xstatic = (Xstatic - Xstatic_mean) / Xstatic_std\n",
    "\n",
    "\n",
    "# look at all data but 1_000 test\n",
    "\n",
    "N_test = 100\n",
    "N_train = len(X2d) - N_test \n",
    "print(f\"Using {N_train} training and {N_test} testing points!\")\n",
    "num_2d_variables= X2d.shape[1]\n",
    "num_3d_variables= X3d.shape[1]\n",
    "num_static_variables= Xstatic.shape[1]\n",
    "num_not_3d_variables = num_2d_variables + num_static_variables\n",
    "num_variables = num_2d_variables + num_3d_variables + num_static_variables\n",
    "print(f\"using {num_static_variables} static variables\")\n",
    "print(f\"using {num_2d_variables} 2d variables\")\n",
    "print(f\"using {num_3d_variables} 3d variables\")\n",
    "names = names_3d + names_2d + names_static\n",
    "print(f\"using variables with names {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class VerticalDataset(Pytree):\n",
    "    X2d: Num[Array, \"N D\"] = None\n",
    "    X3d: Num[Array, \"N D L\"] = None\n",
    "    Xstatic: Num[Array, \"N D\"] = None\n",
    "    y: Num[Array, \"N 1\"] = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        gpx.dataset._check_precision(self.X2d, self.y)\n",
    "        gpx.dataset._check_precision(self.Xstatic, self.y)\n",
    "        gpx.dataset._check_precision(self.X3d, self.y)\n",
    "        \n",
    "    @property\n",
    "    def X(self):\n",
    "        return NotImplementedError(\"Use X2d, X3d or Xstatic instead\")\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VerticalSmoother(gpx.base.Module):\n",
    "    smoother_mean: Float[Array, \"1 D\"]  = param_field(None)\n",
    "    smoother_input_scale: Float[Array, \"1 D\"] = param_field(None, bijector=tfb.Softplus(low=jnp.array(1.0e-5, dtype=jnp.float64)))\n",
    "    Z_levels: Float[Array, \"1 L\"] = static_field(pressure_levels)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.Z_mean = jnp.mean(self.Z_levels)\n",
    "        self.Z_std = jnp.std(self.Z_levels)\n",
    "        self.Z_levels = (self.Z_levels - self.Z_mean) / self.Z_std\n",
    "\n",
    "    def smooth(self) -> Num[Array, \"D L\"]:\n",
    "        smoothing_weights = jnp.exp(-0.5*((self.Z_levels-self.smoother_mean.T)/(self.smoother_input_scale.T))**2) # [D, L]\n",
    "        return  (smoothing_weights/ jnp.sum(smoothing_weights, axis=-1, keepdims=True)) # [D, L]\n",
    "    \n",
    "    def smooth_data(self, dataset: VerticalDataset) -> Num[Array, \"N D\"]:\n",
    "        x3d, x2d, xstatic, y = dataset.X3d, dataset.X2d, dataset.Xstatic, dataset.y\n",
    "        x3d_smooth = jnp.sum(jnp.multiply(self.smooth() , x3d), axis=-1) # [N, D_3d]\n",
    "        x = jnp.hstack([x3d_smooth, x2d, xstatic]) # [N, D_3d + D_2d +D_static]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomConjugatePosterior(gpx.gps.ConjugatePosterior):\n",
    "    smoother: VerticalSmoother = VerticalSmoother()\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: gpx.Dataset,\n",
    "        kernel_between_train: Optional[AbstractKernel] = None,\n",
    "        kernel_with_test: Optional[AbstractKernel] = None,\n",
    "    ) -> GaussianDistribution:\n",
    "\n",
    "        # smooth data to get in form for preds\n",
    "        x,y = self.smoother.smooth_data(train_data)\n",
    "        smoothed_train_data = gpx.Dataset(x, y)\n",
    "        return super().predict(test_inputs, smoothed_train_data, kernel_between_train, kernel_with_test)\n",
    "\n",
    "\n",
    "class CustomAdditiveConjugatePosterior(CustomConjugatePosterior):\n",
    "    def __post__init__(self):\n",
    "        assert isinstance(self.prior.kernel, AdditiveKernel), \"AdditiveConjugatePosterior requires an AdditiveKernel\"\n",
    "\n",
    "    def predict_additive_component(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: gpx.Dataset,\n",
    "        component_list: List[List[int]]\n",
    "    ) -> GaussianDistribution:\n",
    "        r\"\"\"Get the posterior predictive distribution for a specific additive component.\"\"\"\n",
    "        specific_kernel = self.prior.kernel.get_specific_kernel(component_list)\n",
    "        return self.predict(test_inputs, train_data, kernel_with_test = specific_kernel)\n",
    "\n",
    "    def get_sobol_index(self, train_data: gpx.Dataset, component_list: List[int]) -> ScalarFloat:\n",
    "        \"\"\" Return the sobol index for the additive component corresponding to component_list. \"\"\"\n",
    "        if isinstance(component_list[0], List):\n",
    "            raise ValueError(\"Use get_sobol_indicies if you want to calc for multiple components\")\n",
    "        x,y = self.smoother.smooth_data(train_data)\n",
    "        component_posterior = self.predict_additive_component(x, train_data, component_list)\n",
    "        full_posterior= self.predict(x, train_data) # wasteful as only need means\n",
    "        return jnp.var(component_posterior.mean()) / jnp.var(full_posterior.mean())\n",
    "    \n",
    "    def get_sobol_indicies(self, train_data: gpx.Dataset, component_list: List[List[int]]) -> Num[Array, \"c\"]:\n",
    "        if not isinstance(component_list, List):\n",
    "            raise ValueError(\"Use get_sobol_index if you want to calc for single components\")\n",
    "        x,y = self.smoother.smooth_data(train_data)\n",
    "        m_x = self.prior.mean_function(x).T\n",
    "\n",
    "        base_kernels = self.prior.kernel.kernels\n",
    "        Kxx_indiv = jnp.stack([k.gram(x).to_dense() for k in base_kernels], axis=0) # [d, N, N]\n",
    "        interaction_variances = self.prior.kernel.interaction_variances\n",
    "        Kxx_components = [interaction_variances[len(c)]*jnp.prod(Kxx_indiv[c, :, :], axis=0) for c in component_list] \n",
    "        Kxx_components = jnp.stack(Kxx_components, axis=0) # [c, N, N]\n",
    "        assert Kxx_components.shape[0] == len(component_list)\n",
    "\n",
    "        Kxx = self.prior.kernel.gram(x).to_dense() # [N, N]\n",
    "        Sigma = cola.PSD(Kxx + cola.ops.I_like(Kxx) * self.likelihood.obs_stddev**2)\n",
    "\n",
    "        def get_mean_from_covar(K): # [N,N] -> [N, 1]\n",
    "            Sigma_inv_Kxx = cola.solve(Sigma, K)\n",
    "            return m_x.T + jnp.matmul(Sigma_inv_Kxx.T, y - m_x) # [N, 1] \n",
    "\n",
    "        mean_overall =  get_mean_from_covar(Kxx) # [N, 1]\n",
    "        mean_components = jax.vmap(get_mean_from_covar)(Kxx_components) # [c, N, 1]\n",
    "\n",
    "        return jnp.var(mean_components[:,:,0], axis=-1) / jnp.var(mean_overall) # [c]\n",
    "\n",
    "\n",
    "class CustomConjugateMLL(gpx.objectives.ConjugateMLL):\n",
    "    def step(\n",
    "        self,\n",
    "        posterior: CustomConjugatePosterior,\n",
    "        train_data: VerticalDataset,\n",
    "    ) -> ScalarFloat:\n",
    "        x,y = posterior.smoother.smooth_data(train_data)\n",
    "        return super().step(posterior, gpx.Dataset(x, y))\n",
    "\n",
    "\n",
    "\n",
    "def plot_params(model, title=\"\"):\n",
    "    if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "        model = model.posterior\n",
    "    plt.figure()\n",
    "    if isinstance(model.prior.kernel, gpx.kernels.AdditiveKernel):\n",
    "        lengthscales = jnp.array([model.prior.kernel.kernels[i].lengthscale[0] for i in range(num_variables)])\n",
    "    else:\n",
    "        lengthscales = model.prior.kernel.lengthscale\n",
    "    smoothing_weights = model.smoother.smooth() # [4, 21]\n",
    "    for i in range(num_3d_variables):\n",
    "        plt.plot(smoothing_weights[i,:].T,pressure_levels[0,:], label=f\"{names_3d[i]} with lengthscales_ {lengthscales[i]:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.title(title+f\" other lengthscales are {lengthscales[num_3d_variables:]}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "@dataclass()\n",
    "class OrthogonalRBF(gpx.kernels.AbstractKernel):\n",
    "    r\"\"\"todo only for unit gaussian input measure and zero mean.\"\"\"\n",
    "    name: str = \"OrthogonalRBF\"\n",
    "    lengthscale: Union[ScalarFloat, Float[Array, \" D\"]] = gpx.param_field(\n",
    "        jnp.array(1.0), bijector=tfb.Softplus()\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        warnings.warn(\"This kernel is only valid for unit gaussian input measures and zero mean functions.\")\n",
    "\n",
    "    def __call__(self, x: Num[Array, \" D\"], y: Num[Array, \" D\"]) -> ScalarFloat:\n",
    "        r\"\"\"Compute an orthogonal RBF kernel between a pair of arrays.\"\"\"\n",
    "        x = self.slice_input(x) # [d]\n",
    "        y = self.slice_input(y) # [d]\n",
    "        ks = jnp.exp(-0.5 * ((x - y) / self.lengthscale) ** 2) # [d]\n",
    "        ks -=  self._cov_x_s(x) * self._cov_x_s(y) / self._var_s() # [d]\n",
    "        return jnp.prod(ks)\n",
    "    \n",
    "    def _cov_x_s(self,x):\n",
    "        l2 = self.lengthscale ** 2\n",
    "        return jnp.sqrt(l2 / (l2 + 1.0)) * jnp.exp(-0.5 * (x ** 2) / (l2 + 1.0)) # [d]\n",
    "        \n",
    "    def _var_s(self):\n",
    "        return  jnp.sqrt(self.lengthscale ** 2 / (self.lengthscale ** 2 + 2.0)) # [d]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RBF(RBF):\n",
    "    dummy: AbstractKernel = gpx.kernels.RBF()\n",
    "\n",
    "@dataclass\n",
    "class OrthogonalRBF(OrthogonalRBF):\n",
    "    dummy: AbstractKernel = gpx.kernels.RBF()\n",
    "\n",
    "\n",
    "\n",
    "# prep fancy kernel for model\n",
    "# first fit with small data to get init for SVGP\n",
    "num_data_for_init=100\n",
    "D_small = VerticalDataset(\n",
    "    X2d = X2d[:num_data_for_init,:],\n",
    "    X3d = X3d[:num_data_for_init,:,:],\n",
    "    Xstatic = Xstatic[:num_data_for_init,:],\n",
    "    y=Y[:num_data_for_init,:],\n",
    ")\n",
    "\n",
    "lengthscale_bij = tfb.SoftClip(jnp.array(1e-2, dtype=jnp.float64),jnp.array(1e2, dtype=jnp.float64))\n",
    "variance_bij = tfb.SoftClip(jnp.array(1e-4, dtype=jnp.float64),jnp.array(1e5, dtype=jnp.float64))\n",
    "noise_bij = tfb.SoftClip(jnp.array(1e-6, dtype=jnp.float64),jnp.array(1e5, dtype=jnp.float64))\n",
    "smoother = VerticalSmoother(jnp.array([[0.0]*num_3d_variables]), jnp.array([[1.0]*num_3d_variables]))\n",
    "\n",
    "# kernel = RBF(variance = jnp.var(Y), lengthscale=jnp.array([1.0]*num_variables)).replace_bijector(lengthscale=lengthscale_bij)\n",
    "# base_kernels= [RBF(lengthscale=jnp.array([1.0]), active_dims=[i]).replace_bijector(lengthscale=lengthscale_bij).replace_trainable(variance=False) for i in range(num_variables)]\n",
    "base_kernels= [OrthogonalRBF(lengthscale=jnp.array([1.0]), active_dims=[i]).replace_bijector(lengthscale=lengthscale_bij) for i in range(num_variables)]\n",
    "max_interaction = 2\n",
    "kernel = gpx.kernels.AdditiveKernel(kernels=base_kernels,interaction_variances=jnp.array([1.0]*(max_interaction + 1)) * jnp.var(D_small.y), max_interaction_depth=max_interaction).replace_bijector(interaction_variances=variance_bij)\n",
    "\n",
    "\n",
    "prior = gpx.gps.Prior(mean_function= gpx.mean_functions.Zero(), kernel = kernel)\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=num_data_for_init)\n",
    "# posterior = CustomConjugatePosterior(prior=prior, likelihood=likelihood, smoother=smoother)\n",
    "posterior = CustomAdditiveConjugatePosterior(prior=prior, likelihood=likelihood, smoother=smoother)\n",
    "objective = jax.jit(CustomConjugateMLL(negative=True))\n",
    "opt_posterior, history = gpx.fit(\n",
    "    model=posterior,\n",
    "    objective=objective,\n",
    "    train_data=D_small,\n",
    "    optim=ox.adamw(learning_rate=1e-1),\n",
    "    num_iters=400,\n",
    "    key=jr.PRNGKey(42),\n",
    "    safe=False,\n",
    ")\n",
    "plt.plot(history)\n",
    "plot_params(opt_posterior, title=\"initial fit with small data\")\n",
    "\n",
    "\n",
    "\n",
    "# # choose inducing inputs and init SVGP\n",
    "# num_inducing = 100\n",
    "# z = jr.normal(key, (num_inducing , D.X.shape[-1])) # allow this\n",
    "\n",
    "# init_posterior_at_inducing = opt_posterior.predict(z, D_small)\n",
    "# # todo try whitening ?\n",
    "# q = gpx.variational_families.VariationalGaussian(\n",
    "#     posterior=opt_posterior, \n",
    "#     inducing_inputs=z,\n",
    "#     variational_mean= init_posterior_at_inducing.mean()[:,None],\n",
    "#     variational_root_covariance= jnp.linalg.cholesky(init_posterior_at_inducing.covariance() + 1e-5 * jnp.eye(num_inducing)), # todo check this is right!\n",
    "# )\n",
    "\n",
    "# #q = q.replace_trainable(inducing_inputs=False)\n",
    "# # q = q.replace_trainable(variational_mean=False)\n",
    "# # q = q.replace_trainable(variational_root_covariance=False)\n",
    "\n",
    "# objective = jax.jit(gpx.objectives.ELBO(negative=True))\n",
    "\n",
    "\n",
    "# def zero_grads():\n",
    "#     def init_fn(_): \n",
    "#         return ()\n",
    "#     def update_fn(updates, state, params=None):\n",
    "#         return jax.tree_map(jnp.zeros_like, updates), ()\n",
    "#     return ox.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "# optim_pytree = q.replace(\n",
    "#     posterior = zero_grads(), \n",
    "#     variational_mean = ox.adamw(1e-2),\n",
    "#     variational_root_covariance = ox.adamw(1e-2), \n",
    "#     inducing_inputs= ox.adamw(1e-2),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# optim = optim_builder(optim_pytree)\n",
    "\n",
    "# opt_q, history = gpx.fit(\n",
    "#     model=q,\n",
    "#     objective=objective,\n",
    "#     train_data=D,\n",
    "#     optim=optim,\n",
    "#     num_iters=500,\n",
    "#     key=jr.PRNGKey(42),\n",
    "#     batch_size=512,\n",
    "# )\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(history)\n",
    "# plot_params(opt_q,D,  title=\"full fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = [[i] for i in range(num_variables)]\n",
    "idx_2 = []\n",
    "for i in range(num_variables):\n",
    "    for j in range(i+1,num_variables):\n",
    "        idx_2.append([i,j])\n",
    "idxs = idx_1 + idx_2       \n",
    "sobols = opt_posterior.get_sobol_indicies(D_small, idxs)\n",
    "sobols = sobols / jnp.sum(sobols)\n",
    "plt.plot(sobols)\n",
    "k=5\n",
    "top_idx = jax.lax.top_k(sobols, k)[1]\n",
    "for idx in top_idx:\n",
    "    chosen_idx = idxs[idx]\n",
    "    plt.figure()\n",
    "    x_plot = jr.uniform(key, (10_000, num_2d_variables), minval=-2, maxval=2)\n",
    "    posterior_dist = opt_posterior.predict_additive_component(x_plot, D_small, chosen_idx)\n",
    "    mean = posterior_dist.mean()\n",
    "    if len(chosen_idx)==1:\n",
    "        plt.scatter(x_plot[:,chosen_idx[0]],mean) \n",
    "    elif len(chosen_idx)==2:\n",
    "        plt.scatter(x_plot[:,chosen_idx[0]],x_plot[:,chosen_idx[1]],c=mean)\n",
    "    plt.title(f\"variable {[names[i] for i in chosen_idx]} with sobol index {sobols[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
