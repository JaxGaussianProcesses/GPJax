{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jax\n",
    "import json\n",
    "from jax.tree_util import Partial\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "from typing import List, Union\n",
    "\n",
    "import cola\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import (\n",
    "    Array,\n",
    "    Float,\n",
    "    install_import_hook,\n",
    "    Num,\n",
    ")\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "#with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "import gpjax as gpx\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarInt,\n",
    "    ScalarFloat,\n",
    ")\n",
    "from gpjax.distributions import GaussianDistribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "\n",
    "\n",
    "import gpjax as gpx\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "key = jr.PRNGKey(123)\n",
    "\n",
    "import scipy.optimize as spo\n",
    "from dataclasses import dataclass\n",
    "from gpjax.base import param_field, static_field\n",
    "import math\n",
    "from gpjax.kernels.base import AbstractKernel\n",
    "from gpjax.kernels import RBF\n",
    "import optax as ox\n",
    "from gpjax.kernels.stationary.utils import squared_distance\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarFloat,\n",
    ")\n",
    "from jaxtyping import (\n",
    "    Float,\n",
    "    Num,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass()\n",
    "class AdditiveKernel(gpx.kernels.AbstractKernel):\n",
    "    r\"\"\"Build an additive kernel from a list of individual base kernels for a specific maximum interaction depth.\"\"\"\n",
    "    kernels: list[gpx.kernels.AbstractKernel] = None\n",
    "    max_interaction_depth: ScalarInt = gpx.base.static_field(1)\n",
    "    interaction_variances: Float[Array, \" p\"] = gpx.param_field(jnp.array([1.0, 1.0]), bijector=tfb.Softplus())\n",
    "    name: str = \"AdditiveKernel\"\n",
    "\n",
    "    def __post_init__(self): # jax/jit requires specifying max_interaction depth even though this could be infered from length of interaction_variances\n",
    "        assert self.max_interaction_depth == len(self.interaction_variances) - 1, \"Number of interaction variances must be equal to max_interaction_depth + 1\"\n",
    "\n",
    "    def __call__(self, x: Num[Array, \" D\"], y: Num[Array, \" D\"]) -> ScalarFloat:\n",
    "        r\"\"\"Compute the additive kernel between a pair of arrays.\"\"\"\n",
    "        ks = jnp.stack([k(self.slice_input(x),self.slice_input(y)) for k in self.kernels])\n",
    "        return jnp.dot(self._compute_additive_terms_girad_newton(ks), self.interaction_variances)\n",
    "            \n",
    "    @jax.jit   \n",
    "    def _compute_additive_terms_girad_newton(self, ks: Num[Array, \" D\"]) -> ScalarFloat:\n",
    "        r\"\"\"Given a list of inputs, compute a new list containing all products up to order\n",
    "        `max_interaction_depth`. For efficiency, we us the Girad Newton identity \n",
    "        (i.e. O(d^2) instead of exponential).\n",
    "        \"\"\"\n",
    "        powers = jnp.arange(self.max_interaction_depth + 1)[:, None] # [p + 1, 1]\n",
    "        s = jnp.power(ks[None, :],powers) # [p+1, d]\n",
    "        e = jnp.ones(shape=(self.max_interaction_depth+1), dtype=jnp.float64) # lazy init then populate\n",
    "        for n in range(1, self.max_interaction_depth + 1): # has to be for loop because iterative\n",
    "            thing = jax.vmap(lambda k: ((-1.0)**(k-1))*e[n-k]*s[k, :])(jnp.arange(1, n+1))\n",
    "            e = e.at[n].set((1.0/n) *jnp.sum(thing))\n",
    "        return jnp.array(e) # [max_interaction_depth + 1]\n",
    "    \n",
    "    def get_specific_kernel(self, component_list: List[int] = []) -> gpx.kernels.AbstractKernel:\n",
    "        r\"\"\" Get a specific kernel from the additive kernel corresponding to component_list.\"\"\"\n",
    "        var = self.interaction_variances[len(component_list)]\n",
    "        kernel = gpx.kernels.Constant(constant = var)\n",
    "        for i in component_list:\n",
    "            kernel = kernel * self.kernels[i]\n",
    "        return kernel\n",
    "    \n",
    "\n",
    "\n",
    "class AdditivePosterior(gpx.gps.ConjugatePosterior):\n",
    "    r\"\"\"\n",
    "    Build an additive posterior from an additive kernel and a Gaussian likelihood. We have included an\n",
    "    additional method to allow predictions for specific additive components, as specified by a\n",
    "    component_list, e.g. [0, 1] corresponds to the component that takes in the zeroth and first inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __post__init__(self):\n",
    "        assert isinstance(self.prior.kernel, AdditiveKernel), \"AdditivePosterior requires an AdditiveKernel\"\n",
    "\n",
    "    def predict_additive_component(\n",
    "        self,\n",
    "        test_inputs: Num[Array, \"N D\"],\n",
    "        train_data: gpx.Dataset,\n",
    "        component_list: List[List[int]]\n",
    "    ) -> GaussianDistribution:\n",
    "        r\"\"\"Get the posterior predictive distribution for a specific additive component.\"\"\"\n",
    "        specific_kernel = self.prior.kernel.get_specific_kernel(component_list)\n",
    "        Kxx = self.prior.kernel.gram(train_data.X)\n",
    "        Kxt = specific_kernel.cross_covariance(train_data.X, test_inputs)\n",
    "        Sigma = cola.PSD(Kxx + cola.ops.I_like(Kxx) * self.likelihood.obs_stddev**2)\n",
    "        Sigma_inv_Kxt = cola.solve(Sigma, Kxt)\n",
    "        mean =  jnp.matmul(Sigma_inv_Kxt.T, train_data.y)\n",
    "        covariance = specific_kernel.gram(test_inputs) - jnp.matmul(Kxt.T, Sigma_inv_Kxt)\n",
    "        return GaussianDistribution(jnp.atleast_1d(mean.squeeze()), cola.PSD(covariance))\n",
    "\n",
    "    def get_sobol_index(self, train_data: gpx.Dataset, component_list: List[int]) -> ScalarFloat:\n",
    "        \"\"\" Return the sobol index for the additive component corresponding to component_list. \"\"\"\n",
    "        component_posterior = self.predict_additive_component(train_data.X, train_data, component_list)\n",
    "        full_posterior= self.predict(train_data.X, train_data)\n",
    "        return jnp.var(component_posterior.mean()) / jnp.var(full_posterior.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100_000 entries sampled across time/lat/lon over first day of data\n",
    "\n",
    "## X2D has:\n",
    "\n",
    "0\"Sea surface temperature (K)\"\n",
    "\n",
    "1\"Sensible heat flux (W/m^2)\"\n",
    "\n",
    "2\"Latent heat flux (W/m^2)\"\n",
    "\n",
    "3\"Vertically-integrated moisture convergence (kg/m^2)\"\n",
    "\n",
    "4\"Column relative humidity (%)\"\n",
    "\n",
    "\n",
    "## X3D has:\n",
    "\n",
    "0\"Absolute temperature (K)\"\n",
    "\n",
    "1\"Relative humidity (%)\"\n",
    "\n",
    "2\"Specific humidity (kg/kg)\"\n",
    "\n",
    "3\"Geopotential height (m^2 s^-2)\"\n",
    "\n",
    "4\"Zonal wind (m/s)\"\n",
    "\n",
    "5\"Meridional wind (m/s)\"\n",
    "\n",
    "6\"Potential temperature (K)\"\n",
    "\n",
    "7\"Equivalent potential temperature (K)\"\n",
    "\n",
    "8\"Equivalent potential temperature saturation deficit (K)\"\n",
    "\n",
    "9\"Saturated equivalent potential temperature (K)\"\n",
    "\n",
    "10\"MSE-conserving plume buoyancy (m/s^2)\"\n",
    "\n",
    "\n",
    "## static has:\n",
    "\n",
    "0\"Land-sea mask\"\n",
    "\n",
    "1\"Angle of sub-gridscale orography (rad)Anisotropy of sub-gridscale orography\"\n",
    "\n",
    "2\"Standard deviation of sub-gridscale orography\"\n",
    "\n",
    "3\"Slope of sub-gridscale orography\"\n",
    "\n",
    "## Y has:\n",
    "\n",
    "0\"ERA5 Precipitation (mm/hr)\"\n",
    "\n",
    "1\"TRMM Precipitation (mm/hr)\"\n",
    "\n",
    "2\"TRMM Relative Error (%)\"\n",
    "\n",
    "# plev are\n",
    "1000.,   2000.,   3000.,   5000.,   7000.,  10000., 15000.,\n",
    "20000.,  25000.,  30000.,  40000.,  50000.,  60000.,  70000.,80000.,  85000.,  90000.,  92500.,  95000.,  97500., 100000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X2d = jnp.array(jnp.load(\"..//data/100_000_one_day/X2D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "X3d = jnp.array(jnp.load(\"../data/100_000_one_day/X3D_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Xstatic = jnp.array(jnp.load(\"../data/100_000_one_day/XSTATIC_sample.npy\"), dtype=jnp.float64) # [N, D]\n",
    "Y = jnp.array(jnp.load(\"../data/100_000_one_day/Y_sample.npy\"), dtype=jnp.float64) # [N, 1]\n",
    "pressure = jnp.array([[1000,2000,3000,5000,7000,10000,15000,20000,25000,30000,40000,50000,60000,70000,80000, 85000,90000,92500,95000,97500,100000]], dtype=jnp.float64)\n",
    "\n",
    "\n",
    "# look at ERA5 rain\n",
    "Y = Y[:,0:1]  \n",
    "\n",
    "# remove any entries with nan\n",
    "X3d_nan_idx = jnp.isnan(X3d).any(axis=1).any(axis=1)\n",
    "X2d_nan_idx = jnp.isnan(X2d).any(axis=1)\n",
    "Xstatic_nan_idx = jnp.isnan(Xstatic).any(axis=1)\n",
    "Y_nan_idx = jnp.isnan(Y).any(axis=1)\n",
    "any_nan = X3d_nan_idx | X2d_nan_idx | Y_nan_idx | Xstatic_nan_idx\n",
    "no_nan = ~ any_nan\n",
    "print(f\"Removed {any_nan.sum()} entries with nan\")\n",
    "X2d = X2d[no_nan,:]\n",
    "X3d = X3d[no_nan,:,:]\n",
    "Xstatic = Xstatic[no_nan,:]\n",
    "Y = Y[no_nan,:]\n",
    "\n",
    "\n",
    "# just keep the \"prognostic\" inputs that joe considered (for now)\n",
    "# RH, tehta_e^+, theta_e, theta_e^*\n",
    "names_3d =  [\"RH\", \"tehta_e^+\", \"theta_e\", \"theta_e^*\"]\n",
    "idx_3d = [1, 9, 7, 8 ]\n",
    "X3d = X3d[:,idx_3d,:]\n",
    "\n",
    "\n",
    "# # also include his \"normalised\" sigma_o\n",
    "# sigma_o = jnp.where(Xstatic[:,0]<0.5, 0.0, 1.0+jnp.log(1+Xstatic[:,2]))\n",
    "# X2d = sigma_o[:,None]\n",
    "\n",
    "\n",
    "#remove all pressure levels below 500 hPA\n",
    "lowest_idx = 7\n",
    "print(f\"Removed all pressure levels below {pressure[:,lowest_idx]} hPa\")\n",
    "\n",
    "X3d = X3d[:, :, lowest_idx:]\n",
    "pressure_levels = pressure[:,lowest_idx:]\n",
    "\n",
    "# remove no rain days\n",
    "print(f\"Removed {(Y[:,0]>0).sum()} entries with zero rain\")\n",
    "X3d = X3d[Y[:,0]>0,:]\n",
    "X2d = X2d[Y[:,0]>0,:]\n",
    "Y = Y[Y[:,0]>0,:]\n",
    "# also log Y\n",
    "print(f\"Applied log transform to Y\")\n",
    "Y = jnp.log(Y)\n",
    "print(f\"then standardized Y\")\n",
    "Y_mean = jnp.mean(Y)\n",
    "Y_std = jnp.std(Y)\n",
    "Y = (Y - Y_mean) / Y_std\n",
    "\n",
    "# standardize inputs\n",
    "X3d_std = jnp.std(X3d, axis=0)\n",
    "X3d_mean = jnp.mean(X3d,axis=0)\n",
    "X3d = (X3d - X3d_mean) / X3d_std\n",
    "X2d_std = jnp.std(X2d, axis=0)\n",
    "X2d_mean = jnp.mean(X2d,axis=0)\n",
    "X2d = (X2d - X2d_mean) / X2d_std\n",
    "\n",
    "\n",
    "# look at tiny data for now\n",
    "N_train = 10_000\n",
    "print(f\"Only using {N_train} training points!\")\n",
    "X3d_train, X2d_train, Y_train = X3d[:N_train,:],X2d[:N_train,:], Y[:N_train,:]\n",
    "num_2d_variables= X2d_train.shape[-1]\n",
    "X_train = jnp.hstack((X3d_train.reshape(len(X3d_train), -1), X2d_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VerticallyIntegratedKernel(AbstractKernel):\n",
    "    base_kernel: AbstractKernel = gpx.kernels.RBF()\n",
    "    smoother_mean: Float[Array, \" D\"]  = param_field(None)\n",
    "    smoother_input_scale: Float[Array, \" D\"] = param_field(None, bijector=tfb.Softplus(low=jnp.array(1.0e-5, dtype=jnp.float64)))\n",
    "    #smoother_bias: Float[Array, \" D\"] = param_field(None, bijector=tfb.Softplus(low=jnp.array(1.0e-5, dtype=jnp.float64)))\n",
    "    #smoother_output_scale: Float[Array, \" D\"] = param_field(None, bijector=tfb.Softplus(low=jnp.array(1.0e-5, dtype=jnp.float64)))\n",
    "    Z_levels: Float[Array, \" D\"] = static_field(pressure_levels)\n",
    "   \n",
    "    def __post_init__(self):\n",
    "        self.Z_mean = jnp.mean(self.Z_levels)\n",
    "        self.Z_std = jnp.std(self.Z_levels)\n",
    "        self.Z_levels = (self.Z_levels - self.Z_mean) / self.Z_std\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        x, y = self.slice_input(x), self.slice_input(y)\n",
    "        smoothed_x, smoothed_y = self._smooth(x), self._smooth(y)\n",
    "        return self.base_kernel(smoothed_x, smoothed_y )\n",
    "    \n",
    "    def _smooth(self, x): # [d*z] -> [d]\n",
    "        x_to_smooth = x.reshape(-1,self.Z_levels.shape[1]) # [d, z]\n",
    "        smoothing_weights = jnp.exp(-0.5*((self.Z_levels-self.smoother_mean.T)/(self.smoother_input_scale.T))**2) # [d, z]\n",
    "        smoothing_weights = (smoothing_weights/ jnp.sum(smoothing_weights, axis=-1, keepdims=True)) # [d, z]\n",
    "        return jnp.sum(jnp.multiply(smoothing_weights ,x_to_smooth), axis=-1) # [d]\n",
    "    \n",
    "\n",
    "\n",
    "def prep_kernel(D: gpx.Dataset, num_2d_variables:int, bij_lengthscale:bool = True):\n",
    "    lengthscale_bij = tfb.SoftClip(jnp.array(1e-2, dtype=jnp.float64),jnp.array(1e2, dtype=jnp.float64))\n",
    "    base_kernel = RBF(lengthscale=jnp.array([1.0]*4), variance = jnp.var(D.y))\n",
    "    if bij_lengthscale:\n",
    "        base_kernel = base_kernel.replace_bijector( lengthscale = lengthscale_bij)\n",
    "    kernel_3d = VerticallyIntegratedKernel(\n",
    "        base_kernel = base_kernel, \n",
    "        smoother_mean = jnp.array([[0.0]*4]),\n",
    "        smoother_input_scale = jnp.array([[1.0]*4]),\n",
    "        active_dims=jnp.arange(X_train.shape[-1])[:-num_2d_variables]\n",
    "        )\n",
    "    kernel_2d = RBF(lengthscale=jnp.array([0.1]*num_2d_variables), active_dims=jnp.arange(X_train.shape[-1])[-num_2d_variables:])\n",
    "    kernel_2d = kernel_2d.replace_trainable( variance = False)\n",
    "    if bij_lengthscale:\n",
    "        kernel_2d = kernel_2d.replace_bijector( lengthscale = lengthscale_bij)\n",
    "    return kernel_3d * kernel_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_params(model, title=None):\n",
    "    if isinstance(model, gpx.variational_families.AbstractVariationalFamily):\n",
    "        model = model.posterior\n",
    "    plt.figure()\n",
    "    opt_3d_kernel =model.prior.kernel.kernels[0]\n",
    "    lengthscales = opt_3d_kernel.base_kernel.lengthscale\n",
    "    #smoothing_weights = opt_3d_kernel.smoother_bias.T + (1 / (jnp.sqrt(2*math.pi)*opt_3d_kernel.smoother_input_scale.T))*jnp.exp(-0.5*(opt_3d_kernel.Z_levels-opt_3d_kernel.smoother_mean.T)**2/(opt_3d_kernel.smoother_input_scale).T) # [4, 21]\n",
    "    #smoothing_weights = (jnp.exp(-0.5*((opt_3d_kernel.Z_levels-opt_3d_kernel.smoother_mean.T)/(opt_3d_kernel.smoother_input_scale.T))**2)) # [4, 21]\n",
    "    smoothing_weights =  jnp.exp(-0.5*((opt_3d_kernel.Z_levels-opt_3d_kernel.smoother_mean.T)/(opt_3d_kernel.smoother_input_scale.T))**2) # [4, 21]\n",
    "    smoothing_weights = (smoothing_weights/ jnp.sum(smoothing_weights, axis=-1, keepdims=True)) # [4, 21]\n",
    "    for i in range(len(names_3d )):\n",
    "        plt.plot(smoothing_weights[i,:].T,pressure_levels[0,:], label=f\"{names_3d[i]} with lengthscales {lengthscales[i]:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.title(title+f\"other lengthscales are {model.prior.kernel.kernels[1].lengthscale}\")\n",
    "    plt.gca().invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Gamma(gpx.likelihoods.AbstractLikelihood):\n",
    "    v: Float[Array, \"#N\"] = param_field(\n",
    "        jnp.array(10.0), bijector=tfb.Softplus()\n",
    "    )\n",
    "    def link_function(self, f):\n",
    "        #https://www2.imm.dtu.dk/pubdb/edoc/imm6637.pdf\n",
    "        #C = jax.scipy.stats.norm.cdf(f)\n",
    "        C = jnp.exp(f)\n",
    "        return tfd.Gamma(self.v, C, allow_nan_stats=False)\n",
    "\n",
    "    def predict(self, dist: tfd.Distribution) -> tfd.Distribution:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "@dataclass\n",
    "class Exponential(gpx.likelihoods.AbstractLikelihood):\n",
    "    def link_function(self, f):\n",
    "        C = jnp.exp(f)\n",
    "        return tfd.Exponential(1/C)\n",
    "\n",
    "    def predict(self, dist: tfd.Distribution) -> tfd.Distribution:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep fancy kernel for model\n",
    "D = gpx.Dataset(X=X_train,y = Y_train)\n",
    "prior = gpx.gps.Prior(mean_function= gpx.mean_functions.Zero(), kernel = prep_kernel(D, num_2d_variables))\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n",
    "\n",
    "\n",
    "# first fit with small data to get init for SVGP\n",
    "num_data_for_init=100\n",
    "D_small = gpx.Dataset(X = D.X[:num_data_for_init,:],y = D.y[:num_data_for_init,:])\n",
    "posterior = prior * likelihood\n",
    "objective = jax.jit(gpx.objectives.ConjugateMLL(negative=True))\n",
    "#objective = jax.jit(gpx.objectives.NonConjugateMLL(negative=True))\n",
    "opt_posterior, history = gpx.fit(\n",
    "    model=posterior,\n",
    "    objective=objective,\n",
    "    train_data=D_small,\n",
    "    optim=ox.adam(learning_rate=1e-1),\n",
    "    num_iters=100,\n",
    "    key=jr.PRNGKey(42),\n",
    ")\n",
    "plt.plot(history)\n",
    "plot_params(opt_posterior, title=\"initial fit with small data\")\n",
    "\n",
    "\n",
    "\n",
    "# # choose inducing inputs and init SVGP\n",
    "# num_inducing = 100\n",
    "# #z = jr.normal(key, (num_inducing , D.X.shape[-1])) # todo enable this\n",
    "# z = D.X[:num_inducing,:]\n",
    "\n",
    "\n",
    "# init_posterior_at_inducing = opt_posterior.predict(z, D_small)\n",
    "# # todo try whitening ?\n",
    "# q = gpx.variational_families.VariationalGaussian(\n",
    "#     posterior=opt_posterior, \n",
    "#     inducing_inputs=z,\n",
    "#     variational_mean= init_posterior_at_inducing.mean()[:,None],\n",
    "#     variational_root_covariance= jnp.linalg.cholesky(init_posterior_at_inducing.covariance()), # todo check this is right!\n",
    "# )\n",
    "# q = q.replace_trainable(inducing_inputs=False)\n",
    "# q = q.replace_trainable(variational_mean=False)\n",
    "# q = q.replace_trainable(variational_root_covariance=False)\n",
    "\n",
    "# objective = jax.jit(gpx.objectives.ELBO(negative=True))\n",
    "\n",
    "# optim=ox.adam(1e-1)\n",
    "\n",
    "# opt_q, history = gpx.fit(\n",
    "#     model=q,\n",
    "#     objective=objective,\n",
    "#     train_data=D,\n",
    "#     optim=optim,\n",
    "#     num_iters=100,\n",
    "#     key=jr.PRNGKey(42),\n",
    "#     batch_size=128,\n",
    "# )\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(history)\n",
    "# plot_params(opt_q, title=\"full fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nested_fn(fn):\n",
    "  '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
    "  def map_fn(nested_dict):\n",
    "    if not isinstance(nested_dict,dict):\n",
    "      nested_dict = nested_dict.__dict__\n",
    "    return {k: (map_fn(v) if isinstance(v, dict) else fn(k, v))\n",
    "            for k, v in nested_dict.items()}\n",
    "  return map_fn\n",
    "label_fn = map_nested_fn(lambda k, _: k)\n",
    "\n",
    "\n",
    "optim = ox.chain(\n",
    "    ox.multi_transform(\n",
    "      {\n",
    "        \"posterior\": ox.adam(1e-1), \n",
    "        \"variational_mean\": zero_grads(), \n",
    "        \"variational_root_covariance\": zero_grads(),\n",
    "        \"jitter\": zero_grads(), \n",
    "        \"pytree_node\": zero_grads(), \n",
    "        \"trainable\": zero_grads(), \n",
    "        \"inducing_inputs\": zero_grads(),\n",
    "        \"bijector\": zero_grads(),\n",
    "        },label_fn), # opt kernel params\n",
    "    #ox.multi_transform({\"posterior\": zero_grads(), \"variational_mean\": ox.adam(1e-1), \"variational_root_covariance\": ox.adam(1e-1)},label_fn), # opt variational params\n",
    "    )\n",
    "\n",
    "optim.init(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nested_fn(fn):\n",
    "  '''Recursively apply `fn` to the key-value pairs of a nested dict'''\n",
    "  def map_fn(nested_dict):\n",
    "    if not isinstance(nested_dict,dict):\n",
    "      nested_dict = nested_dict.__dict__\n",
    "    return {k: (fn(k, v))\n",
    "            for k, v in nested_dict.items()}\n",
    "  return map_fn\n",
    "\n",
    "label_fn = map_nested_fn(lambda k, _: k)\n",
    "\n",
    "label_fn(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_test = 100\n",
    "# X_test_pred = opt_posterior.predict(X[-N_test:,:], D)\n",
    "# means = X_test_pred.mean()\n",
    "# if isinstance(opt_posterior.likelihood, Exponential):\n",
    "#     pred_means = 1 / jnp.exp(means)\n",
    "# elif isinstance(opt_posterior.likelihood, Gamma):\n",
    "#     pred_means = opt_posterior.likelihood.v / jnp.exp(means)\n",
    "# for i in range(10):\n",
    "#     print(f\"Predict {pred_means[i]} for truth {Y[-N_test+i]}\")\n",
    "\n",
    "# plt.hist(Y[-N_test:,0],alpha=0.5,label=\"true\", bins=20)\n",
    "# plt.hist(pred_means, alpha=0.5, label=\"pred\", bins=20)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spo.Bounds(\n",
    "    lb = \n",
    "    ub = \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub = [jnp.inf,  jnp.inf,  jnp.inf, 0.001,  jnp.inf,  jnp.inf,  jnp.inf,  jnp.inf, 0.001,jnp.inf]\n",
    "lb = [-jnp.inf,  -jnp.inf,  -jnp.inf, 10.0,  -jnp.inf,  -jnp.inf,  -jnp.inf,  -jnp.inf, 10.0,-jnp.inf]\n",
    "bounds = spo.Bounds(lb, ub)\n",
    "\n",
    "jax.tree_util.tree_flatten(posterior)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_util.tree_flatten(posterior)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_util.tree_flatten(posterior)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Z = jnp.array([[20,25,30,40,50,60,70,80,85,90,92.5,95,97.5,100]], dtype=jnp.float64)\n",
    "Z_mean = jnp.mean(Z)\n",
    "Z_std = jnp.std(Z)\n",
    "Z_levels = (Z - Z_mean) / Z_std\n",
    "\n",
    "@dataclass\n",
    "class RBFKernelofRBFKernels(RBF):\n",
    "    smoother_mean: Float[Array, \" D\"]  = param_field(jnp.array([[0.0]*4], dtype=jnp.float64))\n",
    "    smoother_input_scale: Float[Array, \" D\"] = param_field(jnp.array([[1.0]*4], dtype=jnp.float64), bijector=tfb.Softplus())\n",
    "    Z_levels: Float[Array, \" D\"] = static_field(Z_levels)\n",
    "    #smoother_output_scale: Float[Array, \" D\"] = param_field(jnp.array([[1.0]*4], dtype=jnp.float64), bijector=tfb.Softplus())\n",
    "    smoother_bias: Float[Array, \" D\"] = param_field(jnp.array([[0.1]*4], dtype=jnp.float64), bijector=tfb.Softplus())\n",
    "\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        x = jnp.hstack([self._smooth(x[:84]), x[None, 84:85]]) # [1, 5]\n",
    "        y = jnp.hstack([self._smooth(y[:84]), y[None, 84:85]]) # [1, 5]\n",
    "\n",
    "        x = self.slice_input(x) / self.lengthscale\n",
    "        y = self.slice_input(y) / self.lengthscale\n",
    "        K = self.variance * jnp.exp(-0.5 * squared_distance(x, y))\n",
    "        return K.squeeze()\n",
    "    \n",
    "    def _smooth(self, x): # [1,4*21] -> [1, 4]\n",
    "        x_to_smooth = x.reshape(4,21) # [4, 21]\n",
    "        x_to_smooth = x_to_smooth[:,7:] # [4, 14]\n",
    "        smoothing_weights =self.smoother_bias.T + (1 / (jnp.sqrt(2*math.pi)*self.smoother_input_scale.T))*jnp.exp(-0.5*(self.Z_levels-self.smoother_mean.T)**2/(self.smoother_input_scale**2).T) # [4, 21]\n",
    "        nabla = self.Z_levels[:,1:] - self.Z_levels[:,0:-1]\n",
    "        smoothing_weights = jnp.multiply(smoothing_weights[:,:-1],nabla)\n",
    "        x_smoothed = jnp.multiply(smoothing_weights, x_to_smooth[:,:-1]) # [4, 21]\n",
    "        x_smoothed = jnp.sum(x_smoothed.T, axis=0, keepdims=True) # [1, 4]\n",
    "        return x_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Gamma(gpx.likelihoods.AbstractLikelihood):\n",
    "    v: Float[Array, \"#N\"] = param_field(\n",
    "        jnp.array(10.0), bijector=tfb.Softplus()\n",
    "    )\n",
    "    def link_function(self, f):\n",
    "        #https://www2.imm.dtu.dk/pubdb/edoc/imm6637.pdf\n",
    "        #C = jax.scipy.stats.norm.cdf(f)\n",
    "        C = jnp.exp(f)\n",
    "        return tfd.Gamma(self.v, C)\n",
    "\n",
    "\n",
    "    def predict(self, dist: tfd.Distribution) -> tfd.Distribution:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "@dataclass\n",
    "class Exponential(gpx.likelihoods.AbstractLikelihood):\n",
    "    def link_function(self, f):\n",
    "        C = jnp.exp(f)\n",
    "        return tfd.Exponential(C)\n",
    "\n",
    "    def predict(self, dist: tfd.Distribution) -> tfd.Distribution:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = gpx.Dataset(X=X_train,y = Y_train)\n",
    "prior = gpx.Prior(mean_function= gpx.Zero(), kernel=RBFKernelofRBFKernels())\n",
    "likelihood = Exponential(num_datapoints=D.n)\n",
    "posterior = prior * likelihood\n",
    "negative_lpd = jax.jit(gpx.LogPosteriorDensity(negative=True))\n",
    "\n",
    "negative_lpd(posterior, D)\n",
    "\n",
    "opt_posterior, history = gpx.fit(\n",
    "    model=posterior,\n",
    "    objective=negative_lpd,\n",
    "    train_data=D,\n",
    "    optim=ox.adamw(learning_rate=0.1),\n",
    "    num_iters=1000,\n",
    "    key=key,\n",
    ")\n",
    "opt_posterior_old = opt_posterior\n",
    "plt.plot(history)\n",
    "\n",
    "plt.figure()\n",
    "names = [\"rhum\", \"theta_e_plus\", \"theta_e\", \"theta_e_sat\", \"sdor\"]\n",
    "smoothing_weights = opt_posterior.prior.kernel.smoother_bias.T + (1 / (jnp.sqrt(2*math.pi)*opt_posterior.prior.kernel.smoother_input_scale.T))*jnp.exp(-0.5*(opt_posterior.prior.kernel.Z_levels-opt_posterior.prior.kernel.smoother_mean.T)**2/(opt_posterior.prior.kernel.smoother_input_scale).T) # [4, 21]\n",
    "for i in range(4):\n",
    "    plt.plot(smoothing_weights[i,:].T,100*Z.T, label=names[i])\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 100\n",
    "X_test_pred = opt_posterior.predict(X[-N_test:,:], D)\n",
    "means = X_test_pred.mean()\n",
    "if isinstance(opt_posterior.likelihood, Exponential):\n",
    "    pred_means = 1 / jnp.exp(means)\n",
    "elif isinstance(opt_posterior.likelihood, Gamma):\n",
    "    pred_means = opt_posterior.likelihood.v / jnp.exp(means)\n",
    "for i in range(10):\n",
    "    print(f\"Predict {pred_means[i]} for truth {Y[-N_test+i]}\")\n",
    "\n",
    "plt.hist(Y[-N_test:,0],alpha=0.5,label=\"true\", bins=20)\n",
    "plt.hist(pred_means, alpha=0.5, label=\"pred\", bins=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cola\n",
    "from gpjax.lower_cholesky import lower_cholesky\n",
    "class ChainedGammaLogPosteriorDensity(gpx.objectives.AbstractObjective):\n",
    "    def step(self, posteriors, data):\n",
    "       \n",
    "        # Unpack the training data\n",
    "        x, y = data.X, data.y\n",
    "        Kxx_1 = posteriors[0].prior.kernel.gram(x)\n",
    "        Kxx_1 += cola.ops.I_like(Kxx_1) * posterior.prior.jitter\n",
    "        Kxx_1 = cola.PSD(Kxx_1)\n",
    "        Lx_1 = lower_cholesky(Kxx_1)\n",
    "\n",
    "        Kxx_2 = posteriors[1].prior.kernel.gram(x)\n",
    "        Kxx_2 += cola.ops.I_like(Kxx_2) * posterior.prior.jitter\n",
    "        Kxx_2 = cola.PSD(Kxx_2)\n",
    "        Lx_2 = lower_cholesky(Kxx_2)\n",
    "\n",
    "\n",
    "        # Compute the prior mean function\n",
    "        mx_1 = posteriors[0].prior.mean_function(x)\n",
    "        mx_2 = posteriors[1].prior.mean_function(x)\n",
    "\n",
    "        # Whitened function values, wx, corresponding to the inputs, x\n",
    "        wx_1 = posteriors[0].latent\n",
    "        wx_2 = posteriors[1].latent\n",
    "\n",
    "        # f(x) = mx  +  Lx wx\n",
    "        fx_1 = mx_1 + Lx_1 @ wx_1\n",
    "        fx_2 = mx_2 + Lx_2 @ wx_2\n",
    "\n",
    "        # p(y | f(x), θ), where θ are the model hyperparameters\n",
    "        a = jnp.exp(fx_1)\n",
    "        b =jnp.exp(fx_2)\n",
    "        likelihood = tfd.Gamma(a,b)\n",
    "\n",
    "        # Whitened latent function values prior, p(wx | θ) = N(0, I)\n",
    "        latent_prior = tfd.Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "        return self.constant * (\n",
    "            likelihood.log_prob(y).sum() + latent_prior.log_prob(wx_1).sum() + latent_prior.log_prob(wx_2).sum()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpjax.scan import vscan\n",
    "def fit_chained(  # noqa: PLR0913\n",
    "    *,\n",
    "    models,\n",
    "    objective,\n",
    "    train_data,\n",
    "    optim,\n",
    "    key,\n",
    "    num_iters,\n",
    "    unroll=1,\n",
    "    batch_size=-1,\n",
    "):\n",
    "    \n",
    "    # Unconstrained space loss function with stop-gradient rule for non-trainable params.\n",
    "    def loss(models, batch):\n",
    "        model1 = models[0].stop_gradient()\n",
    "        model2 = models[1].stop_gradient()\n",
    "        return objective((model1.constrain(), model2.constrain()), batch)\n",
    "\n",
    "    # Unconstrained space model.\n",
    "    models = (models[0].unconstrain(),models[1].unconstrain())\n",
    "\n",
    "    # Initialise optimiser state.\n",
    "    state = optim.init(models)\n",
    "\n",
    "    # Mini-batch random keys to scan over.\n",
    "    iter_keys = jr.split(key, num_iters)\n",
    "\n",
    "    # Optimisation step.\n",
    "    def step(carry, key):\n",
    "        models, opt_state = carry\n",
    "\n",
    "        if batch_size != -1:\n",
    "            batch = get_batch(train_data, batch_size, key)\n",
    "        else:\n",
    "            batch = train_data\n",
    "\n",
    "        loss_val, loss_gradient = jax.value_and_grad(loss)(models, batch)\n",
    "        updates, opt_state = optim.update(loss_gradient, opt_state, models)\n",
    "        models = ox.apply_updates(models, updates)\n",
    "\n",
    "        carry = models, opt_state\n",
    "        return carry, loss_val\n",
    "\n",
    "    # Optimisation scan.\n",
    "    scan = vscan\n",
    "\n",
    "    # Optimisation loop.\n",
    "    (models, _), history = scan(step, (models, state), (iter_keys), unroll=unroll)\n",
    "\n",
    "    # Constrained space.\n",
    "    models = (models[0].constrain(), models[1].constrain())\n",
    "\n",
    "    return models, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = gpx.Dataset(X=X_train,y = Y_train)\n",
    "posterior1 = prior * Exponential(num_datapoints=D.n) # this likelihood is ignored \n",
    "posterior2 = opt_posterior_old # use from before when fit exponential or gamma as warmstart\n",
    "posteriors = (posterior1, posterior2)\n",
    "\n",
    "opt_posteriors, history = fit_chained(\n",
    "    models=posteriors,\n",
    "    objective=jax.jit(ChainedGammaLogPosteriorDensity(negative=True)),\n",
    "    train_data=D,\n",
    "    optim=ox.adamw(learning_rate=0.1),\n",
    "    num_iters=1000,\n",
    "    key=key,\n",
    ")\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "names = [\"rhum\", \"theta_e_plus\", \"theta_e\", \"theta_e_sat\", \"sdor\"]\n",
    "smoothing_weights = opt_posteriors[0].prior.kernel.smoother_bias.T + (1 / (jnp.sqrt(2*math.pi)*opt_posteriors[0].prior.kernel.smoother_input_scale.T))*jnp.exp(-0.5*(opt_posteriors[0].prior.kernel.Z_levels-opt_posteriors[0].prior.kernel.smoother_mean.T)**2/(opt_posteriors[0].prior.kernel.smoother_input_scale).T) # [4, 21]\n",
    "for i in range(4):\n",
    "    plt.plot(smoothing_weights[i,:].T,100*Z.T, label=names[i])\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "names = [\"rhum\", \"theta_e_plus\", \"theta_e\", \"theta_e_sat\", \"sdor\"]\n",
    "smoothing_weights = opt_posteriors[1].prior.kernel.smoother_bias.T + (1 / (jnp.sqrt(2*math.pi)*opt_posteriors[1].prior.kernel.smoother_input_scale.T))*jnp.exp(-0.5*(opt_posteriors[1].prior.kernel.Z_levels-opt_posteriors[1].prior.kernel.smoother_mean.T)**2/(opt_posteriors[1].prior.kernel.smoother_input_scale).T) # [4, 21]\n",
    "for i in range(4):\n",
    "    plt.plot(smoothing_weights[i,:].T,100*Z.T, label=names[i])\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
