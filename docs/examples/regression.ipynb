{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f9899c",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In this notebook we demonstate how to fit a Gaussian process regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optax as ox\n",
    "from docs.examples.utils import clean_legend\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\"./gpjax.mplstyle\")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a019ce",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "With the necessary modules imported, we simulate a dataset\n",
    "$\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$\n",
    "sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs\n",
    "\n",
    "$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$\n",
    "\n",
    "We store our data $\\mathcal{D}$ as a GPJax `Dataset` and create test inputs and labels\n",
    "for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "noise = 0.3\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n",
    "ytest = f(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c0372",
   "metadata": {},
   "source": [
    "To better understand what we have simulated, we plot both the underlying latent\n",
    "function and the observed data that is subject to Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29061c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\n",
    "ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f787a1e",
   "metadata": {},
   "source": [
    "Our aim in this tutorial will be to reconstruct the latent function from our noisy\n",
    "observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a\n",
    "Gaussian process prior in the next section.\n",
    "\n",
    "## Defining the prior\n",
    "\n",
    "A zero-mean Gaussian process (GP) places a prior distribution over real-valued\n",
    "functions $f(\\cdot)$ where\n",
    "$f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$\n",
    "for any finite collection of inputs $\\boldsymbol{x}$.\n",
    "\n",
    "Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a\n",
    "user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$\n",
    "with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$.\n",
    "The choice of kernel function is critical as, among other things, it governs the\n",
    "smoothness of the outputs that our GP can generate.\n",
    "\n",
    "For simplicity, we consider a radial basis function (RBF) kernel:\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$\n",
    "\n",
    "On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$,\n",
    "we can reciprocate this process in GPJax via defining a `Prior` with our chosen `RBF`\n",
    "kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpx.kernels.RBF()\n",
    "meanf = gpx.mean_functions.Zero()\n",
    "prior = gpx.Prior(mean_function=meanf, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb271a6",
   "metadata": {},
   "source": [
    "\n",
    "The above construction forms the foundation for GPJax's models. Moreover, the GP prior\n",
    "we have just defined can be represented by a\n",
    "[TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/substrates/jax)\n",
    "multivariate Gaussian distribution. Such functionality enables trivial sampling, and\n",
    "the evaluation of the GP's mean and covariance ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dist = prior.predict(xtest)\n",
    "\n",
    "prior_mean = prior_dist.mean()\n",
    "prior_std = prior_dist.variance()\n",
    "samples = prior_dist.sample(seed=key, sample_shape=(20,))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\n",
    "ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    prior_mean - prior_std,\n",
    "    prior_mean + prior_std,\n",
    "    alpha=0.3,\n",
    "    color=cols[1],\n",
    "    label=\"Prior variance\",\n",
    ")\n",
    "ax.legend(loc=\"best\")\n",
    "ax = clean_legend(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b1df9",
   "metadata": {},
   "source": [
    "## Constructing the posterior\n",
    "\n",
    "Having defined our GP, we proceed to define a description of our data\n",
    "$\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the\n",
    "notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of\n",
    "likelihood is a critical in Bayesian modelling, for simplicity we consider a\n",
    "Gaussian with noise parameter $\\alpha$\n",
    "$$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$\n",
    "This is defined in GPJax through calling a `Gaussian` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpx.Gaussian(num_datapoints=D.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68728e4",
   "metadata": {},
   "source": [
    "The posterior is proportional to the prior multiplied by the likelihood, written as\n",
    "\n",
    "  $$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$\n",
    "\n",
    "Mimicking this construct, the posterior is established in GPJax through the `*` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f85916",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db0e8e",
   "metadata": {},
   "source": [
    "<!-- ## Hyperparameter optimisation\n",
    "\n",
    "Our kernel is parameterised by a length-scale $\\ell^2$ and variance parameter\n",
    "$\\sigma^2$, while our likelihood controls the observation noise with $\\alpha^2$.\n",
    "Using Jax's automatic differentiation module, we can take derivatives of  -->\n",
    "\n",
    "## Parameter state\n",
    "\n",
    "As outlined in the [PyTrees](https://jax.readthedocs.io/en/latest/pytrees.html)\n",
    "documentation, parameters are contained within the model and for the leaves of the\n",
    "PyTree. Consequently, in this particular model, we have three parameters: the\n",
    "kernel lengthscale, kernel variance and the observation noise variance. Whilst\n",
    "we have initialised each of these to 1, we can learn Type 2 MLEs for each of\n",
    "these parameters by optimising the marginal log-likelihood (MLL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cf43e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "negative_mll = gpx.objectives.ConjugateMLL(negative=True)\n",
    "negative_mll(posterior, train_data=D)\n",
    "\n",
    "\n",
    "# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n",
    "# optim = ox.chain(\n",
    "#     ox.adam(learning_rate=0.01),\n",
    "#     ox.masked(ox.set_to_zero(), static_tree)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e39c8",
   "metadata": {},
   "source": [
    "For researchers, GPJax has the capacity to print the bibtex citation for objects such\n",
    "as the marginal log-likelihood through the `cite()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpx.cite(negative_mll))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023e03a",
   "metadata": {},
   "source": [
    "JIT-compiling expensive-to-compute functions such as the marginal log-likelihood is\n",
    "advisable. This can be achieved by wrapping the function in `jax.jit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_mll = jit(negative_mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092ba79",
   "metadata": {},
   "source": [
    "Since most optimisers (including here) minimise a given function, we have realised\n",
    "the negative marginal log-likelihood and just-in-time (JIT) compiled this to\n",
    "accelerate training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfc8e8",
   "metadata": {},
   "source": [
    "We can now define an optimiser with `optax`. For this example we'll use the `adam`\n",
    "optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881322ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_posterior, history = gpx.fit(\n",
    "    model=posterior,\n",
    "    objective=negative_mll,\n",
    "    train_data=D,\n",
    "    optim=ox.adam(learning_rate=0.01),\n",
    "    num_iters=500,\n",
    "    safe=True,\n",
    "    key=key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7abd4",
   "metadata": {},
   "source": [
    "The calling of `fit` returns two objects: the optimised posterior and a history of\n",
    "training losses. We can plot the training loss to see how the optimisation has\n",
    "progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history, color=cols[1])\n",
    "ax.set(xlabel=\"Training iteration\", ylabel=\"Negative marginal log likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e51a5",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Equipped with the posterior and a set of optimised hyperparameter values, we are now\n",
    "in a position to query our GP's predictive distribution at novel test inputs. To do\n",
    "this, we use our defined `posterior` and `likelihood` at our test inputs to obtain\n",
    "the predictive distribution as a `Distrax` multivariate Gaussian upon which `mean`\n",
    "and `stddev` can be used to extract the predictive mean and standard deviatation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_posterior.predict(xtest, train_data=D)\n",
    "predictive_dist = opt_posterior.likelihood(latent_dist)\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_std = predictive_dist.stddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85f9d4",
   "metadata": {},
   "source": [
    "With the predictions and their uncertainty acquired, we illustrate the GP's\n",
    "performance at explaining the data $\\mathcal{D}$ and recovering the underlying\n",
    "latent function of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b984cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5, 2.5))\n",
    "ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    alpha=0.2,\n",
    "    label=\"Two sigma\",\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n",
    ")\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f529e",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92044fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder & Daniel Dodd'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "gpjax",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
