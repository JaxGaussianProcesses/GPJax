{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d968859",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Optimisation\n",
    "\n",
    "In this guide we introduce the Bayesian Optimisation (BO) paradigm for\n",
    "optimising black-box functions. We'll assume an understanding of Gaussian processes\n",
    "(GPs), so if you're not familiar with them, check out our [GP introduction notebook](https://docs.jaxgaussianprocesses.com/examples/intro_to_gps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5efe1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook, Float, Int\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import jaxopt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from typing import List, Tuple\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "from gpjax.typing import Array, FunctionalSample, ScalarFloat\n",
    "from jaxopt import ScipyBoundedMinimize\n",
    "\n",
    "key = jr.PRNGKey(42)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af145b9b",
   "metadata": {},
   "source": [
    "## Some Motivating Examples\n",
    "\n",
    "Countless problems in the physical world involve optimising functions for which the\n",
    "explicit functional form is unknown, but which can be expensively queried throughout\n",
    "their domain. For example, within the domain of science the task of designing new\n",
    "molecules with optimised properties ([Griffiths and Lobato,\n",
    "2020](https://pubs.rsc.org/en/content/articlehtml/2019/sc/c9sc04026a)) is incredibly\n",
    "useful. Here, the domain being optimised over is the space of possible molecules, with\n",
    "the objective function depending on the property being optimised, for instance within\n",
    "drug-design this may be the efficacy of the drug. The function from molecules to\n",
    "efficacy is unknown, but can be queried by synthesising a molecule and running an\n",
    "experiment to measure its efficacy. This is clearly an expensive procedure!\n",
    "\n",
    "Within the domain of machine learning, the task of optimising neural network\n",
    "architectures is another example of such a problem (commonly referred to as [Neural\n",
    "Architecture Search (NAS)](https://en.wikipedia.org/wiki/Neural_architecture_search)).\n",
    "Here, the domain is the space of possible neural network architectures, and the\n",
    "objective function is a metric such as the accuracy of the trained model. Again, the\n",
    "function from neural network architectures to accuracy is unknown, but can be queried by\n",
    "training a model with a given architecture and evaluating its accuracy. This is also an\n",
    "expensive procedure, as training models can be incredibly time consuming and\n",
    "computationally demanding.\n",
    "\n",
    "Finally, these problems are ubiquitous within the field of climate science, with\n",
    "([Hellan et al., 2023](https://arxiv.org/abs/2306.04343)) providing several excellent\n",
    "examples. One such example is the task of deciding where to place wind turbines in a\n",
    "wind farm in order to maximise the energy generated. Here, the domain is the space of\n",
    "possible locations for the wind turbines, and the objective function is the energy\n",
    "generated by the wind farm. The function from locations to energy generated is unknown,\n",
    "but could be queried by running a simulation of the wind farm with the turbines placed\n",
    "at a given set of locations. Running such simulations can be expensive, particularly if\n",
    "they are high-fidelity.\n",
    "\n",
    "At the heart of all these problems is the task of optimising a function for which we\n",
    "don't have the explicit functional form, but which we can (expensively) query at any\n",
    "point in its domain. Bayesian optimisation provides a principled framework for solving\n",
    "such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b786ba9",
   "metadata": {},
   "source": [
    "## What is Bayesian Optimisation?\n",
    "\n",
    "Bayesian optimisation (BO) ([Moƒçkus, 1974](https://link.springer.com/chapter/10.1007/3-540-07165-2_55)) provides a principled\n",
    "method for making decisions under uncertainty. The aim of BO is to find the global\n",
    "minimum of a *black-box* objective function, $\\min_{\\mathbf{x} \\in X}\n",
    "f(\\mathbf{x})$. The function $f$ is said to be a *black-box* function because its\n",
    "explicit functional form is unknown. However, it is assumed that one is able to\n",
    "ascertain information about the function by evaluating it at points in its domain,\n",
    "$X$. However, these evaluations are assumed to be *expensive*, as seen in the\n",
    "motivating examples. Therefore, the goal of BO is to minimise $f$ with as few\n",
    "evaluations of the black-box function as possible.\n",
    "\n",
    "As such, BO can be thought of as *sequential decision-making* problem. At each iteration\n",
    "one must choose which point (or batch of points) in a function's domain to evaluate\n",
    "next, drawing on previously observed values to make optimal decisions. In order to do\n",
    "this effectively, we need a way of representing our uncertainty about the black-box\n",
    "function $f$, which we can update in light of observing more data. Gaussian processes\n",
    "will be an ideal tool for this purpose!\n",
    "\n",
    "*Surrogate models* lie at the heart of BO, and are used to model the black-box\n",
    "function. GPs are a natural choice for this model, as they not only provide point\n",
    "estimates for the values taken by the function throughout its domain, but crucially\n",
    "provide a full predictive posterior *distribution* of the range of values the function\n",
    "may take. This rich quantification of uncertainty enables BO to balance *exploration*\n",
    "and *exploitation* in order to efficiently converge upon minima.\n",
    "\n",
    "Having chosen a surrogate model, which we can use to express our current beliefs about\n",
    "the black-box function, ideally we would like a method which can use the surrogate\n",
    "model's posterior distribution to automatically decide which point(s) in the black-box\n",
    "function's domain to query next. This is where *acquisition functions* come in. The\n",
    "acquisition function $\\alpha: X \\to \\mathbb{R}$ is defined over the same domain as the\n",
    "surrogate model, and uses the surrogate model's posterior distribution to quantify the\n",
    "expected *utility*, $U$, of evaluating the black-box function at a given point. Simply\n",
    "put, for each point in the black-box function's domain, $\\mathbf{x} \\in X$, the\n",
    "acquisition function quantifies how useful it would be to evaluate the black-box\n",
    "function at $\\mathbf{x}$ in order to find the minimum of the black-box function, whilst\n",
    "taking into consideration all the datapoints observed so far. Therefore, in order to\n",
    "decide which point to query next we simply choose the point which maximises the\n",
    "acquisition function, using an optimiser such as L-BFGS ([Liu and Nocedal,\n",
    "1989](https://link.springer.com/article/10.1007/BF01589116)).\n",
    "\n",
    "The Bayesian optimisation loop can be summarised as follows, with $i$ denoting the\n",
    "current iteration:\n",
    "\n",
    "1. Select the next point to query, $\\mathbf{x}_{i}$, by maximising the acquisition function $\\alpha$, defined using the surrogate model $\\mathcal{M}_i$ conditioned on previously observed data $\\mathcal{D}_i$:\n",
    "\n",
    "$$\\mathbf{x}_{i} = \\arg\\max_{\\mathbf{x}} \\alpha (\\mathbf{x}; \\mathcal{D}_i,\n",
    "\\mathcal{M}_i)$$\n",
    "\n",
    "2. Evaluate the objective function at $\\mathbf{x}_i$, yielding observation $y_i =\n",
    "   f(\\mathbf{x}_i)$.\n",
    "\n",
    "3. Append the most recent observation to the dataset, $\\mathcal{D}_{i+1} = \\mathcal{D}_i\n",
    "   \\cup \\{(\\mathbf{x}_i, y_i)\\}$.\n",
    "\n",
    "4. Condition the model on the updated dataset to yield $\\mathcal{M}_{i+1}$.\n",
    "\n",
    "This process is repeated until some stopping criterion is met, such as a function\n",
    "evaluation budget being exhausted.\n",
    "\n",
    "There are a plethora of acquisition functions to choose from, each with their own\n",
    "advantages and disadvantages, of which ([Shahriari et al., 2015](https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf))\n",
    "provides an excellent overview.\n",
    "\n",
    "In this guide we will focus on *Thompson sampling*, a conceptually simple yet effective\n",
    "method for characterising the utility of querying points in a black-box function's\n",
    "domain, which will be useful in demonstrating the key aspects of BO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f9ba5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Thompson Sampling\n",
    "\n",
    "Thompson sampling ([Thompson, 1933](https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf)) is a simple method which\n",
    "naturally balances exploration and exploitation. The core idea is to, at each iteration\n",
    "of the BO loop, sample a function, $g$, from the posterior distribution of the surrogate\n",
    "model $\\mathcal{M}_i$, and then evaluate the black-box function at the point(s) which\n",
    "minimise this sample. Given a sample $g$, from the posterior distribution given by the model $\\mathcal{M}_i$ the Thompson sampling utility function is defined as:\n",
    "\n",
    "$$U_{\\text{TS}}(\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i) = - g(\\mathbf{x})$$\n",
    "\n",
    "Note the negative sign; this is included as we want to maximise the *utility* of\n",
    "evaluating the black-box function $f$ at a given point. We interested in finding the\n",
    "minimum of $f$, so we maximise the negative of the sample from the posterior distribution $g$.\n",
    "\n",
    "As a toy example, we shall be applying BO to the widely used [Forrester\n",
    "function](https://www.sfu.ca/~ssurjano/forretal08.html):\n",
    "\n",
    "$$f(x) = (6x - 2)^2 \\sin(12x - 4)$$\n",
    "\n",
    "treating $f$ as a black-box function. Moreover, we shall restrict the domain of the\n",
    "function to $\\mathbf{x} \\in [0, 1]$. The global minimum of this function is located at\n",
    "$x = 0.757$, where $f(x) = -6.021$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5649b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forrester(x: Float[Array, \"N 1\"]) -> Float[Array, \"N 1\"]:\n",
    "    return (6 * x - 2) ** 2 * jnp.sin(12 * x - 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e706c6c",
   "metadata": {},
   "source": [
    "We'll first go through one iteration of the BO loop step-by-step, before wrapping this\n",
    "up in a loop to perform the full optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc22184",
   "metadata": {},
   "source": [
    "First we'll specify the domain over which we wish to optimise the function, as well as\n",
    "sampling some initial points for fitting our surrogate model using a space-filling design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb9d07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lower_bound = jnp.array([0.0])\n",
    "upper_bound = jnp.array([1.0])\n",
    "initial_sample_num = 5\n",
    "\n",
    "initial_x = tfp.mcmc.sample_halton_sequence(\n",
    "    dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n",
    ").reshape(-1, 1)\n",
    "initial_y = forrester(initial_x)\n",
    "D = gpx.Dataset(X=initial_x, y=initial_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378817d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Next we'll define our GP model in the usual way, using a Mat√©rn52 kernel, and fit the\n",
    "kernel parameters by minimising the negative log-marginal likelihood. We'll wrap this in\n",
    "a function as we'll be repeating this process at each iteration of the BO loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_optimised_posterior(\n",
    "    data: gpx.Dataset, prior: gpx.Module, key: Array\n",
    ") -> gpx.Module:\n",
    "    likelihood = gpx.Gaussian(\n",
    "        num_datapoints=data.n, obs_noise=jnp.array(1e-6)\n",
    "    )  # Our function is noise-free, so we set the observation noise to a very small value\n",
    "    likelihood = likelihood.replace_trainable(obs_noise=False)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "\n",
    "    negative_mll = gpx.objectives.ConjugateMLL(negative=True)\n",
    "    negative_mll(posterior, train_data=data)\n",
    "    negative_mll = jit(negative_mll)\n",
    "\n",
    "    opt_posterior, history = gpx.fit(\n",
    "        model=posterior,\n",
    "        train_data=D,\n",
    "        solver=jaxopt.LBFGS(gpx.ConjugateMLL(negative=True), maxiter=500),\n",
    "        safe=True,\n",
    "        key=key,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return opt_posterior\n",
    "\n",
    "\n",
    "mean = gpx.mean_functions.Zero()\n",
    "kernel = gpx.kernels.Matern52()\n",
    "prior = gpx.Prior(mean_function=mean, kernel=kernel)\n",
    "opt_posterior = return_optimised_posterior(D, prior, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a761c1e",
   "metadata": {},
   "source": [
    "We can then sample a function from the posterior distribution of the surrogate model. We\n",
    "will do this using the `sample_approx` method, which generates an approximate sample\n",
    "from the posterior using decoupled sampling introduced in ([Wilson et al.,\n",
    "2020](https://proceedings.mlr.press/v119/wilson20a.html)) and discussed in our [Pathwise\n",
    "Sampling Notebook](https://docs.jaxgaussianprocesses.com/examples/spatial/). This method\n",
    "is used as it enables us to sample from the posterior in a manner which scales linearly\n",
    "with the number of points sampled, $O(N)$, mitigating the cubic cost associated with\n",
    "drawing exact samples from a GP posterior, $O(N^3)$. It also generates more accurate\n",
    "samples than many other methods for drawing approximate samples from a GP posterior.\n",
    "\n",
    "Note that we also define a `utility_fn` which calls the approximate\n",
    "sample but returns the value returned as a scalar. This is because the `sample_approx`\n",
    "function returns an array of shape $[N, B]$, with $N$ being the number of points within\n",
    "each sample and $B$ being the number of samples drawn. We'll only be drawing (and\n",
    "optimising) one sample at a time, and our optimiser requires the function being\n",
    "optimised to return a scalar output (only querying it at $N=1$ points), so we'll remove the axes from the returned value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d700a73",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "approx_sample = opt_posterior.sample_approx(\n",
    "    num_samples=1, train_data=D, key=key, num_features=500\n",
    ")\n",
    "utility_fn = lambda x: approx_sample(x)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9739b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In order to minimise the sample, we'll be using the L-BFGS-B ([Byrd et al., 1995](https://epubs.siam.org/doi/abs/10.1137/0916069)) optimiser from the `jaxopt`\n",
    "library. This is a gradient-based optimiser which performs optimisation within a bounded\n",
    "domain. In order to perform optimisation, this optimiser requires a point to start from.\n",
    "Therefore, we will first query our sample from the posterior at a random set of points,\n",
    "and then use the lowest point from this set of points as the starting point for the\n",
    "optimiser. In this example we'll sample 100 points from the posterior, due to the simple\n",
    "nature of the Forrester function. However, in practice it can be beneficial to\n",
    "adopt a more sophisticated approach, and there are several heuristics available in the\n",
    "literature (see for example ([Le Riche and Picheny,\n",
    "2021](https://arxiv.org/abs/2103.16649))). For instance, one may randomly sample the\n",
    "posterior at a number of points proportional to the dimensionality of the input space,\n",
    "and one may run gradient-based optimisation from multiple of these points, to reduce the\n",
    "risk of converging upon local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01770354",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def optimise_sample(\n",
    "    sample: FunctionalSample,\n",
    "    key: Int[Array, \"\"],\n",
    "    lower_bound: Float[Array, \"D\"],\n",
    "    upper_bound: Float[Array, \"D\"],\n",
    "    num_initial_sample_points: int,\n",
    ") -> ScalarFloat:\n",
    "    initial_sample_points = jr.uniform(\n",
    "        key,\n",
    "        shape=(num_initial_sample_points, lower_bound.shape[0]),\n",
    "        dtype=jnp.float64,\n",
    "        minval=lower_bound,\n",
    "        maxval=upper_bound,\n",
    "    )\n",
    "    initial_sample_y = sample(initial_sample_points)\n",
    "    best_x = jnp.array([initial_sample_points[jnp.argmin(initial_sample_y)]])\n",
    "\n",
    "    # We want to maximise the utility function, but the optimiser performs minimisation. Since we're minimising the sample drawn, the sample is actually the negative utility function.\n",
    "    negative_utility_fn = lambda x: sample(x)[0][0]\n",
    "    lbfgsb = ScipyBoundedMinimize(fun=negative_utility_fn, method=\"l-bfgs-b\")\n",
    "    bounds = (lower_bound, upper_bound)\n",
    "    x_star = lbfgsb.run(best_x, bounds=bounds).params\n",
    "    return x_star\n",
    "\n",
    "\n",
    "x_star = optimise_sample(approx_sample, key, lower_bound, upper_bound, 100)\n",
    "y_star = forrester(x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec19f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Having found the minimum of the sample from the posterior, we can then evaluate the\n",
    "black-box objective function at this point, and append the new observation to our dataset.\n",
    "\n",
    "Below we plot the posterior distribution of the surrogate model, along with the sample\n",
    "drawn from the model, and the minimiser of this sample returned from the optimiser,\n",
    "which we denote with a star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bayes_opt(\n",
    "    posterior: gpx.Module,\n",
    "    sample: FunctionalSample,\n",
    "    dataset: gpx.Dataset,\n",
    "    queried_x: ScalarFloat,\n",
    ") -> None:\n",
    "    plt_x = jnp.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    forrester_y = forrester(plt_x)\n",
    "    sample_y = sample(plt_x)\n",
    "\n",
    "    latent_dist = posterior.predict(plt_x, train_data=dataset)\n",
    "    predictive_dist = posterior.likelihood(latent_dist)\n",
    "\n",
    "    predictive_mean = predictive_dist.mean()\n",
    "    predictive_std = predictive_dist.stddev()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(plt_x, predictive_mean, label=\"Predictive Mean\", color=cols[1])\n",
    "    ax.fill_between(\n",
    "        plt_x.squeeze(),\n",
    "        predictive_mean - 2 * predictive_std,\n",
    "        predictive_mean + 2 * predictive_std,\n",
    "        alpha=0.2,\n",
    "        label=\"Two sigma\",\n",
    "        color=cols[1],\n",
    "    )\n",
    "    ax.plot(\n",
    "        plt_x,\n",
    "        predictive_mean - 2 * predictive_std,\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        color=cols[1],\n",
    "    )\n",
    "    ax.plot(\n",
    "        plt_x,\n",
    "        predictive_mean + 2 * predictive_std,\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        color=cols[1],\n",
    "    )\n",
    "    ax.plot(plt_x, sample_y, label=\"Posterior Sample\")\n",
    "    ax.plot(\n",
    "        plt_x,\n",
    "        forrester_y,\n",
    "        label=\"Forrester Function\",\n",
    "        color=cols[0],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.axvline(x=0.757, linestyle=\":\", color=cols[3], label=\"True Optimum\")\n",
    "    ax.scatter(dataset.X, dataset.y, label=\"Observations\", color=cols[2], zorder=2)\n",
    "    ax.scatter(\n",
    "        queried_x,\n",
    "        sample(queried_x),\n",
    "        label=\"Posterior Sample Optimum\",\n",
    "        marker=\"*\",\n",
    "        color=cols[3],\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_bayes_opt(opt_posterior, approx_sample, D, x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1f294",
   "metadata": {},
   "source": [
    "At this point we can update our model with the newly augmented dataset, and repeat the\n",
    "whole process until some stopping criterion is met. Below we repeat this process for 10\n",
    "iterations, printing out the queried point and the value of the black-box function at\n",
    "each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_iters = 5\n",
    "\n",
    "# Set up initial dataset\n",
    "initial_x = tfp.mcmc.sample_halton_sequence(\n",
    "    dim=1, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n",
    ").reshape(-1, 1)\n",
    "initial_y = forrester(initial_x)\n",
    "D = gpx.Dataset(X=initial_x, y=initial_y)\n",
    "\n",
    "for i in range(bo_iters):\n",
    "    key, subkey = jr.split(key)\n",
    "\n",
    "    # Generate optimised posterior using previously observed data\n",
    "    mean = gpx.mean_functions.Zero()\n",
    "    kernel = gpx.kernels.Matern52()\n",
    "    prior = gpx.Prior(mean_function=mean, kernel=kernel)\n",
    "    opt_posterior = return_optimised_posterior(D, prior, subkey)\n",
    "\n",
    "    # Draw a sample from the posterior, and find the minimiser of it\n",
    "    approx_sample = opt_posterior.sample_approx(\n",
    "        num_samples=1, train_data=D, key=subkey, num_features=500\n",
    "    )\n",
    "    x_star = optimise_sample(\n",
    "        approx_sample, subkey, lower_bound, upper_bound, num_initial_sample_points=100\n",
    "    )\n",
    "\n",
    "    plot_bayes_opt(opt_posterior, approx_sample, D, x_star)\n",
    "\n",
    "    # Evaluate the black-box function at the best point observed so far, and add it to the dataset\n",
    "    y_star = forrester(x_star)\n",
    "    print(f\"Queried Point: {x_star}, Black-Box Function Value: {y_star}\")\n",
    "    D = D + gpx.Dataset(X=x_star, y=y_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0d676",
   "metadata": {},
   "source": [
    "Below we plot the best observed black-box function value against the number of times\n",
    "the black-box function has been evaluated. Note that the first 5 samples are randomly\n",
    "sampled to fit the initial GP model, and we denote the start of using BO to sample with\n",
    "the dotted vertical line.\n",
    "\n",
    "We can see that the BO algorithm quickly converges to the global minimum of the\n",
    "black-box function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222d4f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\n",
    "cumulative_best_y = jax.lax.associative_scan(jax.numpy.minimum, D.y)\n",
    "ax.plot(fn_evaluations, cumulative_best_y)\n",
    "ax.axvline(x=initial_sample_num, linestyle=\":\")\n",
    "ax.axhline(y=-6.0207, linestyle=\"--\", label=\"True Minimum\")\n",
    "ax.set_xlabel(\"Number of Black-Box Function Evaluations\")\n",
    "ax.set_ylabel(\"Best Observed Value\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58299352",
   "metadata": {},
   "source": [
    "### A More Challenging Example - The Six-Hump Camel Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c644c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We'll now apply BO to a more challenging example, the [Six-Hump Camel\n",
    "Function](https://www.sfu.ca/~ssurjano/camel6.html). This is a function of two inputs\n",
    "defined as follows:\n",
    "\n",
    "$$f(x_1, x_2) = (4 - 2.1x_1^2 + \\frac{x_1^4}{3})x_1^2 + x_1x_2 + (-4 + 4x_2^2)x_2^2$$\n",
    "\n",
    "We'll be evaluating it over the domain $x_1 \\in [-2, 2]$ and $x_2 \\in [-1, 1]$. The\n",
    "global minima of this function are located at $\\mathbf{x} = (0.0898, -0.7126)$ and $\\mathbf{x} = (-0.0898, 0.7126)$, where the function takes the value $f(\\mathbf{x}) = -1.0316$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def six_hump_camel(x: Float[Array, \"N 2\"]) -> Float[Array, \"N 1\"]:\n",
    "    x1 = x[..., :1]\n",
    "    x2 = x[..., 1:]\n",
    "    term1 = (4 - 2.1 * x1**2 + x1**4 / 3) * x1**2\n",
    "    term2 = x1 * x2\n",
    "    term3 = (-4 + 4 * x2**2) * x2**2\n",
    "    return term1 + term2 + term3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101bc98",
   "metadata": {},
   "source": [
    "First, we'll visualise the function over the domain of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a16c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = jnp.linspace(-2, 2, 100)\n",
    "x2 = jnp.linspace(-1, 1, 100)\n",
    "x1, x2 = jnp.meshgrid(x1, x2)\n",
    "x = jnp.stack([x1.flatten(), x2.flatten()], axis=1)\n",
    "y = six_hump_camel(x)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf = ax.plot_surface(\n",
    "    x1,\n",
    "    x2,\n",
    "    y.reshape(x1.shape[0], x2.shape[0]),\n",
    "    linewidth=0,\n",
    "    cmap=cm.coolwarm,\n",
    "    antialiased=False,\n",
    ")\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90ee91",
   "metadata": {},
   "source": [
    "For more clarity, we can generate a contour plot of the function which enables us to see\n",
    "the global minima of the function more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b49fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star_one = jnp.array([[0.0898, -0.7126]])\n",
    "x_star_two = jnp.array([[-0.0898, 0.7126]])\n",
    "fig, ax = plt.subplots()\n",
    "contour_plot = ax.contourf(\n",
    "    x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n",
    ")\n",
    "ax.scatter(\n",
    "    x_star_one[0][0], x_star_one[0][1], marker=\"*\", color=cols[2], label=\"Global Minima\"\n",
    ")\n",
    "ax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2])\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "fig.colorbar(contour_plot)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4229fd2",
   "metadata": {},
   "source": [
    "Next, we'll run the BO loop using Thompson sampling as before. This time we'll run the\n",
    "experiment 5 times in order to see how the algorithm performs on average, with different\n",
    "starting points for the initial GP model. This is good practice, as the performance\n",
    "obtained is likely to vary between runs depending on the initialisation samples used to\n",
    "fit the initial GP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a65d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lower_bound = jnp.array([-2.0, -1.0])\n",
    "upper_bound = jnp.array([2.0, 1.0])\n",
    "initial_sample_num = 5\n",
    "bo_iters = 11\n",
    "num_experiments = 5\n",
    "bo_experiment_results = []\n",
    "\n",
    "for experiment in range(num_experiments):\n",
    "    print(f\"Starting Experiment: {experiment + 1}\")\n",
    "    # Set up initial dataset\n",
    "    initial_x = tfp.mcmc.sample_halton_sequence(\n",
    "        dim=2, num_results=initial_sample_num, seed=key, dtype=jnp.float64\n",
    "    )\n",
    "    initial_x = jnp.array(lower_bound + (upper_bound - lower_bound) * initial_x)\n",
    "    initial_y = six_hump_camel(initial_x)\n",
    "    D = gpx.Dataset(X=initial_x, y=initial_y)\n",
    "\n",
    "    for i in range(bo_iters):\n",
    "        key, subkey = jr.split(key)\n",
    "\n",
    "        # Generate optimised posterior\n",
    "        mean = gpx.mean_functions.Zero()\n",
    "        kernel = gpx.kernels.Matern52(\n",
    "            active_dims=[0, 1], lengthscale=jnp.array([1.0, 1.0]), variance=2.0\n",
    "        )\n",
    "        prior = gpx.Prior(mean_function=mean, kernel=kernel)\n",
    "        opt_posterior = return_optimised_posterior(D, prior, subkey)\n",
    "\n",
    "        # Draw a sample from the posterior, and find the minimiser of it\n",
    "        approx_sample = opt_posterior.sample_approx(\n",
    "            num_samples=1, train_data=D, key=subkey, num_features=500\n",
    "        )\n",
    "        x_star = optimise_sample(\n",
    "            approx_sample,\n",
    "            subkey,\n",
    "            lower_bound,\n",
    "            upper_bound,\n",
    "            num_initial_sample_points=1000,\n",
    "        )\n",
    "\n",
    "        # Evaluate the black-box function at the best point observed so far, and add it to the dataset\n",
    "        y_star = six_hump_camel(x_star)\n",
    "        print(\n",
    "            f\"BO Iteration: {i + 1}, Queried Point: {x_star}, Black-Box Function Value: {y_star}\"\n",
    "        )\n",
    "        D = D + gpx.Dataset(X=x_star, y=y_star)\n",
    "    bo_experiment_results.append(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed107c",
   "metadata": {},
   "source": [
    "We'll also run a random benchmark, whereby we randomly sample from the search space for\n",
    "20 iterations. This is a useful benchmark to compare the performance of BO against in\n",
    "order to ascertain how much of an advantage BO provides over such a simple approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6479e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "random_experiment_results = []\n",
    "for i in range(num_experiments):\n",
    "    key, subkey = jr.split(key)\n",
    "    initial_x = bo_experiment_results[i].X[:5]\n",
    "    initial_y = bo_experiment_results[i].y[:5]\n",
    "    final_x = jr.uniform(\n",
    "        key,\n",
    "        shape=(bo_iters, 2),\n",
    "        dtype=jnp.float64,\n",
    "        minval=lower_bound,\n",
    "        maxval=upper_bound,\n",
    "    )\n",
    "    final_y = six_hump_camel(final_x)\n",
    "    random_x = jnp.concatenate([initial_x, final_x], axis=0)\n",
    "    random_y = jnp.concatenate([initial_y, final_y], axis=0)\n",
    "    random_experiment_results.append(gpx.Dataset(X=random_x, y=random_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766bbe7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Finally, we'll process the experiment results to find the log regret at each iteration\n",
    "of the experiments. The regret is defined as the difference between the minimum value of\n",
    "the black-box function observed so far and the true global minimum of the black box\n",
    "function. Mathematically, at time $t$, with observations $\\mathcal{D}_t$, for function\n",
    "$f$ with global minimum $f^*$, the regret is defined as:\n",
    "\n",
    "$$\\text{regret}_t = \\min_{\\mathbf{x} \\in \\mathcal{D_t}}f(\\mathbf{x}) - f^*$$\n",
    "\n",
    "We'll then take the mean and standard deviation of the log of the regret values across\n",
    "the 5 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e143225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_log_regret_statistics(\n",
    "    experiment_results: List[gpx.Dataset],\n",
    "    global_minimum: ScalarFloat,\n",
    ") -> Tuple[Float[Array, \"N 1\"], Float[Array, \"N 1\"]]:\n",
    "    log_regret_results = []\n",
    "    for exp_result in experiment_results:\n",
    "        observations = exp_result.y\n",
    "        cumulative_best_observations = jax.lax.associative_scan(\n",
    "            jax.numpy.minimum, observations\n",
    "        )\n",
    "        regret = cumulative_best_observations - global_minimum\n",
    "        log_regret = jnp.log(regret)\n",
    "        log_regret_results.append(log_regret)\n",
    "\n",
    "    log_regret_results = jnp.array(log_regret_results)\n",
    "    log_regret_mean = jnp.mean(log_regret_results, axis=0)\n",
    "    log_regret_std = jnp.std(log_regret_results, axis=0)\n",
    "    return log_regret_mean, log_regret_std\n",
    "\n",
    "\n",
    "bo_log_regret_mean, bo_log_regret_std = obtain_log_regret_statistics(\n",
    "    bo_experiment_results, -1.031625\n",
    ")\n",
    "(\n",
    "    random_log_regret_mean,\n",
    "    random_log_regret_std,\n",
    ") = obtain_log_regret_statistics(random_experiment_results, -1.031625)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af2a94",
   "metadata": {},
   "source": [
    "Now, when we plot the mean and standard deviation of the log regret at each iteration,\n",
    "we can see that BO outperforms random sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fn_evaluations = jnp.arange(1, bo_iters + initial_sample_num + 1)\n",
    "ax.plot(fn_evaluations, bo_log_regret_mean, label=\"Bayesian Optimisation\")\n",
    "ax.fill_between(\n",
    "    fn_evaluations,\n",
    "    bo_log_regret_mean[:, 0] - bo_log_regret_std[:, 0],\n",
    "    bo_log_regret_mean[:, 0] + bo_log_regret_std[:, 0],\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.plot(fn_evaluations, random_log_regret_mean, label=\"Random Search\")\n",
    "ax.fill_between(\n",
    "    fn_evaluations,\n",
    "    random_log_regret_mean[:, 0] - random_log_regret_std[:, 0],\n",
    "    random_log_regret_mean[:, 0] + random_log_regret_std[:, 0],\n",
    "    alpha=0.2,\n",
    ")\n",
    "ax.axvline(x=initial_sample_num, linestyle=\":\")\n",
    "ax.set_xlabel(\"Number of Black-Box Function Evaluations\")\n",
    "ax.set_ylabel(\"Log Regret\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b122c9",
   "metadata": {},
   "source": [
    "It can also be useful to plot the queried points over the course of a single BO run, in\n",
    "order to gain some insight into how the algorithm queries the search space. Below\n",
    "we do this for the first BO experiment, and can see that the algorithm initially\n",
    "performs some exploration of the search space whilst it is uncertain about the black-box\n",
    "function, but it then hones in one one of the global minima of the function, as we would hope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "contour_plot = ax.contourf(\n",
    "    x1, x2, y.reshape(x1.shape[0], x2.shape[0]), cmap=cm.coolwarm, levels=40\n",
    ")\n",
    "ax.scatter(\n",
    "    x_star_one[0][0],\n",
    "    x_star_one[0][1],\n",
    "    marker=\"*\",\n",
    "    color=cols[2],\n",
    "    label=\"Global Minimum\",\n",
    "    zorder=2,\n",
    ")\n",
    "ax.scatter(x_star_two[0][0], x_star_two[0][1], marker=\"*\", color=cols[2], zorder=2)\n",
    "ax.scatter(\n",
    "    bo_experiment_results[0].X[:, 0],\n",
    "    bo_experiment_results[0].X[:, 1],\n",
    "    marker=\"x\",\n",
    "    color=cols[1],\n",
    "    label=\"Bayesian Optimisation Queries\",\n",
    ")\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "fig.colorbar(contour_plot)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392226d2",
   "metadata": {},
   "source": [
    "### Other Acquisition Functions and Further Reading\n",
    "\n",
    "As mentioned previously, there are many acquisition functions which one may use to\n",
    "characterise the expected utility of querying the black-box function at a given point.\n",
    "We list two of the most popular below:\n",
    "\n",
    "- **Probability of Improvement (PI)** ([Kushner, 1964](https://asmedigitalcollection.asme.org/fluidsengineering/article/86/1/97/392213/A-New-Method-of-Locating-the-Maximum-Point-of-an)): Given the lowest objective function observation\n",
    "  so far, $f(\\mathbf{x}^*)$, PI calculates the probability that the objective function's\n",
    "  value at a given point $\\mathbf{x}$ is lower than $f(\\mathbf{x}^*)$. Given a GP\n",
    "  surrogate model $\\mathcal{M}_i$, PI is defined mathematically as:\n",
    "  $$\n",
    "  \\alpha_{\\text{PI}}(\\mathbf{x}; \\mathcal{D}_i, \\mathcal{M}_i) = \\mathbb{P}[\\mathcal{M}_i (\\mathbf{x}) < f(\\mathbf{x}^*)] = \\Phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)\n",
    "  $$\n",
    "\n",
    "  with $\\Phi(\\cdot)$ denoting the standard normal cumulative distribution function.\n",
    "\n",
    "- **Expected Improvement (EI)** ([Moƒçkus, 1974](https://link.springer.com/chapter/10.1007/3-540-07165-2_55)) - EI goes beyond PI by not only considering the\n",
    "  probability of improving on the current best observed point, but also taking into\n",
    "  account the \\textit{magnitude} of improvement. Mathematically, this is defined as\n",
    "  follows:\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  \\alpha_{\\text{EI}}(\\mathbf{x};\\mathcal{D}_i, \\mathcal{M}_i) &= \\mathbb{E}[(f(\\mathbf{x}^*) - \\mathcal{M}_i(\\mathbf{x}))\\mathbb{I}(\\mathcal{M}_i(\\mathbf{x}) < f(\\mathbf{x}^*))] \\\\\n",
    "  &= \\underbrace{(f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x}))\\Phi\n",
    "  \\left(\\frac{f(\\mathbf{x}^*) -\n",
    "  \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)}_\\text{exploits\n",
    "  areas with low mean} \\\\\n",
    "  &+  \\underbrace{\\sigma_{\\mathcal{M}_i}(\\mathbf{x}) \\phi \\left(\\frac{f(\\mathbf{x}^*) - \\mu_{\\mathcal{M}_i}(\\mathbf{x})}{\\sigma_{\\mathcal{M}_i}(\\mathbf{x})}\\right)}_\\text{explores areas with high variance} \\nonumber\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "  with $\\mathbb{I}(\\cdot)$ denoting the indicator function and $\\phi(\\cdot)$ being the\n",
    "  standard normal probability density function.\n",
    "\n",
    "For those particularly interested in diving deeper into Bayesian optimisation, be sure\n",
    "to check out Shahriari et al.'s \"[Taking the Human Out of the Loop:\n",
    "A Review of Bayesian\n",
    "Optimization](https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf)\",\n",
    "which includes a wide variety of acquisition functions, as well as some examples of more\n",
    "exotic BO problems, such as problems which also feature unknown constraints.\n",
    "\n",
    "## System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872160bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Christie'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
